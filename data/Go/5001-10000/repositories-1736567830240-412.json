{
  "metadata": {
    "timestamp": 1736567830240,
    "page": 412,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "cockroachdb/pebble",
      "stars": 5079,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 0.24609375,
          "content": "# See http://editorconfig.org\n\n[*]\nend_of_line = lf\ninsert_final_newline = true\ncharset = utf-8\n\n# For non-go files, we indent with two spaces. In go files we indent\n# with tabs but still set indent_size to control the github web viewer.\nindent_size=2\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.2099609375,
          "content": "# Github action artifacts.\nartifacts\n# Profiling artifacts.\ncpu.*.prof\nheap.prof\nmutex.prof\ncoverprofile.out\n# Testing artifacts\nmeta.*.test\n\n# Bazel files, generated with 'make gen-bazel'.\n/WORKSPACE\nBUILD.bazel\n\n\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.4521484375,
          "content": "Copyright (c) 2011 The LevelDB-Go Authors. All rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n   * Redistributions of source code must retain the above copyright\nnotice, this list of conditions and the following disclaimer.\n   * Redistributions in binary form must reproduce the above\ncopyright notice, this list of conditions and the following disclaimer\nin the documentation and/or other materials provided with the\ndistribution.\n   * Neither the name of Google Inc. nor the names of its\ncontributors may be used to endorse or promote products derived from\nthis software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 4.0498046875,
          "content": "GO := go\nPKG := ./...\nGOFLAGS :=\nSTRESSFLAGS :=\nTAGS := invariants\nTESTS := .\nCOVER_PROFILE := coverprofile.out\n\n.PHONY: all\nall:\n\t@echo usage:\n\t@echo \"  make test\"\n\t@echo \"  make testrace\"\n\t@echo \"  make stress\"\n\t@echo \"  make stressrace\"\n\t@echo \"  make stressmeta\"\n\t@echo \"  make crossversion-meta\"\n\t@echo \"  make testcoverage\"\n\t@echo \"  make mod-update\"\n\t@echo \"  make gen-bazel\"\n\t@echo \"  make generate\"\n\t@echo \"  make generate-test-data\"\n\t@echo \"  make clean\"\n\noverride testflags :=\n.PHONY: test\ntest:\n\t${GO} test -tags '$(TAGS)' ${testflags} -run ${TESTS} ${PKG}\n\n.PHONY: testcoverage\ntestcoverage:\n\t${GO} test -tags '$(TAGS)' ${testflags} -run ${TESTS} ${PKG} -coverprofile ${COVER_PROFILE}\n\n.PHONY: testrace\ntestrace: testflags += -race -timeout 20m\ntestrace: test\n\ntestasan: testflags += -asan -timeout 20m\ntestasan: test\n\ntestmsan: export CC=clang\ntestmsan: testflags += -msan -timeout 20m\ntestmsan: test\n\n.PHONY: testobjiotracing\ntestobjiotracing:\n\t${GO} test -tags '$(TAGS) pebble_obj_io_tracing' ${testflags} -run ${TESTS} ./objstorage/objstorageprovider/objiotracing\n\n.PHONY: lint\nlint:\n\t${GO} test -tags '$(TAGS)' ${testflags} -run ${TESTS} ./internal/lint\n\n.PHONY: stress stressrace\nstressrace: testflags += -race\nstress stressrace: testflags += -exec 'stress ${STRESSFLAGS}' -timeout 0 -test.v\nstress stressrace: test\n\n.PHONY: stressmeta\nstressmeta: override PKG = ./internal/metamorphic\nstressmeta: override STRESSFLAGS += -p 1\nstressmeta: override TESTS = TestMeta$$\nstressmeta: stress\n\n.PHONY: crossversion-meta\ncrossversion-meta:\n\t$(eval LATEST_RELEASE := $(shell git fetch origin && git branch -r --list '*/crl-release-*' | grep -o 'crl-release-.*$$' | sort | tail -1))\n\tgit checkout ${LATEST_RELEASE}; \\\n\t\t${GO} test -c ./internal/metamorphic -o './internal/metamorphic/crossversion/${LATEST_RELEASE}.test'; \\\n\t\tgit checkout -; \\\n\t\t${GO} test -c ./internal/metamorphic -o './internal/metamorphic/crossversion/head.test'; \\\n\t\t${GO} test -tags '$(TAGS)' ${testflags} -v -run 'TestMetaCrossVersion' ./internal/metamorphic/crossversion --version '${LATEST_RELEASE},${LATEST_RELEASE},${LATEST_RELEASE}.test' --version 'HEAD,HEAD,./head.test'\n\n.PHONY: stress-crossversion\nstress-crossversion:\n\tSTRESS=1 ./scripts/run-crossversion-meta.sh crl-release-23.1 crl-release-23.2 crl-release-24.1 crl-release-24.2 master\n\n.PHONY: gen-bazel\ngen-bazel:\n\t@echo \"Generating WORKSPACE\"\n\t@echo 'workspace(name = \"com_github_cockroachdb_pebble\")' > WORKSPACE\n\t@echo 'Running gazelle...'\n\t${GO} run github.com/bazelbuild/bazel-gazelle/cmd/gazelle@v0.37.0 update --go_prefix=github.com/cockroachdb/pebble --repo_root=.\n\t@echo 'You should now be able to build Cockroach using:'\n\t@echo '  ./dev build short -- --override_repository=com_github_cockroachdb_pebble=${CURDIR}'\n\n.PHONY: clean-bazel\nclean-bazel:\n\tgit clean -dxf WORKSPACE BUILD.bazel '**/BUILD.bazel'\n\n.PHONY: generate\ngenerate:\n\t${GO} generate ${PKG}\n\ngenerate:\n\n# Note that the output of generate-test-data is not deterministic. This should\n# only be run manually as needed.\n.PHONY: generate-test-data\ngenerate-test-data:\n\t${GO} run -tags make_incorrect_manifests ./tool/make_incorrect_manifests.go\n\t${GO} run -tags make_test_find_db ./tool/make_test_find_db.go\n\t${GO} run -tags make_test_sstables ./tool/make_test_sstables.go\n\t${GO} run -tags make_test_remotecat ./tool/make_test_remotecat.go\n\n.PHONY: mod-update\nmod-update:\n\t${GO} get -u\n\t${GO} mod tidy\n\n.PHONY: clean\nclean:\n\trm -f $(patsubst %,%.test,$(notdir $(shell go list ${PKG})))\n\ngit_dirty := $(shell git status -s)\n\n.PHONY: git-clean-check\ngit-clean-check:\nifneq ($(git_dirty),)\n\t@echo \"Git repository is dirty!\"\n\t@false\nelse\n\t@echo \"Git repository is clean.\"\nendif\n\n.PHONY: mod-tidy-check\nmod-tidy-check:\nifneq ($(git_dirty),)\n\t$(error mod-tidy-check must be invoked on a clean repository)\nendif\n\t@${GO} mod tidy\n\t$(MAKE) git-clean-check\n\n.PHONY: format\nformat:\n\tgo install -C internal/devtools github.com/cockroachdb/crlfmt && crlfmt -w -tab 2 .\n\n.PHONY: format-check\nformat-check:\nifneq ($(git_dirty),)\n\t$(error format-check must be invoked on a clean repository)\nendif\n\t$(MAKE) format\n\tgit diff\n\t$(MAKE) git-clean-check\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 10.7041015625,
          "content": "# Pebble [![Build Status](https://github.com/cockroachdb/pebble/actions/workflows/ci.yaml/badge.svg?branch=master)](https://github.com/cockroachdb/pebble/actions/workflows/ci.yaml) [![GoDoc](https://godoc.org/github.com/cockroachdb/pebble?status.svg)](https://godoc.org/github.com/cockroachdb/pebble) <sup><sub><sub>[Coverage](https://storage.googleapis.com/crl-codecover-public/pebble/index.html)</sub></sub></sup>\n\n#### [Nightly benchmarks](https://cockroachdb.github.io/pebble/)\n\nPebble is a LevelDB/RocksDB inspired key-value store focused on\nperformance and internal usage by CockroachDB. Pebble inherits the\nRocksDB file formats and a few extensions such as range deletion\ntombstones, table-level bloom filters, and updates to the MANIFEST\nformat.\n\nPebble intentionally does not aspire to include every feature in RocksDB and\nspecifically targets the use case and feature set needed by CockroachDB:\n\n* Block-based tables\n* Checkpoints\n* Indexed batches\n* Iterator options (lower/upper bound, table filter)\n* Level-based compaction\n* Manual compaction\n* Merge operator\n* Prefix bloom filters\n* Prefix iteration\n* Range deletion tombstones\n* Reverse iteration\n* SSTable ingestion\n* Single delete\n* Snapshots\n* Table-level bloom filters\n\nRocksDB has a large number of features that are not implemented in\nPebble:\n\n* Backups\n* Column families\n* Delete files in range\n* FIFO compaction style\n* Forward iterator / tailing iterator\n* Hash table format\n* Memtable bloom filter\n* Persistent cache\n* Pin iterator key / value\n* Plain table format\n* SSTable ingest-behind\n* Sub-compactions\n* Transactions\n* Universal compaction style\n\n***WARNING***: Pebble may silently corrupt data or behave incorrectly if\nused with a RocksDB database that uses a feature Pebble doesn't\nsupport. Caveat emptor!\n\n## Production Ready\n\nPebble was introduced as an alternative storage engine to RocksDB in\nCockroachDB v20.1 (released May 2020) and was used in production\nsuccessfully at that time. Pebble was made the default storage engine\nin CockroachDB v20.2 (released Nov 2020). Pebble is being used in\nproduction by users of CockroachDB at scale and is considered stable\nand production ready.\n\n## Advantages\n\nPebble offers several improvements over RocksDB:\n\n* Faster reverse iteration via backwards links in the memtable's\n  skiplist.\n* Faster commit pipeline that achieves better concurrency.\n* Seamless merged iteration of indexed batches. The mutations in the\n  batch conceptually occupy another memtable level.\n* L0 sublevels and flush splitting for concurrent compactions out of L0 and\n  reduced read-amplification during heavy write load.\n* Faster LSM edits in LSMs with large numbers of sstables through use of a\n  copy-on-write B-tree to hold file metadata.\n* Delete-only compactions that drop whole sstables that fall within the bounds\n  of a range deletion.\n* Block-property collectors and filters that enable iterators to skip tables,\n  index blocks and data blocks that are irrelevant, according to user-defined\n  properties over key-value pairs.\n* Range keys API, allowing KV pairs defined over a range of keyspace with\n  user-defined semantics and interleaved during iteration.\n* Smaller, more approachable code base.\n\nSee the [Pebble vs RocksDB: Implementation\nDifferences](docs/rocksdb.md) doc for more details on implementation\ndifferences.\n\n## RocksDB Compatibility\n\nPebble `v1` strives for forward compatibility with RocksDB 6.2.1 (the latest\nversion of RocksDB used by CockroachDB). Forward compatibility means that a DB\ngenerated by RocksDB 6.2.1 can be upgraded for use by Pebble. Pebble versions in\nthe `v1` series may open DBs generated by RocksDB 6.2.1. Since its introduction,\nPebble has adopted various backwards-incompatible format changes that are gated\nbehind new 'format major versions'. Pebble `v2` and newer does not support\nopening DBs generated by RocksDB. DBs generated by RocksDB may only be used with\nrecent versions of Pebble after migrating them through format major version\nupgrades using previous versions of Pebble. See the below section of format\nmajor versions.\n\nEven the RocksDB-compatible versions of Pebble only provide compatibility with\nthe subset of functionality and configuration used by CockroachDB. The scope of\nRocksDB functionality and configuration is too large to adequately test and\ndocument all the incompatibilities. The list below contains known\nincompatibilities.\n\n* Pebble's use of WAL recycling is only compatible with RocksDB's\n  `kTolerateCorruptedTailRecords` WAL recovery mode. Older versions of\n  RocksDB would automatically map incompatible WAL recovery modes to\n  `kTolerateCorruptedTailRecords`. New versions of RocksDB will\n  disable WAL recycling.\n* Column families. Pebble does not support column families, nor does\n  it attempt to detect their usage when opening a DB that may contain\n  them.\n* Hash table format. Pebble does not support the hash table sstable\n  format.\n* Plain table format. Pebble does not support the plain table sstable\n  format.\n* SSTable format version 3 and 4. Pebble does not support version 3\n  and version 4 format sstables. The sstable format version is\n  controlled by the `BlockBasedTableOptions::format_version` option.\n  See [#97](https://github.com/cockroachdb/pebble/issues/97).\n\n## Format major versions\n\nOver time Pebble has introduced new physical file formats.  Backwards\nincompatible changes are made through the introduction of 'format major\nversions'. By default, when Pebble opens a database, it defaults to the lowest\nsupported version. In `v1`, this is `FormatMostCompatible`, which is\nbi-directionally compatible with RocksDB 6.2.1 (with the caveats described\nabove).\n\nDatabases created by RocksDB or Pebble versions `v1` and earlier must be upgraded\nto a compatible format major version before running newer Pebble versions. Newer\nPebble versions will refuse to open databases in no longer supported formats.\n\nTo opt into new formats, a user may set `FormatMajorVersion` on the\n[`Options`](https://pkg.go.dev/github.com/cockroachdb/pebble#Options)\nsupplied to\n[`Open`](https://pkg.go.dev/github.com/cockroachdb/pebble#Open), or\nupgrade the format major version at runtime using\n[`DB.RatchetFormatMajorVersion`](https://pkg.go.dev/github.com/cockroachdb/pebble#DB.RatchetFormatMajorVersion).\nFormat major version upgrades are permanent; There is no option to\nreturn to an earlier format.\n\nThe table below outlines the history of format major versions, along with what\nrange of Pebble versions support that format.\n\n| Name                               | Value | Migration  | Pebble support |\n|------------------------------------|-------|------------|----------------|\n| FormatMostCompatible               |   1   | No         | v1             |\n| FormatVersioned                    |   3   | No         | v1             |\n| FormatSetWithDelete                |   4   | No         | v1             |\n| FormatBlockPropertyCollector       |   5   | No         | v1             |\n| FormatSplitUserKeysMarked          |   6   | Background | v1             |\n| FormatSplitUserKeysMarkedCompacted |   7   | Blocking   | v1             |\n| FormatRangeKeys                    |   8   | No         | v1             |\n| FormatMinTableFormatPebblev1       |   9   | No         | v1             |\n| FormatPrePebblev1Marked            |  10   | Background | v1             |\n| FormatSSTableValueBlocks           |  12   | No         | v1             |\n| FormatFlushableIngest              |  13   | No         | v1, v2, master |\n| FormatPrePebblev1MarkedCompacted   |  14   | Blocking   | v1, v2, master |\n| FormatDeleteSizedAndObsolete       |  15   | No         | v1, v2, master |\n| FormatVirtualSSTables              |  16   | No         | v1, v2, master |\n| FormatSyntheticPrefixSuffix        |  17   | No         | v2, master     |\n| FormatFlushableIngestExcises       |  18   | No         | v2, master     |\n| FormatColumnarBlocks               |  19   | No         | v2, master     |\n\nUpgrading to a format major version with 'Background' in the migration\ncolumn may trigger background activity to rewrite physical file\nformats, typically through compactions. Upgrading to a format major\nversion with 'Blocking' in the migration column will block until a\nmigration is complete. The database may continue to serve reads and\nwrites if upgrading a live database through\n`RatchetFormatMajorVersion`, but the method call will not return until\nthe migration is complete.\n\nUpgrading existing stores can be performed via the `RatchetFormatMajorVersion`\nmethod. If the database does not use a custom comparer, merger, or block\nproperty collectors, the `pebble` tool can also be used, at the latest version\nthat supports the format. For example:\n```\n# WARNING: only use if no custom comparer/merger/property collector are necessary.\ngo run github.com/cockroachdb/pebble/cmd/pebble@v1.1.3 db upgrade <db-dir>\n```\n\nFor reference, the table below lists the range of supported Pebble format major\nversions for CockroachDB releases.\n\n| CockroachDB release | Earliest supported                 | Latest supported            |\n|---------------------|------------------------------------|-----------------------------|\n| 20.1 through 21.1   | FormatMostCompatible               | FormatMostCompatible        |\n| 21.2                | FormatMostCompatible               | FormatSetWithDelete         |\n| 21.2                | FormatMostCompatible               | FormatSetWithDelete         |\n| 22.1                | FormatMostCompatible               | FormatSplitUserKeysMarked   |\n| 22.2                | FormatMostCompatible               | FormatPrePebblev1Marked     |\n| 23.1                | FormatSplitUserKeysMarkedCompacted | FormatFlushableIngest       |\n| 23.2                | FormatPrePebblev1Marked            | FormatVirtualSSTables       |\n| 24.1                | FormatFlushableIngest              | FormatSyntheticPrefixSuffix |\n| 24.2                | FormatVirtualSSTables              | FormatSyntheticPrefixSuffix |\n| 24.3                | FormatSyntheticPrefixSuffix        | FormatColumnarBlocks        |\n\n## Pedigree\n\nPebble is based on the incomplete Go version of LevelDB:\n\nhttps://github.com/golang/leveldb\n\nThe Go version of LevelDB is based on the C++ original:\n\nhttps://github.com/google/leveldb\n\nOptimizations and inspiration were drawn from RocksDB:\n\nhttps://github.com/facebook/rocksdb\n\n## Getting Started\n\n### Example Code\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\n\t\"github.com/cockroachdb/pebble\"\n)\n\nfunc main() {\n\tdb, err := pebble.Open(\"demo\", &pebble.Options{})\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tkey := []byte(\"hello\")\n\tif err := db.Set(key, []byte(\"world\"), pebble.Sync); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tvalue, closer, err := db.Get(key)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tfmt.Printf(\"%s %s\\n\", key, value)\n\tif err := closer.Close(); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tif err := db.Close(); err != nil {\n\t\tlog.Fatal(err)\n\t}\n}\n```\n"
        },
        {
          "name": "batch.go",
          "type": "blob",
          "size": 85.287109375,
          "content": "// Copyright 2012 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\t\"sort\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\t\"unsafe\"\n\n\t\"github.com/cockroachdb/crlib/crtime\"\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/batchrepr\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/batchskl\"\n\t\"github.com/cockroachdb/pebble/internal/humanize\"\n\t\"github.com/cockroachdb/pebble/internal/invariants\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan\"\n\t\"github.com/cockroachdb/pebble/internal/private\"\n\t\"github.com/cockroachdb/pebble/internal/rangedel\"\n\t\"github.com/cockroachdb/pebble/internal/rangekey\"\n\t\"github.com/cockroachdb/pebble/internal/rawalloc\"\n\t\"github.com/cockroachdb/pebble/internal/treeprinter\"\n)\n\nconst (\n\tinvalidBatchCount = 1<<32 - 1\n\tmaxVarintLen32    = 5\n\n\tdefaultBatchInitialSize     = 1 << 10 // 1 KB\n\tdefaultBatchMaxRetainedSize = 1 << 20 // 1 MB\n)\n\n// ErrNotIndexed means that a read operation on a batch failed because the\n// batch is not indexed and thus doesn't support reads.\nvar ErrNotIndexed = errors.New(\"pebble: batch not indexed\")\n\n// ErrInvalidBatch indicates that a batch is invalid or otherwise corrupted.\nvar ErrInvalidBatch = batchrepr.ErrInvalidBatch\n\n// ErrBatchTooLarge indicates that a batch is invalid or otherwise corrupted.\nvar ErrBatchTooLarge = base.MarkCorruptionError(errors.Newf(\"pebble: batch too large: >= %s\", humanize.Bytes.Uint64(maxBatchSize)))\n\n// DeferredBatchOp represents a batch operation (eg. set, merge, delete) that is\n// being inserted into the batch. Indexing is not performed on the specified key\n// until Finish is called, hence the name deferred. This struct lets the caller\n// copy or encode keys/values directly into the batch representation instead of\n// copying into an intermediary buffer then having pebble.Batch copy off of it.\ntype DeferredBatchOp struct {\n\tindex *batchskl.Skiplist\n\n\t// Key and Value point to parts of the binary batch representation where\n\t// keys and values should be encoded/copied into. len(Key) and len(Value)\n\t// bytes must be copied into these slices respectively before calling\n\t// Finish(). Changing where these slices point to is not allowed.\n\tKey, Value []byte\n\toffset     uint32\n}\n\n// Finish completes the addition of this batch operation, and adds it to the\n// index if necessary. Must be called once (and exactly once) keys/values\n// have been filled into Key and Value. Not calling Finish or not\n// copying/encoding keys will result in an incomplete index, and calling Finish\n// twice may result in a panic.\nfunc (d DeferredBatchOp) Finish() error {\n\tif d.index != nil {\n\t\tif err := d.index.Add(d.offset); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// A Batch is a sequence of Sets, Merges, Deletes, DeleteRanges, RangeKeySets,\n// RangeKeyUnsets, and/or RangeKeyDeletes that are applied atomically. Batch\n// implements the Reader interface, but only an indexed batch supports reading\n// (without error) via Get or NewIter. A non-indexed batch will return\n// ErrNotIndexed when read from. A batch is not safe for concurrent use, and\n// consumers should use a batch per goroutine or provide their own\n// synchronization.\n//\n// # Indexing\n//\n// Batches can be optionally indexed (see DB.NewIndexedBatch). An indexed batch\n// allows iteration via an Iterator (see Batch.NewIter). The iterator provides\n// a merged view of the operations in the batch and the underlying\n// database. This is implemented by treating the batch as an additional layer\n// in the LSM where every entry in the batch is considered newer than any entry\n// in the underlying database (batch entries have the InternalKeySeqNumBatch\n// bit set). By treating the batch as an additional layer in the LSM, iteration\n// supports all batch operations (i.e. Set, Merge, Delete, DeleteRange,\n// RangeKeySet, RangeKeyUnset, RangeKeyDelete) with minimal effort.\n//\n// The same key can be operated on multiple times in a batch, though only the\n// latest operation will be visible. For example, Put(\"a\", \"b\"), Delete(\"a\")\n// will cause the key \"a\" to not be visible in the batch. Put(\"a\", \"b\"),\n// Put(\"a\", \"c\") will cause a read of \"a\" to return the value \"c\".\n//\n// The batch index is implemented via an skiplist (internal/batchskl). While\n// the skiplist implementation is very fast, inserting into an indexed batch is\n// significantly slower than inserting into a non-indexed batch. Only use an\n// indexed batch if you require reading from it.\n//\n// # Atomic commit\n//\n// The operations in a batch are persisted by calling Batch.Commit which is\n// equivalent to calling DB.Apply(batch). A batch is committed atomically by\n// writing the internal batch representation to the WAL, adding all of the\n// batch operations to the memtable associated with the WAL, and then\n// incrementing the visible sequence number so that subsequent reads can see\n// the effects of the batch operations. If WriteOptions.Sync is true, a call to\n// Batch.Commit will guarantee that the batch is persisted to disk before\n// returning. See commitPipeline for more on the implementation details.\n//\n// # Large batches\n//\n// The size of a batch is limited only by available memory (be aware that\n// indexed batches require considerably additional memory for the skiplist\n// structure). A given WAL file has a single memtable associated with it (this\n// restriction could be removed, but doing so is onerous and complex). And a\n// memtable has a fixed size due to the underlying fixed size arena. Note that\n// this differs from RocksDB where a memtable can grow arbitrarily large using\n// a list of arena chunks. In RocksDB this is accomplished by storing pointers\n// in the arena memory, but that isn't possible in Go.\n//\n// During Batch.Commit, a batch which is larger than a threshold (>\n// MemTableSize/2) is wrapped in a flushableBatch and inserted into the queue\n// of memtables. A flushableBatch forces WAL to be rotated, but that happens\n// anyways when the memtable becomes full so this does not cause significant\n// WAL churn. Because the flushableBatch is readable as another layer in the\n// LSM, Batch.Commit returns as soon as the flushableBatch has been added to\n// the queue of memtables.\n//\n// Internally, a flushableBatch provides Iterator support by sorting the batch\n// contents (the batch is sorted once, when it is added to the memtable\n// queue). Sorting the batch contents and insertion of the contents into a\n// memtable have the same big-O time, but the constant factor dominates\n// here. Sorting is significantly faster and uses significantly less memory.\n//\n// # Internal representation\n//\n// The internal batch representation is a contiguous byte buffer with a fixed\n// 12-byte header, followed by a series of records.\n//\n//\t+-------------+------------+--- ... ---+\n//\t| SeqNum (8B) | Count (4B) |  Entries  |\n//\t+-------------+------------+--- ... ---+\n//\n// Each record has a 1-byte kind tag prefix, followed by 1 or 2 length prefixed\n// strings (varstring):\n//\n//\t+-----------+-----------------+-------------------+\n//\t| Kind (1B) | Key (varstring) | Value (varstring) |\n//\t+-----------+-----------------+-------------------+\n//\n// A varstring is a varint32 followed by N bytes of data. The Kind tags are\n// exactly those specified by InternalKeyKind. The following table shows the\n// format for records of each kind:\n//\n//\tInternalKeyKindDelete         varstring\n//\tInternalKeyKindLogData        varstring\n//\tInternalKeyKindIngestSST      varstring\n//\tInternalKeyKindSet            varstring varstring\n//\tInternalKeyKindMerge          varstring varstring\n//\tInternalKeyKindRangeDelete    varstring varstring\n//\tInternalKeyKindRangeKeySet    varstring varstring\n//\tInternalKeyKindRangeKeyUnset  varstring varstring\n//\tInternalKeyKindRangeKeyDelete varstring varstring\n//\n// The intuitive understanding here are that the arguments to Delete, Set,\n// Merge, DeleteRange and RangeKeyDelete are encoded into the batch. The\n// RangeKeySet and RangeKeyUnset operations are slightly more complicated,\n// encoding their end key, suffix and value [in the case of RangeKeySet] within\n// the Value varstring. For more information on the value encoding for\n// RangeKeySet and RangeKeyUnset, see the internal/rangekey package.\n//\n// The internal batch representation is the on disk format for a batch in the\n// WAL, and thus stable. New record kinds may be added, but the existing ones\n// will not be modified.\ntype Batch struct {\n\tbatchInternal\n\tapplied atomic.Bool\n\t// lifecycle is used to negotiate the lifecycle of a Batch. A Batch and its\n\t// underlying batchInternal.data byte slice may be reused. There are two\n\t// mechanisms for reuse:\n\t//\n\t// 1. The caller may explicitly call [Batch.Reset] to reset the batch to be\n\t//    empty (while retaining the underlying repr's buffer).\n\t// 2. The caller may call [Batch.Close], passing ownership off to Pebble,\n\t//    which may reuse the batch's memory to service new callers to\n\t//    [DB.NewBatch].\n\t//\n\t// There's a complication to reuse: When WAL failover is configured, the\n\t// Pebble commit pipeline may retain a pointer to the batch.data beyond the\n\t// return of [Batch.Commit]. The user of the Batch may commit their batch\n\t// and call Close or Reset before the commit pipeline is finished reading\n\t// the data slice. Recycling immediately would cause a data race.\n\t//\n\t// To resolve this data race, this [lifecycle] atomic is used to determine\n\t// safety and responsibility of reusing a batch. The low bits of the atomic\n\t// are used as a reference count (really just the lowest bit—in practice\n\t// there's only 1 code path that references). The [Batch] is passed into\n\t// [wal.Writer]'s WriteRecord method as a [RefCount] implementation. The\n\t// wal.Writer guarantees that if it will read [Batch.data] after the call to\n\t// WriteRecord returns, it will increment the reference count. When it's\n\t// complete, it'll unreference through invoking [Batch.Unref].\n\t//\n\t// When the committer of a batch indicates intent to recycle a Batch through\n\t// calling [Batch.Reset] or [Batch.Close], the lifecycle atomic is read. If\n\t// an outstanding reference remains, it's unsafe to reuse Batch.data yet. In\n\t// [Batch.Reset] the caller wants to reuse the [Batch] immediately, so we\n\t// discard b.data to recycle the struct but not the underlying byte slice.\n\t// In [Batch.Close], we set a special high bit [batchClosedBit] on lifecycle\n\t// that indicates that the user will not use [Batch] again and we're free to\n\t// recycle it when safe. When the commit pipeline eventually calls\n\t// [Batch.Unref], the [batchClosedBit] is noticed and the batch is\n\t// recycled.\n\tlifecycle atomic.Int32\n}\n\n// batchClosedBit is a bit stored on Batch.lifecycle to indicate that the user\n// called [Batch.Close] to release a Batch, but an open reference count\n// prevented immediate recycling.\nconst batchClosedBit = 1 << 30\n\n// TODO(jackson): Hide the wal.RefCount implementation from the public Batch interface.\n\n// Ref implements wal.RefCount. If the WAL writer may need to read b.data after\n// it returns, it invokes Ref to increment the lifecycle's reference count. When\n// it's finished, it invokes Unref.\nfunc (b *Batch) Ref() {\n\tb.lifecycle.Add(+1)\n}\n\n// Unref implemets wal.RefCount.\nfunc (b *Batch) Unref() {\n\tif v := b.lifecycle.Add(-1); (v ^ batchClosedBit) == 0 {\n\t\t// The [batchClosedBit] high bit is set, and there are no outstanding\n\t\t// references. The user of the Batch called [Batch.Close], expecting the\n\t\t// batch to be recycled. However, our outstanding reference count\n\t\t// prevented recycling. As the last to dereference, we're now\n\t\t// responsible for releasing the batch.\n\t\tb.lifecycle.Store(0)\n\t\tb.release()\n\t}\n}\n\n// batchInternal contains the set of fields within Batch that are non-atomic and\n// capable of being reset using a *b = batchInternal{} struct copy.\ntype batchInternal struct {\n\t// Data is the wire format of a batch's log entry:\n\t//   - 8 bytes for a sequence number of the first batch element,\n\t//     or zeroes if the batch has not yet been applied,\n\t//   - 4 bytes for the count: the number of elements in the batch,\n\t//     or \"\\xff\\xff\\xff\\xff\" if the batch is invalid,\n\t//   - count elements, being:\n\t//     - one byte for the kind\n\t//     - the varint-string user key,\n\t//     - the varint-string value (if kind != delete).\n\t// The sequence number and count are stored in little-endian order.\n\t//\n\t// The data field can be (but is not guaranteed to be) nil for new\n\t// batches. Large batches will set the data field to nil when committed as\n\t// the data has been moved to a flushableBatch and inserted into the queue of\n\t// memtables.\n\tdata     []byte\n\tcomparer *base.Comparer\n\topts     batchOptions\n\n\t// An upper bound on required space to add this batch to a memtable.\n\t// Note that although batches are limited to 4 GiB in size, that limit\n\t// applies to len(data), not the memtable size. The upper bound on the\n\t// size of a memtable node is larger than the overhead of the batch's log\n\t// encoding, so memTableSize is larger than len(data) and may overflow a\n\t// uint32.\n\tmemTableSize uint64\n\n\t// The db to which the batch will be committed. Do not change this field\n\t// after the batch has been created as it might invalidate internal state.\n\t// Batch.memTableSize is only refreshed if Batch.db is set. Setting db to\n\t// nil once it has been set implies that the Batch has encountered an error.\n\tdb *DB\n\n\t// The count of records in the batch. This count will be stored in the batch\n\t// data whenever Repr() is called.\n\tcount uint64\n\n\t// The count of range deletions in the batch. Updated every time a range\n\t// deletion is added.\n\tcountRangeDels uint64\n\n\t// The count of range key sets, unsets and deletes in the batch. Updated\n\t// every time a RANGEKEYSET, RANGEKEYUNSET or RANGEKEYDEL key is added.\n\tcountRangeKeys uint64\n\n\t// A deferredOp struct, stored in the Batch so that a pointer can be returned\n\t// from the *Deferred() methods rather than a value.\n\tdeferredOp DeferredBatchOp\n\n\t// An optional skiplist keyed by offset into data of the entry.\n\tindex         *batchskl.Skiplist\n\trangeDelIndex *batchskl.Skiplist\n\trangeKeyIndex *batchskl.Skiplist\n\n\t// Fragmented range deletion tombstones. Cached the first time a range\n\t// deletion iterator is requested. The cache is invalidated whenever a new\n\t// range deletion is added to the batch. This cache can only be used when\n\t// opening an iterator to read at a batch sequence number >=\n\t// tombstonesSeqNum. This is the case for all new iterators created over a\n\t// batch but it's not the case for all cloned iterators.\n\ttombstones       []keyspan.Span\n\ttombstonesSeqNum base.SeqNum\n\n\t// Fragmented range key spans. Cached the first time a range key iterator is\n\t// requested. The cache is invalidated whenever a new range key\n\t// (RangeKey{Set,Unset,Del}) is added to the batch. This cache can only be\n\t// used when opening an iterator to read at a batch sequence number >=\n\t// tombstonesSeqNum. This is the case for all new iterators created over a\n\t// batch but it's not the case for all cloned iterators.\n\trangeKeys       []keyspan.Span\n\trangeKeysSeqNum base.SeqNum\n\n\t// The flushableBatch wrapper if the batch is too large to fit in the\n\t// memtable.\n\tflushable *flushableBatch\n\n\t// minimumFormatMajorVersion indicates the format major version required in\n\t// order to commit this batch. If an operation requires a particular format\n\t// major version, it ratchets the batch's minimumFormatMajorVersion. When\n\t// the batch is committed, this is validated against the database's current\n\t// format major version.\n\tminimumFormatMajorVersion FormatMajorVersion\n\n\t// Synchronous Apply uses the commit WaitGroup for both publishing the\n\t// seqnum and waiting for the WAL fsync (if needed). Asynchronous\n\t// ApplyNoSyncWait, which implies WriteOptions.Sync is true, uses the commit\n\t// WaitGroup for publishing the seqnum and the fsyncWait WaitGroup for\n\t// waiting for the WAL fsync.\n\t//\n\t// TODO(sumeer): if we find that ApplyNoSyncWait in conjunction with\n\t// SyncWait is causing higher memory usage because of the time duration\n\t// between when the sync is already done, and a goroutine calls SyncWait\n\t// (followed by Batch.Close), we could separate out {fsyncWait, commitErr}\n\t// into a separate struct that is allocated separately (using another\n\t// sync.Pool), and only that struct needs to outlive Batch.Close (which\n\t// could then be called immediately after ApplyNoSyncWait). commitStats\n\t// will also need to be in this separate struct.\n\tcommit    sync.WaitGroup\n\tfsyncWait sync.WaitGroup\n\n\tcommitStats BatchCommitStats\n\n\tcommitErr error\n\n\t// Position bools together to reduce the sizeof the struct.\n\n\t// ingestedSSTBatch indicates that the batch contains one or more key kinds\n\t// of InternalKeyKindIngestSST. If the batch contains key kinds of IngestSST\n\t// then it will only contain key kinds of IngestSST.\n\tingestedSSTBatch bool\n\n\t// committing is set to true when a batch begins to commit. It's used to\n\t// ensure the batch is not mutated concurrently. It is not an atomic\n\t// deliberately, so as to avoid the overhead on batch mutations. This is\n\t// okay, because under correct usage this field will never be accessed\n\t// concurrently. It's only under incorrect usage the memory accesses of this\n\t// variable may violate memory safety. Since we don't use atomics here,\n\t// false negatives are possible.\n\tcommitting bool\n}\n\n// BatchCommitStats exposes stats related to committing a batch.\n//\n// NB: there is no Pebble internal tracing (using LoggerAndTracer) of slow\n// batch commits. The caller can use these stats to do their own tracing as\n// needed.\ntype BatchCommitStats struct {\n\t// TotalDuration is the time spent in DB.{Apply,ApplyNoSyncWait} or\n\t// Batch.Commit, plus the time waiting in Batch.SyncWait. If there is a gap\n\t// between calling ApplyNoSyncWait and calling SyncWait, that gap could\n\t// include some duration in which real work was being done for the commit\n\t// and will not be included here. This missing time is considered acceptable\n\t// since the goal of these stats is to understand user-facing latency.\n\t//\n\t// TotalDuration includes time spent in various queues both inside Pebble\n\t// and outside Pebble (I/O queues, goroutine scheduler queue, mutex wait\n\t// etc.). For some of these queues (which we consider important) the wait\n\t// times are included below -- these expose low-level implementation detail\n\t// and are meant for expert diagnosis and subject to change. There may be\n\t// unaccounted time after subtracting those values from TotalDuration.\n\tTotalDuration time.Duration\n\t// SemaphoreWaitDuration is the wait time for semaphores in\n\t// commitPipeline.Commit.\n\tSemaphoreWaitDuration time.Duration\n\t// WALQueueWaitDuration is the wait time for allocating memory blocks in the\n\t// LogWriter (due to the LogWriter not writing fast enough). At the moment\n\t// this is duration is always zero because a single WAL will allow\n\t// allocating memory blocks up to the entire memtable size. In the future,\n\t// we may pipeline WALs and bound the WAL queued blocks separately, so this\n\t// field is preserved for that possibility.\n\tWALQueueWaitDuration time.Duration\n\t// MemTableWriteStallDuration is the wait caused by a write stall due to too\n\t// many memtables (due to not flushing fast enough).\n\tMemTableWriteStallDuration time.Duration\n\t// L0ReadAmpWriteStallDuration is the wait caused by a write stall due to\n\t// high read amplification in L0 (due to not compacting fast enough out of\n\t// L0).\n\tL0ReadAmpWriteStallDuration time.Duration\n\t// WALRotationDuration is the wait time for WAL rotation, which includes\n\t// syncing and closing the old WAL and creating (or reusing) a new one.\n\tWALRotationDuration time.Duration\n\t// CommitWaitDuration is the wait for publishing the seqnum plus the\n\t// duration for the WAL sync (if requested). The former should be tiny and\n\t// one can assume that this is all due to the WAL sync.\n\tCommitWaitDuration time.Duration\n}\n\nvar _ Reader = (*Batch)(nil)\nvar _ Writer = (*Batch)(nil)\n\nvar batchPool = sync.Pool{\n\tNew: func() interface{} {\n\t\treturn &Batch{}\n\t},\n}\n\ntype indexedBatch struct {\n\tbatch Batch\n\tindex batchskl.Skiplist\n}\n\nvar indexedBatchPool = sync.Pool{\n\tNew: func() interface{} {\n\t\treturn &indexedBatch{}\n\t},\n}\n\nfunc newBatch(db *DB, opts ...BatchOption) *Batch {\n\tb := batchPool.Get().(*Batch)\n\tb.db = db\n\tb.opts.ensureDefaults()\n\tfor _, opt := range opts {\n\t\topt(&b.opts)\n\t}\n\treturn b\n}\n\nfunc newBatchWithSize(db *DB, size int, opts ...BatchOption) *Batch {\n\tb := newBatch(db, opts...)\n\tif cap(b.data) < size {\n\t\tb.data = rawalloc.New(0, size)\n\t}\n\treturn b\n}\n\nfunc newIndexedBatch(db *DB, comparer *Comparer) *Batch {\n\ti := indexedBatchPool.Get().(*indexedBatch)\n\ti.batch.comparer = comparer\n\ti.batch.db = db\n\ti.batch.index = &i.index\n\ti.batch.index.Init(&i.batch.data, comparer.Compare, comparer.AbbreviatedKey)\n\ti.batch.opts.ensureDefaults()\n\treturn &i.batch\n}\n\nfunc newIndexedBatchWithSize(db *DB, comparer *Comparer, size int) *Batch {\n\tb := newIndexedBatch(db, comparer)\n\tif cap(b.data) < size {\n\t\tb.data = rawalloc.New(0, size)\n\t}\n\treturn b\n}\n\n// nextSeqNum returns the batch \"sequence number\" that will be given to the next\n// key written to the batch. During iteration keys within an indexed batch are\n// given a sequence number consisting of their offset within the batch combined\n// with the base.SeqNumBatchBit bit. These sequence numbers are only\n// used during iteration, and the keys are assigned ordinary sequence numbers\n// when the batch is committed.\nfunc (b *Batch) nextSeqNum() base.SeqNum {\n\treturn base.SeqNum(len(b.data)) | base.SeqNumBatchBit\n}\n\nfunc (b *Batch) release() {\n\tif b.db == nil {\n\t\t// The batch was not created using newBatch or newIndexedBatch, or an error\n\t\t// was encountered. We don't try to reuse batches that encountered an error\n\t\t// because they might be stuck somewhere in the system and attempting to\n\t\t// reuse such batches is a recipe for onerous debugging sessions. Instead,\n\t\t// let the GC do its job.\n\t\treturn\n\t}\n\tb.db = nil\n\n\t// NB: This is ugly (it would be cleaner if we could just assign a Batch{}),\n\t// but necessary so that we can use atomic.StoreUint32 for the Batch.applied\n\t// field. Without using an atomic to clear that field the Go race detector\n\t// complains.\n\tb.reset()\n\tb.comparer = nil\n\n\tif b.index == nil {\n\t\tbatchPool.Put(b)\n\t} else {\n\t\tb.index, b.rangeDelIndex, b.rangeKeyIndex = nil, nil, nil\n\t\tindexedBatchPool.Put((*indexedBatch)(unsafe.Pointer(b)))\n\t}\n}\n\nfunc (b *Batch) refreshMemTableSize() error {\n\tb.memTableSize = 0\n\tif len(b.data) < batchrepr.HeaderLen {\n\t\treturn nil\n\t}\n\n\tb.countRangeDels = 0\n\tb.countRangeKeys = 0\n\tb.minimumFormatMajorVersion = 0\n\tfor r := b.Reader(); ; {\n\t\tkind, key, value, ok, err := r.Next()\n\t\tif !ok {\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tbreak\n\t\t}\n\t\tswitch kind {\n\t\tcase InternalKeyKindRangeDelete:\n\t\t\tb.countRangeDels++\n\t\tcase InternalKeyKindRangeKeySet, InternalKeyKindRangeKeyUnset, InternalKeyKindRangeKeyDelete:\n\t\t\tb.countRangeKeys++\n\t\tcase InternalKeyKindSet, InternalKeyKindDelete, InternalKeyKindMerge, InternalKeyKindSingleDelete, InternalKeyKindSetWithDelete:\n\t\t\t// fallthrough\n\t\tcase InternalKeyKindDeleteSized:\n\t\t\tif b.minimumFormatMajorVersion < FormatDeleteSizedAndObsolete {\n\t\t\t\tb.minimumFormatMajorVersion = FormatDeleteSizedAndObsolete\n\t\t\t}\n\t\tcase InternalKeyKindLogData:\n\t\t\t// LogData does not contribute to memtable size.\n\t\t\tcontinue\n\t\tcase InternalKeyKindIngestSST:\n\t\t\tif b.minimumFormatMajorVersion < FormatFlushableIngest {\n\t\t\t\tb.minimumFormatMajorVersion = FormatFlushableIngest\n\t\t\t}\n\t\t\t// This key kind doesn't contribute to the memtable size.\n\t\t\tcontinue\n\t\tcase InternalKeyKindExcise:\n\t\t\tif b.minimumFormatMajorVersion < FormatFlushableIngestExcises {\n\t\t\t\tb.minimumFormatMajorVersion = FormatFlushableIngestExcises\n\t\t\t}\n\t\t\t// This key kind doesn't contribute to the memtable size.\n\t\t\tcontinue\n\t\tdefault:\n\t\t\t// Note In some circumstances this might be temporary memory\n\t\t\t// corruption that can be recovered by discarding the batch and\n\t\t\t// trying again. In other cases, the batch repr might've been\n\t\t\t// already persisted elsewhere, and we'll loop continuously trying\n\t\t\t// to commit the same corrupted batch. The caller is responsible for\n\t\t\t// distinguishing.\n\t\t\treturn errors.Wrapf(ErrInvalidBatch, \"unrecognized kind %v\", kind)\n\t\t}\n\t\tb.memTableSize += memTableEntrySize(len(key), len(value))\n\t}\n\treturn nil\n}\n\n// Apply the operations contained in the batch to the receiver batch.\n//\n// It is safe to modify the contents of the arguments after Apply returns.\n//\n// Apply returns ErrInvalidBatch if the provided batch is invalid in any way.\nfunc (b *Batch) Apply(batch *Batch, _ *WriteOptions) error {\n\tif b.ingestedSSTBatch {\n\t\tpanic(\"pebble: invalid batch application\")\n\t}\n\tif len(batch.data) == 0 {\n\t\treturn nil\n\t}\n\tif len(batch.data) < batchrepr.HeaderLen {\n\t\treturn ErrInvalidBatch\n\t}\n\n\toffset := len(b.data)\n\tif offset == 0 {\n\t\tb.init(offset)\n\t\toffset = batchrepr.HeaderLen\n\t}\n\tb.data = append(b.data, batch.data[batchrepr.HeaderLen:]...)\n\n\tb.setCount(b.Count() + batch.Count())\n\n\tif b.db != nil || b.index != nil {\n\t\t// Only iterate over the new entries if we need to track memTableSize or in\n\t\t// order to update the index.\n\t\tfor iter := batchrepr.Reader(b.data[offset:]); len(iter) > 0; {\n\t\t\toffset := uintptr(unsafe.Pointer(&iter[0])) - uintptr(unsafe.Pointer(&b.data[0]))\n\t\t\tkind, key, value, ok, err := iter.Next()\n\t\t\tif !ok {\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tswitch kind {\n\t\t\tcase InternalKeyKindRangeDelete:\n\t\t\t\tb.countRangeDels++\n\t\t\tcase InternalKeyKindRangeKeySet, InternalKeyKindRangeKeyUnset, InternalKeyKindRangeKeyDelete:\n\t\t\t\tb.countRangeKeys++\n\t\t\tcase InternalKeyKindIngestSST, InternalKeyKindExcise:\n\t\t\t\tpanic(\"pebble: invalid key kind for batch\")\n\t\t\tcase InternalKeyKindLogData:\n\t\t\t\t// LogData does not contribute to memtable size.\n\t\t\t\tcontinue\n\t\t\tcase InternalKeyKindSet, InternalKeyKindDelete, InternalKeyKindMerge,\n\t\t\t\tInternalKeyKindSingleDelete, InternalKeyKindSetWithDelete, InternalKeyKindDeleteSized:\n\t\t\t\t// fallthrough\n\t\t\tdefault:\n\t\t\t\t// Note In some circumstances this might be temporary memory\n\t\t\t\t// corruption that can be recovered by discarding the batch and\n\t\t\t\t// trying again. In other cases, the batch repr might've been\n\t\t\t\t// already persisted elsewhere, and we'll loop continuously\n\t\t\t\t// trying to commit the same corrupted batch. The caller is\n\t\t\t\t// responsible for distinguishing.\n\t\t\t\treturn errors.Wrapf(ErrInvalidBatch, \"unrecognized kind %v\", kind)\n\t\t\t}\n\t\t\tif b.index != nil {\n\t\t\t\tvar err error\n\t\t\t\tswitch kind {\n\t\t\t\tcase InternalKeyKindRangeDelete:\n\t\t\t\t\tb.tombstones = nil\n\t\t\t\t\tb.tombstonesSeqNum = 0\n\t\t\t\t\tif b.rangeDelIndex == nil {\n\t\t\t\t\t\tb.rangeDelIndex = batchskl.NewSkiplist(&b.data, b.comparer.Compare, b.comparer.AbbreviatedKey)\n\t\t\t\t\t}\n\t\t\t\t\terr = b.rangeDelIndex.Add(uint32(offset))\n\t\t\t\tcase InternalKeyKindRangeKeySet, InternalKeyKindRangeKeyUnset, InternalKeyKindRangeKeyDelete:\n\t\t\t\t\tb.rangeKeys = nil\n\t\t\t\t\tb.rangeKeysSeqNum = 0\n\t\t\t\t\tif b.rangeKeyIndex == nil {\n\t\t\t\t\t\tb.rangeKeyIndex = batchskl.NewSkiplist(&b.data, b.comparer.Compare, b.comparer.AbbreviatedKey)\n\t\t\t\t\t}\n\t\t\t\t\terr = b.rangeKeyIndex.Add(uint32(offset))\n\t\t\t\tdefault:\n\t\t\t\t\terr = b.index.Add(uint32(offset))\n\t\t\t\t}\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t\tb.memTableSize += memTableEntrySize(len(key), len(value))\n\t\t}\n\t}\n\treturn nil\n}\n\n// Get gets the value for the given key. It returns ErrNotFound if the Batch\n// does not contain the key.\n//\n// The caller should not modify the contents of the returned slice, but it is\n// safe to modify the contents of the argument after Get returns. The returned\n// slice will remain valid until the returned Closer is closed. On success, the\n// caller MUST call closer.Close() or a memory leak will occur.\nfunc (b *Batch) Get(key []byte) ([]byte, io.Closer, error) {\n\tif b.index == nil {\n\t\treturn nil, nil, ErrNotIndexed\n\t}\n\treturn b.db.getInternal(key, b, nil /* snapshot */)\n}\n\nfunc (b *Batch) prepareDeferredKeyValueRecord(keyLen, valueLen int, kind InternalKeyKind) {\n\tif b.committing {\n\t\tpanic(\"pebble: batch already committing\")\n\t}\n\tif len(b.data) == 0 {\n\t\tb.init(keyLen + valueLen + 2*binary.MaxVarintLen64 + batchrepr.HeaderLen)\n\t}\n\tb.count++\n\tb.memTableSize += memTableEntrySize(keyLen, valueLen)\n\n\tpos := len(b.data)\n\tb.deferredOp.offset = uint32(pos)\n\tb.grow(1 + 2*maxVarintLen32 + keyLen + valueLen)\n\tb.data[pos] = byte(kind)\n\tpos++\n\n\t{\n\t\t// TODO(peter): Manually inlined version binary.PutUvarint(). This is 20%\n\t\t// faster on BenchmarkBatchSet on go1.13. Remove if go1.14 or future\n\t\t// versions show this to not be a performance win.\n\t\tx := uint32(keyLen)\n\t\tfor x >= 0x80 {\n\t\t\tb.data[pos] = byte(x) | 0x80\n\t\t\tx >>= 7\n\t\t\tpos++\n\t\t}\n\t\tb.data[pos] = byte(x)\n\t\tpos++\n\t}\n\n\tb.deferredOp.Key = b.data[pos : pos+keyLen]\n\tpos += keyLen\n\n\t{\n\t\t// TODO(peter): Manually inlined version binary.PutUvarint(). This is 20%\n\t\t// faster on BenchmarkBatchSet on go1.13. Remove if go1.14 or future\n\t\t// versions show this to not be a performance win.\n\t\tx := uint32(valueLen)\n\t\tfor x >= 0x80 {\n\t\t\tb.data[pos] = byte(x) | 0x80\n\t\t\tx >>= 7\n\t\t\tpos++\n\t\t}\n\t\tb.data[pos] = byte(x)\n\t\tpos++\n\t}\n\n\tb.deferredOp.Value = b.data[pos : pos+valueLen]\n\t// Shrink data since varints may be shorter than the upper bound.\n\tb.data = b.data[:pos+valueLen]\n}\n\nfunc (b *Batch) prepareDeferredKeyRecord(keyLen int, kind InternalKeyKind) {\n\tif b.committing {\n\t\tpanic(\"pebble: batch already committing\")\n\t}\n\tif len(b.data) == 0 {\n\t\tb.init(keyLen + binary.MaxVarintLen64 + batchrepr.HeaderLen)\n\t}\n\tb.count++\n\tb.memTableSize += memTableEntrySize(keyLen, 0)\n\n\tpos := len(b.data)\n\tb.deferredOp.offset = uint32(pos)\n\tb.grow(1 + maxVarintLen32 + keyLen)\n\tb.data[pos] = byte(kind)\n\tpos++\n\n\t{\n\t\t// TODO(peter): Manually inlined version binary.PutUvarint(). Remove if\n\t\t// go1.13 or future versions show this to not be a performance win. See\n\t\t// BenchmarkBatchSet.\n\t\tx := uint32(keyLen)\n\t\tfor x >= 0x80 {\n\t\t\tb.data[pos] = byte(x) | 0x80\n\t\t\tx >>= 7\n\t\t\tpos++\n\t\t}\n\t\tb.data[pos] = byte(x)\n\t\tpos++\n\t}\n\n\tb.deferredOp.Key = b.data[pos : pos+keyLen]\n\tb.deferredOp.Value = nil\n\n\t// Shrink data since varint may be shorter than the upper bound.\n\tb.data = b.data[:pos+keyLen]\n}\n\n// AddInternalKey allows the caller to add an internal key of point key or range\n// key kinds (but not RangeDelete) to a batch. Passing in an internal key of\n// kind RangeDelete will result in a panic. Note that the seqnum in the internal\n// key is effectively ignored, even though the Kind is preserved. This is\n// because the batch format does not allow for a per-key seqnum to be specified,\n// only a batch-wide one.\n//\n// Note that non-indexed keys (IngestKeyKind{LogData,IngestSST}) are not\n// supported with this method as they require specialized logic.\nfunc (b *Batch) AddInternalKey(key *base.InternalKey, value []byte, _ *WriteOptions) error {\n\tkeyLen := len(key.UserKey)\n\thasValue := false\n\tswitch kind := key.Kind(); kind {\n\tcase InternalKeyKindRangeDelete:\n\t\tpanic(\"unexpected range delete in AddInternalKey\")\n\tcase InternalKeyKindSingleDelete, InternalKeyKindDelete:\n\t\tb.prepareDeferredKeyRecord(keyLen, kind)\n\t\tb.deferredOp.index = b.index\n\tcase InternalKeyKindRangeKeySet, InternalKeyKindRangeKeyUnset, InternalKeyKindRangeKeyDelete:\n\t\tb.prepareDeferredKeyValueRecord(keyLen, len(value), kind)\n\t\thasValue = true\n\t\tb.incrementRangeKeysCount()\n\tdefault:\n\t\tb.prepareDeferredKeyValueRecord(keyLen, len(value), kind)\n\t\thasValue = true\n\t\tb.deferredOp.index = b.index\n\t}\n\tcopy(b.deferredOp.Key, key.UserKey)\n\tif hasValue {\n\t\tcopy(b.deferredOp.Value, value)\n\t}\n\n\t// TODO(peter): Manually inline DeferredBatchOp.Finish(). Mid-stack inlining\n\t// in go1.13 will remove the need for this.\n\tif b.index != nil {\n\t\tif err := b.index.Add(b.deferredOp.offset); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// Set adds an action to the batch that sets the key to map to the value.\n//\n// It is safe to modify the contents of the arguments after Set returns.\nfunc (b *Batch) Set(key, value []byte, _ *WriteOptions) error {\n\tdeferredOp := b.SetDeferred(len(key), len(value))\n\tcopy(deferredOp.Key, key)\n\tcopy(deferredOp.Value, value)\n\t// TODO(peter): Manually inline DeferredBatchOp.Finish(). Mid-stack inlining\n\t// in go1.13 will remove the need for this.\n\tif b.index != nil {\n\t\tif err := b.index.Add(deferredOp.offset); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// SetDeferred is similar to Set in that it adds a set operation to the batch,\n// except it only takes in key/value lengths instead of complete slices,\n// letting the caller encode into those objects and then call Finish() on the\n// returned object.\nfunc (b *Batch) SetDeferred(keyLen, valueLen int) *DeferredBatchOp {\n\tb.prepareDeferredKeyValueRecord(keyLen, valueLen, InternalKeyKindSet)\n\tb.deferredOp.index = b.index\n\treturn &b.deferredOp\n}\n\n// Merge adds an action to the batch that merges the value at key with the new\n// value. The details of the merge are dependent upon the configured merge\n// operator.\n//\n// It is safe to modify the contents of the arguments after Merge returns.\nfunc (b *Batch) Merge(key, value []byte, _ *WriteOptions) error {\n\tdeferredOp := b.MergeDeferred(len(key), len(value))\n\tcopy(deferredOp.Key, key)\n\tcopy(deferredOp.Value, value)\n\t// TODO(peter): Manually inline DeferredBatchOp.Finish(). Mid-stack inlining\n\t// in go1.13 will remove the need for this.\n\tif b.index != nil {\n\t\tif err := b.index.Add(deferredOp.offset); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// MergeDeferred is similar to Merge in that it adds a merge operation to the\n// batch, except it only takes in key/value lengths instead of complete slices,\n// letting the caller encode into those objects and then call Finish() on the\n// returned object.\nfunc (b *Batch) MergeDeferred(keyLen, valueLen int) *DeferredBatchOp {\n\tb.prepareDeferredKeyValueRecord(keyLen, valueLen, InternalKeyKindMerge)\n\tb.deferredOp.index = b.index\n\treturn &b.deferredOp\n}\n\n// Delete adds an action to the batch that deletes the entry for key.\n//\n// It is safe to modify the contents of the arguments after Delete returns.\nfunc (b *Batch) Delete(key []byte, _ *WriteOptions) error {\n\tdeferredOp := b.DeleteDeferred(len(key))\n\tcopy(deferredOp.Key, key)\n\t// TODO(peter): Manually inline DeferredBatchOp.Finish(). Mid-stack inlining\n\t// in go1.13 will remove the need for this.\n\tif b.index != nil {\n\t\tif err := b.index.Add(deferredOp.offset); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// DeleteDeferred is similar to Delete in that it adds a delete operation to\n// the batch, except it only takes in key/value lengths instead of complete\n// slices, letting the caller encode into those objects and then call Finish()\n// on the returned object.\nfunc (b *Batch) DeleteDeferred(keyLen int) *DeferredBatchOp {\n\tb.prepareDeferredKeyRecord(keyLen, InternalKeyKindDelete)\n\tb.deferredOp.index = b.index\n\treturn &b.deferredOp\n}\n\n// DeleteSized behaves identically to Delete, but takes an additional\n// argument indicating the size of the value being deleted. DeleteSized\n// should be preferred when the caller has the expectation that there exists\n// a single internal KV pair for the key (eg, the key has not been\n// overwritten recently), and the caller knows the size of its value.\n//\n// DeleteSized will record the value size within the tombstone and use it to\n// inform compaction-picking heuristics which strive to reduce space\n// amplification in the LSM. This \"calling your shot\" mechanic allows the\n// storage engine to more accurately estimate and reduce space amplification.\n//\n// It is safe to modify the contents of the arguments after DeleteSized\n// returns.\nfunc (b *Batch) DeleteSized(key []byte, deletedValueSize uint32, _ *WriteOptions) error {\n\tdeferredOp := b.DeleteSizedDeferred(len(key), deletedValueSize)\n\tcopy(b.deferredOp.Key, key)\n\t// TODO(peter): Manually inline DeferredBatchOp.Finish(). Check if in a\n\t// later Go release this is unnecessary.\n\tif b.index != nil {\n\t\tif err := b.index.Add(deferredOp.offset); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// DeleteSizedDeferred is similar to DeleteSized in that it adds a sized delete\n// operation to the batch, except it only takes in key length instead of a\n// complete key slice, letting the caller encode into the DeferredBatchOp.Key\n// slice and then call Finish() on the returned object.\nfunc (b *Batch) DeleteSizedDeferred(keyLen int, deletedValueSize uint32) *DeferredBatchOp {\n\tif b.minimumFormatMajorVersion < FormatDeleteSizedAndObsolete {\n\t\tb.minimumFormatMajorVersion = FormatDeleteSizedAndObsolete\n\t}\n\n\t// Encode the sum of the key length and the value in the value.\n\tv := uint64(deletedValueSize) + uint64(keyLen)\n\n\t// Encode `v` as a varint.\n\tvar buf [binary.MaxVarintLen64]byte\n\tn := 0\n\t{\n\t\tx := v\n\t\tfor x >= 0x80 {\n\t\t\tbuf[n] = byte(x) | 0x80\n\t\t\tx >>= 7\n\t\t\tn++\n\t\t}\n\t\tbuf[n] = byte(x)\n\t\tn++\n\t}\n\n\t// NB: In batch entries and sstable entries, values are stored as\n\t// varstrings. Here, the value is itself a simple varint. This results in an\n\t// unnecessary double layer of encoding:\n\t//     varint(n) varint(deletedValueSize)\n\t// The first varint will always be 1-byte, since a varint-encoded uint64\n\t// will never exceed 128 bytes. This unnecessary extra byte and wrapping is\n\t// preserved to avoid special casing across the database, and in particular\n\t// in sstable block decoding which is performance sensitive.\n\tb.prepareDeferredKeyValueRecord(keyLen, n, InternalKeyKindDeleteSized)\n\tb.deferredOp.index = b.index\n\tcopy(b.deferredOp.Value, buf[:n])\n\treturn &b.deferredOp\n}\n\n// SingleDelete adds an action to the batch that single deletes the entry for key.\n// See Writer.SingleDelete for more details on the semantics of SingleDelete.\n//\n// It is safe to modify the contents of the arguments after SingleDelete returns.\nfunc (b *Batch) SingleDelete(key []byte, _ *WriteOptions) error {\n\tdeferredOp := b.SingleDeleteDeferred(len(key))\n\tcopy(deferredOp.Key, key)\n\t// TODO(peter): Manually inline DeferredBatchOp.Finish(). Mid-stack inlining\n\t// in go1.13 will remove the need for this.\n\tif b.index != nil {\n\t\tif err := b.index.Add(deferredOp.offset); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// SingleDeleteDeferred is similar to SingleDelete in that it adds a single delete\n// operation to the batch, except it only takes in key/value lengths instead of\n// complete slices, letting the caller encode into those objects and then call\n// Finish() on the returned object.\nfunc (b *Batch) SingleDeleteDeferred(keyLen int) *DeferredBatchOp {\n\tb.prepareDeferredKeyRecord(keyLen, InternalKeyKindSingleDelete)\n\tb.deferredOp.index = b.index\n\treturn &b.deferredOp\n}\n\n// DeleteRange deletes all of the point keys (and values) in the range\n// [start,end) (inclusive on start, exclusive on end). DeleteRange does NOT\n// delete overlapping range keys (eg, keys set via RangeKeySet).\n//\n// It is safe to modify the contents of the arguments after DeleteRange\n// returns.\nfunc (b *Batch) DeleteRange(start, end []byte, _ *WriteOptions) error {\n\tdeferredOp := b.DeleteRangeDeferred(len(start), len(end))\n\tcopy(deferredOp.Key, start)\n\tcopy(deferredOp.Value, end)\n\t// TODO(peter): Manually inline DeferredBatchOp.Finish(). Mid-stack inlining\n\t// in go1.13 will remove the need for this.\n\tif deferredOp.index != nil {\n\t\tif err := deferredOp.index.Add(deferredOp.offset); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// DeleteRangeDeferred is similar to DeleteRange in that it adds a delete range\n// operation to the batch, except it only takes in key lengths instead of\n// complete slices, letting the caller encode into those objects and then call\n// Finish() on the returned object. Note that DeferredBatchOp.Key should be\n// populated with the start key, and DeferredBatchOp.Value should be populated\n// with the end key.\nfunc (b *Batch) DeleteRangeDeferred(startLen, endLen int) *DeferredBatchOp {\n\tb.prepareDeferredKeyValueRecord(startLen, endLen, InternalKeyKindRangeDelete)\n\tb.countRangeDels++\n\tif b.index != nil {\n\t\tb.tombstones = nil\n\t\tb.tombstonesSeqNum = 0\n\t\t// Range deletions are rare, so we lazily allocate the index for them.\n\t\tif b.rangeDelIndex == nil {\n\t\t\tb.rangeDelIndex = batchskl.NewSkiplist(&b.data, b.comparer.Compare, b.comparer.AbbreviatedKey)\n\t\t}\n\t\tb.deferredOp.index = b.rangeDelIndex\n\t}\n\treturn &b.deferredOp\n}\n\n// RangeKeySet sets a range key mapping the key range [start, end) at the MVCC\n// timestamp suffix to value. The suffix is optional. If any portion of the key\n// range [start, end) is already set by a range key with the same suffix value,\n// RangeKeySet overrides it.\n//\n// It is safe to modify the contents of the arguments after RangeKeySet returns.\nfunc (b *Batch) RangeKeySet(start, end, suffix, value []byte, _ *WriteOptions) error {\n\tif invariants.Enabled && b.db != nil {\n\t\t// RangeKeySet is only supported on prefix keys.\n\t\tif b.db.opts.Comparer.Split(start) != len(start) {\n\t\t\tpanic(\"RangeKeySet called with suffixed start key\")\n\t\t}\n\t\tif b.db.opts.Comparer.Split(end) != len(end) {\n\t\t\tpanic(\"RangeKeySet called with suffixed end key\")\n\t\t}\n\t}\n\tsuffixValues := [1]rangekey.SuffixValue{{Suffix: suffix, Value: value}}\n\tinternalValueLen := rangekey.EncodedSetValueLen(end, suffixValues[:])\n\n\tdeferredOp := b.rangeKeySetDeferred(len(start), internalValueLen)\n\tcopy(deferredOp.Key, start)\n\tn := rangekey.EncodeSetValue(deferredOp.Value, end, suffixValues[:])\n\tif n != internalValueLen {\n\t\tpanic(\"unexpected internal value length mismatch\")\n\t}\n\n\t// Manually inline DeferredBatchOp.Finish().\n\tif deferredOp.index != nil {\n\t\tif err := deferredOp.index.Add(deferredOp.offset); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (b *Batch) rangeKeySetDeferred(startLen, internalValueLen int) *DeferredBatchOp {\n\tb.prepareDeferredKeyValueRecord(startLen, internalValueLen, InternalKeyKindRangeKeySet)\n\tb.incrementRangeKeysCount()\n\treturn &b.deferredOp\n}\n\nfunc (b *Batch) incrementRangeKeysCount() {\n\tb.countRangeKeys++\n\tif b.index != nil {\n\t\tb.rangeKeys = nil\n\t\tb.rangeKeysSeqNum = 0\n\t\t// Range keys are rare, so we lazily allocate the index for them.\n\t\tif b.rangeKeyIndex == nil {\n\t\t\tb.rangeKeyIndex = batchskl.NewSkiplist(&b.data, b.comparer.Compare, b.comparer.AbbreviatedKey)\n\t\t}\n\t\tb.deferredOp.index = b.rangeKeyIndex\n\t}\n}\n\n// RangeKeyUnset removes a range key mapping the key range [start, end) at the\n// MVCC timestamp suffix. The suffix may be omitted to remove an unsuffixed\n// range key. RangeKeyUnset only removes portions of range keys that fall within\n// the [start, end) key span, and only range keys with suffixes that exactly\n// match the unset suffix.\n//\n// It is safe to modify the contents of the arguments after RangeKeyUnset\n// returns.\nfunc (b *Batch) RangeKeyUnset(start, end, suffix []byte, _ *WriteOptions) error {\n\tif invariants.Enabled && b.db != nil {\n\t\t// RangeKeyUnset is only supported on prefix keys.\n\t\tif b.db.opts.Comparer.Split(start) != len(start) {\n\t\t\tpanic(\"RangeKeyUnset called with suffixed start key\")\n\t\t}\n\t\tif b.db.opts.Comparer.Split(end) != len(end) {\n\t\t\tpanic(\"RangeKeyUnset called with suffixed end key\")\n\t\t}\n\t}\n\tsuffixes := [1][]byte{suffix}\n\tinternalValueLen := rangekey.EncodedUnsetValueLen(end, suffixes[:])\n\n\tdeferredOp := b.rangeKeyUnsetDeferred(len(start), internalValueLen)\n\tcopy(deferredOp.Key, start)\n\tn := rangekey.EncodeUnsetValue(deferredOp.Value, end, suffixes[:])\n\tif n != internalValueLen {\n\t\tpanic(\"unexpected internal value length mismatch\")\n\t}\n\n\t// Manually inline DeferredBatchOp.Finish()\n\tif deferredOp.index != nil {\n\t\tif err := deferredOp.index.Add(deferredOp.offset); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (b *Batch) rangeKeyUnsetDeferred(startLen, internalValueLen int) *DeferredBatchOp {\n\tb.prepareDeferredKeyValueRecord(startLen, internalValueLen, InternalKeyKindRangeKeyUnset)\n\tb.incrementRangeKeysCount()\n\treturn &b.deferredOp\n}\n\n// RangeKeyDelete deletes all of the range keys in the range [start,end)\n// (inclusive on start, exclusive on end). It does not delete point keys (for\n// that use DeleteRange). RangeKeyDelete removes all range keys within the\n// bounds, including those with or without suffixes.\n//\n// It is safe to modify the contents of the arguments after RangeKeyDelete\n// returns.\nfunc (b *Batch) RangeKeyDelete(start, end []byte, _ *WriteOptions) error {\n\tif invariants.Enabled && b.db != nil {\n\t\t// RangeKeyDelete is only supported on prefix keys.\n\t\tif b.db.opts.Comparer.Split(start) != len(start) {\n\t\t\tpanic(\"RangeKeyDelete called with suffixed start key\")\n\t\t}\n\t\tif b.db.opts.Comparer.Split(end) != len(end) {\n\t\t\tpanic(\"RangeKeyDelete called with suffixed end key\")\n\t\t}\n\t}\n\tdeferredOp := b.RangeKeyDeleteDeferred(len(start), len(end))\n\tcopy(deferredOp.Key, start)\n\tcopy(deferredOp.Value, end)\n\t// Manually inline DeferredBatchOp.Finish().\n\tif deferredOp.index != nil {\n\t\tif err := deferredOp.index.Add(deferredOp.offset); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// RangeKeyDeleteDeferred is similar to RangeKeyDelete in that it adds an\n// operation to delete range keys to the batch, except it only takes in key\n// lengths instead of complete slices, letting the caller encode into those\n// objects and then call Finish() on the returned object. Note that\n// DeferredBatchOp.Key should be populated with the start key, and\n// DeferredBatchOp.Value should be populated with the end key.\nfunc (b *Batch) RangeKeyDeleteDeferred(startLen, endLen int) *DeferredBatchOp {\n\tb.prepareDeferredKeyValueRecord(startLen, endLen, InternalKeyKindRangeKeyDelete)\n\tb.incrementRangeKeysCount()\n\treturn &b.deferredOp\n}\n\n// LogData adds the specified to the batch. The data will be written to the\n// WAL, but not added to memtables or sstables. Log data is never indexed,\n// which makes it useful for testing WAL performance.\n//\n// It is safe to modify the contents of the argument after LogData returns.\nfunc (b *Batch) LogData(data []byte, _ *WriteOptions) error {\n\torigCount, origMemTableSize := b.count, b.memTableSize\n\tb.prepareDeferredKeyRecord(len(data), InternalKeyKindLogData)\n\tcopy(b.deferredOp.Key, data)\n\t// Since LogData only writes to the WAL and does not affect the memtable, we\n\t// restore b.count and b.memTableSize to their origin values. Note that\n\t// Batch.count only refers to records that are added to the memtable.\n\tb.count, b.memTableSize = origCount, origMemTableSize\n\treturn nil\n}\n\n// IngestSST adds the FileNum for an sstable to the batch. The data will only be\n// written to the WAL (not added to memtables or sstables).\nfunc (b *Batch) ingestSST(fileNum base.FileNum) {\n\tif b.Empty() {\n\t\tb.ingestedSSTBatch = true\n\t} else if !b.ingestedSSTBatch {\n\t\t// Batch contains other key kinds.\n\t\tpanic(\"pebble: invalid call to ingestSST\")\n\t}\n\n\torigMemTableSize := b.memTableSize\n\tvar buf [binary.MaxVarintLen64]byte\n\tlength := binary.PutUvarint(buf[:], uint64(fileNum))\n\tb.prepareDeferredKeyRecord(length, InternalKeyKindIngestSST)\n\tcopy(b.deferredOp.Key, buf[:length])\n\t// Since IngestSST writes only to the WAL and does not affect the memtable,\n\t// we restore b.memTableSize to its original value. Note that Batch.count\n\t// is not reset because for the InternalKeyKindIngestSST the count is the\n\t// number of sstable paths which have been added to the batch.\n\tb.memTableSize = origMemTableSize\n\tb.minimumFormatMajorVersion = FormatFlushableIngest\n}\n\n// Excise adds the excise span for a flushable ingest containing an excise. The data\n// will only be written to the WAL (not added to memtables or sstables).\nfunc (b *Batch) excise(start, end []byte) {\n\tif b.Empty() {\n\t\tb.ingestedSSTBatch = true\n\t} else if !b.ingestedSSTBatch {\n\t\t// Batch contains other key kinds.\n\t\tpanic(\"pebble: invalid call to excise\")\n\t}\n\n\torigMemTableSize := b.memTableSize\n\tb.prepareDeferredKeyValueRecord(len(start), len(end), InternalKeyKindExcise)\n\tcopy(b.deferredOp.Key, start)\n\tcopy(b.deferredOp.Value, end)\n\t// Since excise writes only to the WAL and does not affect the memtable,\n\t// we restore b.memTableSize to its original value. Note that Batch.count\n\t// is not reset because for the InternalKeyKindIngestSST/Excise the count\n\t// is the number of sstable paths which have been added to the batch.\n\tb.memTableSize = origMemTableSize\n\tb.minimumFormatMajorVersion = FormatFlushableIngestExcises\n}\n\n// Empty returns true if the batch is empty, and false otherwise.\nfunc (b *Batch) Empty() bool {\n\treturn batchrepr.IsEmpty(b.data)\n}\n\n// Len returns the current size of the batch in bytes.\nfunc (b *Batch) Len() int {\n\treturn max(batchrepr.HeaderLen, len(b.data))\n}\n\n// Repr returns the underlying batch representation. It is not safe to modify\n// the contents. Reset() will not change the contents of the returned value,\n// though any other mutation operation may do so.\nfunc (b *Batch) Repr() []byte {\n\tif len(b.data) == 0 {\n\t\tb.init(batchrepr.HeaderLen)\n\t}\n\tbatchrepr.SetCount(b.data, b.Count())\n\treturn b.data\n}\n\n// SetRepr sets the underlying batch representation. The batch takes ownership\n// of the supplied slice. It is not safe to modify it afterwards until the\n// Batch is no longer in use.\n//\n// SetRepr may return ErrInvalidBatch if the supplied slice fails to decode in\n// any way. It will not return an error in any other circumstance.\nfunc (b *Batch) SetRepr(data []byte) error {\n\th, ok := batchrepr.ReadHeader(data)\n\tif !ok {\n\t\treturn ErrInvalidBatch\n\t}\n\tb.data = data\n\tb.count = uint64(h.Count)\n\tvar err error\n\tif b.db != nil {\n\t\t// Only track memTableSize for batches that will be committed to the DB.\n\t\terr = b.refreshMemTableSize()\n\t}\n\treturn err\n}\n\n// NewIter returns an iterator that is unpositioned (Iterator.Valid() will\n// return false). The iterator can be positioned via a call to SeekGE,\n// SeekPrefixGE, SeekLT, First or Last. Only indexed batches support iterators.\n//\n// The returned Iterator observes all of the Batch's existing mutations, but no\n// later mutations. Its view can be refreshed via RefreshBatchSnapshot or\n// SetOptions().\nfunc (b *Batch) NewIter(o *IterOptions) (*Iterator, error) {\n\treturn b.NewIterWithContext(context.Background(), o)\n}\n\n// NewIterWithContext is like NewIter, and additionally accepts a context for\n// tracing.\nfunc (b *Batch) NewIterWithContext(ctx context.Context, o *IterOptions) (*Iterator, error) {\n\tif b.index == nil {\n\t\treturn nil, ErrNotIndexed\n\t}\n\treturn b.db.newIter(ctx, b, newIterOpts{}, o), nil\n}\n\n// NewBatchOnlyIter constructs an iterator that only reads the contents of the\n// batch, and does not overlay the batch mutations on top of the DB state.\n//\n// The returned Iterator observes all of the Batch's existing mutations, but\n// no later mutations. Its view can be refreshed via RefreshBatchSnapshot or\n// SetOptions().\nfunc (b *Batch) NewBatchOnlyIter(ctx context.Context, o *IterOptions) (*Iterator, error) {\n\tif b.index == nil {\n\t\treturn nil, ErrNotIndexed\n\t}\n\treturn b.db.newIter(ctx, b, newIterOpts{batch: batchIterOpts{batchOnly: true}}, o), nil\n}\n\n// newInternalIter creates a new internalIterator that iterates over the\n// contents of the batch.\nfunc (b *Batch) newInternalIter(o *IterOptions) *batchIter {\n\titer := &batchIter{}\n\tb.initInternalIter(o, iter)\n\treturn iter\n}\n\nfunc (b *Batch) initInternalIter(o *IterOptions, iter *batchIter) {\n\t*iter = batchIter{\n\t\tbatch: b,\n\t\titer:  b.index.NewIter(o.GetLowerBound(), o.GetUpperBound()),\n\t\t// NB: We explicitly do not propagate the batch snapshot to the point\n\t\t// key iterator. Filtering point keys within the batch iterator can\n\t\t// cause pathological behavior where a batch iterator advances\n\t\t// significantly farther than necessary filtering many batch keys that\n\t\t// are not visible at the batch sequence number. Instead, the merging\n\t\t// iterator enforces bounds.\n\t\t//\n\t\t// For example, consider an engine that contains the committed keys\n\t\t// 'bar' and 'bax', with no keys between them. Consider a batch\n\t\t// containing keys 1,000 keys within the range [a,z]. All of the\n\t\t// batch keys were added to the batch after the iterator was\n\t\t// constructed, so they are not visible to the iterator. A call to\n\t\t// SeekGE('bax') would seek the LSM iterators and discover the key\n\t\t// 'bax'. It would also seek the batch iterator, landing on the key\n\t\t// 'baz' but discover it that it's not visible. The batch iterator would\n\t\t// next through the rest of the batch's keys, only to discover there are\n\t\t// no visible keys greater than or equal to 'bax'.\n\t\t//\n\t\t// Filtering these batch points within the merging iterator ensures that\n\t\t// the batch iterator never needs to iterate beyond 'baz', because it\n\t\t// already found a smaller, visible key 'bax'.\n\t\tsnapshot: base.SeqNumMax,\n\t}\n}\n\nfunc (b *Batch) newRangeDelIter(o *IterOptions, batchSnapshot base.SeqNum) *keyspan.Iter {\n\t// Construct an iterator even if rangeDelIndex is nil, because it is allowed\n\t// to refresh later, so we need the container to exist.\n\titer := new(keyspan.Iter)\n\tb.initRangeDelIter(o, iter, batchSnapshot)\n\treturn iter\n}\n\nfunc (b *Batch) initRangeDelIter(_ *IterOptions, iter *keyspan.Iter, batchSnapshot base.SeqNum) {\n\tif b.rangeDelIndex == nil {\n\t\titer.Init(b.comparer.Compare, nil)\n\t\treturn\n\t}\n\n\t// Fragment the range tombstones the first time a range deletion iterator is\n\t// requested. The cached tombstones are invalidated if another range\n\t// deletion tombstone is added to the batch. This cache is only guaranteed\n\t// to be correct if we're opening an iterator to read at a batch sequence\n\t// number at least as high as tombstonesSeqNum. The cache is guaranteed to\n\t// include all tombstones up to tombstonesSeqNum, and if any additional\n\t// tombstones were added after that sequence number the cache would've been\n\t// cleared.\n\tnextSeqNum := b.nextSeqNum()\n\tif b.tombstones != nil && b.tombstonesSeqNum <= batchSnapshot {\n\t\titer.Init(b.comparer.Compare, b.tombstones)\n\t\treturn\n\t}\n\n\ttombstones := make([]keyspan.Span, 0, b.countRangeDels)\n\tfrag := &keyspan.Fragmenter{\n\t\tCmp:    b.comparer.Compare,\n\t\tFormat: b.comparer.FormatKey,\n\t\tEmit: func(s keyspan.Span) {\n\t\t\ttombstones = append(tombstones, s)\n\t\t},\n\t}\n\tit := &batchIter{\n\t\tbatch:    b,\n\t\titer:     b.rangeDelIndex.NewIter(nil, nil),\n\t\tsnapshot: batchSnapshot,\n\t}\n\tfragmentRangeDels(frag, it, int(b.countRangeDels))\n\titer.Init(b.comparer.Compare, tombstones)\n\n\t// If we just read all the tombstones in the batch (eg, batchSnapshot was\n\t// set to b.nextSeqNum()), then cache the tombstones so that a subsequent\n\t// call to initRangeDelIter may use them without refragmenting.\n\tif nextSeqNum == batchSnapshot {\n\t\tb.tombstones = tombstones\n\t\tb.tombstonesSeqNum = nextSeqNum\n\t}\n}\n\nfunc fragmentRangeDels(frag *keyspan.Fragmenter, it internalIterator, count int) {\n\t// The memory management here is a bit subtle. The keys and values returned\n\t// by the iterator are slices in Batch.data. Thus the fragmented tombstones\n\t// are slices within Batch.data. If additional entries are added to the\n\t// Batch, Batch.data may be reallocated. The references in the fragmented\n\t// tombstones will remain valid, pointing into the old Batch.data. GC for\n\t// the win.\n\n\t// Use a single []keyspan.Key buffer to avoid allocating many\n\t// individual []keyspan.Key slices with a single element each.\n\tkeyBuf := make([]keyspan.Key, 0, count)\n\tfor kv := it.First(); kv != nil; kv = it.Next() {\n\t\ts := rangedel.Decode(kv.K, kv.InPlaceValue(), keyBuf)\n\t\tkeyBuf = s.Keys[len(s.Keys):]\n\n\t\t// Set a fixed capacity to avoid accidental overwriting.\n\t\ts.Keys = s.Keys[:len(s.Keys):len(s.Keys)]\n\t\tfrag.Add(s)\n\t}\n\tfrag.Finish()\n}\n\nfunc (b *Batch) newRangeKeyIter(o *IterOptions, batchSnapshot base.SeqNum) *keyspan.Iter {\n\t// Construct an iterator even if rangeKeyIndex is nil, because it is allowed\n\t// to refresh later, so we need the container to exist.\n\titer := new(keyspan.Iter)\n\tb.initRangeKeyIter(o, iter, batchSnapshot)\n\treturn iter\n}\n\nfunc (b *Batch) initRangeKeyIter(_ *IterOptions, iter *keyspan.Iter, batchSnapshot base.SeqNum) {\n\tif b.rangeKeyIndex == nil {\n\t\titer.Init(b.comparer.Compare, nil)\n\t\treturn\n\t}\n\n\t// Fragment the range keys the first time a range key iterator is requested.\n\t// The cached spans are invalidated if another range key is added to the\n\t// batch. This cache is only guaranteed to be correct if we're opening an\n\t// iterator to read at a batch sequence number at least as high as\n\t// rangeKeysSeqNum. The cache is guaranteed to include all range keys up to\n\t// rangeKeysSeqNum, and if any additional range keys were added after that\n\t// sequence number the cache would've been cleared.\n\tnextSeqNum := b.nextSeqNum()\n\tif b.rangeKeys != nil && b.rangeKeysSeqNum <= batchSnapshot {\n\t\titer.Init(b.comparer.Compare, b.rangeKeys)\n\t\treturn\n\t}\n\n\trangeKeys := make([]keyspan.Span, 0, b.countRangeKeys)\n\tfrag := &keyspan.Fragmenter{\n\t\tCmp:    b.comparer.Compare,\n\t\tFormat: b.comparer.FormatKey,\n\t\tEmit: func(s keyspan.Span) {\n\t\t\trangeKeys = append(rangeKeys, s)\n\t\t},\n\t}\n\tit := &batchIter{\n\t\tbatch:    b,\n\t\titer:     b.rangeKeyIndex.NewIter(nil, nil),\n\t\tsnapshot: batchSnapshot,\n\t}\n\tfragmentRangeKeys(frag, it, int(b.countRangeKeys))\n\titer.Init(b.comparer.Compare, rangeKeys)\n\n\t// If we just read all the range keys in the batch (eg, batchSnapshot was\n\t// set to b.nextSeqNum()), then cache the range keys so that a subsequent\n\t// call to initRangeKeyIter may use them without refragmenting.\n\tif nextSeqNum == batchSnapshot {\n\t\tb.rangeKeys = rangeKeys\n\t\tb.rangeKeysSeqNum = nextSeqNum\n\t}\n}\n\nfunc fragmentRangeKeys(frag *keyspan.Fragmenter, it internalIterator, count int) error {\n\t// The memory management here is a bit subtle. The keys and values\n\t// returned by the iterator are slices in Batch.data. Thus the\n\t// fragmented key spans are slices within Batch.data. If additional\n\t// entries are added to the Batch, Batch.data may be reallocated. The\n\t// references in the fragmented keys will remain valid, pointing into\n\t// the old Batch.data. GC for the win.\n\n\t// Use a single []keyspan.Key buffer to avoid allocating many\n\t// individual []keyspan.Key slices with a single element each.\n\tkeyBuf := make([]keyspan.Key, 0, count)\n\tfor kv := it.First(); kv != nil; kv = it.Next() {\n\t\ts, err := rangekey.Decode(kv.K, kv.InPlaceValue(), keyBuf)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tkeyBuf = s.Keys[len(s.Keys):]\n\n\t\t// Set a fixed capacity to avoid accidental overwriting.\n\t\ts.Keys = s.Keys[:len(s.Keys):len(s.Keys)]\n\t\tfrag.Add(s)\n\t}\n\tfrag.Finish()\n\treturn nil\n}\n\n// Commit applies the batch to its parent writer.\nfunc (b *Batch) Commit(o *WriteOptions) error {\n\treturn b.db.Apply(b, o)\n}\n\n// Close closes the batch without committing it.\nfunc (b *Batch) Close() error {\n\t// The storage engine commit pipeline may retain a pointer to b.data beyond\n\t// when Commit() returns. This is possible when configured for WAL failover;\n\t// we don't know if we might need to read the batch data again until the\n\t// batch has been durably synced [even if the committer doesn't care to wait\n\t// for the sync and Sync()=false].\n\t//\n\t// We still want to recycle these batches. The b.lifecycle atomic negotiates\n\t// the batch's lifecycle. If the commit pipeline still might read b.data,\n\t// b.lifecycle will be nonzeroed [the low bits hold a ref count].\n\tfor {\n\t\tv := b.lifecycle.Load()\n\t\tswitch {\n\t\tcase v == 0:\n\t\t\t// A zero value indicates that the commit pipeline has no\n\t\t\t// outstanding references to the batch. The commit pipeline is\n\t\t\t// required to acquire a ref synchronously, so there is no risk that\n\t\t\t// the commit pipeline will grab a ref after the call to release. We\n\t\t\t// can simply release the batch.\n\t\t\tb.release()\n\t\t\treturn nil\n\t\tcase (v & batchClosedBit) != 0:\n\t\t\t// The batch has a batchClosedBit: This batch has already been closed.\n\t\t\treturn ErrClosed\n\t\tdefault:\n\t\t\t// There's an outstanding reference. Set the batch released bit so\n\t\t\t// that the commit pipeline knows it should release the batch when\n\t\t\t// it unrefs.\n\t\t\tif b.lifecycle.CompareAndSwap(v, v|batchClosedBit) {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\t// CAS Failed—this indicates the outstanding reference just\n\t\t\t// decremented (or the caller illegally closed the batch twice).\n\t\t\t// Loop to reload.\n\t\t}\n\t}\n}\n\n// Indexed returns true if the batch is indexed (i.e. supports read\n// operations).\nfunc (b *Batch) Indexed() bool {\n\treturn b.index != nil\n}\n\n// init ensures that the batch data slice is initialized to meet the\n// minimum required size and allocates space for the batch header.\nfunc (b *Batch) init(size int) {\n\tb.opts.ensureDefaults()\n\tn := b.opts.initialSizeBytes\n\tfor n < size {\n\t\tn *= 2\n\t}\n\tif cap(b.data) < n {\n\t\tb.data = rawalloc.New(batchrepr.HeaderLen, n)\n\t}\n\tb.data = b.data[:batchrepr.HeaderLen]\n\tclear(b.data) // Zero the sequence number in the header\n}\n\n// Reset resets the batch for reuse. The underlying byte slice (that is\n// returned by Repr()) may not be modified. It is only necessary to call this\n// method if a batch is explicitly being reused. Close automatically takes are\n// of releasing resources when appropriate for batches that are internally\n// being reused.\nfunc (b *Batch) Reset() {\n\t// In some configurations (WAL failover) the commit pipeline may retain\n\t// b.data beyond a call to commit the batch. When this happens, b.lifecycle\n\t// is nonzero (see the comment above b.lifecycle). In this case it's unsafe\n\t// to mutate b.data, so we discard it. Note that Reset must not be called on\n\t// a closed batch, so v > 0 implies a non-zero ref count and not\n\t// batchClosedBit being set.\n\tif v := b.lifecycle.Load(); v > 0 {\n\t\tb.data = nil\n\t}\n\tb.reset()\n}\n\nfunc (b *Batch) reset() {\n\t// Zero out the struct, retaining only the fields necessary for manual\n\t// reuse.\n\tb.batchInternal = batchInternal{\n\t\tdata:     b.data,\n\t\tcomparer: b.comparer,\n\t\topts:     b.opts,\n\t\tindex:    b.index,\n\t\tdb:       b.db,\n\t}\n\tb.applied.Store(false)\n\tif b.data != nil {\n\t\tif cap(b.data) > b.opts.maxRetainedSizeBytes {\n\t\t\t// If the capacity of the buffer is larger than our maximum\n\t\t\t// retention size, don't re-use it. Let it be GC-ed instead.\n\t\t\t// This prevents the memory from an unusually large batch from\n\t\t\t// being held on to indefinitely.\n\t\t\tb.data = nil\n\t\t} else {\n\t\t\t// Otherwise, reset the buffer for re-use.\n\t\t\tb.data = b.data[:batchrepr.HeaderLen]\n\t\t\tclear(b.data)\n\t\t}\n\t}\n\tif b.index != nil {\n\t\tb.index.Init(&b.data, b.comparer.Compare, b.comparer.AbbreviatedKey)\n\t}\n}\n\nfunc (b *Batch) grow(n int) {\n\tnewSize := len(b.data) + n\n\tif uint64(newSize) >= maxBatchSize {\n\t\tpanic(ErrBatchTooLarge)\n\t}\n\tif newSize > cap(b.data) {\n\t\tnewCap := 2 * cap(b.data)\n\t\tfor newCap < newSize {\n\t\t\tnewCap *= 2\n\t\t}\n\t\tnewData := rawalloc.New(len(b.data), newCap)\n\t\tcopy(newData, b.data)\n\t\tb.data = newData\n\t}\n\tb.data = b.data[:newSize]\n}\n\nfunc (b *Batch) setSeqNum(seqNum base.SeqNum) {\n\tbatchrepr.SetSeqNum(b.data, seqNum)\n}\n\n// SeqNum returns the batch sequence number which is applied to the first\n// record in the batch. The sequence number is incremented for each subsequent\n// record. It returns zero if the batch is empty.\nfunc (b *Batch) SeqNum() base.SeqNum {\n\tif len(b.data) == 0 {\n\t\tb.init(batchrepr.HeaderLen)\n\t}\n\treturn batchrepr.ReadSeqNum(b.data)\n}\n\nfunc (b *Batch) setCount(v uint32) {\n\tb.count = uint64(v)\n}\n\n// Count returns the count of memtable-modifying operations in this batch. All\n// operations with the except of LogData increment this count. For IngestSSTs,\n// count is only used to indicate the number of SSTs ingested in the record, the\n// batch isn't applied to the memtable.\nfunc (b *Batch) Count() uint32 {\n\tif b.count > math.MaxUint32 {\n\t\tpanic(batchrepr.ErrInvalidBatch)\n\t}\n\treturn uint32(b.count)\n}\n\n// Reader returns a batchrepr.Reader for the current batch contents. If the\n// batch is mutated, the new entries will not be visible to the reader.\nfunc (b *Batch) Reader() batchrepr.Reader {\n\tif len(b.data) == 0 {\n\t\tb.init(batchrepr.HeaderLen)\n\t}\n\treturn batchrepr.Read(b.data)\n}\n\n// SyncWait is to be used in conjunction with DB.ApplyNoSyncWait.\nfunc (b *Batch) SyncWait() error {\n\tnow := crtime.NowMono()\n\tb.fsyncWait.Wait()\n\tif b.commitErr != nil {\n\t\tb.db = nil // prevent batch reuse on error\n\t}\n\twaitDuration := now.Elapsed()\n\tb.commitStats.CommitWaitDuration += waitDuration\n\tb.commitStats.TotalDuration += waitDuration\n\treturn b.commitErr\n}\n\n// CommitStats returns stats related to committing the batch. Should be called\n// after Batch.Commit, DB.Apply. If DB.ApplyNoSyncWait is used, should be\n// called after Batch.SyncWait.\nfunc (b *Batch) CommitStats() BatchCommitStats {\n\treturn b.commitStats\n}\n\n// Note: batchIter mirrors the implementation of flushableBatchIter. Keep the\n// two in sync.\ntype batchIter struct {\n\tbatch *Batch\n\titer  batchskl.Iterator\n\tkv    base.InternalKV\n\terr   error\n\t// snapshot holds a batch \"sequence number\" at which the batch is being\n\t// read. This sequence number has the InternalKeySeqNumBatch bit set, so it\n\t// encodes an offset within the batch. Only batch entries earlier than the\n\t// offset are visible during iteration.\n\tsnapshot base.SeqNum\n}\n\n// batchIter implements the base.InternalIterator interface.\nvar _ base.InternalIterator = (*batchIter)(nil)\n\nfunc (i *batchIter) String() string {\n\treturn \"batch\"\n}\n\nfunc (i *batchIter) SeekGE(key []byte, flags base.SeekGEFlags) *base.InternalKV {\n\t// Ignore TrySeekUsingNext if the view of the batch changed.\n\tif flags.TrySeekUsingNext() && flags.BatchJustRefreshed() {\n\t\tflags = flags.DisableTrySeekUsingNext()\n\t}\n\n\ti.err = nil // clear cached iteration error\n\tikey := i.iter.SeekGE(key, flags)\n\tfor ikey != nil && ikey.SeqNum() >= i.snapshot {\n\t\tikey = i.iter.Next()\n\t}\n\tif ikey == nil {\n\t\ti.kv = base.InternalKV{}\n\t\treturn nil\n\t}\n\ti.kv.K = *ikey\n\ti.kv.V = base.MakeInPlaceValue(i.value())\n\treturn &i.kv\n}\n\nfunc (i *batchIter) SeekPrefixGE(prefix, key []byte, flags base.SeekGEFlags) *base.InternalKV {\n\tkv := i.SeekGE(key, flags)\n\tif kv == nil {\n\t\treturn nil\n\t}\n\t// If the key doesn't have the sought prefix, return nil.\n\tif !bytes.Equal(i.batch.comparer.Split.Prefix(kv.K.UserKey), prefix) {\n\t\ti.kv = base.InternalKV{}\n\t\treturn nil\n\t}\n\treturn kv\n}\n\nfunc (i *batchIter) SeekLT(key []byte, flags base.SeekLTFlags) *base.InternalKV {\n\ti.err = nil // clear cached iteration error\n\tikey := i.iter.SeekLT(key)\n\tfor ikey != nil && ikey.SeqNum() >= i.snapshot {\n\t\tikey = i.iter.Prev()\n\t}\n\tif ikey == nil {\n\t\ti.kv = base.InternalKV{}\n\t\treturn nil\n\t}\n\ti.kv.K = *ikey\n\ti.kv.V = base.MakeInPlaceValue(i.value())\n\treturn &i.kv\n}\n\nfunc (i *batchIter) First() *base.InternalKV {\n\ti.err = nil // clear cached iteration error\n\tikey := i.iter.First()\n\tfor ikey != nil && ikey.SeqNum() >= i.snapshot {\n\t\tikey = i.iter.Next()\n\t}\n\tif ikey == nil {\n\t\ti.kv = base.InternalKV{}\n\t\treturn nil\n\t}\n\ti.kv.K = *ikey\n\ti.kv.V = base.MakeInPlaceValue(i.value())\n\treturn &i.kv\n}\n\nfunc (i *batchIter) Last() *base.InternalKV {\n\ti.err = nil // clear cached iteration error\n\tikey := i.iter.Last()\n\tfor ikey != nil && ikey.SeqNum() >= i.snapshot {\n\t\tikey = i.iter.Prev()\n\t}\n\tif ikey == nil {\n\t\ti.kv = base.InternalKV{}\n\t\treturn nil\n\t}\n\ti.kv.K = *ikey\n\ti.kv.V = base.MakeInPlaceValue(i.value())\n\treturn &i.kv\n}\n\nfunc (i *batchIter) Next() *base.InternalKV {\n\tikey := i.iter.Next()\n\tfor ikey != nil && ikey.SeqNum() >= i.snapshot {\n\t\tikey = i.iter.Next()\n\t}\n\tif ikey == nil {\n\t\ti.kv = base.InternalKV{}\n\t\treturn nil\n\t}\n\ti.kv.K = *ikey\n\ti.kv.V = base.MakeInPlaceValue(i.value())\n\treturn &i.kv\n}\n\nfunc (i *batchIter) NextPrefix(succKey []byte) *base.InternalKV {\n\t// Because NextPrefix was invoked `succKey` must be ≥ the key at i's current\n\t// position. Seek the arena iterator using TrySeekUsingNext.\n\tikey := i.iter.SeekGE(succKey, base.SeekGEFlagsNone.EnableTrySeekUsingNext())\n\tfor ikey != nil && ikey.SeqNum() >= i.snapshot {\n\t\tikey = i.iter.Next()\n\t}\n\tif ikey == nil {\n\t\ti.kv = base.InternalKV{}\n\t\treturn nil\n\t}\n\ti.kv.K = *ikey\n\ti.kv.V = base.MakeInPlaceValue(i.value())\n\treturn &i.kv\n}\n\nfunc (i *batchIter) Prev() *base.InternalKV {\n\tikey := i.iter.Prev()\n\tfor ikey != nil && ikey.SeqNum() >= i.snapshot {\n\t\tikey = i.iter.Prev()\n\t}\n\tif ikey == nil {\n\t\ti.kv = base.InternalKV{}\n\t\treturn nil\n\t}\n\ti.kv.K = *ikey\n\ti.kv.V = base.MakeInPlaceValue(i.value())\n\treturn &i.kv\n}\n\nfunc (i *batchIter) value() []byte {\n\toffset, _, keyEnd := i.iter.KeyInfo()\n\tdata := i.batch.data\n\tif len(data[offset:]) == 0 {\n\t\ti.err = base.CorruptionErrorf(\"corrupted batch\")\n\t\treturn nil\n\t}\n\n\tswitch InternalKeyKind(data[offset]) {\n\tcase InternalKeyKindSet, InternalKeyKindMerge, InternalKeyKindRangeDelete,\n\t\tInternalKeyKindRangeKeySet, InternalKeyKindRangeKeyUnset, InternalKeyKindRangeKeyDelete,\n\t\tInternalKeyKindDeleteSized:\n\t\t_, value, ok := batchrepr.DecodeStr(data[keyEnd:])\n\t\tif !ok {\n\t\t\treturn nil\n\t\t}\n\t\treturn value\n\tdefault:\n\t\treturn nil\n\t}\n}\n\nfunc (i *batchIter) Error() error {\n\treturn i.err\n}\n\nfunc (i *batchIter) Close() error {\n\t_ = i.iter.Close()\n\treturn i.err\n}\n\nfunc (i *batchIter) SetBounds(lower, upper []byte) {\n\ti.iter.SetBounds(lower, upper)\n}\n\nfunc (i *batchIter) SetContext(_ context.Context) {}\n\n// DebugTree is part of the InternalIterator interface.\nfunc (i *batchIter) DebugTree(tp treeprinter.Node) {\n\ttp.Childf(\"%T(%p)\", i, i)\n}\n\ntype flushableBatchEntry struct {\n\t// offset is the byte offset of the record within the batch repr.\n\toffset uint32\n\t// index is the 0-based ordinal number of the record within the batch. Used\n\t// to compute the seqnum for the record.\n\tindex uint32\n\t// key{Start,End} are the start and end byte offsets of the key within the\n\t// batch repr. Cached to avoid decoding the key length on every\n\t// comparison. The value is stored starting at keyEnd.\n\tkeyStart uint32\n\tkeyEnd   uint32\n}\n\n// flushableBatch wraps an existing batch and provides the interfaces needed\n// for making the batch flushable (i.e. able to mimic a memtable).\ntype flushableBatch struct {\n\tcmp      Compare\n\tcomparer *base.Comparer\n\tdata     []byte\n\n\t// The base sequence number for the entries in the batch. This is the same\n\t// value as Batch.seqNum() and is cached here for performance.\n\tseqNum base.SeqNum\n\n\t// A slice of offsets and indices for the entries in the batch. Used to\n\t// implement flushableBatchIter. Unlike the indexing on a normal batch, a\n\t// flushable batch is indexed such that batch entry i will be given the\n\t// sequence number flushableBatch.seqNum+i.\n\t//\n\t// Sorted in increasing order of key and decreasing order of offset (since\n\t// higher offsets correspond to higher sequence numbers).\n\t//\n\t// Does not include range deletion entries or range key entries.\n\toffsets []flushableBatchEntry\n\n\t// Fragmented range deletion tombstones.\n\ttombstones []keyspan.Span\n\n\t// Fragmented range keys.\n\trangeKeys []keyspan.Span\n}\n\nvar _ flushable = (*flushableBatch)(nil)\n\n// newFlushableBatch creates a new batch that implements the flushable\n// interface. This allows the batch to act like a memtable and be placed in the\n// queue of flushable memtables. Note that the flushable batch takes ownership\n// of the batch data.\nfunc newFlushableBatch(batch *Batch, comparer *Comparer) (*flushableBatch, error) {\n\tb := &flushableBatch{\n\t\tdata:     batch.data,\n\t\tcmp:      comparer.Compare,\n\t\tcomparer: comparer,\n\t\toffsets:  make([]flushableBatchEntry, 0, batch.Count()),\n\t}\n\tif b.data != nil {\n\t\t// Note that this sequence number is not correct when this batch has not\n\t\t// been applied since the sequence number has not been assigned yet. The\n\t\t// correct sequence number will be set later. But it is correct when the\n\t\t// batch is being replayed from the WAL.\n\t\tb.seqNum = batch.SeqNum()\n\t}\n\tvar rangeDelOffsets []flushableBatchEntry\n\tvar rangeKeyOffsets []flushableBatchEntry\n\tif len(b.data) > batchrepr.HeaderLen {\n\t\t// Non-empty batch.\n\t\tvar index uint32\n\t\tfor iter := batchrepr.Read(b.data); len(iter) > 0; {\n\t\t\toffset := uintptr(unsafe.Pointer(&iter[0])) - uintptr(unsafe.Pointer(&b.data[0]))\n\t\t\tkind, key, _, ok, err := iter.Next()\n\t\t\tif !ok {\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tentry := flushableBatchEntry{\n\t\t\t\toffset: uint32(offset),\n\t\t\t\tindex:  uint32(index),\n\t\t\t}\n\t\t\tif keySize := uint32(len(key)); keySize == 0 {\n\t\t\t\t// Must add 2 to the offset. One byte encodes `kind` and the next\n\t\t\t\t// byte encodes `0`, which is the length of the key.\n\t\t\t\tentry.keyStart = uint32(offset) + 2\n\t\t\t\tentry.keyEnd = entry.keyStart\n\t\t\t} else {\n\t\t\t\tentry.keyStart = uint32(uintptr(unsafe.Pointer(&key[0])) -\n\t\t\t\t\tuintptr(unsafe.Pointer(&b.data[0])))\n\t\t\t\tentry.keyEnd = entry.keyStart + keySize\n\t\t\t}\n\t\t\tswitch kind {\n\t\t\tcase InternalKeyKindRangeDelete:\n\t\t\t\trangeDelOffsets = append(rangeDelOffsets, entry)\n\t\t\tcase InternalKeyKindRangeKeySet, InternalKeyKindRangeKeyUnset, InternalKeyKindRangeKeyDelete:\n\t\t\t\trangeKeyOffsets = append(rangeKeyOffsets, entry)\n\t\t\tcase InternalKeyKindLogData:\n\t\t\t\t// Skip it; we never want to iterate over LogDatas.\n\t\t\t\tcontinue\n\t\t\tcase InternalKeyKindSet, InternalKeyKindDelete, InternalKeyKindMerge,\n\t\t\t\tInternalKeyKindSingleDelete, InternalKeyKindSetWithDelete, InternalKeyKindDeleteSized:\n\t\t\t\tb.offsets = append(b.offsets, entry)\n\t\t\tdefault:\n\t\t\t\t// Note In some circumstances this might be temporary memory\n\t\t\t\t// corruption that can be recovered by discarding the batch and\n\t\t\t\t// trying again. In other cases, the batch repr might've been\n\t\t\t\t// already persisted elsewhere, and we'll loop continuously trying\n\t\t\t\t// to commit the same corrupted batch. The caller is responsible for\n\t\t\t\t// distinguishing.\n\t\t\t\treturn nil, errors.Wrapf(ErrInvalidBatch, \"unrecognized kind %v\", kind)\n\t\t\t}\n\t\t\t// NB: index (used for entry.offset above) must not reach the\n\t\t\t// batch.count, because the offset is used in conjunction with the\n\t\t\t// batch's sequence number to assign sequence numbers to keys within\n\t\t\t// the batch. If we assign KV's indexes as high as batch.count,\n\t\t\t// we'll begin assigning keys sequence numbers that weren't\n\t\t\t// allocated.\n\t\t\tif index >= uint32(batch.count) {\n\t\t\t\treturn nil, base.AssertionFailedf(\"pebble: batch entry index %d ≥ batch.count %d\", index, batch.count)\n\t\t\t}\n\t\t\tindex++\n\t\t}\n\t}\n\n\t// Sort all of offsets, rangeDelOffsets and rangeKeyOffsets, using *batch's\n\t// sort.Interface implementation.\n\tpointOffsets := b.offsets\n\tsort.Sort(b)\n\tb.offsets = rangeDelOffsets\n\tsort.Sort(b)\n\tb.offsets = rangeKeyOffsets\n\tsort.Sort(b)\n\tb.offsets = pointOffsets\n\n\tif len(rangeDelOffsets) > 0 {\n\t\tfrag := &keyspan.Fragmenter{\n\t\t\tCmp:    b.cmp,\n\t\t\tFormat: b.comparer.FormatKey,\n\t\t\tEmit: func(s keyspan.Span) {\n\t\t\t\tb.tombstones = append(b.tombstones, s)\n\t\t\t},\n\t\t}\n\t\tit := &flushableBatchIter{\n\t\t\tbatch:   b,\n\t\t\tdata:    b.data,\n\t\t\toffsets: rangeDelOffsets,\n\t\t\tcmp:     b.cmp,\n\t\t\tindex:   -1,\n\t\t}\n\t\tfragmentRangeDels(frag, it, len(rangeDelOffsets))\n\t}\n\tif len(rangeKeyOffsets) > 0 {\n\t\tfrag := &keyspan.Fragmenter{\n\t\t\tCmp:    b.cmp,\n\t\t\tFormat: b.comparer.FormatKey,\n\t\t\tEmit: func(s keyspan.Span) {\n\t\t\t\tb.rangeKeys = append(b.rangeKeys, s)\n\t\t\t},\n\t\t}\n\t\tit := &flushableBatchIter{\n\t\t\tbatch:   b,\n\t\t\tdata:    b.data,\n\t\t\toffsets: rangeKeyOffsets,\n\t\t\tcmp:     b.cmp,\n\t\t\tindex:   -1,\n\t\t}\n\t\tfragmentRangeKeys(frag, it, len(rangeKeyOffsets))\n\t}\n\treturn b, nil\n}\n\nfunc (b *flushableBatch) setSeqNum(seqNum base.SeqNum) {\n\tif b.seqNum != 0 {\n\t\tpanic(fmt.Sprintf(\"pebble: flushableBatch.seqNum already set: %d\", b.seqNum))\n\t}\n\tb.seqNum = seqNum\n\tfor i := range b.tombstones {\n\t\tfor j := range b.tombstones[i].Keys {\n\t\t\tb.tombstones[i].Keys[j].Trailer = base.MakeTrailer(\n\t\t\t\tb.tombstones[i].Keys[j].SeqNum()+seqNum,\n\t\t\t\tb.tombstones[i].Keys[j].Kind(),\n\t\t\t)\n\t\t}\n\t}\n\tfor i := range b.rangeKeys {\n\t\tfor j := range b.rangeKeys[i].Keys {\n\t\t\tb.rangeKeys[i].Keys[j].Trailer = base.MakeTrailer(\n\t\t\t\tb.rangeKeys[i].Keys[j].SeqNum()+seqNum,\n\t\t\t\tb.rangeKeys[i].Keys[j].Kind(),\n\t\t\t)\n\t\t}\n\t}\n}\n\nfunc (b *flushableBatch) Len() int {\n\treturn len(b.offsets)\n}\n\nfunc (b *flushableBatch) Less(i, j int) bool {\n\tei := &b.offsets[i]\n\tej := &b.offsets[j]\n\tki := b.data[ei.keyStart:ei.keyEnd]\n\tkj := b.data[ej.keyStart:ej.keyEnd]\n\tswitch c := b.cmp(ki, kj); {\n\tcase c < 0:\n\t\treturn true\n\tcase c > 0:\n\t\treturn false\n\tdefault:\n\t\treturn ei.offset > ej.offset\n\t}\n}\n\nfunc (b *flushableBatch) Swap(i, j int) {\n\tb.offsets[i], b.offsets[j] = b.offsets[j], b.offsets[i]\n}\n\n// newIter is part of the flushable interface.\nfunc (b *flushableBatch) newIter(o *IterOptions) internalIterator {\n\treturn &flushableBatchIter{\n\t\tbatch:   b,\n\t\tdata:    b.data,\n\t\toffsets: b.offsets,\n\t\tcmp:     b.cmp,\n\t\tindex:   -1,\n\t\tlower:   o.GetLowerBound(),\n\t\tupper:   o.GetUpperBound(),\n\t}\n}\n\n// newFlushIter is part of the flushable interface.\nfunc (b *flushableBatch) newFlushIter(o *IterOptions) internalIterator {\n\treturn &flushFlushableBatchIter{\n\t\tflushableBatchIter: flushableBatchIter{\n\t\t\tbatch:   b,\n\t\t\tdata:    b.data,\n\t\t\toffsets: b.offsets,\n\t\t\tcmp:     b.cmp,\n\t\t\tindex:   -1,\n\t\t},\n\t}\n}\n\n// newRangeDelIter is part of the flushable interface.\nfunc (b *flushableBatch) newRangeDelIter(o *IterOptions) keyspan.FragmentIterator {\n\tif len(b.tombstones) == 0 {\n\t\treturn nil\n\t}\n\treturn keyspan.NewIter(b.cmp, b.tombstones)\n}\n\n// newRangeKeyIter is part of the flushable interface.\nfunc (b *flushableBatch) newRangeKeyIter(o *IterOptions) keyspan.FragmentIterator {\n\tif len(b.rangeKeys) == 0 {\n\t\treturn nil\n\t}\n\treturn keyspan.NewIter(b.cmp, b.rangeKeys)\n}\n\n// containsRangeKeys is part of the flushable interface.\nfunc (b *flushableBatch) containsRangeKeys() bool { return len(b.rangeKeys) > 0 }\n\n// inuseBytes is part of the flushable interface.\nfunc (b *flushableBatch) inuseBytes() uint64 {\n\treturn uint64(len(b.data) - batchrepr.HeaderLen)\n}\n\n// totalBytes is part of the flushable interface.\nfunc (b *flushableBatch) totalBytes() uint64 {\n\treturn uint64(cap(b.data))\n}\n\n// readyForFlush is part of the flushable interface.\nfunc (b *flushableBatch) readyForFlush() bool {\n\t// A flushable batch is always ready for flush; it must be flushed together\n\t// with the previous memtable.\n\treturn true\n}\n\n// computePossibleOverlaps is part of the flushable interface.\nfunc (b *flushableBatch) computePossibleOverlaps(\n\tfn func(bounded) shouldContinue, bounded ...bounded,\n) {\n\tcomputePossibleOverlapsGenericImpl[*flushableBatch](b, b.cmp, fn, bounded)\n}\n\n// Note: flushableBatchIter mirrors the implementation of batchIter. Keep the\n// two in sync.\ntype flushableBatchIter struct {\n\t// Members to be initialized by creator.\n\tbatch *flushableBatch\n\t// The bytes backing the batch. Always the same as batch.data?\n\tdata []byte\n\t// The sorted entries. This is not always equal to batch.offsets.\n\toffsets []flushableBatchEntry\n\tcmp     Compare\n\t// Must be initialized to -1. It is the index into offsets that represents\n\t// the current iterator position.\n\tindex int\n\n\t// For internal use by the implementation.\n\tkv  base.InternalKV\n\terr error\n\n\t// Optionally initialize to bounds of iteration, if any.\n\tlower []byte\n\tupper []byte\n}\n\n// flushableBatchIter implements the base.InternalIterator interface.\nvar _ base.InternalIterator = (*flushableBatchIter)(nil)\n\nfunc (i *flushableBatchIter) String() string {\n\treturn \"flushable-batch\"\n}\n\n// SeekGE implements internalIterator.SeekGE, as documented in the pebble\n// package. Ignore flags.TrySeekUsingNext() since we don't expect this\n// optimization to provide much benefit here at the moment.\nfunc (i *flushableBatchIter) SeekGE(key []byte, flags base.SeekGEFlags) *base.InternalKV {\n\ti.err = nil // clear cached iteration error\n\tikey := base.MakeSearchKey(key)\n\ti.index = sort.Search(len(i.offsets), func(j int) bool {\n\t\treturn base.InternalCompare(i.cmp, ikey, i.getKey(j)) <= 0\n\t})\n\tif i.index >= len(i.offsets) {\n\t\treturn nil\n\t}\n\tkv := i.getKV(i.index)\n\tif i.upper != nil && i.cmp(kv.K.UserKey, i.upper) >= 0 {\n\t\ti.index = len(i.offsets)\n\t\treturn nil\n\t}\n\treturn kv\n}\n\n// SeekPrefixGE implements internalIterator.SeekPrefixGE, as documented in the\n// pebble package.\nfunc (i *flushableBatchIter) SeekPrefixGE(\n\tprefix, key []byte, flags base.SeekGEFlags,\n) *base.InternalKV {\n\tkv := i.SeekGE(key, flags)\n\tif kv == nil {\n\t\treturn nil\n\t}\n\t// If the key doesn't have the sought prefix, return nil.\n\tif !bytes.Equal(i.batch.comparer.Split.Prefix(kv.K.UserKey), prefix) {\n\t\treturn nil\n\t}\n\treturn kv\n}\n\n// SeekLT implements internalIterator.SeekLT, as documented in the pebble\n// package.\nfunc (i *flushableBatchIter) SeekLT(key []byte, flags base.SeekLTFlags) *base.InternalKV {\n\ti.err = nil // clear cached iteration error\n\tikey := base.MakeSearchKey(key)\n\ti.index = sort.Search(len(i.offsets), func(j int) bool {\n\t\treturn base.InternalCompare(i.cmp, ikey, i.getKey(j)) <= 0\n\t})\n\ti.index--\n\tif i.index < 0 {\n\t\treturn nil\n\t}\n\tkv := i.getKV(i.index)\n\tif i.lower != nil && i.cmp(kv.K.UserKey, i.lower) < 0 {\n\t\ti.index = -1\n\t\treturn nil\n\t}\n\treturn kv\n}\n\n// First implements internalIterator.First, as documented in the pebble\n// package.\nfunc (i *flushableBatchIter) First() *base.InternalKV {\n\ti.err = nil // clear cached iteration error\n\tif len(i.offsets) == 0 {\n\t\treturn nil\n\t}\n\ti.index = 0\n\tkv := i.getKV(i.index)\n\tif i.upper != nil && i.cmp(kv.K.UserKey, i.upper) >= 0 {\n\t\ti.index = len(i.offsets)\n\t\treturn nil\n\t}\n\treturn kv\n}\n\n// Last implements internalIterator.Last, as documented in the pebble\n// package.\nfunc (i *flushableBatchIter) Last() *base.InternalKV {\n\ti.err = nil // clear cached iteration error\n\tif len(i.offsets) == 0 {\n\t\treturn nil\n\t}\n\ti.index = len(i.offsets) - 1\n\tkv := i.getKV(i.index)\n\tif i.lower != nil && i.cmp(kv.K.UserKey, i.lower) < 0 {\n\t\ti.index = -1\n\t\treturn nil\n\t}\n\treturn kv\n}\n\n// Note: flushFlushableBatchIter.Next mirrors the implementation of\n// flushableBatchIter.Next due to performance. Keep the two in sync.\nfunc (i *flushableBatchIter) Next() *base.InternalKV {\n\tif i.index == len(i.offsets) {\n\t\treturn nil\n\t}\n\ti.index++\n\tif i.index == len(i.offsets) {\n\t\treturn nil\n\t}\n\tkv := i.getKV(i.index)\n\tif i.upper != nil && i.cmp(kv.K.UserKey, i.upper) >= 0 {\n\t\ti.index = len(i.offsets)\n\t\treturn nil\n\t}\n\treturn kv\n}\n\nfunc (i *flushableBatchIter) Prev() *base.InternalKV {\n\tif i.index < 0 {\n\t\treturn nil\n\t}\n\ti.index--\n\tif i.index < 0 {\n\t\treturn nil\n\t}\n\tkv := i.getKV(i.index)\n\tif i.lower != nil && i.cmp(kv.K.UserKey, i.lower) < 0 {\n\t\ti.index = -1\n\t\treturn nil\n\t}\n\treturn kv\n}\n\n// Note: flushFlushableBatchIter.NextPrefix mirrors the implementation of\n// flushableBatchIter.NextPrefix due to performance. Keep the two in sync.\nfunc (i *flushableBatchIter) NextPrefix(succKey []byte) *base.InternalKV {\n\treturn i.SeekGE(succKey, base.SeekGEFlagsNone.EnableTrySeekUsingNext())\n}\n\nfunc (i *flushableBatchIter) getKey(index int) InternalKey {\n\te := &i.offsets[index]\n\tkind := InternalKeyKind(i.data[e.offset])\n\tkey := i.data[e.keyStart:e.keyEnd]\n\treturn base.MakeInternalKey(key, i.batch.seqNum+base.SeqNum(e.index), kind)\n}\n\nfunc (i *flushableBatchIter) getKV(index int) *base.InternalKV {\n\ti.kv = base.InternalKV{\n\t\tK: i.getKey(index),\n\t\tV: i.extractValue(),\n\t}\n\treturn &i.kv\n}\n\nfunc (i *flushableBatchIter) extractValue() base.LazyValue {\n\tp := i.data[i.offsets[i.index].offset:]\n\tif len(p) == 0 {\n\t\ti.err = base.CorruptionErrorf(\"corrupted batch\")\n\t\treturn base.LazyValue{}\n\t}\n\tkind := InternalKeyKind(p[0])\n\tif kind > InternalKeyKindMax {\n\t\ti.err = base.CorruptionErrorf(\"corrupted batch\")\n\t\treturn base.LazyValue{}\n\t}\n\tvar value []byte\n\tvar ok bool\n\tswitch kind {\n\tcase InternalKeyKindSet, InternalKeyKindMerge, InternalKeyKindRangeDelete,\n\t\tInternalKeyKindRangeKeySet, InternalKeyKindRangeKeyUnset, InternalKeyKindRangeKeyDelete,\n\t\tInternalKeyKindDeleteSized:\n\t\tkeyEnd := i.offsets[i.index].keyEnd\n\t\t_, value, ok = batchrepr.DecodeStr(i.data[keyEnd:])\n\t\tif !ok {\n\t\t\ti.err = base.CorruptionErrorf(\"corrupted batch\")\n\t\t\treturn base.LazyValue{}\n\t\t}\n\t}\n\treturn base.MakeInPlaceValue(value)\n}\n\nfunc (i *flushableBatchIter) Valid() bool {\n\treturn i.index >= 0 && i.index < len(i.offsets)\n}\n\nfunc (i *flushableBatchIter) Error() error {\n\treturn i.err\n}\n\nfunc (i *flushableBatchIter) Close() error {\n\treturn i.err\n}\n\nfunc (i *flushableBatchIter) SetBounds(lower, upper []byte) {\n\ti.lower = lower\n\ti.upper = upper\n}\n\nfunc (i *flushableBatchIter) SetContext(_ context.Context) {}\n\n// DebugTree is part of the InternalIterator interface.\nfunc (i *flushableBatchIter) DebugTree(tp treeprinter.Node) {\n\ttp.Childf(\"%T(%p)\", i, i)\n}\n\n// flushFlushableBatchIter is similar to flushableBatchIter but it keeps track\n// of number of bytes iterated.\ntype flushFlushableBatchIter struct {\n\tflushableBatchIter\n}\n\n// flushFlushableBatchIter implements the base.InternalIterator interface.\nvar _ base.InternalIterator = (*flushFlushableBatchIter)(nil)\n\nfunc (i *flushFlushableBatchIter) String() string {\n\treturn \"flushable-batch\"\n}\n\nfunc (i *flushFlushableBatchIter) SeekGE(key []byte, flags base.SeekGEFlags) *base.InternalKV {\n\tpanic(\"pebble: SeekGE unimplemented\")\n}\n\nfunc (i *flushFlushableBatchIter) SeekPrefixGE(\n\tprefix, key []byte, flags base.SeekGEFlags,\n) *base.InternalKV {\n\tpanic(\"pebble: SeekPrefixGE unimplemented\")\n}\n\nfunc (i *flushFlushableBatchIter) SeekLT(key []byte, flags base.SeekLTFlags) *base.InternalKV {\n\tpanic(\"pebble: SeekLT unimplemented\")\n}\n\nfunc (i *flushFlushableBatchIter) First() *base.InternalKV {\n\ti.err = nil // clear cached iteration error\n\treturn i.flushableBatchIter.First()\n}\n\nfunc (i *flushFlushableBatchIter) NextPrefix(succKey []byte) *base.InternalKV {\n\tpanic(\"pebble: Prev unimplemented\")\n}\n\n// Note: flushFlushableBatchIter.Next mirrors the implementation of\n// flushableBatchIter.Next due to performance. Keep the two in sync.\nfunc (i *flushFlushableBatchIter) Next() *base.InternalKV {\n\tif i.index == len(i.offsets) {\n\t\treturn nil\n\t}\n\ti.index++\n\tif i.index == len(i.offsets) {\n\t\treturn nil\n\t}\n\treturn i.getKV(i.index)\n}\n\nfunc (i flushFlushableBatchIter) Prev() *base.InternalKV {\n\tpanic(\"pebble: Prev unimplemented\")\n}\n\n// batchOptions holds the parameters to configure batch.\ntype batchOptions struct {\n\tinitialSizeBytes     int\n\tmaxRetainedSizeBytes int\n}\n\n// ensureDefaults creates batch options with default values.\nfunc (o *batchOptions) ensureDefaults() {\n\tif o.initialSizeBytes <= 0 {\n\t\to.initialSizeBytes = defaultBatchInitialSize\n\t}\n\tif o.maxRetainedSizeBytes <= 0 {\n\t\to.maxRetainedSizeBytes = defaultBatchMaxRetainedSize\n\t}\n}\n\n// BatchOption allows customizing the batch.\ntype BatchOption func(*batchOptions)\n\n// WithInitialSizeBytes sets a custom initial size for the batch. Defaults\n// to 1KB.\nfunc WithInitialSizeBytes(s int) BatchOption {\n\treturn func(opts *batchOptions) {\n\t\topts.initialSizeBytes = s\n\t}\n}\n\n// WithMaxRetainedSizeBytes sets a custom max size for the batch to be\n// re-used. Any batch which exceeds the max retained size would be GC-ed.\n// Defaults to 1MB.\nfunc WithMaxRetainedSizeBytes(s int) BatchOption {\n\treturn func(opts *batchOptions) {\n\t\topts.maxRetainedSizeBytes = s\n\t}\n}\n\n// batchSort returns iterators for the sorted contents of the batch. It is\n// intended for testing use only. The batch.Sort dance is done to prevent\n// exposing this method in the public pebble interface.\nfunc batchSort(\n\ti interface{},\n) (\n\tpoints internalIterator,\n\trangeDels keyspan.FragmentIterator,\n\trangeKeys keyspan.FragmentIterator,\n) {\n\tb := i.(*Batch)\n\tif b.Indexed() {\n\t\tpointIter := b.newInternalIter(nil)\n\t\trangeDelIter := b.newRangeDelIter(nil, math.MaxUint64)\n\t\trangeKeyIter := b.newRangeKeyIter(nil, math.MaxUint64)\n\t\treturn pointIter, rangeDelIter, rangeKeyIter\n\t}\n\tf, err := newFlushableBatch(b, b.db.opts.Comparer)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn f.newIter(nil), f.newRangeDelIter(nil), f.newRangeKeyIter(nil)\n}\n\nfunc init() {\n\tprivate.BatchSort = batchSort\n}\n"
        },
        {
          "name": "batch_test.go",
          "type": "blob",
          "size": 46.5498046875,
          "content": "// Copyright 2012 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\t\"math/rand/v2\"\n\t\"strings\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/datadriven\"\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/batchrepr\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/batchskl\"\n\t\"github.com/cockroachdb/pebble/internal/datadrivenutil\"\n\t\"github.com/cockroachdb/pebble/internal/itertest\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan\"\n\t\"github.com/cockroachdb/pebble/internal/testkeys\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestBatch(t *testing.T) {\n\ttestBatch(t, 0)\n\ttestBatch(t, defaultBatchInitialSize)\n}\n\nfunc testBatch(t *testing.T, size int) {\n\ttype testCase struct {\n\t\tkind       InternalKeyKind\n\t\tkey, value string\n\t\tvalueInt   uint32\n\t}\n\n\tverifyTestCases := func(b *Batch, testCases []testCase, indexedPointKindsOnly bool) {\n\t\tr := b.Reader()\n\n\t\tfor _, tc := range testCases {\n\t\t\tif indexedPointKindsOnly && (tc.kind == InternalKeyKindLogData || tc.kind == InternalKeyKindIngestSST ||\n\t\t\t\ttc.kind == InternalKeyKindRangeDelete) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tkind, k, v, ok, err := r.Next()\n\t\t\tif !ok {\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t}\n\t\t\t\tt.Fatalf(\"next returned !ok: test case = %v\", tc)\n\t\t\t}\n\t\t\tkey, value := string(k), string(v)\n\t\t\tif kind != tc.kind || key != tc.key || value != tc.value {\n\t\t\t\tt.Errorf(\"got (%d, %q, %q), want (%d, %q, %q)\",\n\t\t\t\t\tkind, key, value, tc.kind, tc.key, tc.value)\n\t\t\t}\n\t\t}\n\t\tif len(r) != 0 {\n\t\t\tt.Errorf(\"reader was not exhausted: remaining bytes = %q\", r)\n\t\t}\n\t}\n\n\tencodeFileNum := func(n base.FileNum) string {\n\t\treturn string(binary.AppendUvarint(nil, uint64(n)))\n\t}\n\tdecodeFileNum := func(d []byte) base.FileNum {\n\t\tval, n := binary.Uvarint(d)\n\t\tif n <= 0 {\n\t\t\tt.Fatalf(\"invalid filenum encoding\")\n\t\t}\n\t\treturn base.FileNum(val)\n\t}\n\n\t// RangeKeySet and RangeKeyUnset are untested here because they don't expose\n\t// deferred variants. This is a consequence of these keys' more complex\n\t// value encodings.\n\ttestCases := []testCase{\n\t\t{InternalKeyKindIngestSST, encodeFileNum(1), \"\", 0},\n\t\t{InternalKeyKindSet, \"roses\", \"red\", 0},\n\t\t{InternalKeyKindSet, \"violets\", \"blue\", 0},\n\t\t{InternalKeyKindDelete, \"roses\", \"\", 0},\n\t\t{InternalKeyKindSingleDelete, \"roses\", \"\", 0},\n\t\t{InternalKeyKindSet, \"\", \"\", 0},\n\t\t{InternalKeyKindSet, \"\", \"non-empty\", 0},\n\t\t{InternalKeyKindDelete, \"\", \"\", 0},\n\t\t{InternalKeyKindSingleDelete, \"\", \"\", 0},\n\t\t{InternalKeyKindSet, \"grass\", \"green\", 0},\n\t\t{InternalKeyKindSet, \"grass\", \"greener\", 0},\n\t\t{InternalKeyKindSet, \"eleventy\", strings.Repeat(\"!!11!\", 100), 0},\n\t\t{InternalKeyKindDelete, \"nosuchkey\", \"\", 0},\n\t\t{InternalKeyKindDeleteSized, \"eleventy\", string(binary.AppendUvarint([]byte(nil), 508)), 500},\n\t\t{InternalKeyKindSingleDelete, \"nosuchkey\", \"\", 0},\n\t\t{InternalKeyKindSet, \"binarydata\", \"\\x00\", 0},\n\t\t{InternalKeyKindSet, \"binarydata\", \"\\xff\", 0},\n\t\t{InternalKeyKindMerge, \"merge\", \"mergedata\", 0},\n\t\t{InternalKeyKindMerge, \"merge\", \"\", 0},\n\t\t{InternalKeyKindMerge, \"\", \"\", 0},\n\t\t{InternalKeyKindRangeDelete, \"a\", \"b\", 0},\n\t\t{InternalKeyKindRangeDelete, \"\", \"\", 0},\n\t\t{InternalKeyKindLogData, \"logdata\", \"\", 0},\n\t\t{InternalKeyKindLogData, \"\", \"\", 0},\n\t\t{InternalKeyKindRangeKeyDelete, \"grass\", \"green\", 0},\n\t\t{InternalKeyKindRangeKeyDelete, \"\", \"\", 0},\n\t\t{InternalKeyKindDeleteSized, \"nosuchkey\", string(binary.AppendUvarint([]byte(nil), 11)), 2},\n\t}\n\tb := newBatchWithSize(nil, size)\n\tfor _, tc := range testCases {\n\t\tswitch tc.kind {\n\t\tcase InternalKeyKindSet:\n\t\t\t_ = b.Set([]byte(tc.key), []byte(tc.value), nil)\n\t\tcase InternalKeyKindMerge:\n\t\t\t_ = b.Merge([]byte(tc.key), []byte(tc.value), nil)\n\t\tcase InternalKeyKindDelete:\n\t\t\t_ = b.Delete([]byte(tc.key), nil)\n\t\tcase InternalKeyKindDeleteSized:\n\t\t\t_ = b.DeleteSized([]byte(tc.key), tc.valueInt, nil)\n\t\tcase InternalKeyKindSingleDelete:\n\t\t\t_ = b.SingleDelete([]byte(tc.key), nil)\n\t\tcase InternalKeyKindRangeDelete:\n\t\t\t_ = b.DeleteRange([]byte(tc.key), []byte(tc.value), nil)\n\t\tcase InternalKeyKindLogData:\n\t\t\t_ = b.LogData([]byte(tc.key), nil)\n\t\tcase InternalKeyKindRangeKeyDelete:\n\t\t\t_ = b.RangeKeyDelete([]byte(tc.key), []byte(tc.value), nil)\n\t\tcase InternalKeyKindIngestSST:\n\t\t\tb.ingestSST(decodeFileNum([]byte(tc.key)))\n\t\t}\n\t}\n\tverifyTestCases(b, testCases, false /* indexedKindsOnly */)\n\n\tb.Reset()\n\t// Run the same operations, this time using the Deferred variants of each\n\t// operation (eg. SetDeferred).\n\tfor _, tc := range testCases {\n\t\tkey := []byte(tc.key)\n\t\tvalue := []byte(tc.value)\n\t\tswitch tc.kind {\n\t\tcase InternalKeyKindSet:\n\t\t\td := b.SetDeferred(len(key), len(value))\n\t\t\tcopy(d.Key, key)\n\t\t\tcopy(d.Value, value)\n\t\t\td.Finish()\n\t\tcase InternalKeyKindMerge:\n\t\t\td := b.MergeDeferred(len(key), len(value))\n\t\t\tcopy(d.Key, key)\n\t\t\tcopy(d.Value, value)\n\t\t\td.Finish()\n\t\tcase InternalKeyKindDelete:\n\t\t\td := b.DeleteDeferred(len(key))\n\t\t\tcopy(d.Key, key)\n\t\t\tcopy(d.Value, value)\n\t\t\td.Finish()\n\t\tcase InternalKeyKindDeleteSized:\n\t\t\td := b.DeleteSizedDeferred(len(tc.key), tc.valueInt)\n\t\t\tcopy(d.Key, key)\n\t\t\td.Finish()\n\t\tcase InternalKeyKindSingleDelete:\n\t\t\td := b.SingleDeleteDeferred(len(key))\n\t\t\tcopy(d.Key, key)\n\t\t\tcopy(d.Value, value)\n\t\t\td.Finish()\n\t\tcase InternalKeyKindRangeDelete:\n\t\t\td := b.DeleteRangeDeferred(len(key), len(value))\n\t\t\tcopy(d.Key, key)\n\t\t\tcopy(d.Value, value)\n\t\t\td.Finish()\n\t\tcase InternalKeyKindLogData:\n\t\t\t_ = b.LogData([]byte(tc.key), nil)\n\t\tcase InternalKeyKindIngestSST:\n\t\t\tb.ingestSST(decodeFileNum([]byte(tc.key)))\n\t\tcase InternalKeyKindRangeKeyDelete:\n\t\t\td := b.RangeKeyDeleteDeferred(len(key), len(value))\n\t\t\tcopy(d.Key, key)\n\t\t\tcopy(d.Value, value)\n\t\t\td.Finish()\n\t\t}\n\t}\n\tverifyTestCases(b, testCases, false /* indexedKindsOnly */)\n\n\tb.Reset()\n\t// Run the same operations, this time using AddInternalKey instead of the\n\t// Kind-specific methods.\n\tfor _, tc := range testCases {\n\t\tif tc.kind == InternalKeyKindLogData || tc.kind == InternalKeyKindIngestSST ||\n\t\t\ttc.kind == InternalKeyKindRangeDelete {\n\t\t\tcontinue\n\t\t}\n\t\tkey := []byte(tc.key)\n\t\tvalue := []byte(tc.value)\n\t\tb.AddInternalKey(&InternalKey{UserKey: key, Trailer: base.MakeTrailer(0, tc.kind)}, value, nil)\n\t}\n\tverifyTestCases(b, testCases, true /* indexedKindsOnly */)\n}\n\nfunc TestBatchPreAlloc(t *testing.T) {\n\tvar cases = []struct {\n\t\tsize int\n\t\texp  int\n\t}{\n\t\t{0, defaultBatchInitialSize},\n\t\t{defaultBatchInitialSize, defaultBatchInitialSize},\n\t\t{2 * defaultBatchInitialSize, 2 * defaultBatchInitialSize},\n\t}\n\tfor _, c := range cases {\n\t\tb := newBatchWithSize(nil, c.size)\n\t\tb.Set([]byte{0x1}, []byte{0x2}, nil)\n\t\tif cap(b.data) != c.exp {\n\t\t\tt.Errorf(\"Unexpected memory space, required: %d, got: %d\", c.exp, cap(b.data))\n\t\t}\n\t}\n}\n\nfunc TestBatchIngestSST(t *testing.T) {\n\t// Verify that Batch.IngestSST has the correct batch count and memtable\n\t// size.\n\tvar b Batch\n\tb.ingestSST(1)\n\trequire.Equal(t, int(b.Count()), 1)\n\tb.ingestSST(2)\n\trequire.Equal(t, int(b.Count()), 2)\n\trequire.Equal(t, int(b.memTableSize), 0)\n\trequire.Equal(t, b.ingestedSSTBatch, true)\n}\n\nfunc TestBatchLen(t *testing.T) {\n\tvar b Batch\n\n\trequireLenAndReprEq := func(size int) {\n\t\trequire.Equal(t, size, b.Len())\n\t\trequire.Equal(t, size, len(b.Repr()))\n\t}\n\n\trequireLenAndReprEq(batchrepr.HeaderLen)\n\n\tkey := \"test-key\"\n\tvalue := \"test-value\"\n\n\terr := b.Set([]byte(key), []byte(value), nil)\n\trequire.NoError(t, err)\n\n\trequireLenAndReprEq(33)\n\n\terr = b.Delete([]byte(key), nil)\n\trequire.NoError(t, err)\n\n\trequireLenAndReprEq(43)\n}\n\nfunc TestBatchEmpty(t *testing.T) {\n\ttestBatchEmpty(t, 0)\n\ttestBatchEmpty(t, defaultBatchInitialSize)\n\ttestBatchEmpty(t, 0, WithInitialSizeBytes(2<<10), WithMaxRetainedSizeBytes(2<<20))\n}\n\nfunc testBatchEmpty(t *testing.T, size int, opts ...BatchOption) {\n\tb := newBatchWithSize(nil, size, opts...)\n\trequire.True(t, b.Empty())\n\n\tops := []func(*Batch) error{\n\t\tfunc(b *Batch) error { return b.Set(nil, nil, nil) },\n\t\tfunc(b *Batch) error { return b.Merge(nil, nil, nil) },\n\t\tfunc(b *Batch) error { return b.Delete(nil, nil) },\n\t\tfunc(b *Batch) error { return b.DeleteRange(nil, nil, nil) },\n\t\tfunc(b *Batch) error { return b.LogData(nil, nil) },\n\t\tfunc(b *Batch) error { return b.RangeKeySet(nil, nil, nil, nil, nil) },\n\t\tfunc(b *Batch) error { return b.RangeKeyUnset(nil, nil, nil, nil) },\n\t\tfunc(b *Batch) error { return b.RangeKeyDelete(nil, nil, nil) },\n\t}\n\n\tfor _, op := range ops {\n\t\trequire.NoError(t, op(b))\n\t\trequire.False(t, b.Empty())\n\t\tb.Reset()\n\t\trequire.True(t, b.Empty())\n\t\t// Reset may choose to reuse b.data, so clear it to the zero value in\n\t\t// order to test the lazy initialization of b.data.\n\t\tb = newBatchWithSize(nil, size)\n\t}\n\n\t_ = b.Reader()\n\trequire.True(t, b.Empty())\n\tb.Reset()\n\trequire.True(t, b.Empty())\n\tb = newBatchWithSize(nil, size)\n\n\trequire.Equal(t, base.SeqNumZero, b.SeqNum())\n\trequire.True(t, b.Empty())\n\tb.Reset()\n\trequire.True(t, b.Empty())\n\tb = &Batch{}\n\n\td, err := Open(\"\", &Options{\n\t\tFS: vfs.NewMem(),\n\t})\n\trequire.NoError(t, err)\n\tdefer d.Close()\n\tib := newIndexedBatch(d, DefaultComparer)\n\titer, _ := ib.NewIter(nil)\n\trequire.False(t, iter.First())\n\titer2, err := iter.Clone(CloneOptions{})\n\trequire.NoError(t, err)\n\trequire.NoError(t, iter.Close())\n\t_, err = iter.Clone(CloneOptions{})\n\trequire.True(t, err != nil)\n\trequire.False(t, iter2.First())\n\trequire.NoError(t, iter2.Close())\n\titer3, err := ib.NewBatchOnlyIter(context.Background(), nil)\n\trequire.NoError(t, err)\n\trequire.False(t, iter3.First())\n\t_, err = iter3.Clone(CloneOptions{})\n\trequire.Error(t, err)\n\trequire.NoError(t, iter3.Close())\n}\n\nfunc TestBatchApplyNoSyncWait(t *testing.T) {\n\tdb, err := Open(\"\", &Options{\n\t\tFS: vfs.NewMem(),\n\t})\n\trequire.NoError(t, err)\n\tdefer db.Close()\n\tvar batches []*Batch\n\toptions := &WriteOptions{Sync: true}\n\tfor i := 0; i < 10000; i++ {\n\t\tb := db.NewBatch()\n\t\tstr := fmt.Sprintf(\"a%d\", i)\n\t\trequire.NoError(t, b.Set([]byte(str), []byte(str), nil))\n\t\trequire.NoError(t, db.ApplyNoSyncWait(b, options))\n\t\t// k-v pair is visible even if not yet synced.\n\t\tval, closer, err := db.Get([]byte(str))\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, str, string(val))\n\t\tcloser.Close()\n\t\tbatches = append(batches, b)\n\t}\n\tfor _, b := range batches {\n\t\trequire.NoError(t, b.SyncWait())\n\t\tb.Close()\n\t}\n}\n\nfunc TestBatchReset(t *testing.T) {\n\tdb, err := Open(\"\", &Options{\n\t\tFS: vfs.NewMem(),\n\t})\n\trequire.NoError(t, err)\n\tdefer db.Close()\n\tkey := \"test-key\"\n\tvalue := \"test-value\"\n\tb := db.NewBatch()\n\trequire.NoError(t, b.Set([]byte(key), []byte(value), nil))\n\tdd := b.DeleteRangeDeferred(len(key), len(value))\n\tcopy(dd.Key, key)\n\tcopy(dd.Value, value)\n\tdd.Finish()\n\n\trequire.NoError(t, b.RangeKeySet([]byte(key), []byte(value), []byte(value), []byte(value), nil))\n\n\tb.setSeqNum(100)\n\tb.applied.Store(true)\n\tb.commitErr = errors.New(\"test-error\")\n\tb.commit.Add(1)\n\tb.fsyncWait.Add(1)\n\trequire.Equal(t, uint32(3), b.Count())\n\trequire.Equal(t, uint64(1), b.countRangeDels)\n\trequire.Equal(t, uint64(1), b.countRangeKeys)\n\trequire.True(t, len(b.data) > 0)\n\trequire.True(t, b.SeqNum() > 0)\n\trequire.True(t, b.memTableSize > 0)\n\trequire.NotEqual(t, b.deferredOp, DeferredBatchOp{})\n\t// At this point b.data has not been modified since the db.NewBatch() and is\n\t// either nil or contains a byte slice of length batchHeaderLen, with a 0\n\t// seqnum encoded in data[0:8] and an arbitrary count encoded in data[8:12].\n\t// If we simply called b.Reset now and later used b.data to initialize\n\t// expected, the count in expected will also be arbitrary. So we fix the\n\t// count in b.data now by calling b.Repr(). This call isn't essential, since\n\t// we will call b.Repr() again, and just shows that it fixes the count in\n\t// b.data.\n\th, ok := batchrepr.ReadHeader(b.Repr())\n\trequire.True(t, ok)\n\trequire.Equal(t, uint32(3), h.Count)\n\n\tb.Reset()\n\trequire.Equal(t, db, b.db)\n\trequire.Equal(t, false, b.applied.Load())\n\trequire.Nil(t, b.commitErr)\n\trequire.Equal(t, uint32(0), b.Count())\n\trequire.Equal(t, uint64(0), b.countRangeDels)\n\trequire.Equal(t, uint64(0), b.countRangeKeys)\n\trequire.Equal(t, batchrepr.HeaderLen, len(b.data))\n\trequire.Equal(t, base.SeqNumZero, b.SeqNum())\n\trequire.Equal(t, uint64(0), b.memTableSize)\n\trequire.Equal(t, FormatMajorVersion(0x00), b.minimumFormatMajorVersion)\n\trequire.Equal(t, b.deferredOp, DeferredBatchOp{})\n\t_ = b.Repr()\n\n\tvar expected Batch\n\trequire.NoError(t, expected.SetRepr(b.data))\n\texpected.db = db\n\t// Batch options should remain same after reset.\n\texpected.opts = b.opts\n\trequire.Equal(t, &expected, b)\n\n\t// Reset batch can be used to write and commit a new record.\n\tb.Set([]byte(key), []byte(value), nil)\n\trequire.NoError(t, db.Apply(b, nil))\n\tv, closer, err := db.Get([]byte(key))\n\trequire.NoError(t, err)\n\tdefer closer.Close()\n\trequire.Equal(t, v, []byte(value))\n}\n\nfunc TestBatchReuse(t *testing.T) {\n\tdb, err := Open(\"\", &Options{\n\t\tFS: vfs.NewMem(),\n\t})\n\trequire.NoError(t, err)\n\n\tvar buf bytes.Buffer\n\tbatches := map[string]*Batch{}\n\tdatadriven.RunTest(t, \"testdata/batch_reuse\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tbuf.Reset()\n\t\tswitch td.Cmd {\n\t\tcase \"run\":\n\t\t\tlines := datadrivenutil.Lines(td.Input)\n\t\t\tfor len(lines) > 0 {\n\t\t\t\tl := lines.Next()\n\t\t\t\tfields := l.Fields('.', '(', ')', '\"')\n\t\t\t\tif len(l) > 0 && l[0] == '#' {\n\t\t\t\t\t// Comment.\n\t\t\t\t\tfmt.Fprintln(&buf, l)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tswitch {\n\t\t\t\tcase fields.Index(1) == \"=\":\n\t\t\t\t\tswitch {\n\t\t\t\t\tcase fields.Index(2).Str() == \"db\" && fields.Index(3).Str() == \"NewBatch\":\n\t\t\t\t\t\t// Command of the form: b = db.NewBatch()\n\t\t\t\t\t\tbatches[fields.Index(0).Str()] = db.NewBatch()\n\t\t\t\t\t\tfmt.Fprintln(&buf, l)\n\t\t\t\t\tcase fields.Index(2).Str() == \"new\" && fields.Index(3).Str() == \"Batch\":\n\t\t\t\t\t\t// Command of the form: b = new(Batch)\n\t\t\t\t\t\tbatches[fields.Index(0).Str()] = new(Batch)\n\t\t\t\t\t\tfmt.Fprintln(&buf, l)\n\t\t\t\t\tdefault:\n\t\t\t\t\t\treturn fmt.Sprintf(\"unrecognized batch constructor: %s\", l)\n\t\t\t\t\t}\n\t\t\t\tcase fields.Index(1) == \"Set\":\n\t\t\t\t\t// Command of the form: b1.Set(\"foo\", \"bar\")\n\t\t\t\t\tbatches[fields.Index(0).Str()].Set(\n\t\t\t\t\t\tfields.Index(2).Bytes(),\n\t\t\t\t\t\tfields.Index(3).Bytes(),\n\t\t\t\t\t\tnil,\n\t\t\t\t\t)\n\t\t\t\t\tfmt.Fprintln(&buf, l)\n\t\t\t\tcase fields.Index(1) == \"lifecycle\":\n\t\t\t\t\t// Command of the form: b1.lifecycle\n\t\t\t\t\tv := batches[fields.Index(0).Str()].lifecycle.Load()\n\t\t\t\t\tfmt.Fprintf(&buf, \"%s = %b\\n\", l, v)\n\t\t\t\tcase fields.Index(1) == \"refData\":\n\t\t\t\t\t// Command of the form: b1.refData()\n\t\t\t\t\tbatches[fields.Index(0).Str()].Ref()\n\t\t\t\t\tfmt.Fprintf(&buf, \"%s\\n\", l)\n\t\t\t\tcase fields.Index(1) == \"unrefData\":\n\t\t\t\t\t// Command of the form: b1.unrefData()\n\t\t\t\t\tbatches[fields.Index(0).Str()].Unref()\n\t\t\t\t\tfmt.Fprintf(&buf, \"%s\\n\", l)\n\t\t\t\tcase fields.Index(1) == \"Close\":\n\t\t\t\t\t// Command of the form: b1.Close()\n\t\t\t\t\terr := batches[fields.Index(0).Str()].Close()\n\t\t\t\t\tfmt.Fprintf(&buf, \"%s = %v\\n\", l, err)\n\t\t\t\tcase fields.Index(1) == \"Len\":\n\t\t\t\t\t// Command of the form: b1.Len()\n\t\t\t\t\tfmt.Fprintf(&buf, \"%s = %d\\n\", l, batches[fields.Index(0).Str()].Len())\n\t\t\t\tcase fields.Index(1) == \"Reset\":\n\t\t\t\t\t// Command of the form: b1.Reset()\n\t\t\t\t\tbatches[fields.Index(0).Str()].Reset()\n\t\t\t\t\tfmt.Fprintf(&buf, \"%s\\n\", l)\n\t\t\t\tcase fields.Index(0) == \"cap\" && fields.Index(2) == \"data\":\n\t\t\t\t\t// Command of the form: cap(b1.data)\n\t\t\t\t\tv := cap(batches[fields.Index(1).Str()].data)\n\t\t\t\t\tfmt.Fprintf(&buf, \"%s = %d\\n\", l, v)\n\t\t\t\tdefault:\n\t\t\t\t\treturn fmt.Sprintf(\"unrecognized `run` subcommand: %+v\", fields)\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn buf.String()\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unrecognized command %q\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc TestIndexedBatchReset(t *testing.T) {\n\tindexCount := func(sl *batchskl.Skiplist) int {\n\t\tcount := 0\n\t\titer := sl.NewIter(nil, nil)\n\t\tdefer iter.Close()\n\t\tfor k := iter.First(); k != nil; k = iter.Next() {\n\t\t\tcount++\n\t\t}\n\t\treturn count\n\t}\n\tdb, err := Open(\"\", &Options{\n\t\tFS: vfs.NewMem(),\n\t})\n\trequire.NoError(t, err)\n\tdefer db.Close()\n\tb := newIndexedBatch(db, DefaultComparer)\n\tstart := \"start-key\"\n\tend := \"end-key\"\n\tkey := \"test-key\"\n\tvalue := \"test-value\"\n\tb.DeleteRange([]byte(start), []byte(end), nil)\n\tb.Set([]byte(key), []byte(value), nil)\n\trequire.NoError(t, b.\n\t\tRangeKeySet([]byte(start), []byte(end), []byte(\"suffix\"), []byte(value), nil))\n\trequire.NotNil(t, b.rangeKeyIndex)\n\trequire.NotNil(t, b.rangeDelIndex)\n\trequire.NotNil(t, b.index)\n\trequire.Equal(t, 1, indexCount(b.index))\n\n\tb.Reset()\n\trequire.NotNil(t, b.comparer)\n\trequire.NotNil(t, b.index)\n\trequire.Nil(t, b.rangeDelIndex)\n\trequire.Nil(t, b.rangeKeyIndex)\n\n\tcount := func(ib *Batch) int {\n\t\titer, _ := ib.NewIter(nil)\n\t\tdefer iter.Close()\n\t\titer2, err := iter.Clone(CloneOptions{})\n\t\trequire.NoError(t, err)\n\t\tdefer iter2.Close()\n\t\titer3, err := ib.NewBatchOnlyIter(context.Background(), nil)\n\t\trequire.NoError(t, err)\n\t\tdefer iter3.Close()\n\t\tvar count [3]int\n\t\tfor i, it := range []*Iterator{iter, iter2, iter3} {\n\t\t\tfor it.First(); it.Valid(); it.Next() {\n\t\t\t\tcount[i]++\n\t\t\t}\n\t\t}\n\t\trequire.Equal(t, count[0], count[1])\n\t\trequire.Equal(t, count[0], count[2])\n\t\treturn count[0]\n\t}\n\tcontains := func(ib *Batch, key, value string) bool {\n\t\titer, _ := ib.NewIter(nil)\n\t\tdefer iter.Close()\n\t\titer2, err := iter.Clone(CloneOptions{})\n\t\trequire.NoError(t, err)\n\t\tdefer iter2.Close()\n\t\titer3, err := ib.NewBatchOnlyIter(context.Background(), nil)\n\t\trequire.NoError(t, err)\n\t\tdefer iter3.Close()\n\t\tvar found [3]bool\n\t\tfor i, it := range []*Iterator{iter, iter2, iter3} {\n\t\t\tfor it.First(); it.Valid(); it.Next() {\n\t\t\t\tif string(it.Key()) == key &&\n\t\t\t\t\tstring(it.Value()) == value {\n\t\t\t\t\tfound[i] = true\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\trequire.Equal(t, found[0], found[1])\n\t\trequire.Equal(t, found[0], found[2])\n\t\treturn found[0]\n\t}\n\t// Set a key and check whether the key-value pair is visible.\n\tb.Set([]byte(key), []byte(value), nil)\n\trequire.Equal(t, 1, indexCount(b.index))\n\trequire.Equal(t, 1, count(b))\n\trequire.True(t, contains(b, key, value))\n\n\t// Use range delete to delete the above inserted key-value pair.\n\tb.DeleteRange([]byte(key), []byte(value), nil)\n\trequire.NotNil(t, b.rangeDelIndex)\n\trequire.Equal(t, 1, indexCount(b.rangeDelIndex))\n\trequire.Equal(t, 0, count(b))\n\trequire.False(t, contains(b, key, value))\n}\n\n// TestIndexedBatchMutation tests mutating an indexed batch with an open\n// iterator.\nfunc TestIndexedBatchMutation(t *testing.T) {\n\topts := &Options{\n\t\tComparer:           testkeys.Comparer,\n\t\tFS:                 vfs.NewMem(),\n\t\tFormatMajorVersion: internalFormatNewest,\n\t}\n\td, err := Open(\"\", opts)\n\trequire.NoError(t, err)\n\tdefer func() { d.Close() }()\n\n\tb := newIndexedBatch(d, DefaultComparer)\n\titers := map[string]*Iterator{}\n\tdefer func() {\n\t\tfor _, iter := range iters {\n\t\t\trequire.NoError(t, iter.Close())\n\t\t}\n\t}()\n\n\tdatadriven.RunTest(t, \"testdata/indexed_batch_mutation\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"batch\":\n\t\t\twriteBatch := newBatch(d)\n\t\t\tif err := runBatchDefineCmd(td, writeBatch); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tif err := writeBatch.Commit(nil); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\t\tcase \"new-batch-iter\":\n\t\t\tname := td.CmdArgs[0].String()\n\t\t\titers[name], _ = b.NewIter(&IterOptions{\n\t\t\t\tKeyTypes: IterKeyTypePointsAndRanges,\n\t\t\t})\n\t\t\treturn \"\"\n\t\tcase \"new-batch-only-iter\":\n\t\t\tname := td.CmdArgs[0].String()\n\t\t\titers[name], _ = b.NewBatchOnlyIter(context.Background(), &IterOptions{\n\t\t\t\tKeyTypes: IterKeyTypePointsAndRanges,\n\t\t\t})\n\t\t\treturn \"\"\n\t\tcase \"new-db-iter\":\n\t\t\tname := td.CmdArgs[0].String()\n\t\t\titers[name], _ = d.NewIter(&IterOptions{\n\t\t\t\tKeyTypes: IterKeyTypePointsAndRanges,\n\t\t\t})\n\t\t\treturn \"\"\n\t\tcase \"new-batch\":\n\t\t\tif b != nil {\n\t\t\t\trequire.NoError(t, b.Close())\n\t\t\t}\n\t\t\tb = newIndexedBatch(d, opts.Comparer)\n\t\t\tif err := runBatchDefineCmd(td, b); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\t\tcase \"flush\":\n\t\t\trequire.NoError(t, d.Flush())\n\t\t\treturn \"\"\n\t\tcase \"iter\":\n\t\t\tvar iter string\n\t\t\ttd.ScanArgs(t, \"iter\", &iter)\n\t\t\treturn runIterCmd(td, iters[iter], false /* closeIter */)\n\t\tcase \"mutate\":\n\t\t\tmut := newBatch(d)\n\t\t\tif err := runBatchDefineCmd(td, mut); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tif err := b.Apply(mut, nil); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\t\tcase \"clone\":\n\t\t\tvar from, to string\n\t\t\tvar refreshBatchView bool\n\t\t\ttd.ScanArgs(t, \"from\", &from)\n\t\t\ttd.ScanArgs(t, \"to\", &to)\n\t\t\ttd.ScanArgs(t, \"refresh-batch\", &refreshBatchView)\n\t\t\tvar err error\n\t\t\titers[to], err = iters[from].Clone(CloneOptions{RefreshBatchView: refreshBatchView})\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\t\tcase \"reset\":\n\t\t\tfor key, iter := range iters {\n\t\t\t\tif err := iter.Close(); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\tdelete(iters, key)\n\t\t\t}\n\t\t\tif d != nil {\n\t\t\t\tif err := d.Close(); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t}\n\t\t\topts.FS = vfs.NewMem()\n\t\t\td, err = Open(\"\", opts)\n\t\t\trequire.NoError(t, err)\n\t\t\treturn \"\"\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unrecognized command %q\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc TestIndexedBatch_GlobalVisibility(t *testing.T) {\n\topts := &Options{\n\t\tFS:                 vfs.NewMem(),\n\t\tFormatMajorVersion: internalFormatNewest,\n\t\tComparer:           testkeys.Comparer,\n\t}\n\td, err := Open(\"\", opts)\n\trequire.NoError(t, err)\n\tdefer d.Close()\n\n\trequire.NoError(t, d.Set([]byte(\"foo\"), []byte(\"foo\"), nil))\n\n\t// Create an iterator over an empty indexed batch.\n\tb := newIndexedBatch(d, DefaultComparer)\n\titerOpts := IterOptions{KeyTypes: IterKeyTypePointsAndRanges}\n\titer, _ := b.NewIter(&iterOpts)\n\tdefer iter.Close()\n\n\t// Mutate the database's committed state.\n\tmut := newBatch(d)\n\trequire.NoError(t, mut.Set([]byte(\"bar\"), []byte(\"bar\"), nil))\n\trequire.NoError(t, mut.DeleteRange([]byte(\"e\"), []byte(\"g\"), nil))\n\trequire.NoError(t, mut.RangeKeySet([]byte(\"a\"), []byte(\"c\"), []byte(\"@1\"), []byte(\"v\"), nil))\n\trequire.NoError(t, mut.Commit(nil))\n\n\tscanIter := func() string {\n\t\tvar buf bytes.Buffer\n\t\tfor valid := iter.First(); valid; valid = iter.Next() {\n\t\t\tfmt.Fprintf(&buf, \"%s: (\", iter.Key())\n\t\t\thasPoint, hasRange := iter.HasPointAndRange()\n\t\t\tif hasPoint {\n\t\t\t\tfmt.Fprintf(&buf, \"%s,\", iter.Value())\n\t\t\t} else {\n\t\t\t\tfmt.Fprintf(&buf, \".,\")\n\t\t\t}\n\t\t\tif hasRange {\n\t\t\t\tstart, end := iter.RangeBounds()\n\t\t\t\tfmt.Fprintf(&buf, \"[%s-%s)\", start, end)\n\t\t\t\twriteRangeKeys(&buf, iter)\n\t\t\t} else {\n\t\t\t\tfmt.Fprintf(&buf, \".\")\n\t\t\t}\n\t\t\tfmt.Fprintln(&buf, \")\")\n\t\t}\n\t\treturn strings.TrimSpace(buf.String())\n\t}\n\t// Scanning the iterator should only see the point key written before the\n\t// iterator was constructed.\n\trequire.Equal(t, `foo: (foo,.)`, scanIter())\n\n\t// After calling SetOptions, the iterator should still only see the point\n\t// key written before the iterator was constructed. SetOptions refreshes the\n\t// iterator's view of its own indexed batch, but not committed state.\n\titer.SetOptions(&iterOpts)\n\trequire.Equal(t, `foo: (foo,.)`, scanIter())\n}\n\nfunc TestFlushableBatchReset(t *testing.T) {\n\tvar b Batch\n\tvar err error\n\tb.flushable, err = newFlushableBatch(&b, DefaultComparer)\n\trequire.NoError(t, err)\n\n\tb.Reset()\n\trequire.Nil(t, b.flushable)\n}\n\nfunc TestBatchIncrement(t *testing.T) {\n\ttestCases := []uint32{\n\t\t0x00000000,\n\t\t0x00000001,\n\t\t0x00000002,\n\t\t0x0000007f,\n\t\t0x00000080,\n\t\t0x000000fe,\n\t\t0x000000ff,\n\t\t0x00000100,\n\t\t0x00000101,\n\t\t0x000001ff,\n\t\t0x00000200,\n\t\t0x00000fff,\n\t\t0x00001234,\n\t\t0x0000fffe,\n\t\t0x0000ffff,\n\t\t0x00010000,\n\t\t0x00010001,\n\t\t0x000100fe,\n\t\t0x000100ff,\n\t\t0x00020100,\n\t\t0x03fffffe,\n\t\t0x03ffffff,\n\t\t0x04000000,\n\t\t0x04000001,\n\t\t0x7fffffff,\n\t\t0xfffffffe,\n\t}\n\tfor _, tc := range testCases {\n\t\tvar buf [batchrepr.HeaderLen]byte\n\t\tbinary.LittleEndian.PutUint32(buf[8:12], tc)\n\t\tvar b Batch\n\t\tb.SetRepr(buf[:])\n\t\tb.count++\n\t\twant := tc + 1\n\t\th, ok := batchrepr.ReadHeader(b.Repr())\n\t\trequire.True(t, ok)\n\t\tif h.Count != want {\n\t\t\tt.Errorf(\"input=%d: got %d, want %d\", tc, h.Count, want)\n\t\t}\n\t}\n\n\terr := func() (err error) {\n\t\tdefer func() {\n\t\t\tif v := recover(); v != nil {\n\t\t\t\tif verr, ok := v.(error); ok {\n\t\t\t\t\terr = verr\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\t\tvar buf [batchrepr.HeaderLen]byte\n\t\tbinary.LittleEndian.PutUint32(buf[8:12], 0xffffffff)\n\t\tvar b Batch\n\t\tb.SetRepr(buf[:])\n\t\tb.count++\n\t\tb.Repr()\n\t\treturn nil\n\t}()\n\tif err != ErrInvalidBatch {\n\t\tt.Fatalf(\"expected %v, but found %v\", ErrInvalidBatch, err)\n\t}\n}\n\nfunc TestBatchOpDoesIncrement(t *testing.T) {\n\tvar b Batch\n\tkey := []byte(\"foo\")\n\tvalue := []byte(\"bar\")\n\n\tif b.Count() != 0 {\n\t\tt.Fatalf(\"new batch has a nonzero count: %d\", b.Count())\n\t}\n\n\t// Should increment count by 1\n\t_ = b.Set(key, value, nil)\n\tif b.Count() != 1 {\n\t\tt.Fatalf(\"expected count: %d, got %d\", 1, b.Count())\n\t}\n\n\tvar b2 Batch\n\t// Should increment count by 1 each\n\t_ = b2.Set(key, value, nil)\n\t_ = b2.Delete(key, nil)\n\tif b2.Count() != 2 {\n\t\tt.Fatalf(\"expected count: %d, got %d\", 2, b2.Count())\n\t}\n\n\t// Should increment count by b2.count()\n\t_ = b.Apply(&b2, nil)\n\tif b.Count() != 3 {\n\t\tt.Fatalf(\"expected count: %d, got %d\", 3, b.Count())\n\t}\n\n\t// Should increment count by 1\n\t_ = b.Merge(key, value, nil)\n\tif b.Count() != 4 {\n\t\tt.Fatalf(\"expected count: %d, got %d\", 4, b.Count())\n\t}\n\n\t// Should NOT increment count.\n\t_ = b.LogData([]byte(\"foobarbaz\"), nil)\n\tif b.Count() != 4 {\n\t\tt.Fatalf(\"expected count: %d, got %d\", 4, b.Count())\n\t}\n}\n\nfunc TestBatchGet(t *testing.T) {\n\ttestCases := []struct {\n\t\tmethod       string\n\t\tmemTableSize uint64\n\t}{\n\t\t{\"build\", 64 << 20},\n\t\t{\"build\", 2 << 10},\n\t\t{\"apply\", 64 << 20},\n\t}\n\n\tfor _, c := range testCases {\n\t\tt.Run(fmt.Sprintf(\"%s,mem=%d\", c.method, c.memTableSize), func(t *testing.T) {\n\t\t\td, err := Open(\"\", &Options{\n\t\t\t\tFS:           vfs.NewMem(),\n\t\t\t\tMemTableSize: c.memTableSize,\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Open: %v\", err)\n\t\t\t}\n\t\t\tdefer d.Close()\n\t\t\tvar b *Batch\n\n\t\t\tdatadriven.RunTest(t, \"testdata/batch_get\", func(t *testing.T, td *datadriven.TestData) string {\n\t\t\t\tswitch td.Cmd {\n\t\t\t\tcase \"define\":\n\t\t\t\t\tswitch c.method {\n\t\t\t\t\tcase \"build\":\n\t\t\t\t\t\tb = d.NewIndexedBatch()\n\t\t\t\t\tcase \"apply\":\n\t\t\t\t\t\tb = d.NewBatch()\n\t\t\t\t\t}\n\n\t\t\t\t\tif err := runBatchDefineCmd(td, b); err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\n\t\t\t\t\tswitch c.method {\n\t\t\t\t\tcase \"apply\":\n\t\t\t\t\t\ttmp := d.NewIndexedBatch()\n\t\t\t\t\t\ttmp.Apply(b, nil)\n\t\t\t\t\t\tb = tmp\n\t\t\t\t\t}\n\t\t\t\t\treturn \"\"\n\n\t\t\t\tcase \"commit\":\n\t\t\t\t\tif err := b.Commit(nil); err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t\tb = nil\n\t\t\t\t\treturn \"\"\n\n\t\t\t\tcase \"get\":\n\t\t\t\t\tif len(td.CmdArgs) != 1 {\n\t\t\t\t\t\treturn fmt.Sprintf(\"%s expects 1 argument\", td.Cmd)\n\t\t\t\t\t}\n\t\t\t\t\tv, closer, err := b.Get([]byte(td.CmdArgs[0].String()))\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t\tdefer closer.Close()\n\t\t\t\t\treturn string(v)\n\n\t\t\t\tdefault:\n\t\t\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t\t\t}\n\t\t\t})\n\t\t})\n\t}\n}\n\nfunc TestBatchIter(t *testing.T) {\n\tvar b *Batch\n\n\tfor _, method := range []string{\"build\", \"apply\"} {\n\t\tfor _, testdata := range []string{\n\t\t\t\"testdata/internal_iter_next\", \"testdata/internal_iter_bounds\"} {\n\t\t\tt.Run(method, func(t *testing.T) {\n\t\t\t\tdatadriven.RunTest(t, testdata, func(t *testing.T, d *datadriven.TestData) string {\n\t\t\t\t\tswitch d.Cmd {\n\t\t\t\t\tcase \"define\":\n\t\t\t\t\t\tswitch method {\n\t\t\t\t\t\tcase \"build\":\n\t\t\t\t\t\t\tb = newIndexedBatch(nil, DefaultComparer)\n\t\t\t\t\t\tcase \"apply\":\n\t\t\t\t\t\t\tb = newBatch(nil)\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tfor _, key := range strings.Split(d.Input, \"\\n\") {\n\t\t\t\t\t\t\tj := strings.Index(key, \":\")\n\t\t\t\t\t\t\tikey := base.ParseInternalKey(key[:j])\n\t\t\t\t\t\t\tvalue := []byte(key[j+1:])\n\t\t\t\t\t\t\tb.Set(ikey.UserKey, value, nil)\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tswitch method {\n\t\t\t\t\t\tcase \"apply\":\n\t\t\t\t\t\t\ttmp := newIndexedBatch(nil, DefaultComparer)\n\t\t\t\t\t\t\ttmp.Apply(b, nil)\n\t\t\t\t\t\t\tb = tmp\n\t\t\t\t\t\t}\n\t\t\t\t\t\treturn \"\"\n\n\t\t\t\t\tcase \"iter\":\n\t\t\t\t\t\tvar options IterOptions\n\t\t\t\t\t\tfor _, arg := range d.CmdArgs {\n\t\t\t\t\t\t\tswitch arg.Key {\n\t\t\t\t\t\t\tcase \"lower\":\n\t\t\t\t\t\t\t\tif len(arg.Vals) != 1 {\n\t\t\t\t\t\t\t\t\treturn fmt.Sprintf(\n\t\t\t\t\t\t\t\t\t\t\"%s expects at most 1 value for lower\", d.Cmd)\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\toptions.LowerBound = []byte(arg.Vals[0])\n\t\t\t\t\t\t\tcase \"upper\":\n\t\t\t\t\t\t\t\tif len(arg.Vals) != 1 {\n\t\t\t\t\t\t\t\t\treturn fmt.Sprintf(\n\t\t\t\t\t\t\t\t\t\t\"%s expects at most 1 value for upper\", d.Cmd)\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\toptions.UpperBound = []byte(arg.Vals[0])\n\t\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t\t\treturn fmt.Sprintf(\"unknown arg: %s\", arg.Key)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\titer := b.newInternalIter(&options)\n\t\t\t\t\t\tdefer iter.Close()\n\t\t\t\t\t\treturn itertest.RunInternalIterCmd(t, d, iter)\n\n\t\t\t\t\tdefault:\n\t\t\t\t\t\treturn fmt.Sprintf(\"unknown command: %s\", d.Cmd)\n\t\t\t\t\t}\n\t\t\t\t})\n\t\t\t})\n\t\t}\n\t}\n}\n\nfunc TestBatchRangeOps(t *testing.T) {\n\tvar b *Batch\n\n\tdatadriven.RunTest(t, \"testdata/batch_range_ops\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"clear\":\n\t\t\tb = nil\n\t\t\treturn \"\"\n\n\t\tcase \"apply\":\n\t\t\tif b == nil {\n\t\t\t\tb = newIndexedBatch(nil, DefaultComparer)\n\t\t\t}\n\t\t\tt := newBatch(nil)\n\t\t\tif err := runBatchDefineCmd(td, t); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tif err := b.Apply(t, nil); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"define\":\n\t\t\tif b == nil {\n\t\t\t\tb = newIndexedBatch(nil, DefaultComparer)\n\t\t\t}\n\t\t\tif err := runBatchDefineCmd(td, b); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"scan\":\n\t\t\tif len(td.CmdArgs) > 1 {\n\t\t\t\treturn fmt.Sprintf(\"%s expects at most 1 argument\", td.Cmd)\n\t\t\t}\n\t\t\tvar fragmentIter keyspan.FragmentIterator\n\t\t\tvar internalIter base.InternalIterator\n\t\t\tswitch {\n\t\t\tcase td.HasArg(\"range-del\"):\n\t\t\t\tfragmentIter = b.newRangeDelIter(nil, math.MaxUint64)\n\t\t\t\tdefer fragmentIter.Close()\n\t\t\tcase td.HasArg(\"range-key\"):\n\t\t\t\tfragmentIter = b.newRangeKeyIter(nil, math.MaxUint64)\n\t\t\t\tdefer fragmentIter.Close()\n\t\t\tdefault:\n\t\t\t\tinternalIter = b.newInternalIter(nil)\n\t\t\t\tdefer internalIter.Close()\n\t\t\t}\n\n\t\t\tvar buf bytes.Buffer\n\t\t\tif fragmentIter != nil {\n\t\t\t\ts, err := fragmentIter.First()\n\t\t\t\tfor ; s != nil; s, err = fragmentIter.Next() {\n\t\t\t\t\tfor i := range s.Keys {\n\t\t\t\t\t\ts.Keys[i].Trailer = base.MakeTrailer(\n\t\t\t\t\t\t\ts.Keys[i].SeqNum()&^base.SeqNumBatchBit,\n\t\t\t\t\t\t\ts.Keys[i].Kind(),\n\t\t\t\t\t\t)\n\t\t\t\t\t}\n\t\t\t\t\tfmt.Fprintln(&buf, s)\n\t\t\t\t}\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tfor kv := internalIter.First(); kv != nil; kv = internalIter.Next() {\n\t\t\t\t\tkv.K.SetSeqNum(kv.K.SeqNum() &^ base.SeqNumBatchBit)\n\t\t\t\t\tfmt.Fprintf(&buf, \"%s:%s\\n\", kv.K, kv.InPlaceValue())\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn buf.String()\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc TestBatchTooLarge(t *testing.T) {\n\tvar b Batch\n\tvar result interface{}\n\tfunc() {\n\t\tdefer func() {\n\t\t\tif r := recover(); r != nil {\n\t\t\t\tresult = r\n\t\t\t}\n\t\t}()\n\t\tb.grow(maxBatchSize)\n\t}()\n\trequire.EqualValues(t, ErrBatchTooLarge, result)\n}\n\nfunc TestFlushableBatchIter(t *testing.T) {\n\tvar b *flushableBatch\n\tdatadriven.RunTest(t, \"testdata/internal_iter_next\", func(t *testing.T, d *datadriven.TestData) string {\n\t\tswitch d.Cmd {\n\t\tcase \"define\":\n\t\t\tbatch := newBatch(nil)\n\t\t\tfor _, key := range strings.Split(d.Input, \"\\n\") {\n\t\t\t\tj := strings.Index(key, \":\")\n\t\t\t\tikey := base.ParseInternalKey(key[:j])\n\t\t\t\tvalue := []byte(fmt.Sprint(ikey.SeqNum()))\n\t\t\t\tbatch.Set(ikey.UserKey, value, nil)\n\t\t\t}\n\t\t\tvar err error\n\t\t\tb, err = newFlushableBatch(batch, DefaultComparer)\n\t\t\trequire.NoError(t, err)\n\t\t\treturn \"\"\n\n\t\tcase \"iter\":\n\t\t\titer := b.newIter(nil)\n\t\t\tdefer iter.Close()\n\t\t\treturn itertest.RunInternalIterCmd(t, d, iter)\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", d.Cmd)\n\t\t}\n\t})\n}\n\nfunc TestFlushableBatch(t *testing.T) {\n\tvar b *flushableBatch\n\tdatadriven.RunTest(t, \"testdata/flushable_batch\", func(t *testing.T, d *datadriven.TestData) string {\n\t\tswitch d.Cmd {\n\t\tcase \"define\":\n\t\t\tbatch := newBatch(nil)\n\t\t\tfor _, key := range strings.Split(d.Input, \"\\n\") {\n\t\t\t\tj := strings.Index(key, \":\")\n\t\t\t\tikey := base.ParseInternalKey(key[:j])\n\t\t\t\tvalue := []byte(key[j+1:])\n\t\t\t\tif len(value) == 0 {\n\t\t\t\t\tvalue = []byte(fmt.Sprintf(\"%d\", ikey.SeqNum()))\n\t\t\t\t}\n\t\t\t\tswitch ikey.Kind() {\n\t\t\t\tcase InternalKeyKindDelete:\n\t\t\t\t\trequire.NoError(t, batch.Delete(ikey.UserKey, nil))\n\t\t\t\tcase InternalKeyKindSet:\n\t\t\t\t\trequire.NoError(t, batch.Set(ikey.UserKey, value, nil))\n\t\t\t\tcase InternalKeyKindMerge:\n\t\t\t\t\trequire.NoError(t, batch.Merge(ikey.UserKey, value, nil))\n\t\t\t\tcase InternalKeyKindLogData:\n\t\t\t\t\trequire.NoError(t, batch.LogData(ikey.UserKey, nil))\n\t\t\t\tcase InternalKeyKindRangeDelete:\n\t\t\t\t\trequire.NoError(t, batch.DeleteRange(ikey.UserKey, value, nil))\n\t\t\t\tcase InternalKeyKindRangeKeyDelete:\n\t\t\t\t\trequire.NoError(t, batch.RangeKeyDelete(ikey.UserKey, value, nil))\n\t\t\t\tcase InternalKeyKindRangeKeySet:\n\t\t\t\t\trequire.NoError(t, batch.RangeKeySet(ikey.UserKey, value, value, value, nil))\n\t\t\t\tcase InternalKeyKindRangeKeyUnset:\n\t\t\t\t\trequire.NoError(t, batch.RangeKeyUnset(ikey.UserKey, value, value, nil))\n\t\t\t\t}\n\t\t\t}\n\t\t\tvar err error\n\t\t\tb, err = newFlushableBatch(batch, DefaultComparer)\n\t\t\trequire.NoError(t, err)\n\t\t\treturn \"\"\n\n\t\tcase \"iter\":\n\t\t\tvar opts IterOptions\n\t\t\tfor _, arg := range d.CmdArgs {\n\t\t\t\tif len(arg.Vals) != 1 {\n\t\t\t\t\treturn fmt.Sprintf(\"%s: %s=<value>\", d.Cmd, arg.Key)\n\t\t\t\t}\n\t\t\t\tswitch arg.Key {\n\t\t\t\tcase \"lower\":\n\t\t\t\t\topts.LowerBound = []byte(arg.Vals[0])\n\t\t\t\tcase \"upper\":\n\t\t\t\t\topts.UpperBound = []byte(arg.Vals[0])\n\t\t\t\tdefault:\n\t\t\t\t\treturn fmt.Sprintf(\"%s: unknown arg: %s\", d.Cmd, arg.Key)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\titer := b.newIter(&opts)\n\t\t\tdefer iter.Close()\n\t\t\treturn itertest.RunInternalIterCmd(t, d, iter)\n\n\t\tcase \"dump\":\n\t\t\tif len(d.CmdArgs) != 1 || len(d.CmdArgs[0].Vals) != 1 || d.CmdArgs[0].Key != \"seq\" {\n\t\t\t\treturn \"dump seq=<value>\\n\"\n\t\t\t}\n\t\t\tseqNum := base.ParseSeqNum(d.CmdArgs[0].Vals[0])\n\t\t\tb.setSeqNum(seqNum)\n\n\t\t\tvar buf bytes.Buffer\n\n\t\t\titer := b.newIter(nil)\n\t\t\tfor kv := iter.First(); kv != nil; kv = iter.Next() {\n\t\t\t\tfmt.Fprintf(&buf, \"%s:%s\\n\", kv.K, kv.InPlaceValue())\n\t\t\t}\n\t\t\titer.Close()\n\n\t\t\tif rangeDelIter := b.newRangeDelIter(nil); rangeDelIter != nil {\n\t\t\t\tscanKeyspanIterator(&buf, rangeDelIter)\n\t\t\t\trangeDelIter.Close()\n\t\t\t}\n\t\t\tif rangeKeyIter := b.newRangeKeyIter(nil); rangeKeyIter != nil {\n\t\t\t\tscanKeyspanIterator(&buf, rangeKeyIter)\n\t\t\t\trangeKeyIter.Close()\n\t\t\t}\n\t\t\treturn buf.String()\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", d.Cmd)\n\t\t}\n\t})\n}\n\nfunc TestFlushableBatchDeleteRange(t *testing.T) {\n\tvar fb *flushableBatch\n\tvar input string\n\n\tdatadriven.RunTest(t, \"testdata/delete_range\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"clear\":\n\t\t\tinput = \"\"\n\t\t\treturn \"\"\n\n\t\tcase \"define\":\n\t\t\tb := newBatch(nil)\n\t\t\t// NB: We can't actually add to the flushable batch as we can to a\n\t\t\t// memtable (which shares the \"testdata/delete_range\" data), so we fake\n\t\t\t// it by concatenating the input and rebuilding the flushable batch from\n\t\t\t// scratch.\n\t\t\tinput += \"\\n\" + td.Input\n\t\t\ttd.Input = input\n\t\t\tif err := runBatchDefineCmd(td, b); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tvar err error\n\t\t\tfb, err = newFlushableBatch(b, DefaultComparer)\n\t\t\trequire.NoError(t, err)\n\t\t\treturn \"\"\n\n\t\tcase \"scan\":\n\t\t\tvar buf bytes.Buffer\n\t\t\tif td.HasArg(\"range-del\") {\n\t\t\t\tfi := fb.newRangeDelIter(nil)\n\t\t\t\tdefer fi.Close()\n\t\t\t\tscanKeyspanIterator(&buf, fi)\n\t\t\t} else {\n\t\t\t\tii := fb.newIter(nil)\n\t\t\t\tdefer ii.Close()\n\t\t\t\tscanInternalIter(&buf, ii)\n\t\t\t}\n\t\t\treturn buf.String()\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc scanInternalIter(w io.Writer, ii internalIterator) {\n\tfor kv := ii.First(); kv != nil; kv = ii.Next() {\n\t\tfmt.Fprintf(w, \"%s:%s\\n\", kv.K, kv.InPlaceValue())\n\t}\n}\n\nfunc scanKeyspanIterator(w io.Writer, ki keyspan.FragmentIterator) {\n\ts, err := ki.First()\n\tfor ; s != nil; s, err = ki.Next() {\n\t\tfmt.Fprintln(w, s)\n\t}\n\tif err != nil {\n\t\tfmt.Fprintf(w, \"err=%q\", err.Error())\n\t}\n}\n\nfunc TestEmptyFlushableBatch(t *testing.T) {\n\t// Verify that we can create a flushable batch on an empty batch.\n\tfb, err := newFlushableBatch(newBatch(nil), DefaultComparer)\n\trequire.NoError(t, err)\n\tit := fb.newIter(nil)\n\trequire.Nil(t, it.First())\n}\n\nfunc TestBatchCommitStats(t *testing.T) {\n\ttestFunc := func() error {\n\t\tdb, err := Open(\"\", &Options{\n\t\t\tFS: vfs.NewMem(),\n\t\t})\n\t\trequire.NoError(t, err)\n\t\tdefer db.Close()\n\t\tb := db.NewBatch()\n\t\tdefer b.Close()\n\t\tstats := b.CommitStats()\n\t\trequire.Equal(t, BatchCommitStats{}, stats)\n\n\t\t// The stall code peers into the internals, instead of adding general\n\t\t// purpose hooks, to avoid changing production code. We can revisit this\n\t\t// choice if it becomes hard to maintain.\n\n\t\t// Commit semaphore stall funcs.\n\t\tvar unstallCommitSemaphore func()\n\t\tstallCommitSemaphore := func() {\n\t\t\tcommitPipeline := db.commit\n\t\t\tcommitSemaphoreReserved := 0\n\t\t\tdone := false\n\t\t\tfor !done {\n\t\t\t\tselect {\n\t\t\t\tcase commitPipeline.commitQueueSem <- struct{}{}:\n\t\t\t\t\tcommitSemaphoreReserved++\n\t\t\t\tdefault:\n\t\t\t\t\tdone = true\n\t\t\t\t}\n\t\t\t\tif done {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tunstallCommitSemaphore = func() {\n\t\t\t\tfor i := 0; i < commitSemaphoreReserved; i++ {\n\t\t\t\t\t<-commitPipeline.commitQueueSem\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// Memstable stall funcs.\n\t\tvar unstallMemtable func()\n\t\tstallMemtable := func() {\n\t\t\tdb.mu.Lock()\n\t\t\tdefer db.mu.Unlock()\n\t\t\tprev := db.opts.MemTableStopWritesThreshold\n\t\t\tdb.opts.MemTableStopWritesThreshold = 0\n\t\t\tunstallMemtable = func() {\n\t\t\t\tdb.mu.Lock()\n\t\t\t\tdefer db.mu.Unlock()\n\t\t\t\tdb.opts.MemTableStopWritesThreshold = prev\n\t\t\t\tdb.mu.compact.cond.Broadcast()\n\t\t\t}\n\t\t}\n\n\t\t// L0 read-amp stall funcs.\n\t\tvar unstallL0ReadAmp func()\n\t\tstallL0ReadAmp := func() {\n\t\t\tdb.mu.Lock()\n\t\t\tdefer db.mu.Unlock()\n\t\t\tprev := db.opts.L0StopWritesThreshold\n\t\t\tdb.opts.L0StopWritesThreshold = 0\n\t\t\tunstallL0ReadAmp = func() {\n\t\t\t\tdb.mu.Lock()\n\t\t\t\tdefer db.mu.Unlock()\n\t\t\t\tdb.opts.L0StopWritesThreshold = prev\n\t\t\t\tdb.mu.compact.cond.Broadcast()\n\t\t\t}\n\t\t}\n\n\t\t// Commit wait stall funcs.\n\t\tvar unstallCommitWait func()\n\t\tstallCommitWait := func() {\n\t\t\tb.commit.Add(1)\n\t\t\tunstallCommitWait = func() {\n\t\t\t\tb.commit.Done()\n\t\t\t}\n\t\t}\n\n\t\t// Stall everything.\n\t\tstallCommitSemaphore()\n\t\tstallMemtable()\n\t\tstallL0ReadAmp()\n\t\tstallCommitWait()\n\n\t\t// Exceed initialMemTableSize -- this is needed to make stallMemtable work.\n\t\trequire.NoError(t, b.Set(make([]byte, initialMemTableSize), nil, nil))\n\n\t\tvar commitWG sync.WaitGroup\n\t\tcommitWG.Add(1)\n\t\tgo func() {\n\t\t\trequire.NoError(t, db.Apply(b, &WriteOptions{Sync: true}))\n\t\t\tcommitWG.Done()\n\t\t}()\n\t\t// Unstall things in the order that the stalls will happen.\n\t\tsleepDuration := 10 * time.Millisecond\n\t\ttime.Sleep(sleepDuration)\n\t\tunstallCommitSemaphore()\n\t\ttime.Sleep(sleepDuration)\n\t\tunstallMemtable()\n\t\ttime.Sleep(sleepDuration)\n\t\tunstallL0ReadAmp()\n\t\ttime.Sleep(sleepDuration)\n\t\tunstallCommitWait()\n\n\t\t// Wait for Apply to return.\n\t\tcommitWG.Wait()\n\t\tstats = b.CommitStats()\n\t\texpectedDuration := (2 * sleepDuration) / 3\n\t\tif expectedDuration > stats.SemaphoreWaitDuration {\n\t\t\treturn errors.Errorf(\"SemaphoreWaitDuration %s is too low\",\n\t\t\t\tstats.SemaphoreWaitDuration.String())\n\t\t}\n\t\tif expectedDuration > stats.MemTableWriteStallDuration {\n\t\t\treturn errors.Errorf(\"MemTableWriteStallDuration %s is too low\",\n\t\t\t\tstats.MemTableWriteStallDuration.String())\n\t\t}\n\t\tif expectedDuration > stats.L0ReadAmpWriteStallDuration {\n\t\t\treturn errors.Errorf(\"L0ReadAmpWriteStallDuration %s is too low\",\n\t\t\t\tstats.L0ReadAmpWriteStallDuration)\n\t\t}\n\t\tif expectedDuration > stats.CommitWaitDuration {\n\t\t\treturn errors.Errorf(\"CommitWaitDuration %s is too low\",\n\t\t\t\tstats.CommitWaitDuration)\n\t\t}\n\t\tif 5*expectedDuration > stats.TotalDuration {\n\t\t\treturn errors.Errorf(\"TotalDuration %s is too low\",\n\t\t\t\tstats.TotalDuration)\n\t\t}\n\t\treturn nil\n\t}\n\t// Try a few times, and succeed if one of them succeeds.\n\tvar err error\n\tfor i := 0; i < 5; i++ {\n\t\terr = testFunc()\n\t\tif err == nil {\n\t\t\tbreak\n\t\t}\n\t}\n\trequire.NoError(t, err)\n}\n\n// TestBatchLogDataMemtableSize tests that LogDatas never contribute to memtable\n// size.\nfunc TestBatchLogDataMemtableSize(t *testing.T) {\n\t// Create a batch with Set(\"foo\", \"bar\") and a LogData. Only the Set should\n\t// contribute to the batch's memtable size.\n\tb := Batch{}\n\trequire.NoError(t, b.Set([]byte(\"foo\"), []byte(\"bar\"), nil))\n\trequire.Equal(t, uint64(201), b.memTableSize)\n\trequire.NoError(t, b.LogData([]byte(\"baxbarbaz\"), nil))\n\trequire.Equal(t, uint64(201), b.memTableSize)\n\n\tt.Run(\"SetRepr\", func(t *testing.T) {\n\t\t// Setting another batch's repr using SetRepr should result in a\n\t\t// recalculation of the memtable size that matches.\n\t\ta := Batch{}\n\t\ta.db = new(DB)\n\t\trequire.NoError(t, a.SetRepr(b.Repr()))\n\t\trequire.Equal(t, uint64(201), a.memTableSize)\n\t})\n\tt.Run(\"Apply\", func(t *testing.T) {\n\t\t// Applying another batch using apply should result in a recalculation\n\t\t// of the memtable size that matches.\n\t\ta := Batch{}\n\t\ta.db = new(DB)\n\t\trequire.NoError(t, a.Apply(&b, nil))\n\t\trequire.Equal(t, uint64(201), a.memTableSize)\n\t})\n}\n\nfunc BenchmarkBatchSet(b *testing.B) {\n\tvalue := make([]byte, 10)\n\tfor i := range value {\n\t\tvalue[i] = byte(i)\n\t}\n\tkey := make([]byte, 8)\n\tbatch := newBatch(nil)\n\n\tb.ResetTimer()\n\n\tconst batchSize = 1000\n\tfor i := 0; i < b.N; i += batchSize {\n\t\tend := i + batchSize\n\t\tif end > b.N {\n\t\t\tend = b.N\n\t\t}\n\n\t\tfor j := i; j < end; j++ {\n\t\t\tbinary.BigEndian.PutUint64(key, uint64(j))\n\t\t\tbatch.Set(key, value, nil)\n\t\t}\n\t\tbatch.Reset()\n\t}\n\n\tb.StopTimer()\n}\n\nfunc BenchmarkIndexedBatchSet(b *testing.B) {\n\tvalue := make([]byte, 10)\n\tfor i := range value {\n\t\tvalue[i] = byte(i)\n\t}\n\tkey := make([]byte, 8)\n\tbatch := newIndexedBatch(nil, DefaultComparer)\n\n\tb.ResetTimer()\n\n\tconst batchSize = 1000\n\tfor i := 0; i < b.N; i += batchSize {\n\t\tend := i + batchSize\n\t\tif end > b.N {\n\t\t\tend = b.N\n\t\t}\n\n\t\tfor j := i; j < end; j++ {\n\t\t\tbinary.BigEndian.PutUint64(key, uint64(j))\n\t\t\tbatch.Set(key, value, nil)\n\t\t}\n\t\tbatch.Reset()\n\t}\n\n\tb.StopTimer()\n}\n\nfunc BenchmarkBatchSetDeferred(b *testing.B) {\n\tvalue := make([]byte, 10)\n\tfor i := range value {\n\t\tvalue[i] = byte(i)\n\t}\n\tkey := make([]byte, 8)\n\tbatch := newBatch(nil)\n\n\tb.ResetTimer()\n\n\tconst batchSize = 1000\n\tfor i := 0; i < b.N; i += batchSize {\n\t\tend := i + batchSize\n\t\tif end > b.N {\n\t\t\tend = b.N\n\t\t}\n\n\t\tfor j := i; j < end; j++ {\n\t\t\tbinary.BigEndian.PutUint64(key, uint64(j))\n\t\t\tdeferredOp := batch.SetDeferred(len(key), len(value))\n\n\t\t\tcopy(deferredOp.Key, key)\n\t\t\tcopy(deferredOp.Value, value)\n\n\t\t\tdeferredOp.Finish()\n\t\t}\n\t\tbatch.Reset()\n\t}\n\n\tb.StopTimer()\n}\n\nfunc BenchmarkIndexedBatchSetDeferred(b *testing.B) {\n\tvalue := make([]byte, 10)\n\tfor i := range value {\n\t\tvalue[i] = byte(i)\n\t}\n\tkey := make([]byte, 8)\n\tbatch := newIndexedBatch(nil, DefaultComparer)\n\n\tb.ResetTimer()\n\n\tconst batchSize = 1000\n\tfor i := 0; i < b.N; i += batchSize {\n\t\tend := i + batchSize\n\t\tif end > b.N {\n\t\t\tend = b.N\n\t\t}\n\n\t\tfor j := i; j < end; j++ {\n\t\t\tbinary.BigEndian.PutUint64(key, uint64(j))\n\t\t\tdeferredOp := batch.SetDeferred(len(key), len(value))\n\n\t\t\tcopy(deferredOp.Key, key)\n\t\t\tcopy(deferredOp.Value, value)\n\n\t\t\tdeferredOp.Finish()\n\t\t}\n\t\tbatch.Reset()\n\t}\n\n\tb.StopTimer()\n}\n\nfunc TestBatchMemTableSizeOverflow(t *testing.T) {\n\topts := &Options{\n\t\tFS: vfs.NewMem(),\n\t}\n\topts.EnsureDefaults()\n\td, err := Open(\"\", opts)\n\trequire.NoError(t, err)\n\n\tbigValue := make([]byte, 1000)\n\tb := d.NewBatch()\n\n\t// memTableSize can overflow as a uint32.\n\tb.memTableSize = math.MaxUint32 - 50\n\tfor i := 0; i < 10; i++ {\n\t\tk := fmt.Sprintf(\"key-%05d\", i)\n\t\trequire.NoError(t, b.Set([]byte(k), bigValue, nil))\n\t}\n\trequire.Greater(t, b.memTableSize, uint64(math.MaxUint32))\n\trequire.NoError(t, b.Close())\n\trequire.NoError(t, d.Close())\n}\n\n// TestBatchSpanCaching stress tests the caching of keyspan.Spans for range\n// tombstones and range keys.\nfunc TestBatchSpanCaching(t *testing.T) {\n\topts := &Options{\n\t\tComparer:           testkeys.Comparer,\n\t\tFS:                 vfs.NewMem(),\n\t\tFormatMajorVersion: internalFormatNewest,\n\t}\n\td, err := Open(\"\", opts)\n\trequire.NoError(t, err)\n\tdefer d.Close()\n\n\tks := testkeys.Alpha(1)\n\tb := d.NewIndexedBatch()\n\tfor i := int64(0); i < ks.Count(); i++ {\n\t\tk := testkeys.Key(ks, i)\n\t\trequire.NoError(t, b.Set(k, k, nil))\n\t}\n\n\tseed := uint64(time.Now().UnixNano())\n\tt.Logf(\"seed = %d\", seed)\n\trng := rand.New(rand.NewPCG(seed, seed))\n\titers := make([][]*Iterator, ks.Count())\n\tdefer func() {\n\t\tfor _, keyIters := range iters {\n\t\t\tfor _, iter := range keyIters {\n\t\t\t\t_ = iter.Close()\n\t\t\t}\n\t\t}\n\t}()\n\n\t// This test begins with one point key for every letter of the alphabet.\n\t// Over the course of the test, point keys are 'replaced' with range keys\n\t// with narrow bounds from left to right. Iterators are created at random,\n\t// sometimes from the batch and sometimes by cloning existing iterators.\n\n\tcheckIter := func(iter *Iterator, nextKey int64) {\n\t\tvar i int64\n\t\tfor valid := iter.First(); valid; valid = iter.Next() {\n\t\t\thasPoint, hasRange := iter.HasPointAndRange()\n\t\t\trequire.Equal(t, testkeys.Key(ks, i), iter.Key())\n\t\t\tif i < nextKey {\n\t\t\t\t// This key should not exist as a point key, just a range key.\n\t\t\t\trequire.False(t, hasPoint)\n\t\t\t\trequire.True(t, hasRange)\n\t\t\t} else {\n\t\t\t\trequire.True(t, hasPoint)\n\t\t\t\trequire.False(t, hasRange)\n\t\t\t}\n\t\t\ti++\n\t\t}\n\t\trequire.Equal(t, ks.Count(), i)\n\t}\n\n\t// Each iteration of the below loop either reads or writes.\n\t//\n\t// A write iteration writes a new RANGEDEL and RANGEKEYSET into the batch,\n\t// covering a single point key seeded above. Writing these two span keys\n\t// together 'replaces' the point key with a range key. Each write iteration\n\t// ratchets nextWriteKey so the next write iteration will write the next\n\t// key.\n\t//\n\t// A read iteration creates a new iterator and ensures its state is\n\t// expected: some prefix of only point keys, followed by a suffix of only\n\t// range keys. Iterators created through Clone should observe the point keys\n\t// that existed when the cloned iterator was created.\n\tfor nextWriteKey := int64(0); nextWriteKey < ks.Count(); {\n\t\tp := rng.Float64()\n\t\tswitch {\n\t\tcase p < .10: /* 10 % */\n\t\t\t// Write a new range deletion and range key.\n\t\t\tstart := testkeys.Key(ks, nextWriteKey)\n\t\t\tend := append(start, 0x00)\n\t\t\trequire.NoError(t, b.DeleteRange(start, end, nil))\n\t\t\trequire.NoError(t, b.RangeKeySet(start, end, nil, []byte(\"foo\"), nil))\n\t\t\tnextWriteKey++\n\t\tcase p < .55: /* 45 % */\n\t\t\t// Create a new iterator directly from the batch and check that it\n\t\t\t// observes the correct state.\n\t\t\titer, _ := b.NewIter(&IterOptions{KeyTypes: IterKeyTypePointsAndRanges})\n\t\t\tcheckIter(iter, nextWriteKey)\n\t\t\titers[nextWriteKey] = append(iters[nextWriteKey], iter)\n\t\tdefault: /* 45 % */\n\t\t\t// Create a new iterator through cloning a random existing iterator\n\t\t\t// and check that it observes the right state.\n\t\t\treadKey := rng.Int64N(nextWriteKey + 1)\n\t\t\titersForReadKey := iters[readKey]\n\t\t\tif len(itersForReadKey) == 0 {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\titer, err := itersForReadKey[rng.IntN(len(itersForReadKey))].Clone(CloneOptions{})\n\t\t\trequire.NoError(t, err)\n\t\t\tcheckIter(iter, readKey)\n\t\t\titers[readKey] = append(iters[readKey], iter)\n\t\t}\n\t}\n}\n\nfunc TestBatchOption(t *testing.T) {\n\tfor _, tc := range []struct {\n\t\tname     string\n\t\topts     []BatchOption\n\t\texpected *Batch\n\t}{\n\t\t{\n\t\t\tname: \"default\",\n\t\t\topts: nil,\n\t\t\texpected: &Batch{batchInternal: batchInternal{\n\t\t\t\topts: batchOptions{\n\t\t\t\t\tinitialSizeBytes:     defaultBatchInitialSize,\n\t\t\t\t\tmaxRetainedSizeBytes: defaultBatchMaxRetainedSize,\n\t\t\t\t},\n\t\t\t}},\n\t\t},\n\t\t{\n\t\t\tname: \"with_custom_initial_size\",\n\t\t\topts: []BatchOption{WithInitialSizeBytes(2 << 10)},\n\t\t\texpected: &Batch{batchInternal: batchInternal{\n\t\t\t\topts: batchOptions{\n\t\t\t\t\tinitialSizeBytes:     2 << 10,\n\t\t\t\t\tmaxRetainedSizeBytes: defaultBatchMaxRetainedSize,\n\t\t\t\t},\n\t\t\t}},\n\t\t},\n\t\t{\n\t\t\tname: \"with_custom_max_retained_size\",\n\t\t\topts: []BatchOption{WithMaxRetainedSizeBytes(2 << 10)},\n\t\t\texpected: &Batch{batchInternal: batchInternal{\n\t\t\t\topts: batchOptions{\n\t\t\t\t\tinitialSizeBytes:     defaultBatchInitialSize,\n\t\t\t\t\tmaxRetainedSizeBytes: 2 << 10,\n\t\t\t\t},\n\t\t\t}},\n\t\t},\n\t} {\n\t\tb := newBatch(nil, tc.opts...)\n\t\t// newBatch returns batch from the pool so it is possible for len(data) to be > 0\n\t\tb.data = nil\n\t\trequire.Equal(t, tc.expected, b)\n\t}\n}\n"
        },
        {
          "name": "batchrepr",
          "type": "tree",
          "content": null
        },
        {
          "name": "bloom",
          "type": "tree",
          "content": null
        },
        {
          "name": "cache.go",
          "type": "blob",
          "size": 0.7939453125,
          "content": "// Copyright 2019 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport \"github.com/cockroachdb/pebble/internal/cache\"\n\n// Cache exports the cache.Cache type.\ntype Cache = cache.Cache\n\n// NewCache creates a new cache of the specified size. Memory for the cache is\n// allocated on demand, not during initialization. The cache is created with a\n// reference count of 1. Each DB it is associated with adds a reference, so the\n// creator of the cache should usually release their reference after the DB is\n// created.\n//\n//\tc := pebble.NewCache(...)\n//\tdefer c.Unref()\n//\td, err := pebble.Open(pebble.Options{Cache: c})\nfunc NewCache(size int64) *cache.Cache {\n\treturn cache.New(size)\n}\n"
        },
        {
          "name": "checkpoint.go",
          "type": "blob",
          "size": 15.4697265625,
          "content": "// Copyright 2019 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"io\"\n\t\"os\"\n\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/errors/oserror\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/record\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/cockroachdb/pebble/vfs/atomicfs\"\n)\n\n// checkpointOptions hold the optional parameters to construct checkpoint\n// snapshots.\ntype checkpointOptions struct {\n\t// flushWAL set to true will force a flush and sync of the WAL prior to\n\t// checkpointing.\n\tflushWAL bool\n\n\t// If set, any SSTs that don't overlap with these spans are excluded from a checkpoint.\n\trestrictToSpans []CheckpointSpan\n}\n\n// CheckpointOption set optional parameters used by `DB.Checkpoint`.\ntype CheckpointOption func(*checkpointOptions)\n\n// WithFlushedWAL enables flushing and syncing the WAL prior to constructing a\n// checkpoint. This guarantees that any writes committed before calling\n// DB.Checkpoint will be part of that checkpoint.\n//\n// Note that this setting can only be useful in cases when some writes are\n// performed with Sync = false. Otherwise, the guarantee will already be met.\n//\n// Passing this option is functionally equivalent to calling\n// DB.LogData(nil, Sync) right before DB.Checkpoint.\nfunc WithFlushedWAL() CheckpointOption {\n\treturn func(opt *checkpointOptions) {\n\t\topt.flushWAL = true\n\t}\n}\n\n// WithRestrictToSpans specifies spans of interest for the checkpoint. Any SSTs\n// that don't overlap with any of these spans are excluded from the checkpoint.\n//\n// Note that the checkpoint can still surface keys outside of these spans (from\n// the WAL and from SSTs that partially overlap with these spans). Moreover,\n// these surface keys aren't necessarily \"valid\" in that they could have been\n// modified but the SST containing the modification is excluded.\nfunc WithRestrictToSpans(spans []CheckpointSpan) CheckpointOption {\n\treturn func(opt *checkpointOptions) {\n\t\topt.restrictToSpans = spans\n\t}\n}\n\n// CheckpointSpan is a key range [Start, End) (inclusive on Start, exclusive on\n// End) of interest for a checkpoint.\ntype CheckpointSpan struct {\n\tStart []byte\n\tEnd   []byte\n}\n\n// excludeFromCheckpoint returns true if an SST file should be excluded from the\n// checkpoint because it does not overlap with the spans of interest\n// (opt.restrictToSpans).\nfunc excludeFromCheckpoint(f *fileMetadata, opt *checkpointOptions, cmp Compare) bool {\n\tif len(opt.restrictToSpans) == 0 {\n\t\t// Option not set; don't exclude anything.\n\t\treturn false\n\t}\n\tfor _, s := range opt.restrictToSpans {\n\t\tspanBounds := base.UserKeyBoundsEndExclusive(s.Start, s.End)\n\t\tif f.Overlaps(cmp, &spanBounds) {\n\t\t\treturn false\n\t\t}\n\t}\n\t// None of the restrictToSpans overlapped; we can exclude this file.\n\treturn true\n}\n\n// mkdirAllAndSyncParents creates destDir and any of its missing parents.\n// Those missing parents, as well as the closest existing ancestor, are synced.\n// Returns a handle to the directory created at destDir.\nfunc mkdirAllAndSyncParents(fs vfs.FS, destDir string) (vfs.File, error) {\n\t// Collect paths for all directories between destDir (excluded) and its\n\t// closest existing ancestor (included).\n\tvar parentPaths []string\n\tfor parentPath := fs.PathDir(destDir); ; parentPath = fs.PathDir(parentPath) {\n\t\tparentPaths = append(parentPaths, parentPath)\n\t\tif fs.PathDir(parentPath) == parentPath {\n\t\t\tbreak\n\t\t}\n\t\t_, err := fs.Stat(parentPath)\n\t\tif err == nil {\n\t\t\t// Exit loop at the closest existing ancestor.\n\t\t\tbreak\n\t\t}\n\t\tif !oserror.IsNotExist(err) {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\t// Create destDir and any of its missing parents.\n\tif err := fs.MkdirAll(destDir, 0755); err != nil {\n\t\treturn nil, err\n\t}\n\t// Sync all the parent directories up to the closest existing ancestor,\n\t// included.\n\tfor _, parentPath := range parentPaths {\n\t\tparentDir, err := fs.OpenDir(parentPath)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\terr = parentDir.Sync()\n\t\tif err != nil {\n\t\t\t_ = parentDir.Close()\n\t\t\treturn nil, err\n\t\t}\n\t\terr = parentDir.Close()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\treturn fs.OpenDir(destDir)\n}\n\n// Checkpoint constructs a snapshot of the DB instance in the specified\n// directory. The WAL, MANIFEST, OPTIONS, and sstables will be copied into the\n// snapshot. Hard links will be used when possible. Beware of the significant\n// space overhead for a checkpoint if hard links are disabled. Also beware that\n// even if hard links are used, the space overhead for the checkpoint will\n// increase over time as the DB performs compactions.\n//\n// Note that shared files in a checkpoint could get deleted if the DB is\n// restarted after a checkpoint operation, as the reference for the checkpoint\n// is only maintained in memory. This is okay as long as users of Checkpoint\n// crash shortly afterwards with a \"poison file\" preventing further restarts.\nfunc (d *DB) Checkpoint(\n\tdestDir string, opts ...CheckpointOption,\n) (\n\tckErr error, /* used in deferred cleanup */\n) {\n\topt := &checkpointOptions{}\n\tfor _, fn := range opts {\n\t\tfn(opt)\n\t}\n\n\tif _, err := d.opts.FS.Stat(destDir); !oserror.IsNotExist(err) {\n\t\tif err == nil {\n\t\t\treturn &os.PathError{\n\t\t\t\tOp:   \"checkpoint\",\n\t\t\t\tPath: destDir,\n\t\t\t\tErr:  oserror.ErrExist,\n\t\t\t}\n\t\t}\n\t\treturn err\n\t}\n\n\tif opt.flushWAL && !d.opts.DisableWAL {\n\t\t// Write an empty log-data record to flush and sync the WAL.\n\t\tif err := d.LogData(nil /* data */, Sync); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Disable file deletions.\n\td.mu.Lock()\n\td.disableFileDeletions()\n\tdefer func() {\n\t\td.mu.Lock()\n\t\tdefer d.mu.Unlock()\n\t\td.enableFileDeletions()\n\t}()\n\n\t// TODO(peter): RocksDB provides the option to roll the manifest if the\n\t// MANIFEST size is too large. Should we do this too?\n\n\t// Lock the manifest before getting the current version. We need the\n\t// length of the manifest that we read to match the current version that\n\t// we read, otherwise we might copy a versionEdit not reflected in the\n\t// sstables we copy/link.\n\td.mu.versions.logLock()\n\t// Get the the current version and the current manifest file number.\n\tcurrent := d.mu.versions.currentVersion()\n\tformatVers := d.FormatMajorVersion()\n\tmanifestFileNum := d.mu.versions.manifestFileNum\n\tmanifestSize := d.mu.versions.manifest.Size()\n\toptionsFileNum := d.optionsFileNum\n\n\tvirtualBackingFiles := make(map[base.DiskFileNum]struct{})\n\td.mu.versions.virtualBackings.ForEach(func(backing *fileBacking) {\n\t\tvirtualBackingFiles[backing.DiskFileNum] = struct{}{}\n\t})\n\n\t// Acquire the logs while holding mutexes to ensure we don't race with a\n\t// flush that might mark a log that's relevant to `current` as obsolete\n\t// before our call to List.\n\tallLogicalLogs := d.mu.log.manager.List()\n\n\t// Release the manifest and DB.mu so we don't block other operations on\n\t// the database.\n\td.mu.versions.logUnlock()\n\td.mu.Unlock()\n\n\t// Wrap the normal filesystem with one which wraps newly created files with\n\t// vfs.NewSyncingFile.\n\tfs := vfs.NewSyncingFS(d.opts.FS, vfs.SyncingFileOptions{\n\t\tNoSyncOnClose: d.opts.NoSyncOnClose,\n\t\tBytesPerSync:  d.opts.BytesPerSync,\n\t})\n\n\t// Create the dir and its parents (if necessary), and sync them.\n\tvar dir vfs.File\n\tdefer func() {\n\t\tif dir != nil {\n\t\t\t_ = dir.Close()\n\t\t}\n\t\tif ckErr != nil {\n\t\t\t// Attempt to cleanup on error.\n\t\t\t_ = fs.RemoveAll(destDir)\n\t\t}\n\t}()\n\tdir, ckErr = mkdirAllAndSyncParents(fs, destDir)\n\tif ckErr != nil {\n\t\treturn ckErr\n\t}\n\n\t{\n\t\t// Copy the OPTIONS.\n\t\tsrcPath := base.MakeFilepath(fs, d.dirname, fileTypeOptions, optionsFileNum)\n\t\tdestPath := fs.PathJoin(destDir, fs.PathBase(srcPath))\n\t\tckErr = copyCheckpointOptions(fs, srcPath, destPath)\n\t\tif ckErr != nil {\n\t\t\treturn ckErr\n\t\t}\n\t}\n\n\t{\n\t\t// Set the format major version in the destination directory.\n\t\tvar versionMarker *atomicfs.Marker\n\t\tversionMarker, _, ckErr = atomicfs.LocateMarker(fs, destDir, formatVersionMarkerName)\n\t\tif ckErr != nil {\n\t\t\treturn ckErr\n\t\t}\n\n\t\t// We use the marker to encode the active format version in the\n\t\t// marker filename. Unlike other uses of the atomic marker,\n\t\t// there is no file with the filename `formatVers.String()` on\n\t\t// the filesystem.\n\t\tckErr = versionMarker.Move(formatVers.String())\n\t\tif ckErr != nil {\n\t\t\treturn ckErr\n\t\t}\n\t\tckErr = versionMarker.Close()\n\t\tif ckErr != nil {\n\t\t\treturn ckErr\n\t\t}\n\t}\n\n\tvar excludedFiles map[deletedFileEntry]*fileMetadata\n\tvar remoteFiles []base.DiskFileNum\n\t// Set of FileBacking.DiskFileNum which will be required by virtual sstables\n\t// in the checkpoint.\n\trequiredVirtualBackingFiles := make(map[base.DiskFileNum]struct{})\n\t// Link or copy the sstables.\n\tfor l := range current.Levels {\n\t\titer := current.Levels[l].Iter()\n\t\tfor f := iter.First(); f != nil; f = iter.Next() {\n\t\t\tif excludeFromCheckpoint(f, opt, d.cmp) {\n\t\t\t\tif excludedFiles == nil {\n\t\t\t\t\texcludedFiles = make(map[deletedFileEntry]*fileMetadata)\n\t\t\t\t}\n\t\t\t\texcludedFiles[deletedFileEntry{\n\t\t\t\t\tLevel:   l,\n\t\t\t\t\tFileNum: f.FileNum,\n\t\t\t\t}] = f\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tfileBacking := f.FileBacking\n\t\t\tif f.Virtual {\n\t\t\t\tif _, ok := requiredVirtualBackingFiles[fileBacking.DiskFileNum]; ok {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\trequiredVirtualBackingFiles[fileBacking.DiskFileNum] = struct{}{}\n\t\t\t}\n\t\t\tmeta, err := d.objProvider.Lookup(fileTypeTable, fileBacking.DiskFileNum)\n\t\t\tif err != nil {\n\t\t\t\tckErr = err\n\t\t\t\treturn ckErr\n\t\t\t}\n\t\t\tif meta.IsRemote() {\n\t\t\t\t// We don't copy remote files. This is desirable as checkpointing is\n\t\t\t\t// supposed to be a fast operation, and references to remote files can\n\t\t\t\t// always be resolved by any checkpoint readers by reading the object\n\t\t\t\t// catalog. We don't add this file to excludedFiles either, as that'd\n\t\t\t\t// cause it to be deleted in the second manifest entry which is also\n\t\t\t\t// inaccurate.\n\t\t\t\tremoteFiles = append(remoteFiles, meta.DiskFileNum)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tsrcPath := base.MakeFilepath(fs, d.dirname, fileTypeTable, fileBacking.DiskFileNum)\n\t\t\tdestPath := fs.PathJoin(destDir, fs.PathBase(srcPath))\n\t\t\tckErr = vfs.LinkOrCopy(fs, srcPath, destPath)\n\t\t\tif ckErr != nil {\n\t\t\t\treturn ckErr\n\t\t\t}\n\t\t}\n\t}\n\n\tvar removeBackingTables []base.DiskFileNum\n\tfor diskFileNum := range virtualBackingFiles {\n\t\tif _, ok := requiredVirtualBackingFiles[diskFileNum]; !ok {\n\t\t\t// The backing sstable associated with fileNum is no longer\n\t\t\t// required.\n\t\t\tremoveBackingTables = append(removeBackingTables, diskFileNum)\n\t\t}\n\t}\n\n\tckErr = d.writeCheckpointManifest(\n\t\tfs, formatVers, destDir, dir, manifestFileNum, manifestSize,\n\t\texcludedFiles, removeBackingTables,\n\t)\n\tif ckErr != nil {\n\t\treturn ckErr\n\t}\n\tif len(remoteFiles) > 0 {\n\t\tckErr = d.objProvider.CheckpointState(fs, destDir, fileTypeTable, remoteFiles)\n\t\tif ckErr != nil {\n\t\t\treturn ckErr\n\t\t}\n\t}\n\n\t// Copy the WAL files. We copy rather than link because WAL file recycling\n\t// will cause the WAL files to be reused which would invalidate the\n\t// checkpoint. It's possible allLogicalLogs includes logs that are not\n\t// relevant (beneath the version's MinUnflushedLogNum). These extra files\n\t// are harmless. The earlier (wal.Manager).List call will not include\n\t// obsolete logs that are sitting in the recycler or have already been\n\t// passed off to the cleanup manager for deletion.\n\t//\n\t// TODO(jackson): It would be desirable to copy all recycling and obsolete\n\t// WALs to aid corruption postmortem debugging should we need them.\n\tfor _, log := range allLogicalLogs {\n\t\tfor i := 0; i < log.NumSegments(); i++ {\n\t\t\tsrcFS, srcPath := log.SegmentLocation(i)\n\t\t\tdestPath := fs.PathJoin(destDir, srcFS.PathBase(srcPath))\n\t\t\tckErr = vfs.CopyAcrossFS(srcFS, srcPath, fs, destPath)\n\t\t\tif ckErr != nil {\n\t\t\t\treturn ckErr\n\t\t\t}\n\t\t}\n\t}\n\n\t// Sync and close the checkpoint directory.\n\tckErr = dir.Sync()\n\tif ckErr != nil {\n\t\treturn ckErr\n\t}\n\tckErr = dir.Close()\n\tdir = nil\n\treturn ckErr\n}\n\n// copyCheckpointOptions copies an OPTIONS file, commenting out some options\n// that existed on the original database but no longer apply to the checkpointed\n// database. For example, the entire [WAL Failover] stanza is commented out\n// because Checkpoint will copy all WAL segment files from both the primary and\n// secondary WAL directories into the checkpoint.\nfunc copyCheckpointOptions(fs vfs.FS, srcPath, dstPath string) error {\n\tvar buf bytes.Buffer\n\tf, err := fs.Open(srcPath)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer f.Close()\n\tb, err := io.ReadAll(f)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// Copy the OPTIONS file verbatim, but commenting out the [WAL Failover]\n\t// section.\n\terr = parseOptions(string(b), parseOptionsFuncs{\n\t\tvisitNewSection: func(startOff, endOff int, section string) error {\n\t\t\tif section == \"WAL Failover\" {\n\t\t\t\tbuf.WriteString(\"# \")\n\t\t\t}\n\t\t\tbuf.Write(b[startOff:endOff])\n\t\t\treturn nil\n\t\t},\n\t\tvisitKeyValue: func(startOff, endOff int, section, key, value string) error {\n\t\t\tif section == \"WAL Failover\" {\n\t\t\t\tbuf.WriteString(\"# \")\n\t\t\t}\n\t\t\tbuf.Write(b[startOff:endOff])\n\t\t\treturn nil\n\t\t},\n\t\tvisitCommentOrWhitespace: func(startOff, endOff int, line string) error {\n\t\t\tbuf.Write(b[startOff:endOff])\n\t\t\treturn nil\n\t\t},\n\t})\n\tif err != nil {\n\t\treturn err\n\t}\n\tnf, err := fs.Create(dstPath, vfs.WriteCategoryUnspecified)\n\tif err != nil {\n\t\treturn err\n\t}\n\t_, err = io.Copy(nf, &buf)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn errors.CombineErrors(nf.Sync(), nf.Close())\n}\n\nfunc (d *DB) writeCheckpointManifest(\n\tfs vfs.FS,\n\tformatVers FormatMajorVersion,\n\tdestDirPath string,\n\tdestDir vfs.File,\n\tmanifestFileNum base.DiskFileNum,\n\tmanifestSize int64,\n\texcludedFiles map[deletedFileEntry]*fileMetadata,\n\tremoveBackingTables []base.DiskFileNum,\n) error {\n\t// Copy the MANIFEST, and create a pointer to it. We copy rather\n\t// than link because additional version edits added to the\n\t// MANIFEST after we took our snapshot of the sstables will\n\t// reference sstables that aren't in our checkpoint. For a\n\t// similar reason, we need to limit how much of the MANIFEST we\n\t// copy.\n\t// If some files are excluded from the checkpoint, also append a block that\n\t// records those files as deleted.\n\tif err := func() error {\n\t\tsrcPath := base.MakeFilepath(fs, d.dirname, fileTypeManifest, manifestFileNum)\n\t\tdestPath := fs.PathJoin(destDirPath, fs.PathBase(srcPath))\n\t\tsrc, err := fs.Open(srcPath, vfs.SequentialReadsOption)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdefer src.Close()\n\n\t\tdst, err := fs.Create(destPath, vfs.WriteCategoryUnspecified)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdefer dst.Close()\n\n\t\t// Copy all existing records. We need to copy at the record level in case we\n\t\t// need to append another record with the excluded files (we cannot simply\n\t\t// append a record after a raw data copy; see\n\t\t// https://github.com/cockroachdb/cockroach/issues/100935).\n\t\tr := record.NewReader(&io.LimitedReader{R: src, N: manifestSize}, manifestFileNum)\n\t\tw := record.NewWriter(dst)\n\t\tfor {\n\t\t\trr, err := r.Next()\n\t\t\tif err != nil {\n\t\t\t\tif err == io.EOF {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\trw, err := w.Next()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif _, err := io.Copy(rw, rr); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\n\t\tif len(excludedFiles) > 0 {\n\t\t\t// Write out an additional VersionEdit that deletes the excluded SST files.\n\t\t\tve := versionEdit{\n\t\t\t\tDeletedFiles:         excludedFiles,\n\t\t\t\tRemovedBackingTables: removeBackingTables,\n\t\t\t}\n\n\t\t\trw, err := w.Next()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif err := ve.Encode(rw); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tif err := w.Close(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn dst.Sync()\n\t}(); err != nil {\n\t\treturn err\n\t}\n\n\tvar manifestMarker *atomicfs.Marker\n\tmanifestMarker, _, err := atomicfs.LocateMarker(fs, destDirPath, manifestMarkerName)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err := manifestMarker.Move(base.MakeFilename(fileTypeManifest, manifestFileNum)); err != nil {\n\t\treturn err\n\t}\n\treturn manifestMarker.Close()\n}\n"
        },
        {
          "name": "checkpoint_test.go",
          "type": "blob",
          "size": 11.6787109375,
          "content": "// Copyright 2019 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"math/rand/v2\"\n\t\"runtime\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"testing\"\n\n\t\"github.com/cockroachdb/datadriven\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/objstorage/remote\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc testCheckpointImpl(t *testing.T, ddFile string, createOnShared bool) {\n\tdbs := make(map[string]*DB)\n\tdefer func() {\n\t\tfor _, db := range dbs {\n\t\t\tif db.closed.Load() == nil {\n\t\t\t\trequire.NoError(t, db.Close())\n\t\t\t}\n\t\t}\n\t}()\n\n\tmem := vfs.NewMem()\n\tvar memLog base.InMemLogger\n\tremoteMem := remote.NewInMem()\n\tmakeOptions := func() *Options {\n\t\topts := &Options{\n\t\t\tFS:                          vfs.WithLogging(mem, memLog.Infof),\n\t\t\tFormatMajorVersion:          internalFormatNewest,\n\t\t\tL0CompactionThreshold:       10,\n\t\t\tDisableAutomaticCompactions: true,\n\t\t\tLogger:                      testLogger{t},\n\t\t}\n\t\topts.Experimental.EnableColumnarBlocks = func() bool { return true }\n\t\topts.Experimental.RemoteStorage = remote.MakeSimpleFactory(map[remote.Locator]remote.Storage{\n\t\t\t\"\": remoteMem,\n\t\t})\n\t\tif createOnShared {\n\t\t\topts.Experimental.CreateOnShared = remote.CreateOnSharedAll\n\t\t}\n\t\topts.DisableTableStats = true\n\t\topts.private.testingAlwaysWaitForCleanup = true\n\t\treturn opts\n\t}\n\n\tdatadriven.RunTest(t, ddFile, func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"batch\":\n\t\t\tif len(td.CmdArgs) != 1 {\n\t\t\t\treturn \"batch <db>\"\n\t\t\t}\n\t\t\tmemLog.Reset()\n\t\t\td := dbs[td.CmdArgs[0].String()]\n\t\t\tb := d.NewBatch()\n\t\t\tif err := runBatchDefineCmd(td, b); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tif err := b.Commit(Sync); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn memLog.String()\n\n\t\tcase \"checkpoint\":\n\t\t\tif len(td.CmdArgs) < 2 {\n\t\t\t\treturn \"checkpoint <db> <dir> [restrict=(start-end, ...)]\"\n\t\t\t}\n\t\t\tvar opts []CheckpointOption\n\t\t\tif len(td.CmdArgs) == 3 {\n\t\t\t\tvar spans []CheckpointSpan\n\t\t\t\tfor _, v := range td.CmdArgs[2].Vals {\n\t\t\t\t\tsplits := strings.SplitN(v, \"-\", 2)\n\t\t\t\t\tif len(splits) != 2 {\n\t\t\t\t\t\treturn fmt.Sprintf(\"invalid restrict range %q\", v)\n\t\t\t\t\t}\n\t\t\t\t\tspans = append(spans, CheckpointSpan{\n\t\t\t\t\t\tStart: []byte(splits[0]),\n\t\t\t\t\t\tEnd:   []byte(splits[1]),\n\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t\topts = append(opts, WithRestrictToSpans(spans))\n\t\t\t}\n\t\t\tmemLog.Reset()\n\t\t\td := dbs[td.CmdArgs[0].String()]\n\t\t\tif err := d.Checkpoint(td.CmdArgs[1].String(), opts...); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tif td.HasArg(\"nondeterministic\") {\n\t\t\t\tmemLog.Reset()\n\t\t\t\treturn \"\"\n\t\t\t}\n\t\t\treturn memLog.String()\n\n\t\tcase \"ingest-and-excise\":\n\t\t\td := dbs[td.CmdArgs[0].String()]\n\n\t\t\t// Hacky but the command doesn't expect a db string. Get rid of it.\n\t\t\ttd.CmdArgs = td.CmdArgs[1:]\n\t\t\tif err := runIngestAndExciseCmd(td, d, mem); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"build\":\n\t\t\td := dbs[td.CmdArgs[0].String()]\n\n\t\t\t// Hacky but the command doesn't expect a db string. Get rid of it.\n\t\t\ttd.CmdArgs = td.CmdArgs[1:]\n\t\t\tif err := runBuildCmd(td, d, mem); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"lsm\":\n\t\t\td := dbs[td.CmdArgs[0].String()]\n\n\t\t\t// Hacky but the command doesn't expect a db string. Get rid of it.\n\t\t\ttd.CmdArgs = td.CmdArgs[1:]\n\t\t\treturn runLSMCmd(td, d)\n\n\t\tcase \"compact\":\n\t\t\tif len(td.CmdArgs) != 1 {\n\t\t\t\treturn \"compact <db>\"\n\t\t\t}\n\t\t\tmemLog.Reset()\n\t\t\td := dbs[td.CmdArgs[0].String()]\n\t\t\tif err := d.Compact(nil, []byte(\"\\xff\"), false); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\td.TestOnlyWaitForCleaning()\n\t\t\treturn memLog.String()\n\n\t\tcase \"print-backing\":\n\t\t\t// prints contents of the file backing map in the version. Used to\n\t\t\t// test whether the checkpoint removed the filebackings correctly.\n\t\t\tif len(td.CmdArgs) != 1 {\n\t\t\t\treturn \"print-backing <db>\"\n\t\t\t}\n\t\t\td := dbs[td.CmdArgs[0].String()]\n\t\t\td.mu.Lock()\n\t\t\td.mu.versions.logLock()\n\t\t\tfileNums := d.mu.versions.virtualBackings.DiskFileNums()\n\t\t\td.mu.versions.logUnlock()\n\t\t\td.mu.Unlock()\n\n\t\t\tvar buf bytes.Buffer\n\t\t\tfor _, f := range fileNums {\n\t\t\t\tbuf.WriteString(fmt.Sprintf(\"%s\\n\", f.String()))\n\t\t\t}\n\t\t\treturn buf.String()\n\n\t\tcase \"close\":\n\t\t\tif len(td.CmdArgs) != 1 {\n\t\t\t\treturn \"close <db>\"\n\t\t\t}\n\t\t\td := dbs[td.CmdArgs[0].String()]\n\t\t\trequire.NoError(t, d.Close())\n\t\t\treturn \"\"\n\n\t\tcase \"flush\":\n\t\t\tif len(td.CmdArgs) != 1 {\n\t\t\t\treturn \"flush <db>\"\n\t\t\t}\n\t\t\tmemLog.Reset()\n\t\t\td := dbs[td.CmdArgs[0].String()]\n\t\t\tif err := d.Flush(); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn memLog.String()\n\n\t\tcase \"list\":\n\t\t\tif len(td.CmdArgs) != 1 {\n\t\t\t\treturn \"list <dir>\"\n\t\t\t}\n\t\t\tpaths, err := mem.List(td.CmdArgs[0].String())\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tsort.Strings(paths)\n\t\t\treturn fmt.Sprintf(\"%s\\n\", strings.Join(paths, \"\\n\"))\n\n\t\tcase \"open\":\n\t\t\tif len(td.CmdArgs) < 1 {\n\t\t\t\treturn \"open <dir> [readonly]\"\n\t\t\t}\n\t\t\topts := makeOptions()\n\t\t\trequire.NoError(t, parseDBOptionsArgs(opts, td.CmdArgs[1:]))\n\n\t\t\tmemLog.Reset()\n\t\t\tdir := td.CmdArgs[0].String()\n\t\t\tif _, ok := dbs[dir]; ok {\n\t\t\t\trequire.NoError(t, dbs[dir].Close())\n\t\t\t\tdbs[dir] = nil\n\t\t\t}\n\n\t\t\td, err := Open(dir, opts)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tdbs[dir] = d\n\t\t\tif len(dbs) == 1 && createOnShared {\n\t\t\t\t// This is the first db. Set a creator ID.\n\t\t\t\tif err := d.SetCreatorID(1); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t}\n\t\t\twaitForCompactionsAndTableStats(d)\n\n\t\t\tif td.HasArg(\"nondeterministic\") {\n\t\t\t\tmemLog.Reset()\n\t\t\t\treturn \"\"\n\t\t\t}\n\t\t\treturn memLog.String()\n\n\t\tcase \"scan\":\n\t\t\tif len(td.CmdArgs) != 1 {\n\t\t\t\treturn \"scan <db>\"\n\t\t\t}\n\t\t\tmemLog.Reset()\n\t\t\td := dbs[td.CmdArgs[0].String()]\n\t\t\titer, _ := d.NewIter(nil)\n\t\t\tfor valid := iter.First(); valid; valid = iter.Next() {\n\t\t\t\tmemLog.Infof(\"%s %s\", iter.Key(), iter.Value())\n\t\t\t}\n\t\t\tmemLog.Infof(\".\")\n\t\t\tif err := iter.Close(); err != nil {\n\t\t\t\tmemLog.Infof(\"%v\\n\", err)\n\t\t\t}\n\t\t\treturn memLog.String()\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc TestCopyCheckpointOptions(t *testing.T) {\n\tfs := vfs.NewMem()\n\tdatadriven.RunTest(t, \"testdata/copy_checkpoint_options\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"copy\":\n\t\t\tf, err := fs.Create(\"old\", vfs.WriteCategoryUnspecified)\n\t\t\trequire.NoError(t, err)\n\t\t\t_, err = io.WriteString(f, td.Input)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.NoError(t, f.Close())\n\n\t\t\tif err := copyCheckpointOptions(fs, \"old\", \"new\"); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\n\t\t\tf, err = fs.Open(\"new\")\n\t\t\trequire.NoError(t, err)\n\t\t\tnewFile, err := io.ReadAll(f)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.NoError(t, f.Close())\n\t\t\treturn string(newFile)\n\t\tdefault:\n\t\t\tpanic(fmt.Sprintf(\"unrecognized command %q\", td.Cmd))\n\t\t}\n\t})\n}\n\nfunc TestCheckpoint(t *testing.T) {\n\tt.Run(\"shared=false\", func(t *testing.T) {\n\t\ttestCheckpointImpl(t, \"testdata/checkpoint\", false /* createOnShared */)\n\t})\n\tt.Run(\"shared=true\", func(t *testing.T) {\n\t\tif runtime.GOOS == \"windows\" {\n\t\t\tt.Skipf(\"skipped on windows\")\n\t\t}\n\t\ttestCheckpointImpl(t, \"testdata/checkpoint_shared\", true /* createOnShared */)\n\t})\n}\n\nfunc TestCheckpointCompaction(t *testing.T) {\n\tfs := vfs.NewMem()\n\td, err := Open(\"\", &Options{FS: fs, Logger: testLogger{t: t}})\n\trequire.NoError(t, err)\n\n\tctx, cancel := context.WithCancel(context.Background())\n\n\tvar wg sync.WaitGroup\n\twg.Add(4)\n\tgo func() {\n\t\tdefer cancel()\n\t\tdefer wg.Done()\n\t\tfor i := 0; ctx.Err() == nil; i++ {\n\t\t\tif err := d.Set([]byte(fmt.Sprintf(\"key%06d\", i)), nil, nil); err != nil {\n\t\t\t\tt.Error(err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n\tgo func() {\n\t\tdefer cancel()\n\t\tdefer wg.Done()\n\t\tfor ctx.Err() == nil {\n\t\t\tif err := d.Compact([]byte(\"key\"), []byte(\"key999999\"), false); err != nil {\n\t\t\t\tt.Error(err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n\tcheck := make(chan string, 100)\n\tgo func() {\n\t\tdefer cancel()\n\t\tdefer close(check)\n\t\tdefer wg.Done()\n\t\tfor i := 0; ctx.Err() == nil && i < 200; i++ {\n\t\t\tdir := fmt.Sprintf(\"checkpoint%06d\", i)\n\t\t\tif err := d.Checkpoint(dir); err != nil {\n\t\t\t\tt.Error(err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn\n\t\t\tcase check <- dir:\n\t\t\t}\n\t\t}\n\t}()\n\tgo func() {\n\t\topts := &Options{FS: fs, Logger: testLogger{t: t}}\n\t\tdefer cancel()\n\t\tdefer wg.Done()\n\t\tfor dir := range check {\n\t\t\td2, err := Open(dir, opts)\n\t\t\tif err != nil {\n\t\t\t\tt.Error(err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// Check the checkpoint has all the sstables that the manifest\n\t\t\t// claims it has.\n\t\t\ttableInfos, _ := d2.SSTables()\n\t\t\tfor _, tables := range tableInfos {\n\t\t\t\tfor _, tbl := range tables {\n\t\t\t\t\tif tbl.Virtual {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tif _, err := fs.Stat(base.MakeFilepath(fs, dir, base.FileTypeTable, base.PhysicalTableDiskFileNum(tbl.FileNum))); err != nil {\n\t\t\t\t\t\tt.Error(err)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif err := d2.Close(); err != nil {\n\t\t\t\tt.Error(err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n\t<-ctx.Done()\n\twg.Wait()\n\trequire.NoError(t, d.Close())\n}\n\nfunc TestCheckpointFlushWAL(t *testing.T) {\n\tconst checkpointPath = \"checkpoints/checkpoint\"\n\tfs := vfs.NewCrashableMem()\n\topts := &Options{FS: fs, Logger: testLogger{t: t}}\n\tkey, value := []byte(\"key\"), []byte(\"value\")\n\n\t// Create a checkpoint from an unsynced DB.\n\t{\n\t\td, err := Open(\"\", opts)\n\t\trequire.NoError(t, err)\n\t\t{\n\t\t\twb := d.NewBatch()\n\t\t\terr = wb.Set(key, value, nil)\n\t\t\trequire.NoError(t, err)\n\t\t\terr = d.Apply(wb, NoSync)\n\t\t\trequire.NoError(t, err)\n\t\t}\n\t\terr = d.Checkpoint(checkpointPath, WithFlushedWAL())\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, d.Close())\n\t\tfs = fs.CrashClone(vfs.CrashCloneCfg{UnsyncedDataPercent: 0})\n\t}\n\n\t// Check that the WAL has been flushed in the checkpoint.\n\t{\n\t\tfiles, err := fs.List(checkpointPath)\n\t\trequire.NoError(t, err)\n\t\thasLogFile := false\n\t\tfor _, f := range files {\n\t\t\tinfo, err := fs.Stat(fs.PathJoin(checkpointPath, f))\n\t\t\trequire.NoError(t, err)\n\t\t\tif strings.HasSuffix(f, \".log\") {\n\t\t\t\thasLogFile = true\n\t\t\t\trequire.NotZero(t, info.Size())\n\t\t\t}\n\t\t}\n\t\trequire.True(t, hasLogFile)\n\t}\n\n\t// Check that the checkpoint contains the expected data.\n\t{\n\t\td, err := Open(checkpointPath, opts)\n\t\trequire.NoError(t, err)\n\t\titer, _ := d.NewIter(nil)\n\t\trequire.True(t, iter.First())\n\t\trequire.Equal(t, key, iter.Key())\n\t\trequire.Equal(t, value, iter.Value())\n\t\trequire.False(t, iter.Next())\n\t\trequire.NoError(t, iter.Close())\n\t\trequire.NoError(t, d.Close())\n\t}\n}\n\nfunc TestCheckpointManyFiles(t *testing.T) {\n\tif testing.Short() {\n\t\tt.Skip(\"skipping because of short flag\")\n\t}\n\tconst checkpointPath = \"checkpoint\"\n\topts := &Options{\n\t\tFS:                          vfs.NewMem(),\n\t\tFormatMajorVersion:          internalFormatNewest,\n\t\tDisableAutomaticCompactions: true,\n\t\tLogger:                      testLogger{t},\n\t}\n\t// Disable compression to speed up the test.\n\topts.EnsureDefaults()\n\tfor i := range opts.Levels {\n\t\topts.Levels[i].Compression = func() Compression { return NoCompression }\n\t}\n\n\td, err := Open(\"\", opts)\n\trequire.NoError(t, err)\n\tdefer d.Close()\n\n\tmkKey := func(x int) []byte {\n\t\treturn []byte(fmt.Sprintf(\"key%06d\", x))\n\t}\n\t// We want to test the case where the appended record with the excluded files\n\t// makes the manifest cross 32KB. This will happen for a range of values\n\t// around 450.\n\tn := 400 + rand.IntN(100)\n\tfor i := 0; i < n; i++ {\n\t\terr := d.Set(mkKey(i), nil, nil)\n\t\trequire.NoError(t, err)\n\t\terr = d.Flush()\n\t\trequire.NoError(t, err)\n\t}\n\terr = d.Checkpoint(checkpointPath, WithRestrictToSpans([]CheckpointSpan{\n\t\t{\n\t\t\tStart: mkKey(0),\n\t\t\tEnd:   mkKey(10),\n\t\t},\n\t}))\n\trequire.NoError(t, err)\n\n\t// Open the checkpoint and iterate through all the keys.\n\t{\n\t\td, err := Open(checkpointPath, opts)\n\t\trequire.NoError(t, err)\n\t\titer, _ := d.NewIter(nil)\n\t\trequire.True(t, iter.First())\n\t\trequire.NoError(t, iter.Error())\n\t\tn := 1\n\t\tfor iter.Next() {\n\t\t\tn++\n\t\t}\n\t\trequire.NoError(t, iter.Error())\n\t\trequire.NoError(t, iter.Close())\n\t\trequire.NoError(t, d.Close())\n\t\trequire.Equal(t, 10, n)\n\t}\n}\n"
        },
        {
          "name": "cmd",
          "type": "tree",
          "content": null
        },
        {
          "name": "cockroachkvs",
          "type": "tree",
          "content": null
        },
        {
          "name": "commit.go",
          "type": "blob",
          "size": 19.7919921875,
          "content": "// Copyright 2018 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"runtime\"\n\t\"sync\"\n\t\"sync/atomic\"\n\n\t\"github.com/cockroachdb/crlib/crtime\"\n\t\"github.com/cockroachdb/pebble/batchrepr\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/record\"\n)\n\n// commitQueue is a lock-free fixed-size single-producer, multi-consumer\n// queue. The single producer can enqueue (push) to the head, and consumers can\n// dequeue (pop) from the tail.\n//\n// It has the added feature that it nils out unused slots to avoid unnecessary\n// retention of objects.\ntype commitQueue struct {\n\t// headTail packs together a 32-bit head index and a 32-bit tail index. Both\n\t// are indexes into slots modulo len(slots)-1.\n\t//\n\t// tail = index of oldest data in queue\n\t// head = index of next slot to fill\n\t//\n\t// Slots in the range [tail, head) are owned by consumers.  A consumer\n\t// continues to own a slot outside this range until it nils the slot, at\n\t// which point ownership passes to the producer.\n\t//\n\t// The head index is stored in the most-significant bits so that we can\n\t// atomically add to it and the overflow is harmless.\n\theadTail atomic.Uint64\n\n\t// slots is a ring buffer of values stored in this queue. The size must be a\n\t// power of 2. A slot is in use until *both* the tail index has moved beyond\n\t// it and the slot value has been set to nil. The slot value is set to nil\n\t// atomically by the consumer and read atomically by the producer.\n\tslots [record.SyncConcurrency]atomic.Pointer[Batch]\n}\n\nconst dequeueBits = 32\n\nfunc (q *commitQueue) unpack(ptrs uint64) (head, tail uint32) {\n\tconst mask = 1<<dequeueBits - 1\n\thead = uint32((ptrs >> dequeueBits) & mask)\n\ttail = uint32(ptrs & mask)\n\treturn\n}\n\nfunc (q *commitQueue) pack(head, tail uint32) uint64 {\n\tconst mask = 1<<dequeueBits - 1\n\treturn (uint64(head) << dequeueBits) |\n\t\tuint64(tail&mask)\n}\n\nfunc (q *commitQueue) enqueue(b *Batch) {\n\tptrs := q.headTail.Load()\n\thead, tail := q.unpack(ptrs)\n\tif (tail+uint32(len(q.slots)))&(1<<dequeueBits-1) == head {\n\t\t// Queue is full. This should never be reached because commitPipeline.commitQueueSem\n\t\t// limits the number of concurrent operations.\n\t\tpanic(\"pebble: not reached\")\n\t}\n\tslot := &q.slots[head&uint32(len(q.slots)-1)]\n\n\t// Check if the head slot has been released by dequeueApplied.\n\tfor slot.Load() != nil {\n\t\t// Another goroutine is still cleaning up the tail, so the queue is\n\t\t// actually still full. We spin because this should resolve itself\n\t\t// momentarily.\n\t\truntime.Gosched()\n\t}\n\n\t// The head slot is free, so we own it.\n\tslot.Store(b)\n\n\t// Increment head. This passes ownership of slot to dequeueApplied and acts as a\n\t// store barrier for writing the slot.\n\tq.headTail.Add(1 << dequeueBits)\n}\n\n// dequeueApplied removes the earliest enqueued Batch, if it is applied.\n//\n// Returns nil if the commit queue is empty or the earliest Batch is not yet\n// applied.\nfunc (q *commitQueue) dequeueApplied() *Batch {\n\tfor {\n\t\tptrs := q.headTail.Load()\n\t\thead, tail := q.unpack(ptrs)\n\t\tif tail == head {\n\t\t\t// Queue is empty.\n\t\t\treturn nil\n\t\t}\n\n\t\tslot := &q.slots[tail&uint32(len(q.slots)-1)]\n\t\tb := slot.Load()\n\t\tif b == nil || !b.applied.Load() {\n\t\t\t// The batch is not ready to be dequeued, or another goroutine has\n\t\t\t// already dequeued it.\n\t\t\treturn nil\n\t\t}\n\n\t\t// Confirm head and tail (for our speculative check above) and increment\n\t\t// tail. If this succeeds, then we own the slot at tail.\n\t\tptrs2 := q.pack(head, tail+1)\n\t\tif q.headTail.CompareAndSwap(ptrs, ptrs2) {\n\t\t\t// We now own slot.\n\t\t\t//\n\t\t\t// Tell enqueue that we're done with this slot. Zeroing the slot is also\n\t\t\t// important so we don't leave behind references that could keep this object\n\t\t\t// live longer than necessary.\n\t\t\tslot.Store(nil)\n\t\t\t// At this point enqueue owns the slot.\n\t\t\treturn b\n\t\t}\n\t}\n}\n\n// commitEnv contains the environment that a commitPipeline interacts\n// with. This allows fine-grained testing of commitPipeline behavior without\n// construction of an entire DB.\ntype commitEnv struct {\n\t// The next sequence number to give to a batch. Protected by\n\t// commitPipeline.mu.\n\tlogSeqNum *base.AtomicSeqNum\n\t// The visible sequence number at which reads should be performed. Ratcheted\n\t// upwards atomically as batches are applied to the memtable.\n\tvisibleSeqNum *base.AtomicSeqNum\n\n\t// Apply the batch to the specified memtable. Called concurrently.\n\tapply func(b *Batch, mem *memTable) error\n\t// Write the batch to the WAL. If wg != nil, the data will be persisted\n\t// asynchronously and done will be called on wg upon completion. If wg != nil\n\t// and err != nil, a failure to persist the WAL will populate *err. Returns\n\t// the memtable the batch should be applied to. Serial execution enforced by\n\t// commitPipeline.mu.\n\twrite func(b *Batch, wg *sync.WaitGroup, err *error) (*memTable, error)\n}\n\n// A commitPipeline manages the stages of committing a set of mutations\n// (contained in a single Batch) atomically to the DB. The steps are\n// conceptually:\n//\n//  1. Write the batch to the WAL and optionally sync the WAL\n//  2. Apply the mutations in the batch to the memtable\n//\n// These two simple steps are made complicated by the desire for high\n// performance. In the absence of concurrency, performance is limited by how\n// fast a batch can be written (and synced) to the WAL and then added to the\n// memtable, both of which are outside the purview of the commit\n// pipeline. Performance under concurrency is the primary concern of the commit\n// pipeline, though it also needs to maintain two invariants:\n//\n//  1. Batches need to be written to the WAL in sequence number order.\n//  2. Batches need to be made visible for reads in sequence number order. This\n//     invariant arises from the use of a single sequence number which\n//     indicates which mutations are visible.\n//\n// Taking these invariants into account, let's revisit the work the commit\n// pipeline needs to perform. Writing the batch to the WAL is necessarily\n// serialized as there is a single WAL object. The order of the entries in the\n// WAL defines the sequence number order. Note that writing to the WAL is\n// extremely fast, usually just a memory copy. Applying the mutations in a\n// batch to the memtable can occur concurrently as the underlying skiplist\n// supports concurrent insertions. Publishing the visible sequence number is\n// another serialization point, but one with a twist: the visible sequence\n// number cannot be bumped until the mutations for earlier batches have\n// finished applying to the memtable (the visible sequence number only ratchets\n// up). Lastly, if requested, the commit waits for the WAL to sync. Note that\n// waiting for the WAL sync after ratcheting the visible sequence number allows\n// another goroutine to read committed data before the WAL has synced. This is\n// similar behavior to RocksDB's manual WAL flush functionality. Application\n// code needs to protect against this if necessary.\n//\n// The full outline of the commit pipeline operation is as follows:\n//\n//\twith commitPipeline mutex locked:\n//\t  assign batch sequence number\n//\t  write batch to WAL\n//\t(optionally) add batch to WAL sync list\n//\tapply batch to memtable (concurrently)\n//\twait for earlier batches to apply\n//\tratchet read sequence number\n//\t(optionally) wait for the WAL to sync\n//\n// As soon as a batch has been written to the WAL, the commitPipeline mutex is\n// released allowing another batch to write to the WAL. Each commit operation\n// individually applies its batch to the memtable providing concurrency. The\n// WAL sync happens concurrently with applying to the memtable (see\n// commitPipeline.syncLoop).\n//\n// The \"waits for earlier batches to apply\" work is more complicated than might\n// be expected. The obvious approach would be to keep a queue of pending\n// batches and for each batch to wait for the previous batch to finish\n// committing. This approach was tried initially and turned out to be too\n// slow. The problem is that it causes excessive goroutine activity as each\n// committing goroutine needs to wake up in order for the next goroutine to be\n// unblocked. The approach taken in the current code is conceptually similar,\n// though it avoids waking a goroutine to perform work that another goroutine\n// can perform. A commitQueue (a single-producer, multiple-consumer queue)\n// holds the ordered list of committing batches. Addition to the queue is done\n// while holding commitPipeline.mutex ensuring the same ordering of batches in\n// the queue as the ordering in the WAL. When a batch finishes applying to the\n// memtable, it atomically updates its Batch.applied field. Ratcheting of the\n// visible sequence number is done by commitPipeline.publish which loops\n// dequeueing \"applied\" batches and ratcheting the visible sequence number. If\n// we hit an unapplied batch at the head of the queue we can block as we know\n// that committing of that unapplied batch will eventually find our (applied)\n// batch in the queue. See commitPipeline.publish for additional commentary.\ntype commitPipeline struct {\n\t// WARNING: The following struct `commitQueue` contains fields which will\n\t// be accessed atomically.\n\t//\n\t// Go allocations are guaranteed to be 64-bit aligned which we take advantage\n\t// of by placing the 64-bit fields which we access atomically at the beginning\n\t// of the commitPipeline struct.\n\t// For more information, see https://golang.org/pkg/sync/atomic/#pkg-note-BUG.\n\t// Queue of pending batches to commit.\n\tpending commitQueue\n\tenv     commitEnv\n\t// The commit path has two queues:\n\t// - commitPipeline.pending contains batches whose seqnums have not yet been\n\t//   published. It is a lock-free single producer multi consumer queue.\n\t// - LogWriter.flusher.syncQ contains state for batches that have asked for\n\t//   a sync. It is a lock-free single producer single consumer queue.\n\t// These lock-free queues have a fixed capacity. And since they are\n\t// lock-free, we cannot do blocking waits when pushing onto these queues, in\n\t// case they are full. Additionally, adding to these queues happens while\n\t// holding commitPipeline.mu, and we don't want to block while holding that\n\t// mutex since it is also needed by other code.\n\t//\n\t// Popping from these queues is independent and for a particular batch can\n\t// occur in either order, though it is more common that popping from the\n\t// commitPipeline.pending will happen first.\n\t//\n\t// Due to these constraints, we reserve a unit of space in each queue before\n\t// acquiring commitPipeline.mu, which also ensures that the push operation\n\t// is guaranteed to have space in the queue. The commitQueueSem and\n\t// logSyncQSem are used for this reservation.\n\tcommitQueueSem chan struct{}\n\tlogSyncQSem    chan struct{}\n\tingestSem      chan struct{}\n\t// The mutex to use for synchronizing access to logSeqNum and serializing\n\t// calls to commitEnv.write().\n\tmu sync.Mutex\n}\n\nfunc newCommitPipeline(env commitEnv) *commitPipeline {\n\tp := &commitPipeline{\n\t\tenv: env,\n\t\t// The capacity of both commitQueue.slots and syncQueue.slots is set to\n\t\t// record.SyncConcurrency, which also determines the value of these\n\t\t// semaphores. We used to have a single semaphore, which required that the\n\t\t// capacity of these queues be the same. Now that we have two semaphores,\n\t\t// the capacity of these queues could be changed to be different. Say half\n\t\t// of the batches asked to be synced, but syncing took 5x the latency of\n\t\t// adding to the memtable and publishing. Then syncQueue.slots could be\n\t\t// sized as 0.5*5 of the commitQueue.slots. We can explore this if we find\n\t\t// that LogWriterMetrics.SyncQueueLen has high utilization under some\n\t\t// workloads.\n\t\t//\n\t\t// NB: the commit concurrency is one less than SyncConcurrency because we\n\t\t// have to allow one \"slot\" for a concurrent WAL rotation which will close\n\t\t// and sync the WAL.\n\t\tcommitQueueSem: make(chan struct{}, record.SyncConcurrency-1),\n\t\tlogSyncQSem:    make(chan struct{}, record.SyncConcurrency-1),\n\t\tingestSem:      make(chan struct{}, 1),\n\t}\n\treturn p\n}\n\n// directWrite is used to directly write to the WAL. commitPipeline.mu must be\n// held while this is called. DB.mu must not be held. directWrite will only\n// return once the WAL sync is complete. Note that DirectWrite is a special case\n// function which is currently only used when ingesting sstables as a flushable.\n// Reason carefully about the correctness argument when calling this function\n// from any context.\nfunc (p *commitPipeline) directWrite(b *Batch) error {\n\tvar syncWG sync.WaitGroup\n\tvar syncErr error\n\tsyncWG.Add(1)\n\tp.logSyncQSem <- struct{}{}\n\t_, err := p.env.write(b, &syncWG, &syncErr)\n\tsyncWG.Wait()\n\terr = firstError(err, syncErr)\n\treturn err\n}\n\n// Commit the specified batch, writing it to the WAL, optionally syncing the\n// WAL, and applying the batch to the memtable. Upon successful return the\n// batch's mutations will be visible for reading.\n// REQUIRES: noSyncWait => syncWAL\nfunc (p *commitPipeline) Commit(b *Batch, syncWAL bool, noSyncWait bool) error {\n\tif b.Empty() {\n\t\treturn nil\n\t}\n\n\tcommitStartTime := crtime.NowMono()\n\t// Acquire semaphores.\n\tp.commitQueueSem <- struct{}{}\n\tif syncWAL {\n\t\tp.logSyncQSem <- struct{}{}\n\t}\n\tb.commitStats.SemaphoreWaitDuration = commitStartTime.Elapsed()\n\n\t// Prepare the batch for committing: enqueuing the batch in the pending\n\t// queue, determining the batch sequence number and writing the data to the\n\t// WAL.\n\t//\n\t// NB: We set Batch.commitErr on error so that the batch won't be a candidate\n\t// for reuse. See Batch.release().\n\tmem, err := p.prepare(b, syncWAL, noSyncWait)\n\tif err != nil {\n\t\tb.db = nil // prevent batch reuse on error\n\t\t// NB: we are not doing <-p.commitQueueSem since the batch is still\n\t\t// sitting in the pending queue. We should consider fixing this by also\n\t\t// removing the batch from the pending queue.\n\t\treturn err\n\t}\n\n\t// Apply the batch to the memtable.\n\tif err := p.env.apply(b, mem); err != nil {\n\t\tb.db = nil // prevent batch reuse on error\n\t\t// NB: we are not doing <-p.commitQueueSem since the batch is still\n\t\t// sitting in the pending queue. We should consider fixing this by also\n\t\t// removing the batch from the pending queue.\n\t\treturn err\n\t}\n\n\t// Publish the batch sequence number.\n\tp.publish(b)\n\n\t<-p.commitQueueSem\n\n\tif !noSyncWait {\n\t\t// Already waited for commit, so look at the error.\n\t\tif b.commitErr != nil {\n\t\t\tb.db = nil // prevent batch reuse on error\n\t\t\terr = b.commitErr\n\t\t}\n\t}\n\t// Else noSyncWait. The LogWriter can be concurrently writing to\n\t// b.commitErr. We will read b.commitErr in Batch.SyncWait after the\n\t// LogWriter is done writing.\n\n\tb.commitStats.TotalDuration = commitStartTime.Elapsed()\n\n\treturn err\n}\n\n// AllocateSeqNum allocates count sequence numbers, invokes the prepare\n// callback, then the apply callback, and then publishes the sequence\n// numbers. AllocateSeqNum does not write to the WAL or add entries to the\n// memtable. AllocateSeqNum can be used to sequence an operation such as\n// sstable ingestion within the commit pipeline. The prepare callback is\n// invoked with commitPipeline.mu held, but note that DB.mu is not held and\n// must be locked if necessary.\nfunc (p *commitPipeline) AllocateSeqNum(\n\tcount int, prepare func(seqNum base.SeqNum), apply func(seqNum base.SeqNum),\n) {\n\t// This method is similar to Commit and prepare. Be careful about trying to\n\t// share additional code with those methods because Commit and prepare are\n\t// performance critical code paths.\n\n\tb := newBatch(nil)\n\tdefer b.Close()\n\n\t// Give the batch a count of 1 so that the log and visible sequence number\n\t// are incremented correctly.\n\tb.data = make([]byte, batchrepr.HeaderLen)\n\tb.setCount(uint32(count))\n\tb.commit.Add(1)\n\n\tp.commitQueueSem <- struct{}{}\n\n\tp.mu.Lock()\n\n\t// Enqueue the batch in the pending queue. Note that while the pending queue\n\t// is lock-free, we want the order of batches to be the same as the sequence\n\t// number order.\n\tp.pending.enqueue(b)\n\n\t// Assign the batch a sequence number. Note that we use atomic operations\n\t// here to handle concurrent reads of logSeqNum. commitPipeline.mu provides\n\t// mutual exclusion for other goroutines writing to logSeqNum.\n\tlogSeqNum := p.env.logSeqNum.Add(base.SeqNum(count)) - base.SeqNum(count)\n\tseqNum := logSeqNum\n\tif seqNum == 0 {\n\t\t// We can't use the value 0 for the global seqnum during ingestion, because\n\t\t// 0 indicates no global seqnum. So allocate one more seqnum.\n\t\tp.env.logSeqNum.Add(1)\n\t\tseqNum++\n\t}\n\tb.setSeqNum(seqNum)\n\n\t// Wait for any outstanding writes to the memtable to complete. This is\n\t// necessary for ingestion so that the check for memtable overlap can see any\n\t// writes that were sequenced before the ingestion. The spin loop is\n\t// unfortunate, but obviates the need for additional synchronization.\n\tfor {\n\t\tvisibleSeqNum := p.env.visibleSeqNum.Load()\n\t\tif visibleSeqNum == logSeqNum {\n\t\t\tbreak\n\t\t}\n\t\truntime.Gosched()\n\t}\n\n\t// Invoke the prepare callback. Note the lack of error reporting. Even if the\n\t// callback internally fails, the sequence number needs to be published in\n\t// order to allow the commit pipeline to proceed.\n\tprepare(b.SeqNum())\n\n\tp.mu.Unlock()\n\n\t// Invoke the apply callback.\n\tapply(b.SeqNum())\n\n\t// Publish the sequence number.\n\tp.publish(b)\n\n\t<-p.commitQueueSem\n}\n\nfunc (p *commitPipeline) prepare(b *Batch, syncWAL bool, noSyncWait bool) (*memTable, error) {\n\tn := uint64(b.Count())\n\tif n == invalidBatchCount {\n\t\treturn nil, ErrInvalidBatch\n\t}\n\tvar syncWG *sync.WaitGroup\n\tvar syncErr *error\n\tswitch {\n\tcase !syncWAL:\n\t\t// Only need to wait for the publish.\n\t\tb.commit.Add(1)\n\t// Remaining cases represent syncWAL=true.\n\tcase noSyncWait:\n\t\tsyncErr = &b.commitErr\n\t\tsyncWG = &b.fsyncWait\n\t\t// Only need to wait synchronously for the publish. The user will\n\t\t// (asynchronously) wait on the batch's fsyncWait.\n\t\tb.commit.Add(1)\n\t\tb.fsyncWait.Add(1)\n\tcase !noSyncWait:\n\t\tsyncErr = &b.commitErr\n\t\tsyncWG = &b.commit\n\t\t// Must wait for both the publish and the WAL fsync.\n\t\tb.commit.Add(2)\n\t}\n\n\tp.mu.Lock()\n\n\t// Enqueue the batch in the pending queue. Note that while the pending queue\n\t// is lock-free, we want the order of batches to be the same as the sequence\n\t// number order.\n\tp.pending.enqueue(b)\n\n\t// Assign the batch a sequence number. Note that we use atomic operations\n\t// here to handle concurrent reads of logSeqNum. commitPipeline.mu provides\n\t// mutual exclusion for other goroutines writing to logSeqNum.\n\tb.setSeqNum(p.env.logSeqNum.Add(base.SeqNum(n)) - base.SeqNum(n))\n\n\t// Write the data to the WAL.\n\tmem, err := p.env.write(b, syncWG, syncErr)\n\n\tp.mu.Unlock()\n\n\treturn mem, err\n}\n\nfunc (p *commitPipeline) publish(b *Batch) {\n\t// Mark the batch as applied.\n\tb.applied.Store(true)\n\n\t// Loop dequeuing applied batches from the pending queue. If our batch was\n\t// the head of the pending queue we are guaranteed that either we'll publish\n\t// it or someone else will dequeueApplied and publish it. If our batch is not the\n\t// head of the queue then either we'll dequeueApplied applied batches and reach our\n\t// batch or there is an unapplied batch blocking us. When that unapplied\n\t// batch applies it will go through the same process and publish our batch\n\t// for us.\n\tfor {\n\t\tt := p.pending.dequeueApplied()\n\t\tif t == nil {\n\t\t\t// Wait for another goroutine to publish us. We might also be waiting for\n\t\t\t// the WAL sync to finish.\n\t\t\tnow := crtime.NowMono()\n\t\t\tb.commit.Wait()\n\t\t\tb.commitStats.CommitWaitDuration += now.Elapsed()\n\t\t\tbreak\n\t\t}\n\t\tif !t.applied.Load() {\n\t\t\tpanic(\"not reached\")\n\t\t}\n\n\t\t// We're responsible for publishing the sequence number for batch t, but\n\t\t// another concurrent goroutine might sneak in and publish the sequence\n\t\t// number for a subsequent batch. That's ok as all we're guaranteeing is\n\t\t// that the sequence number ratchets up.\n\t\tfor {\n\t\t\tcurSeqNum := p.env.visibleSeqNum.Load()\n\t\t\tnewSeqNum := t.SeqNum() + base.SeqNum(t.Count())\n\t\t\tif newSeqNum <= curSeqNum {\n\t\t\t\t// t's sequence number has already been published.\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tif p.env.visibleSeqNum.CompareAndSwap(curSeqNum, newSeqNum) {\n\t\t\t\t// We successfully published t's sequence number.\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\n\t\tt.commit.Done()\n\t}\n}\n"
        },
        {
          "name": "commit_test.go",
          "type": "blob",
          "size": 12.025390625,
          "content": "// Copyright 2018 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"io\"\n\t\"math/rand/v2\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/pebble/internal/arenaskl\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/invariants\"\n\t\"github.com/cockroachdb/pebble/record\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/stretchr/testify/require\"\n)\n\ntype testCommitEnv struct {\n\tlogSeqNum     base.AtomicSeqNum\n\tvisibleSeqNum base.AtomicSeqNum\n\twriteCount    atomic.Uint64\n\tapplyBuf      struct {\n\t\tsync.Mutex\n\t\tbuf []uint64\n\t}\n\tqueueSemChan chan struct{}\n}\n\nfunc (e *testCommitEnv) env() commitEnv {\n\treturn commitEnv{\n\t\tlogSeqNum:     &e.logSeqNum,\n\t\tvisibleSeqNum: &e.visibleSeqNum,\n\t\tapply:         e.apply,\n\t\twrite:         e.write,\n\t}\n}\n\nfunc (e *testCommitEnv) apply(b *Batch, mem *memTable) error {\n\te.applyBuf.Lock()\n\te.applyBuf.buf = append(e.applyBuf.buf, uint64(b.SeqNum()))\n\te.applyBuf.Unlock()\n\treturn nil\n}\n\nfunc (e *testCommitEnv) write(b *Batch, wg *sync.WaitGroup, _ *error) (*memTable, error) {\n\te.writeCount.Add(1)\n\tif wg != nil {\n\t\twg.Done()\n\t\t<-e.queueSemChan\n\t}\n\treturn nil, nil\n}\n\nfunc TestCommitQueue(t *testing.T) {\n\tvar q commitQueue\n\tvar batches [16]Batch\n\tfor i := range batches {\n\t\tq.enqueue(&batches[i])\n\t}\n\tif b := q.dequeueApplied(); b != nil {\n\t\tt.Fatalf(\"unexpectedly dequeued batch: %p\", b)\n\t}\n\tbatches[1].applied.Store(true)\n\tif b := q.dequeueApplied(); b != nil {\n\t\tt.Fatalf(\"unexpectedly dequeued batch: %p\", b)\n\t}\n\tfor i := range batches {\n\t\tbatches[i].applied.Store(true)\n\t\tif b := q.dequeueApplied(); b != &batches[i] {\n\t\t\tt.Fatalf(\"%d: expected batch %p, but found %p\", i, &batches[i], b)\n\t\t}\n\t}\n\tif b := q.dequeueApplied(); b != nil {\n\t\tt.Fatalf(\"unexpectedly dequeued batch: %p\", b)\n\t}\n}\n\nfunc TestCommitPipeline(t *testing.T) {\n\tvar e testCommitEnv\n\tp := newCommitPipeline(e.env())\n\n\tn := 10000\n\tif invariants.RaceEnabled {\n\t\t// Under race builds we have to limit the concurrency or we hit the\n\t\t// following error:\n\t\t//\n\t\t//   race: limit on 8128 simultaneously alive goroutines is exceeded, dying\n\t\tn = 1000\n\t}\n\n\tvar wg sync.WaitGroup\n\twg.Add(n)\n\tfor i := 0; i < n; i++ {\n\t\tgo func(i int) {\n\t\t\tdefer wg.Done()\n\t\t\tvar b Batch\n\t\t\t_ = b.Set([]byte(fmt.Sprint(i)), nil, nil)\n\t\t\t_ = p.Commit(&b, false, false)\n\t\t}(i)\n\t}\n\twg.Wait()\n\n\tif s := e.writeCount.Load(); uint64(n) != s {\n\t\tt.Fatalf(\"expected %d written batches, but found %d\", n, s)\n\t}\n\tif n != len(e.applyBuf.buf) {\n\t\tt.Fatalf(\"expected %d written batches, but found %d\",\n\t\t\tn, len(e.applyBuf.buf))\n\t}\n\tif s := e.logSeqNum.Load(); base.SeqNum(n) != s {\n\t\tt.Fatalf(\"expected %d, but found %d\", n, s)\n\t}\n\tif s := e.visibleSeqNum.Load(); base.SeqNum(n) != s {\n\t\tt.Fatalf(\"expected %d, but found %d\", n, s)\n\t}\n}\n\nfunc TestCommitPipelineSync(t *testing.T) {\n\tn := 10000\n\tif invariants.RaceEnabled {\n\t\t// Under race builds we have to limit the concurrency or we hit the\n\t\t// following error:\n\t\t//\n\t\t//   race: limit on 8128 simultaneously alive goroutines is exceeded, dying\n\t\tn = 1000\n\t}\n\n\tfor _, noSyncWait := range []bool{false, true} {\n\t\tt.Run(fmt.Sprintf(\"no-sync-wait=%t\", noSyncWait), func(t *testing.T) {\n\t\t\tvar e testCommitEnv\n\t\t\tp := newCommitPipeline(e.env())\n\t\t\te.queueSemChan = p.logSyncQSem\n\n\t\t\tvar wg sync.WaitGroup\n\t\t\twg.Add(n)\n\t\t\tfor i := 0; i < n; i++ {\n\t\t\t\tgo func(i int) {\n\t\t\t\t\tdefer wg.Done()\n\t\t\t\t\tvar b Batch\n\t\t\t\t\trequire.NoError(t, b.Set([]byte(fmt.Sprint(i)), nil, nil))\n\t\t\t\t\trequire.NoError(t, p.Commit(&b, true, noSyncWait))\n\t\t\t\t\tif noSyncWait {\n\t\t\t\t\t\trequire.NoError(t, b.SyncWait())\n\t\t\t\t\t}\n\t\t\t\t}(i)\n\t\t\t}\n\t\t\twg.Wait()\n\t\t\tif s := e.writeCount.Load(); uint64(n) != s {\n\t\t\t\tt.Fatalf(\"expected %d written batches, but found %d\", n, s)\n\t\t\t}\n\t\t\tif n != len(e.applyBuf.buf) {\n\t\t\t\tt.Fatalf(\"expected %d written batches, but found %d\",\n\t\t\t\t\tn, len(e.applyBuf.buf))\n\t\t\t}\n\t\t\tif s := e.logSeqNum.Load(); base.SeqNum(n) != s {\n\t\t\t\tt.Fatalf(\"expected %d, but found %d\", n, s)\n\t\t\t}\n\t\t\tif s := e.visibleSeqNum.Load(); base.SeqNum(n) != s {\n\t\t\t\tt.Fatalf(\"expected %d, but found %d\", n, s)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestCommitPipelineAllocateSeqNum(t *testing.T) {\n\tvar e testCommitEnv\n\tp := newCommitPipeline(e.env())\n\n\tconst n = 10\n\tvar wg sync.WaitGroup\n\twg.Add(n)\n\tvar prepareCount atomic.Uint64\n\tvar applyCount atomic.Uint64\n\tfor i := 1; i <= n; i++ {\n\t\tgo func(i int) {\n\t\t\tdefer wg.Done()\n\t\t\tp.AllocateSeqNum(i, func(_ base.SeqNum) {\n\t\t\t\tprepareCount.Add(1)\n\t\t\t}, func(_ base.SeqNum) {\n\t\t\t\tapplyCount.Add(1)\n\t\t\t})\n\t\t}(i)\n\t}\n\twg.Wait()\n\n\tif s := prepareCount.Load(); n != s {\n\t\tt.Fatalf(\"expected %d prepares, but found %d\", n, s)\n\t}\n\tif s := applyCount.Load(); n != s {\n\t\tt.Fatalf(\"expected %d applies, but found %d\", n, s)\n\t}\n\t// AllocateSeqNum always returns a non-zero sequence number causing the\n\t// values we see to be offset from 1.\n\tconst total = 1 + 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10\n\tif s := e.logSeqNum.Load(); total != s {\n\t\tt.Fatalf(\"expected %d, but found %d\", total, s)\n\t}\n\tif s := e.visibleSeqNum.Load(); total != s {\n\t\tt.Fatalf(\"expected %d, but found %d\", total, s)\n\t}\n}\n\ntype syncDelayFile struct {\n\tvfs.File\n\tdone chan struct{}\n}\n\nfunc (f *syncDelayFile) Sync() error {\n\t<-f.done\n\treturn nil\n}\n\nfunc TestCommitPipelineWALClose(t *testing.T) {\n\t// This test stresses the edge case of N goroutines blocked in the\n\t// commitPipeline waiting for the log to sync when we concurrently decide to\n\t// rotate and close the log.\n\n\tmem := vfs.NewMem()\n\tf, err := mem.Create(\"test-wal\", vfs.WriteCategoryUnspecified)\n\trequire.NoError(t, err)\n\n\t// syncDelayFile will block on the done channel befor returning from Sync\n\t// call.\n\tsf := &syncDelayFile{\n\t\tFile: f,\n\t\tdone: make(chan struct{}),\n\t}\n\n\t// A basic commitEnv which writes to a WAL.\n\tvar wal *record.LogWriter\n\tvar walDone sync.WaitGroup\n\ttestEnv := commitEnv{\n\t\tlogSeqNum:     new(base.AtomicSeqNum),\n\t\tvisibleSeqNum: new(base.AtomicSeqNum),\n\t\tapply: func(b *Batch, mem *memTable) error {\n\t\t\t// At this point, we've called SyncRecord but the sync is blocked.\n\t\t\twalDone.Done()\n\t\t\treturn nil\n\t\t},\n\t\twrite: func(b *Batch, syncWG *sync.WaitGroup, syncErr *error) (*memTable, error) {\n\t\t\t_, err := wal.SyncRecord(b.data, syncWG, syncErr)\n\t\t\treturn nil, err\n\t\t},\n\t}\n\tp := newCommitPipeline(testEnv)\n\twal = record.NewLogWriter(sf, 0 /* logNum */, record.LogWriterConfig{\n\t\tWALFsyncLatency: prometheus.NewHistogram(prometheus.HistogramOpts{}),\n\t\tQueueSemChan:    p.logSyncQSem,\n\t})\n\n\t// Launch N (commitConcurrency) goroutines which each create a batch and\n\t// commit it with sync==true. Because of the syncDelayFile, none of these\n\t// operations can complete until syncDelayFile.done is closed.\n\terrCh := make(chan error, cap(p.commitQueueSem))\n\twalDone.Add(cap(errCh))\n\tfor i := 0; i < cap(errCh); i++ {\n\t\tgo func(i int) {\n\t\t\tb := &Batch{}\n\t\t\tif err := b.LogData([]byte(\"foo\"), nil); err != nil {\n\t\t\t\terrCh <- err\n\t\t\t\treturn\n\t\t\t}\n\t\t\terrCh <- p.Commit(b, true /* sync */, false)\n\t\t}(i)\n\t}\n\n\t// Wait for all of the WAL writes to queue up. This ensures we don't violate\n\t// the concurrency requirements of LogWriter, and also ensures all of the WAL\n\t// writes are queued.\n\twalDone.Wait()\n\tclose(sf.done)\n\n\t// Close the WAL. A \"queue is full\" panic means that something is broken.\n\trequire.NoError(t, wal.Close())\n\tfor i := 0; i < cap(errCh); i++ {\n\t\trequire.NoError(t, <-errCh)\n\t}\n}\n\n// TestCommitPipelineLogDataSeqNum ensures committing a KV and a LogData\n// concurrently never publishes the KV's sequence number before it's been fully\n// applied to the memtable (which would violate the consistency of iterators\n// to which that sequence number is visible).\n//\n// A LogData batch reads the 'next sequence number' without incrementing it,\n// effectively sharing the sequence number with the next key committed. It may\n// finish applying to the memtable before the KV that shares its sequence\n// number. However, sequence number publishing ratchets the visible sequence\n// number to the batch's first seqnum + number of batch entries ..., so for e.g.\n// with first seqnum = 5 and number of entries = 3, it will ratchet to 8. This\n// means all seqnums strictly less than 8 are visible. So a LogData batch which\n// also grabbed the first seqnum = 5 before this batch, will ratchet to 5 + 0,\n// which is a noop.\nfunc TestCommitPipelineLogDataSeqNum(t *testing.T) {\n\tvar testEnv commitEnv\n\ttestEnv = commitEnv{\n\t\tlogSeqNum:     new(base.AtomicSeqNum),\n\t\tvisibleSeqNum: new(base.AtomicSeqNum),\n\t\tapply: func(b *Batch, mem *memTable) error {\n\t\t\t// Jitter a delay in memtable application to get test coverage of\n\t\t\t// varying interleavings of which batch completes memtable\n\t\t\t// application first.\n\t\t\ttime.Sleep(time.Duration(rand.Float64() * 20.0 * float64(time.Millisecond)))\n\t\t\t// Ensure that our sequence number is not published before we've\n\t\t\t// returned from apply.\n\t\t\t//\n\t\t\t// If b is the Set(\"foo\",\"bar\") batch, the LogData batch sharing the\n\t\t\t// sequence number may have already entered commitPipeline.publish,\n\t\t\t// but the sequence number it publishes should not be high enough to\n\t\t\t// make this batch's KV visible.\n\t\t\t//\n\t\t\t// It may set visibleSeqNum = b.SeqNum(), but seqnum X is not\n\t\t\t// considered visible until the visibleSeqNum is >X.\n\t\t\trequire.False(t, base.Visible(\n\t\t\t\tb.SeqNum(),                   // Seqnum of the first KV in the batch b\n\t\t\t\ttestEnv.visibleSeqNum.Load(), // Snapshot seqnum\n\t\t\t\tbase.SeqNumMax,               // Indexed batch \"seqnum\" (unused here)\n\t\t\t))\n\t\t\treturn nil\n\t\t},\n\t\twrite: func(b *Batch, syncWG *sync.WaitGroup, syncErr *error) (*memTable, error) {\n\t\t\tif syncWG != nil {\n\t\t\t\tsyncWG.Done()\n\t\t\t}\n\t\t\treturn nil, nil\n\t\t},\n\t}\n\ttestEnv.logSeqNum.Store(base.SeqNumStart)\n\ttestEnv.visibleSeqNum.Store(base.SeqNumStart)\n\tp := newCommitPipeline(testEnv)\n\n\tvar wg sync.WaitGroup\n\twg.Add(2)\n\tgo func() {\n\t\tdefer wg.Done()\n\t\tb := &Batch{}\n\t\trequire.NoError(t, b.Set([]byte(\"foo\"), []byte(\"bar\"), nil))\n\t\trequire.NoError(t, p.Commit(b, false /* sync */, false))\n\t}()\n\tgo func() {\n\t\tdefer wg.Done()\n\t\tb := &Batch{}\n\t\trequire.NoError(t, b.LogData([]byte(\"foo\"), nil))\n\t\trequire.NoError(t, p.Commit(b, false /* sync */, false))\n\t}()\n\twg.Wait()\n}\n\nfunc BenchmarkCommitPipeline(b *testing.B) {\n\tfor _, noSyncWait := range []bool{false, true} {\n\t\tfor _, parallelism := range []int{1, 2, 4, 8, 16, 32, 64, 128} {\n\t\t\tb.Run(fmt.Sprintf(\"no-sync-wait=%t/parallel=%d\", noSyncWait, parallelism),\n\t\t\t\tfunc(b *testing.B) {\n\t\t\t\t\tb.SetParallelism(parallelism)\n\t\t\t\t\tmem := newMemTable(memTableOptions{})\n\t\t\t\t\tvar wal *record.LogWriter\n\t\t\t\t\tnullCommitEnv := commitEnv{\n\t\t\t\t\t\tlogSeqNum:     new(base.AtomicSeqNum),\n\t\t\t\t\t\tvisibleSeqNum: new(base.AtomicSeqNum),\n\t\t\t\t\t\tapply: func(b *Batch, mem *memTable) error {\n\t\t\t\t\t\t\terr := mem.apply(b, b.SeqNum())\n\t\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\t\treturn err\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tmem.writerUnref()\n\t\t\t\t\t\t\treturn nil\n\t\t\t\t\t\t},\n\t\t\t\t\t\twrite: func(b *Batch, syncWG *sync.WaitGroup, syncErr *error) (*memTable, error) {\n\t\t\t\t\t\t\tfor {\n\t\t\t\t\t\t\t\terr := mem.prepare(b)\n\t\t\t\t\t\t\t\tif err == arenaskl.ErrArenaFull {\n\t\t\t\t\t\t\t\t\tmem = newMemTable(memTableOptions{})\n\t\t\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\t\t\treturn nil, err\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t_, err := wal.SyncRecord(b.data, syncWG, syncErr)\n\t\t\t\t\t\t\treturn mem, err\n\t\t\t\t\t\t},\n\t\t\t\t\t}\n\t\t\t\t\tp := newCommitPipeline(nullCommitEnv)\n\t\t\t\t\twal = record.NewLogWriter(io.Discard, 0, /* logNum */\n\t\t\t\t\t\trecord.LogWriterConfig{\n\t\t\t\t\t\t\tWALFsyncLatency: prometheus.NewHistogram(prometheus.HistogramOpts{}),\n\t\t\t\t\t\t\tQueueSemChan:    p.logSyncQSem,\n\t\t\t\t\t\t})\n\t\t\t\t\tconst keySize = 8\n\t\t\t\t\tb.SetBytes(2 * keySize)\n\t\t\t\t\tb.ResetTimer()\n\n\t\t\t\t\tb.RunParallel(func(pb *testing.PB) {\n\t\t\t\t\t\trng := rand.New(rand.NewPCG(0, uint64(time.Now().UnixNano())))\n\t\t\t\t\t\tbuf := make([]byte, keySize)\n\n\t\t\t\t\t\tfor pb.Next() {\n\t\t\t\t\t\t\tbatch := newBatch(nil)\n\t\t\t\t\t\t\tbinary.BigEndian.PutUint64(buf, rng.Uint64())\n\t\t\t\t\t\t\tbatch.Set(buf, buf, nil)\n\t\t\t\t\t\t\tif err := p.Commit(batch, true /* sync */, noSyncWait); err != nil {\n\t\t\t\t\t\t\t\tb.Fatal(err)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif noSyncWait {\n\t\t\t\t\t\t\t\tif err := batch.SyncWait(); err != nil {\n\t\t\t\t\t\t\t\t\tb.Fatal(err)\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tbatch.Close()\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\t\t\t\t})\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "compaction.go",
          "type": "blob",
          "size": 115.8154296875,
          "content": "// Copyright 2013 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"math\"\n\t\"runtime/pprof\"\n\t\"slices\"\n\t\"sort\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/crlib/crtime\"\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/compact\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan/keyspanimpl\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n\t\"github.com/cockroachdb/pebble/internal/sstableinternal\"\n\t\"github.com/cockroachdb/pebble/objstorage\"\n\t\"github.com/cockroachdb/pebble/objstorage/objstorageprovider/objiotracing\"\n\t\"github.com/cockroachdb/pebble/objstorage/remote\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n)\n\nvar errEmptyTable = errors.New(\"pebble: empty table\")\n\n// ErrCancelledCompaction is returned if a compaction is cancelled by a\n// concurrent excise or ingest-split operation.\nvar ErrCancelledCompaction = errors.New(\"pebble: compaction cancelled by a concurrent operation, will retry compaction\")\n\nvar flushLabels = pprof.Labels(\"pebble\", \"flush\", \"output-level\", \"L0\")\nvar gcLabels = pprof.Labels(\"pebble\", \"gc\")\n\n// expandedCompactionByteSizeLimit is the maximum number of bytes in all\n// compacted files. We avoid expanding the lower level file set of a compaction\n// if it would make the total compaction cover more than this many bytes.\nfunc expandedCompactionByteSizeLimit(opts *Options, level int, availBytes uint64) uint64 {\n\tv := uint64(25 * opts.Level(level).TargetFileSize)\n\n\t// Never expand a compaction beyond half the available capacity, divided\n\t// by the maximum number of concurrent compactions. Each of the concurrent\n\t// compactions may expand up to this limit, so this attempts to limit\n\t// compactions to half of available disk space. Note that this will not\n\t// prevent compaction picking from pursuing compactions that are larger\n\t// than this threshold before expansion.\n\tdiskMax := (availBytes / 2) / uint64(opts.MaxConcurrentCompactions())\n\tif v > diskMax {\n\t\tv = diskMax\n\t}\n\treturn v\n}\n\n// maxGrandparentOverlapBytes is the maximum bytes of overlap with level+1\n// before we stop building a single file in a level-1 to level compaction.\nfunc maxGrandparentOverlapBytes(opts *Options, level int) uint64 {\n\treturn uint64(10 * opts.Level(level).TargetFileSize)\n}\n\n// maxReadCompactionBytes is used to prevent read compactions which\n// are too wide.\nfunc maxReadCompactionBytes(opts *Options, level int) uint64 {\n\treturn uint64(10 * opts.Level(level).TargetFileSize)\n}\n\n// noCloseIter wraps around a FragmentIterator, intercepting and eliding\n// calls to Close. It is used during compaction to ensure that rangeDelIters\n// are not closed prematurely.\ntype noCloseIter struct {\n\tkeyspan.FragmentIterator\n}\n\nfunc (i *noCloseIter) Close() {}\n\ntype compactionLevel struct {\n\tlevel int\n\tfiles manifest.LevelSlice\n\t// l0SublevelInfo contains information about L0 sublevels being compacted.\n\t// It's only set for the start level of a compaction starting out of L0 and\n\t// is nil for all other compactions.\n\tl0SublevelInfo []sublevelInfo\n}\n\nfunc (cl compactionLevel) Clone() compactionLevel {\n\tnewCL := compactionLevel{\n\t\tlevel: cl.level,\n\t\tfiles: cl.files,\n\t}\n\treturn newCL\n}\nfunc (cl compactionLevel) String() string {\n\treturn fmt.Sprintf(`Level %d, Files %s`, cl.level, cl.files)\n}\n\n// compactionWritable is a objstorage.Writable wrapper that, on every write,\n// updates a metric in `versions` on bytes written by in-progress compactions so\n// far. It also increments a per-compaction `written` int.\ntype compactionWritable struct {\n\tobjstorage.Writable\n\n\tversions *versionSet\n\twritten  *int64\n}\n\n// Write is part of the objstorage.Writable interface.\nfunc (c *compactionWritable) Write(p []byte) error {\n\tif err := c.Writable.Write(p); err != nil {\n\t\treturn err\n\t}\n\n\t*c.written += int64(len(p))\n\tc.versions.incrementCompactionBytes(int64(len(p)))\n\treturn nil\n}\n\ntype compactionKind int\n\nconst (\n\tcompactionKindDefault compactionKind = iota\n\tcompactionKindFlush\n\t// compactionKindMove denotes a move compaction where the input file is\n\t// retained and linked in a new level without being obsoleted.\n\tcompactionKindMove\n\t// compactionKindCopy denotes a copy compaction where the input file is\n\t// copied byte-by-byte into a new file with a new FileNum in the output level.\n\tcompactionKindCopy\n\t// compactionKindDeleteOnly denotes a compaction that only deletes input\n\t// files. It can occur when wide range tombstones completely contain sstables.\n\tcompactionKindDeleteOnly\n\tcompactionKindElisionOnly\n\tcompactionKindRead\n\tcompactionKindTombstoneDensity\n\tcompactionKindRewrite\n\tcompactionKindIngestedFlushable\n)\n\nfunc (k compactionKind) String() string {\n\tswitch k {\n\tcase compactionKindDefault:\n\t\treturn \"default\"\n\tcase compactionKindFlush:\n\t\treturn \"flush\"\n\tcase compactionKindMove:\n\t\treturn \"move\"\n\tcase compactionKindDeleteOnly:\n\t\treturn \"delete-only\"\n\tcase compactionKindElisionOnly:\n\t\treturn \"elision-only\"\n\tcase compactionKindRead:\n\t\treturn \"read\"\n\tcase compactionKindTombstoneDensity:\n\t\treturn \"tombstone-density\"\n\tcase compactionKindRewrite:\n\t\treturn \"rewrite\"\n\tcase compactionKindIngestedFlushable:\n\t\treturn \"ingested-flushable\"\n\tcase compactionKindCopy:\n\t\treturn \"copy\"\n\t}\n\treturn \"?\"\n}\n\n// compaction is a table compaction from one level to the next, starting from a\n// given version.\ntype compaction struct {\n\t// cancel is a bool that can be used by other goroutines to signal a compaction\n\t// to cancel, such as if a conflicting excise operation raced it to manifest\n\t// application. Only holders of the manifest lock will write to this atomic.\n\tcancel atomic.Bool\n\n\tkind compactionKind\n\t// isDownload is true if this compaction was started as part of a Download\n\t// operation. In this case kind is compactionKindCopy or\n\t// compactionKindRewrite.\n\tisDownload bool\n\n\tcmp       Compare\n\tequal     Equal\n\tcomparer  *base.Comparer\n\tformatKey base.FormatKey\n\tlogger    Logger\n\tversion   *version\n\tstats     base.InternalIteratorStats\n\tbeganAt   time.Time\n\t// versionEditApplied is set to true when a compaction has completed and the\n\t// resulting version has been installed (if successful), but the compaction\n\t// goroutine is still cleaning up (eg, deleting obsolete files).\n\tversionEditApplied bool\n\tbufferPool         sstable.BufferPool\n\n\t// startLevel is the level that is being compacted. Inputs from startLevel\n\t// and outputLevel will be merged to produce a set of outputLevel files.\n\tstartLevel *compactionLevel\n\n\t// outputLevel is the level that files are being produced in. outputLevel is\n\t// equal to startLevel+1 except when:\n\t//    - if startLevel is 0, the output level equals compactionPicker.baseLevel().\n\t//    - in multilevel compaction, the output level is the lowest level involved in\n\t//      the compaction\n\t// A compaction's outputLevel is nil for delete-only compactions.\n\toutputLevel *compactionLevel\n\n\t// extraLevels point to additional levels in between the input and output\n\t// levels that get compacted in multilevel compactions\n\textraLevels []*compactionLevel\n\n\tinputs []compactionLevel\n\n\t// maxOutputFileSize is the maximum size of an individual table created\n\t// during compaction.\n\tmaxOutputFileSize uint64\n\t// maxOverlapBytes is the maximum number of bytes of overlap allowed for a\n\t// single output table with the tables in the grandparent level.\n\tmaxOverlapBytes uint64\n\n\t// flushing contains the flushables (aka memtables) that are being flushed.\n\tflushing flushableList\n\t// bytesWritten contains the number of bytes that have been written to outputs.\n\tbytesWritten int64\n\n\t// The boundaries of the input data.\n\tsmallest InternalKey\n\tlargest  InternalKey\n\n\t// A list of fragment iterators to close when the compaction finishes. Used by\n\t// input iteration to keep rangeDelIters open for the lifetime of the\n\t// compaction, and only close them when the compaction finishes.\n\tclosers []*noCloseIter\n\n\t// grandparents are the tables in level+2 that overlap with the files being\n\t// compacted. Used to determine output table boundaries. Do not assume that the actual files\n\t// in the grandparent when this compaction finishes will be the same.\n\tgrandparents manifest.LevelSlice\n\n\t// Boundaries at which flushes to L0 should be split. Determined by\n\t// L0Sublevels. If nil, flushes aren't split.\n\tl0Limits [][]byte\n\n\tdelElision      compact.TombstoneElision\n\trangeKeyElision compact.TombstoneElision\n\n\t// allowedZeroSeqNum is true if seqnums can be zeroed if there are no\n\t// snapshots requiring them to be kept. This determination is made by\n\t// looking for an sstable which overlaps the bounds of the compaction at a\n\t// lower level in the LSM during runCompaction.\n\tallowedZeroSeqNum bool\n\n\t// deletionHints are set if this is a compactionKindDeleteOnly. Used to figure\n\t// out whether an input must be deleted in its entirety, or excised into\n\t// virtual sstables.\n\tdeletionHints []deleteCompactionHint\n\n\t// exciseEnabled is set to true if this is a compactionKindDeleteOnly and\n\t// this compaction is allowed to excise files.\n\texciseEnabled bool\n\n\tmetrics map[int]*LevelMetrics\n\n\tpickerMetrics compactionPickerMetrics\n\n\tslot base.CompactionSlot\n}\n\n// inputLargestSeqNumAbsolute returns the maximum LargestSeqNumAbsolute of any\n// input sstables.\nfunc (c *compaction) inputLargestSeqNumAbsolute() base.SeqNum {\n\tvar seqNum base.SeqNum\n\tfor _, cl := range c.inputs {\n\t\tcl.files.Each(func(m *manifest.FileMetadata) {\n\t\t\tseqNum = max(seqNum, m.LargestSeqNumAbsolute)\n\t\t})\n\t}\n\treturn seqNum\n}\n\nfunc (c *compaction) makeInfo(jobID JobID) CompactionInfo {\n\tinfo := CompactionInfo{\n\t\tJobID:       int(jobID),\n\t\tReason:      c.kind.String(),\n\t\tInput:       make([]LevelInfo, 0, len(c.inputs)),\n\t\tAnnotations: []string{},\n\t}\n\tif c.isDownload {\n\t\tinfo.Reason = \"download,\" + info.Reason\n\t}\n\tfor _, cl := range c.inputs {\n\t\tinputInfo := LevelInfo{Level: cl.level, Tables: nil}\n\t\titer := cl.files.Iter()\n\t\tfor m := iter.First(); m != nil; m = iter.Next() {\n\t\t\tinputInfo.Tables = append(inputInfo.Tables, m.TableInfo())\n\t\t}\n\t\tinfo.Input = append(info.Input, inputInfo)\n\t}\n\tif c.outputLevel != nil {\n\t\tinfo.Output.Level = c.outputLevel.level\n\n\t\t// If there are no inputs from the output level (eg, a move\n\t\t// compaction), add an empty LevelInfo to info.Input.\n\t\tif len(c.inputs) > 0 && c.inputs[len(c.inputs)-1].level != c.outputLevel.level {\n\t\t\tinfo.Input = append(info.Input, LevelInfo{Level: c.outputLevel.level})\n\t\t}\n\t} else {\n\t\t// For a delete-only compaction, set the output level to L6. The\n\t\t// output level is not meaningful here, but complicating the\n\t\t// info.Output interface with a pointer doesn't seem worth the\n\t\t// semantic distinction.\n\t\tinfo.Output.Level = numLevels - 1\n\t}\n\n\tfor i, score := range c.pickerMetrics.scores {\n\t\tinfo.Input[i].Score = score\n\t}\n\tinfo.SingleLevelOverlappingRatio = c.pickerMetrics.singleLevelOverlappingRatio\n\tinfo.MultiLevelOverlappingRatio = c.pickerMetrics.multiLevelOverlappingRatio\n\tif len(info.Input) > 2 {\n\t\tinfo.Annotations = append(info.Annotations, \"multilevel\")\n\t}\n\treturn info\n}\n\nfunc (c *compaction) userKeyBounds() base.UserKeyBounds {\n\treturn base.UserKeyBoundsFromInternal(c.smallest, c.largest)\n}\n\nfunc newCompaction(\n\tpc *pickedCompaction,\n\topts *Options,\n\tbeganAt time.Time,\n\tprovider objstorage.Provider,\n\tslot base.CompactionSlot,\n) *compaction {\n\tc := &compaction{\n\t\tkind:              compactionKindDefault,\n\t\tcmp:               pc.cmp,\n\t\tequal:             opts.Comparer.Equal,\n\t\tcomparer:          opts.Comparer,\n\t\tformatKey:         opts.Comparer.FormatKey,\n\t\tinputs:            pc.inputs,\n\t\tsmallest:          pc.smallest,\n\t\tlargest:           pc.largest,\n\t\tlogger:            opts.Logger,\n\t\tversion:           pc.version,\n\t\tbeganAt:           beganAt,\n\t\tmaxOutputFileSize: pc.maxOutputFileSize,\n\t\tmaxOverlapBytes:   pc.maxOverlapBytes,\n\t\tpickerMetrics:     pc.pickerMetrics,\n\t\tslot:              slot,\n\t}\n\tc.startLevel = &c.inputs[0]\n\tif pc.startLevel.l0SublevelInfo != nil {\n\t\tc.startLevel.l0SublevelInfo = pc.startLevel.l0SublevelInfo\n\t}\n\tc.outputLevel = &c.inputs[1]\n\tif c.slot == nil {\n\t\tc.slot = opts.Experimental.CompactionLimiter.TookWithoutPermission(context.TODO())\n\t\tc.slot.CompactionSelected(c.startLevel.level, c.outputLevel.level, c.startLevel.files.SizeSum())\n\t}\n\n\tif len(pc.extraLevels) > 0 {\n\t\tc.extraLevels = pc.extraLevels\n\t\tc.outputLevel = &c.inputs[len(c.inputs)-1]\n\t}\n\t// Compute the set of outputLevel+1 files that overlap this compaction (these\n\t// are the grandparent sstables).\n\tif c.outputLevel.level+1 < numLevels {\n\t\tc.grandparents = c.version.Overlaps(c.outputLevel.level+1, c.userKeyBounds())\n\t}\n\tc.delElision, c.rangeKeyElision = compact.SetupTombstoneElision(\n\t\tc.cmp, c.version, c.outputLevel.level, base.UserKeyBoundsFromInternal(c.smallest, c.largest),\n\t)\n\tc.kind = pc.kind\n\n\tif c.kind == compactionKindDefault && c.outputLevel.files.Empty() && !c.hasExtraLevelData() &&\n\t\tc.startLevel.files.Len() == 1 && c.grandparents.SizeSum() <= c.maxOverlapBytes {\n\t\t// This compaction can be converted into a move or copy from one level\n\t\t// to the next. We avoid such a move if there is lots of overlapping\n\t\t// grandparent data. Otherwise, the move could create a parent file\n\t\t// that will require a very expensive merge later on.\n\t\titer := c.startLevel.files.Iter()\n\t\tmeta := iter.First()\n\t\tisRemote := false\n\t\t// We should always be passed a provider, except in some unit tests.\n\t\tif provider != nil {\n\t\t\tisRemote = !objstorage.IsLocalTable(provider, meta.FileBacking.DiskFileNum)\n\t\t}\n\t\t// Avoid a trivial move or copy if all of these are true, as rewriting a\n\t\t// new file is better:\n\t\t//\n\t\t// 1) The source file is a virtual sstable\n\t\t// 2) The existing file `meta` is on non-remote storage\n\t\t// 3) The output level prefers shared storage\n\t\tmustCopy := !isRemote && remote.ShouldCreateShared(opts.Experimental.CreateOnShared, c.outputLevel.level)\n\t\tif mustCopy {\n\t\t\t// If the source is virtual, it's best to just rewrite the file as all\n\t\t\t// conditions in the above comment are met.\n\t\t\tif !meta.Virtual {\n\t\t\t\tc.kind = compactionKindCopy\n\t\t\t}\n\t\t} else {\n\t\t\tc.kind = compactionKindMove\n\t\t}\n\t}\n\treturn c\n}\n\nfunc newDeleteOnlyCompaction(\n\topts *Options,\n\tcur *version,\n\tinputs []compactionLevel,\n\tbeganAt time.Time,\n\thints []deleteCompactionHint,\n\texciseEnabled bool,\n) *compaction {\n\tc := &compaction{\n\t\tkind:          compactionKindDeleteOnly,\n\t\tcmp:           opts.Comparer.Compare,\n\t\tequal:         opts.Comparer.Equal,\n\t\tcomparer:      opts.Comparer,\n\t\tformatKey:     opts.Comparer.FormatKey,\n\t\tlogger:        opts.Logger,\n\t\tversion:       cur,\n\t\tbeganAt:       beganAt,\n\t\tinputs:        inputs,\n\t\tdeletionHints: hints,\n\t\texciseEnabled: exciseEnabled,\n\t}\n\n\t// Set c.smallest, c.largest.\n\tfiles := make([]manifest.LevelIterator, 0, len(inputs))\n\tfor _, in := range inputs {\n\t\tfiles = append(files, in.files.Iter())\n\t}\n\tc.smallest, c.largest = manifest.KeyRange(opts.Comparer.Compare, files...)\n\treturn c\n}\n\nfunc adjustGrandparentOverlapBytesForFlush(c *compaction, flushingBytes uint64) {\n\t// Heuristic to place a lower bound on compaction output file size\n\t// caused by Lbase. Prior to this heuristic we have observed an L0 in\n\t// production with 310K files of which 290K files were < 10KB in size.\n\t// Our hypothesis is that it was caused by L1 having 2600 files and\n\t// ~10GB, such that each flush got split into many tiny files due to\n\t// overlapping with most of the files in Lbase.\n\t//\n\t// The computation below is general in that it accounts\n\t// for flushing different volumes of data (e.g. we may be flushing\n\t// many memtables). For illustration, we consider the typical\n\t// example of flushing a 64MB memtable. So 12.8MB output,\n\t// based on the compression guess below. If the compressed bytes\n\t// guess is an over-estimate we will end up with smaller files,\n\t// and if an under-estimate we will end up with larger files.\n\t// With a 2MB target file size, 7 files. We are willing to accept\n\t// 4x the number of files, if it results in better write amplification\n\t// when later compacting to Lbase, i.e., ~450KB files (target file\n\t// size / 4).\n\t//\n\t// Note that this is a pessimistic heuristic in that\n\t// fileCountUpperBoundDueToGrandparents could be far from the actual\n\t// number of files produced due to the grandparent limits. For\n\t// example, in the extreme, consider a flush that overlaps with 1000\n\t// files in Lbase f0...f999, and the initially calculated value of\n\t// maxOverlapBytes will cause splits at f10, f20,..., f990, which\n\t// means an upper bound file count of 100 files. Say the input bytes\n\t// in the flush are such that acceptableFileCount=10. We will fatten\n\t// up maxOverlapBytes by 10x to ensure that the upper bound file count\n\t// drops to 10. However, it is possible that in practice, even without\n\t// this change, we would have produced no more than 10 files, and that\n\t// this change makes the files unnecessarily wide. Say the input bytes\n\t// are distributed such that 10% are in f0...f9, 10% in f10...f19, ...\n\t// 10% in f80...f89 and 10% in f990...f999. The original value of\n\t// maxOverlapBytes would have actually produced only 10 sstables. But\n\t// by increasing maxOverlapBytes by 10x, we may produce 1 sstable that\n\t// spans f0...f89, i.e., a much wider sstable than necessary.\n\t//\n\t// We could produce a tighter estimate of\n\t// fileCountUpperBoundDueToGrandparents if we had knowledge of the key\n\t// distribution of the flush. The 4x multiplier mentioned earlier is\n\t// a way to try to compensate for this pessimism.\n\t//\n\t// TODO(sumeer): we don't have compression info for the data being\n\t// flushed, but it is likely that existing files that overlap with\n\t// this flush in Lbase are representative wrt compression ratio. We\n\t// could store the uncompressed size in FileMetadata and estimate\n\t// the compression ratio.\n\tconst approxCompressionRatio = 0.2\n\tapproxOutputBytes := approxCompressionRatio * float64(flushingBytes)\n\tapproxNumFilesBasedOnTargetSize :=\n\t\tint(math.Ceil(approxOutputBytes / float64(c.maxOutputFileSize)))\n\tacceptableFileCount := float64(4 * approxNumFilesBasedOnTargetSize)\n\t// The byte calculation is linear in numGrandparentFiles, but we will\n\t// incur this linear cost in compact.Runner.TableSplitLimit() too, so we are\n\t// also willing to pay it now. We could approximate this cheaply by using the\n\t// mean file size of Lbase.\n\tgrandparentFileBytes := c.grandparents.SizeSum()\n\tfileCountUpperBoundDueToGrandparents :=\n\t\tfloat64(grandparentFileBytes) / float64(c.maxOverlapBytes)\n\tif fileCountUpperBoundDueToGrandparents > acceptableFileCount {\n\t\tc.maxOverlapBytes = uint64(\n\t\t\tfloat64(c.maxOverlapBytes) *\n\t\t\t\t(fileCountUpperBoundDueToGrandparents / acceptableFileCount))\n\t}\n}\n\nfunc newFlush(\n\topts *Options, cur *version, baseLevel int, flushing flushableList, beganAt time.Time,\n) (*compaction, error) {\n\tc := &compaction{\n\t\tkind:              compactionKindFlush,\n\t\tcmp:               opts.Comparer.Compare,\n\t\tequal:             opts.Comparer.Equal,\n\t\tcomparer:          opts.Comparer,\n\t\tformatKey:         opts.Comparer.FormatKey,\n\t\tlogger:            opts.Logger,\n\t\tversion:           cur,\n\t\tbeganAt:           beganAt,\n\t\tinputs:            []compactionLevel{{level: -1}, {level: 0}},\n\t\tmaxOutputFileSize: math.MaxUint64,\n\t\tmaxOverlapBytes:   math.MaxUint64,\n\t\tflushing:          flushing,\n\t}\n\tc.startLevel = &c.inputs[0]\n\tc.outputLevel = &c.inputs[1]\n\n\t// Flush slots are always taken without permission.\n\t//\n\t// NB: CompactionLimiter defaults to a no-op limiter unless one is implemented\n\t// and passed-in as an option during Open.\n\tslot := opts.Experimental.CompactionLimiter.TookWithoutPermission(context.TODO())\n\tvar flushingSize uint64\n\tfor i := range flushing {\n\t\tflushingSize += flushing[i].totalBytes()\n\t}\n\tslot.CompactionSelected(-1, 0, flushingSize)\n\tc.slot = slot\n\n\tif len(flushing) > 0 {\n\t\tif _, ok := flushing[0].flushable.(*ingestedFlushable); ok {\n\t\t\tif len(flushing) != 1 {\n\t\t\t\tpanic(\"pebble: ingestedFlushable must be flushed one at a time.\")\n\t\t\t}\n\t\t\tc.kind = compactionKindIngestedFlushable\n\t\t\treturn c, nil\n\t\t}\n\t}\n\n\t// Make sure there's no ingestedFlushable after the first flushable in the\n\t// list.\n\tfor _, f := range flushing {\n\t\tif _, ok := f.flushable.(*ingestedFlushable); ok {\n\t\t\tpanic(\"pebble: flushing shouldn't contain ingestedFlushable flushable\")\n\t\t}\n\t}\n\n\tif cur.L0Sublevels != nil {\n\t\tc.l0Limits = cur.L0Sublevels.FlushSplitKeys()\n\t}\n\n\tsmallestSet, largestSet := false, false\n\tupdatePointBounds := func(iter internalIterator) {\n\t\tif kv := iter.First(); kv != nil {\n\t\t\tif !smallestSet ||\n\t\t\t\tbase.InternalCompare(c.cmp, c.smallest, kv.K) > 0 {\n\t\t\t\tsmallestSet = true\n\t\t\t\tc.smallest = kv.K.Clone()\n\t\t\t}\n\t\t}\n\t\tif kv := iter.Last(); kv != nil {\n\t\t\tif !largestSet ||\n\t\t\t\tbase.InternalCompare(c.cmp, c.largest, kv.K) < 0 {\n\t\t\t\tlargestSet = true\n\t\t\t\tc.largest = kv.K.Clone()\n\t\t\t}\n\t\t}\n\t}\n\n\tupdateRangeBounds := func(iter keyspan.FragmentIterator) error {\n\t\t// File bounds require s != nil && !s.Empty(). We only need to check for\n\t\t// s != nil here, as the memtable's FragmentIterator would never surface\n\t\t// empty spans.\n\t\tif s, err := iter.First(); err != nil {\n\t\t\treturn err\n\t\t} else if s != nil {\n\t\t\tif key := s.SmallestKey(); !smallestSet ||\n\t\t\t\tbase.InternalCompare(c.cmp, c.smallest, key) > 0 {\n\t\t\t\tsmallestSet = true\n\t\t\t\tc.smallest = key.Clone()\n\t\t\t}\n\t\t}\n\t\tif s, err := iter.Last(); err != nil {\n\t\t\treturn err\n\t\t} else if s != nil {\n\t\t\tif key := s.LargestKey(); !largestSet ||\n\t\t\t\tbase.InternalCompare(c.cmp, c.largest, key) < 0 {\n\t\t\t\tlargestSet = true\n\t\t\t\tc.largest = key.Clone()\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}\n\n\tvar flushingBytes uint64\n\tfor i := range flushing {\n\t\tf := flushing[i]\n\t\tupdatePointBounds(f.newIter(nil))\n\t\tif rangeDelIter := f.newRangeDelIter(nil); rangeDelIter != nil {\n\t\t\tif err := updateRangeBounds(rangeDelIter); err != nil {\n\t\t\t\tc.slot.Release(0)\n\t\t\t\tc.slot = nil\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t\tif rangeKeyIter := f.newRangeKeyIter(nil); rangeKeyIter != nil {\n\t\t\tif err := updateRangeBounds(rangeKeyIter); err != nil {\n\t\t\t\tc.slot.Release(0)\n\t\t\t\tc.slot = nil\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t\tflushingBytes += f.inuseBytes()\n\t}\n\n\tif opts.FlushSplitBytes > 0 {\n\t\tc.maxOutputFileSize = uint64(opts.Level(0).TargetFileSize)\n\t\tc.maxOverlapBytes = maxGrandparentOverlapBytes(opts, 0)\n\t\tc.grandparents = c.version.Overlaps(baseLevel, c.userKeyBounds())\n\t\tadjustGrandparentOverlapBytesForFlush(c, flushingBytes)\n\t}\n\n\t// We don't elide tombstones for flushes.\n\tc.delElision, c.rangeKeyElision = compact.NoTombstoneElision(), compact.NoTombstoneElision()\n\treturn c, nil\n}\n\nfunc (c *compaction) hasExtraLevelData() bool {\n\tif len(c.extraLevels) == 0 {\n\t\t// not a multi level compaction\n\t\treturn false\n\t} else if c.extraLevels[0].files.Empty() {\n\t\t// a multi level compaction without data in the intermediate input level;\n\t\t// e.g. for a multi level compaction with levels 4,5, and 6, this could\n\t\t// occur if there is no files to compact in 5, or in 5 and 6 (i.e. a move).\n\t\treturn false\n\t}\n\treturn true\n}\n\n// errorOnUserKeyOverlap returns an error if the last two written sstables in\n// this compaction have revisions of the same user key present in both sstables,\n// when it shouldn't (eg. when splitting flushes).\nfunc (c *compaction) errorOnUserKeyOverlap(ve *versionEdit) error {\n\tif n := len(ve.NewFiles); n > 1 {\n\t\tmeta := ve.NewFiles[n-1].Meta\n\t\tprevMeta := ve.NewFiles[n-2].Meta\n\t\tif !prevMeta.Largest.IsExclusiveSentinel() &&\n\t\t\tc.cmp(prevMeta.Largest.UserKey, meta.Smallest.UserKey) >= 0 {\n\t\t\treturn errors.Errorf(\"pebble: compaction split user key across two sstables: %s in %s and %s\",\n\t\t\t\tprevMeta.Largest.Pretty(c.formatKey),\n\t\t\t\tprevMeta.FileNum,\n\t\t\t\tmeta.FileNum)\n\t\t}\n\t}\n\treturn nil\n}\n\n// allowZeroSeqNum returns true if seqnum's can be zeroed if there are no\n// snapshots requiring them to be kept. It performs this determination by\n// looking at the TombstoneElision values which are set up based on sstables\n// which overlap the bounds of the compaction at a lower level in the LSM.\nfunc (c *compaction) allowZeroSeqNum() bool {\n\t// TODO(peter): we disable zeroing of seqnums during flushing to match\n\t// RocksDB behavior and to avoid generating overlapping sstables during\n\t// DB.replayWAL. When replaying WAL files at startup, we flush after each\n\t// WAL is replayed building up a single version edit that is\n\t// applied. Because we don't apply the version edit after each flush, this\n\t// code doesn't know that L0 contains files and zeroing of seqnums should\n\t// be disabled. That is fixable, but it seems safer to just match the\n\t// RocksDB behavior for now.\n\treturn len(c.flushing) == 0 && c.delElision.ElidesEverything() && c.rangeKeyElision.ElidesEverything()\n}\n\n// newInputIters returns an iterator over all the input tables in a compaction.\nfunc (c *compaction) newInputIters(\n\tnewIters tableNewIters, newRangeKeyIter keyspanimpl.TableNewSpanIter,\n) (\n\tpointIter internalIterator,\n\trangeDelIter, rangeKeyIter keyspan.FragmentIterator,\n\tretErr error,\n) {\n\t// Validate the ordering of compaction input files for defense in depth.\n\tif len(c.flushing) == 0 {\n\t\tif c.startLevel.level >= 0 {\n\t\t\terr := manifest.CheckOrdering(c.cmp, c.formatKey,\n\t\t\t\tmanifest.Level(c.startLevel.level), c.startLevel.files.Iter())\n\t\t\tif err != nil {\n\t\t\t\treturn nil, nil, nil, err\n\t\t\t}\n\t\t}\n\t\terr := manifest.CheckOrdering(c.cmp, c.formatKey,\n\t\t\tmanifest.Level(c.outputLevel.level), c.outputLevel.files.Iter())\n\t\tif err != nil {\n\t\t\treturn nil, nil, nil, err\n\t\t}\n\t\tif c.startLevel.level == 0 {\n\t\t\tif c.startLevel.l0SublevelInfo == nil {\n\t\t\t\tpanic(\"l0SublevelInfo not created for compaction out of L0\")\n\t\t\t}\n\t\t\tfor _, info := range c.startLevel.l0SublevelInfo {\n\t\t\t\terr := manifest.CheckOrdering(c.cmp, c.formatKey,\n\t\t\t\t\tinfo.sublevel, info.Iter())\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, nil, nil, err\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif len(c.extraLevels) > 0 {\n\t\t\tif len(c.extraLevels) > 1 {\n\t\t\t\tpanic(\"n>2 multi level compaction not implemented yet\")\n\t\t\t}\n\t\t\tinterLevel := c.extraLevels[0]\n\t\t\terr := manifest.CheckOrdering(c.cmp, c.formatKey,\n\t\t\t\tmanifest.Level(interLevel.level), interLevel.files.Iter())\n\t\t\tif err != nil {\n\t\t\t\treturn nil, nil, nil, err\n\t\t\t}\n\t\t}\n\t}\n\n\t// There are three classes of keys that a compaction needs to process: point\n\t// keys, range deletion tombstones and range keys. Collect all iterators for\n\t// all these classes of keys from all the levels. We'll aggregate them\n\t// together farther below.\n\t//\n\t// numInputLevels is an approximation of the number of iterator levels. Due\n\t// to idiosyncrasies in iterator construction, we may (rarely) exceed this\n\t// initial capacity.\n\tnumInputLevels := max(len(c.flushing), len(c.inputs))\n\titers := make([]internalIterator, 0, numInputLevels)\n\trangeDelIters := make([]keyspan.FragmentIterator, 0, numInputLevels)\n\trangeKeyIters := make([]keyspan.FragmentIterator, 0, numInputLevels)\n\n\t// If construction of the iterator inputs fails, ensure that we close all\n\t// the consitutent iterators.\n\tdefer func() {\n\t\tif retErr != nil {\n\t\t\tfor _, iter := range iters {\n\t\t\t\tif iter != nil {\n\t\t\t\t\titer.Close()\n\t\t\t\t}\n\t\t\t}\n\t\t\tfor _, rangeDelIter := range rangeDelIters {\n\t\t\t\trangeDelIter.Close()\n\t\t\t}\n\t\t}\n\t}()\n\titerOpts := IterOptions{\n\t\tCategory: categoryCompaction,\n\t\tlogger:   c.logger,\n\t}\n\n\t// Populate iters, rangeDelIters and rangeKeyIters with the appropriate\n\t// constituent iterators. This depends on whether this is a flush or a\n\t// compaction.\n\tif len(c.flushing) != 0 {\n\t\t// If flushing, we need to build the input iterators over the memtables\n\t\t// stored in c.flushing.\n\t\tfor i := range c.flushing {\n\t\t\tf := c.flushing[i]\n\t\t\titers = append(iters, f.newFlushIter(nil))\n\t\t\trangeDelIter := f.newRangeDelIter(nil)\n\t\t\tif rangeDelIter != nil {\n\t\t\t\trangeDelIters = append(rangeDelIters, rangeDelIter)\n\t\t\t}\n\t\t\tif rangeKeyIter := f.newRangeKeyIter(nil); rangeKeyIter != nil {\n\t\t\t\trangeKeyIters = append(rangeKeyIters, rangeKeyIter)\n\t\t\t}\n\t\t}\n\t} else {\n\t\taddItersForLevel := func(level *compactionLevel, l manifest.Layer) error {\n\t\t\t// Add a *levelIter for point iterators. Because we don't call\n\t\t\t// initRangeDel, the levelIter will close and forget the range\n\t\t\t// deletion iterator when it steps on to a new file. Surfacing range\n\t\t\t// deletions to compactions are handled below.\n\t\t\titers = append(iters, newLevelIter(context.Background(),\n\t\t\t\titerOpts, c.comparer, newIters, level.files.Iter(), l, internalIterOpts{\n\t\t\t\t\tcompaction: true,\n\t\t\t\t\tbufferPool: &c.bufferPool,\n\t\t\t\t\tstats:      &c.stats,\n\t\t\t\t}))\n\t\t\t// TODO(jackson): Use keyspanimpl.LevelIter to avoid loading all the range\n\t\t\t// deletions into memory upfront. (See #2015, which reverted this.) There\n\t\t\t// will be no user keys that are split between sstables within a level in\n\t\t\t// Cockroach 23.1, which unblocks this optimization.\n\n\t\t\t// Add the range deletion iterator for each file as an independent level\n\t\t\t// in mergingIter, as opposed to making a levelIter out of those. This\n\t\t\t// is safer as levelIter expects all keys coming from underlying\n\t\t\t// iterators to be in order. Due to compaction / tombstone writing\n\t\t\t// logic in finishOutput(), it is possible for range tombstones to not\n\t\t\t// be strictly ordered across all files in one level.\n\t\t\t//\n\t\t\t// Consider this example from the metamorphic tests (also repeated in\n\t\t\t// finishOutput()), consisting of three L3 files with their bounds\n\t\t\t// specified in square brackets next to the file name:\n\t\t\t//\n\t\t\t// ./000240.sst   [tmgc#391,MERGE-tmgc#391,MERGE]\n\t\t\t// tmgc#391,MERGE [786e627a]\n\t\t\t// tmgc-udkatvs#331,RANGEDEL\n\t\t\t//\n\t\t\t// ./000241.sst   [tmgc#384,MERGE-tmgc#384,MERGE]\n\t\t\t// tmgc#384,MERGE [666c7070]\n\t\t\t// tmgc-tvsalezade#383,RANGEDEL\n\t\t\t// tmgc-tvsalezade#331,RANGEDEL\n\t\t\t//\n\t\t\t// ./000242.sst   [tmgc#383,RANGEDEL-tvsalezade#72057594037927935,RANGEDEL]\n\t\t\t// tmgc-tvsalezade#383,RANGEDEL\n\t\t\t// tmgc#375,SET [72646c78766965616c72776865676e79]\n\t\t\t// tmgc-tvsalezade#356,RANGEDEL\n\t\t\t//\n\t\t\t// Here, the range tombstone in 000240.sst falls \"after\" one in\n\t\t\t// 000241.sst, despite 000240.sst being ordered \"before\" 000241.sst for\n\t\t\t// levelIter's purposes. While each file is still consistent before its\n\t\t\t// bounds, it's safer to have all rangedel iterators be visible to\n\t\t\t// mergingIter.\n\t\t\titer := level.files.Iter()\n\t\t\tfor f := iter.First(); f != nil; f = iter.Next() {\n\t\t\t\trangeDelIter, err := c.newRangeDelIter(newIters, iter.Take(), iterOpts, l)\n\t\t\t\tif err != nil {\n\t\t\t\t\t// The error will already be annotated with the BackingFileNum, so\n\t\t\t\t\t// we annotate it with the FileNum.\n\t\t\t\t\treturn errors.Wrapf(err, \"pebble: could not open table %s\", errors.Safe(f.FileNum))\n\t\t\t\t}\n\t\t\t\tif rangeDelIter == nil {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\trangeDelIters = append(rangeDelIters, rangeDelIter)\n\t\t\t\tc.closers = append(c.closers, rangeDelIter)\n\t\t\t}\n\n\t\t\t// Check if this level has any range keys.\n\t\t\thasRangeKeys := false\n\t\t\tfor f := iter.First(); f != nil; f = iter.Next() {\n\t\t\t\tif f.HasRangeKeys {\n\t\t\t\t\thasRangeKeys = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif hasRangeKeys {\n\t\t\t\tnewRangeKeyIterWrapper := func(ctx context.Context, file *manifest.FileMetadata, iterOptions keyspan.SpanIterOptions) (keyspan.FragmentIterator, error) {\n\t\t\t\t\trangeKeyIter, err := newRangeKeyIter(ctx, file, iterOptions)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn nil, err\n\t\t\t\t\t} else if rangeKeyIter == nil {\n\t\t\t\t\t\treturn emptyKeyspanIter, nil\n\t\t\t\t\t}\n\t\t\t\t\t// Ensure that the range key iter is not closed until the compaction is\n\t\t\t\t\t// finished. This is necessary because range key processing\n\t\t\t\t\t// requires the range keys to be held in memory for up to the\n\t\t\t\t\t// lifetime of the compaction.\n\t\t\t\t\tnoCloseIter := &noCloseIter{rangeKeyIter}\n\t\t\t\t\tc.closers = append(c.closers, noCloseIter)\n\n\t\t\t\t\t// We do not need to truncate range keys to sstable boundaries, or\n\t\t\t\t\t// only read within the file's atomic compaction units, unlike with\n\t\t\t\t\t// range tombstones. This is because range keys were added after we\n\t\t\t\t\t// stopped splitting user keys across sstables, so all the range keys\n\t\t\t\t\t// in this sstable must wholly lie within the file's bounds.\n\t\t\t\t\treturn noCloseIter, err\n\t\t\t\t}\n\t\t\t\tli := keyspanimpl.NewLevelIter(\n\t\t\t\t\tcontext.Background(), keyspan.SpanIterOptions{}, c.cmp,\n\t\t\t\t\tnewRangeKeyIterWrapper, level.files.Iter(), l, manifest.KeyTypeRange,\n\t\t\t\t)\n\t\t\t\trangeKeyIters = append(rangeKeyIters, li)\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\n\t\tfor i := range c.inputs {\n\t\t\t// If the level is annotated with l0SublevelInfo, expand it into one\n\t\t\t// level per sublevel.\n\t\t\t// TODO(jackson): Perform this expansion even earlier when we pick the\n\t\t\t// compaction?\n\t\t\tif len(c.inputs[i].l0SublevelInfo) > 0 {\n\t\t\t\tfor _, info := range c.startLevel.l0SublevelInfo {\n\t\t\t\t\tsublevelCompactionLevel := &compactionLevel{0, info.LevelSlice, nil}\n\t\t\t\t\tif err := addItersForLevel(sublevelCompactionLevel, info.sublevel); err != nil {\n\t\t\t\t\t\treturn nil, nil, nil, err\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif err := addItersForLevel(&c.inputs[i], manifest.Level(c.inputs[i].level)); err != nil {\n\t\t\t\treturn nil, nil, nil, err\n\t\t\t}\n\t\t}\n\t}\n\n\t// If there's only one constituent point iterator, we can avoid the overhead\n\t// of a *mergingIter. This is possible, for example, when performing a flush\n\t// of a single memtable. Otherwise, combine all the iterators into a merging\n\t// iter.\n\tpointIter = iters[0]\n\tif len(iters) > 1 {\n\t\tpointIter = newMergingIter(c.logger, &c.stats, c.cmp, nil, iters...)\n\t}\n\n\t// In normal operation, levelIter iterates over the point operations in a\n\t// level, and initializes a rangeDelIter pointer for the range deletions in\n\t// each table. During compaction, we want to iterate over the merged view of\n\t// point operations and range deletions. In order to do this we create one\n\t// levelIter per level to iterate over the point operations, and collect up\n\t// all the range deletion files.\n\t//\n\t// The range deletion levels are combined with a keyspanimpl.MergingIter. The\n\t// resulting merged rangedel iterator is then included using an\n\t// InterleavingIter.\n\t// TODO(jackson): Consider using a defragmenting iterator to stitch together\n\t// logical range deletions that were fragmented due to previous file\n\t// boundaries.\n\tif len(rangeDelIters) > 0 {\n\t\tmi := &keyspanimpl.MergingIter{}\n\t\tmi.Init(c.comparer, keyspan.NoopTransform, new(keyspanimpl.MergingBuffers), rangeDelIters...)\n\t\trangeDelIter = mi\n\t}\n\n\t// If there are range key iterators, we need to combine them using\n\t// keyspanimpl.MergingIter, and then interleave them among the points.\n\tif len(rangeKeyIters) > 0 {\n\t\tmi := &keyspanimpl.MergingIter{}\n\t\tmi.Init(c.comparer, keyspan.NoopTransform, new(keyspanimpl.MergingBuffers), rangeKeyIters...)\n\t\t// TODO(radu): why do we have a defragmenter here but not above?\n\t\tdi := &keyspan.DefragmentingIter{}\n\t\tdi.Init(c.comparer, mi, keyspan.DefragmentInternal, keyspan.StaticDefragmentReducer, new(keyspan.DefragmentingBuffers))\n\t\trangeKeyIter = di\n\t}\n\treturn pointIter, rangeDelIter, rangeKeyIter, nil\n}\n\nfunc (c *compaction) newRangeDelIter(\n\tnewIters tableNewIters, f manifest.LevelFile, opts IterOptions, l manifest.Layer,\n) (*noCloseIter, error) {\n\topts.layer = l\n\titerSet, err := newIters(context.Background(), f.FileMetadata, &opts,\n\t\tinternalIterOpts{\n\t\t\tcompaction: true,\n\t\t\tbufferPool: &c.bufferPool,\n\t\t}, iterRangeDeletions)\n\tif err != nil {\n\t\treturn nil, err\n\t} else if iterSet.rangeDeletion == nil {\n\t\t// The file doesn't contain any range deletions.\n\t\treturn nil, nil\n\t}\n\t// Ensure that rangeDelIter is not closed until the compaction is\n\t// finished. This is necessary because range tombstone processing\n\t// requires the range tombstones to be held in memory for up to the\n\t// lifetime of the compaction.\n\treturn &noCloseIter{iterSet.rangeDeletion}, nil\n}\n\nfunc (c *compaction) String() string {\n\tif len(c.flushing) != 0 {\n\t\treturn \"flush\\n\"\n\t}\n\n\tvar buf bytes.Buffer\n\tfor level := c.startLevel.level; level <= c.outputLevel.level; level++ {\n\t\ti := level - c.startLevel.level\n\t\tfmt.Fprintf(&buf, \"%d:\", level)\n\t\titer := c.inputs[i].files.Iter()\n\t\tfor f := iter.First(); f != nil; f = iter.Next() {\n\t\t\tfmt.Fprintf(&buf, \" %s:%s-%s\", f.FileNum, f.Smallest, f.Largest)\n\t\t}\n\t\tfmt.Fprintf(&buf, \"\\n\")\n\t}\n\treturn buf.String()\n}\n\ntype manualCompaction struct {\n\t// Count of the retries either due to too many concurrent compactions, or a\n\t// concurrent compaction to overlapping levels.\n\tretries     int\n\tlevel       int\n\toutputLevel int\n\tdone        chan error\n\tstart       []byte\n\tend         []byte\n\tsplit       bool\n}\n\ntype readCompaction struct {\n\tlevel int\n\t// [start, end] key ranges are used for de-duping.\n\tstart []byte\n\tend   []byte\n\n\t// The file associated with the compaction.\n\t// If the file no longer belongs in the same\n\t// level, then we skip the compaction.\n\tfileNum base.FileNum\n}\n\nfunc (d *DB) addInProgressCompaction(c *compaction) {\n\td.mu.compact.inProgress[c] = struct{}{}\n\tvar isBase, isIntraL0 bool\n\tfor _, cl := range c.inputs {\n\t\titer := cl.files.Iter()\n\t\tfor f := iter.First(); f != nil; f = iter.Next() {\n\t\t\tif f.IsCompacting() {\n\t\t\t\td.opts.Logger.Fatalf(\"L%d->L%d: %s already being compacted\", c.startLevel.level, c.outputLevel.level, f.FileNum)\n\t\t\t}\n\t\t\tf.SetCompactionState(manifest.CompactionStateCompacting)\n\t\t\tif c.startLevel != nil && c.outputLevel != nil && c.startLevel.level == 0 {\n\t\t\t\tif c.outputLevel.level == 0 {\n\t\t\t\t\tf.IsIntraL0Compacting = true\n\t\t\t\t\tisIntraL0 = true\n\t\t\t\t} else {\n\t\t\t\t\tisBase = true\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif (isIntraL0 || isBase) && c.version.L0Sublevels != nil {\n\t\tl0Inputs := []manifest.LevelSlice{c.startLevel.files}\n\t\tif isIntraL0 {\n\t\t\tl0Inputs = append(l0Inputs, c.outputLevel.files)\n\t\t}\n\t\tif err := c.version.L0Sublevels.UpdateStateForStartedCompaction(l0Inputs, isBase); err != nil {\n\t\t\td.opts.Logger.Fatalf(\"could not update state for compaction: %s\", err)\n\t\t}\n\t}\n}\n\n// Removes compaction markers from files in a compaction. The rollback parameter\n// indicates whether the compaction state should be rolled back to its original\n// state in the case of an unsuccessful compaction.\n//\n// DB.mu must be held when calling this method, however this method can drop and\n// re-acquire that mutex. All writes to the manifest for this compaction should\n// have completed by this point.\nfunc (d *DB) clearCompactingState(c *compaction, rollback bool) {\n\tc.versionEditApplied = true\n\tif c.slot != nil {\n\t\tpanic(\"pebble: compaction slot should have been released before clearing compacting state\")\n\t}\n\tfor _, cl := range c.inputs {\n\t\titer := cl.files.Iter()\n\t\tfor f := iter.First(); f != nil; f = iter.Next() {\n\t\t\tif !f.IsCompacting() {\n\t\t\t\td.opts.Logger.Fatalf(\"L%d->L%d: %s not being compacted\", c.startLevel.level, c.outputLevel.level, f.FileNum)\n\t\t\t}\n\t\t\tif !rollback {\n\t\t\t\t// On success all compactions other than move and delete-only compactions\n\t\t\t\t// transition the file into the Compacted state. Move-compacted files\n\t\t\t\t// become eligible for compaction again and transition back to NotCompacting.\n\t\t\t\t// Delete-only compactions could, on rare occasion, leave files untouched\n\t\t\t\t// (eg. if files have a loose bound), so we revert them all to NotCompacting\n\t\t\t\t// just in case they need to be compacted again.\n\t\t\t\tif c.kind != compactionKindMove && c.kind != compactionKindDeleteOnly {\n\t\t\t\t\tf.SetCompactionState(manifest.CompactionStateCompacted)\n\t\t\t\t} else {\n\t\t\t\t\tf.SetCompactionState(manifest.CompactionStateNotCompacting)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// Else, on rollback, all input files unconditionally transition back to\n\t\t\t\t// NotCompacting.\n\t\t\t\tf.SetCompactionState(manifest.CompactionStateNotCompacting)\n\t\t\t}\n\t\t\tf.IsIntraL0Compacting = false\n\t\t}\n\t}\n\tl0InProgress := inProgressL0Compactions(d.getInProgressCompactionInfoLocked(c))\n\tfunc() {\n\t\t// InitCompactingFileInfo requires that no other manifest writes be\n\t\t// happening in parallel with it, i.e. we're not in the midst of installing\n\t\t// another version. Otherwise, it's possible that we've created another\n\t\t// L0Sublevels instance, but not added it to the versions list, causing\n\t\t// all the indices in FileMetadata to be inaccurate. To ensure this,\n\t\t// grab the manifest lock.\n\t\td.mu.versions.logLock()\n\t\tdefer d.mu.versions.logUnlock()\n\t\td.mu.versions.currentVersion().L0Sublevels.InitCompactingFileInfo(l0InProgress)\n\t}()\n}\n\nfunc (d *DB) calculateDiskAvailableBytes() uint64 {\n\tif space, err := d.opts.FS.GetDiskUsage(d.dirname); err == nil {\n\t\td.diskAvailBytes.Store(space.AvailBytes)\n\t\treturn space.AvailBytes\n\t} else if !errors.Is(err, vfs.ErrUnsupported) {\n\t\td.opts.EventListener.BackgroundError(err)\n\t}\n\treturn d.diskAvailBytes.Load()\n}\n\n// maybeScheduleFlush schedules a flush if necessary.\n//\n// d.mu must be held when calling this.\nfunc (d *DB) maybeScheduleFlush() {\n\tif d.mu.compact.flushing || d.closed.Load() != nil || d.opts.ReadOnly {\n\t\treturn\n\t}\n\tif len(d.mu.mem.queue) <= 1 {\n\t\treturn\n\t}\n\n\tif !d.passedFlushThreshold() {\n\t\treturn\n\t}\n\n\td.mu.compact.flushing = true\n\tgo d.flush()\n}\n\nfunc (d *DB) passedFlushThreshold() bool {\n\tvar n int\n\tvar size uint64\n\tfor ; n < len(d.mu.mem.queue)-1; n++ {\n\t\tif !d.mu.mem.queue[n].readyForFlush() {\n\t\t\tbreak\n\t\t}\n\t\tif d.mu.mem.queue[n].flushForced {\n\t\t\t// A flush was forced. Pretend the memtable size is the configured\n\t\t\t// size. See minFlushSize below.\n\t\t\tsize += d.opts.MemTableSize\n\t\t} else {\n\t\t\tsize += d.mu.mem.queue[n].totalBytes()\n\t\t}\n\t}\n\tif n == 0 {\n\t\t// None of the immutable memtables are ready for flushing.\n\t\treturn false\n\t}\n\n\t// Only flush once the sum of the queued memtable sizes exceeds half the\n\t// configured memtable size. This prevents flushing of memtables at startup\n\t// while we're undergoing the ramp period on the memtable size. See\n\t// DB.newMemTable().\n\tminFlushSize := d.opts.MemTableSize / 2\n\treturn size >= minFlushSize\n}\n\nfunc (d *DB) maybeScheduleDelayedFlush(tbl *memTable, dur time.Duration) {\n\tvar mem *flushableEntry\n\tfor _, m := range d.mu.mem.queue {\n\t\tif m.flushable == tbl {\n\t\t\tmem = m\n\t\t\tbreak\n\t\t}\n\t}\n\tif mem == nil || mem.flushForced {\n\t\treturn\n\t}\n\tdeadline := d.timeNow().Add(dur)\n\tif !mem.delayedFlushForcedAt.IsZero() && deadline.After(mem.delayedFlushForcedAt) {\n\t\t// Already scheduled to flush sooner than within `dur`.\n\t\treturn\n\t}\n\tmem.delayedFlushForcedAt = deadline\n\tgo func() {\n\t\ttimer := time.NewTimer(dur)\n\t\tdefer timer.Stop()\n\n\t\tselect {\n\t\tcase <-d.closedCh:\n\t\t\treturn\n\t\tcase <-mem.flushed:\n\t\t\treturn\n\t\tcase <-timer.C:\n\t\t\td.commit.mu.Lock()\n\t\t\tdefer d.commit.mu.Unlock()\n\t\t\td.mu.Lock()\n\t\t\tdefer d.mu.Unlock()\n\n\t\t\t// NB: The timer may fire concurrently with a call to Close.  If a\n\t\t\t// Close call beat us to acquiring d.mu, d.closed holds ErrClosed,\n\t\t\t// and it's too late to flush anything. Otherwise, the Close call\n\t\t\t// will block on locking d.mu until we've finished scheduling the\n\t\t\t// flush and set `d.mu.compact.flushing` to true. Close will wait\n\t\t\t// for the current flush to complete.\n\t\t\tif d.closed.Load() != nil {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tif d.mu.mem.mutable == tbl {\n\t\t\t\td.makeRoomForWrite(nil)\n\t\t\t} else {\n\t\t\t\tmem.flushForced = true\n\t\t\t}\n\t\t\td.maybeScheduleFlush()\n\t\t}\n\t}()\n}\n\nfunc (d *DB) flush() {\n\tpprof.Do(context.Background(), flushLabels, func(context.Context) {\n\t\tflushingWorkStart := crtime.NowMono()\n\t\td.mu.Lock()\n\t\tdefer d.mu.Unlock()\n\t\tidleDuration := flushingWorkStart.Sub(d.mu.compact.noOngoingFlushStartTime)\n\t\tvar bytesFlushed uint64\n\t\tvar err error\n\t\tif bytesFlushed, err = d.flush1(); err != nil {\n\t\t\t// TODO(peter): count consecutive flush errors and backoff.\n\t\t\td.opts.EventListener.BackgroundError(err)\n\t\t}\n\t\td.mu.compact.flushing = false\n\t\td.mu.compact.noOngoingFlushStartTime = crtime.NowMono()\n\t\tworkDuration := d.mu.compact.noOngoingFlushStartTime.Sub(flushingWorkStart)\n\t\td.mu.compact.flushWriteThroughput.Bytes += int64(bytesFlushed)\n\t\td.mu.compact.flushWriteThroughput.WorkDuration += workDuration\n\t\td.mu.compact.flushWriteThroughput.IdleDuration += idleDuration\n\t\t// More flush work may have arrived while we were flushing, so schedule\n\t\t// another flush if needed.\n\t\td.maybeScheduleFlush()\n\t\t// The flush may have produced too many files in a level, so schedule a\n\t\t// compaction if needed.\n\t\td.maybeScheduleCompaction()\n\t\td.mu.compact.cond.Broadcast()\n\t})\n}\n\n// runIngestFlush is used to generate a flush version edit for sstables which\n// were ingested as flushables. Both DB.mu and the manifest lock must be held\n// while runIngestFlush is called.\nfunc (d *DB) runIngestFlush(c *compaction) (*manifest.VersionEdit, error) {\n\tif len(c.flushing) != 1 {\n\t\tpanic(\"pebble: ingestedFlushable must be flushed one at a time.\")\n\t}\n\tdefer func() {\n\t\tc.slot.Release(0 /* totalBytesWritten */)\n\t\tc.slot = nil\n\t}()\n\n\t// Construct the VersionEdit, levelMetrics etc.\n\tc.metrics = make(map[int]*LevelMetrics, numLevels)\n\t// Finding the target level for ingestion must use the latest version\n\t// after the logLock has been acquired.\n\tc.version = d.mu.versions.currentVersion()\n\n\tbaseLevel := d.mu.versions.picker.getBaseLevel()\n\tve := &versionEdit{}\n\tvar ingestSplitFiles []ingestSplitFile\n\tingestFlushable := c.flushing[0].flushable.(*ingestedFlushable)\n\n\tupdateLevelMetricsOnExcise := func(m *fileMetadata, level int, added []newFileEntry) {\n\t\tlevelMetrics := c.metrics[level]\n\t\tif levelMetrics == nil {\n\t\t\tlevelMetrics = &LevelMetrics{}\n\t\t\tc.metrics[level] = levelMetrics\n\t\t}\n\t\tlevelMetrics.NumFiles--\n\t\tlevelMetrics.Size -= int64(m.Size)\n\t\tfor i := range added {\n\t\t\tlevelMetrics.NumFiles++\n\t\t\tlevelMetrics.Size += int64(added[i].Meta.Size)\n\t\t}\n\t}\n\n\tsuggestSplit := d.opts.Experimental.IngestSplit != nil && d.opts.Experimental.IngestSplit() &&\n\t\td.FormatMajorVersion() >= FormatVirtualSSTables\n\n\tif suggestSplit || ingestFlushable.exciseSpan.Valid() {\n\t\t// We could add deleted files to ve.\n\t\tve.DeletedFiles = make(map[manifest.DeletedFileEntry]*manifest.FileMetadata)\n\t}\n\n\tctx := context.Background()\n\toverlapChecker := &overlapChecker{\n\t\tcomparer: d.opts.Comparer,\n\t\tnewIters: d.newIters,\n\t\topts: IterOptions{\n\t\t\tlogger:   d.opts.Logger,\n\t\t\tCategory: categoryIngest,\n\t\t},\n\t\tv: c.version,\n\t}\n\treplacedFiles := make(map[base.FileNum][]newFileEntry)\n\tfor _, file := range ingestFlushable.files {\n\t\tvar fileToSplit *fileMetadata\n\t\tvar level int\n\n\t\t// This file fits perfectly within the excise span, so we can slot it at L6.\n\t\tif ingestFlushable.exciseSpan.Valid() &&\n\t\t\tingestFlushable.exciseSpan.Contains(d.cmp, file.FileMetadata.Smallest) &&\n\t\t\tingestFlushable.exciseSpan.Contains(d.cmp, file.FileMetadata.Largest) {\n\t\t\tlevel = 6\n\t\t} else {\n\t\t\t// TODO(radu): this can perform I/O; we should not do this while holding DB.mu.\n\t\t\tlsmOverlap, err := overlapChecker.DetermineLSMOverlap(ctx, file.UserKeyBounds())\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tlevel, fileToSplit, err = ingestTargetLevel(\n\t\t\t\tctx, d.cmp, lsmOverlap, baseLevel, d.mu.compact.inProgress, file.FileMetadata, suggestSplit,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\n\t\t// Add the current flushableIngest file to the version.\n\t\tve.NewFiles = append(ve.NewFiles, newFileEntry{Level: level, Meta: file.FileMetadata})\n\t\tif fileToSplit != nil {\n\t\t\tingestSplitFiles = append(ingestSplitFiles, ingestSplitFile{\n\t\t\t\tingestFile: file.FileMetadata,\n\t\t\t\tsplitFile:  fileToSplit,\n\t\t\t\tlevel:      level,\n\t\t\t})\n\t\t}\n\t\tlevelMetrics := c.metrics[level]\n\t\tif levelMetrics == nil {\n\t\t\tlevelMetrics = &LevelMetrics{}\n\t\t\tc.metrics[level] = levelMetrics\n\t\t}\n\t\tlevelMetrics.BytesIngested += file.Size\n\t\tlevelMetrics.TablesIngested++\n\t}\n\tif ingestFlushable.exciseSpan.Valid() {\n\t\t// Iterate through all levels and find files that intersect with exciseSpan.\n\t\tfor l := range c.version.Levels {\n\t\t\toverlaps := c.version.Overlaps(l, base.UserKeyBoundsEndExclusive(ingestFlushable.exciseSpan.Start, ingestFlushable.exciseSpan.End))\n\t\t\titer := overlaps.Iter()\n\n\t\t\tfor m := iter.First(); m != nil; m = iter.Next() {\n\t\t\t\tnewFiles, err := d.excise(context.TODO(), ingestFlushable.exciseSpan.UserKeyBounds(), m, ve, l)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\n\t\t\t\tif _, ok := ve.DeletedFiles[deletedFileEntry{\n\t\t\t\t\tLevel:   l,\n\t\t\t\t\tFileNum: m.FileNum,\n\t\t\t\t}]; !ok {\n\t\t\t\t\t// We did not excise this file.\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\treplacedFiles[m.FileNum] = newFiles\n\t\t\t\tupdateLevelMetricsOnExcise(m, l, newFiles)\n\t\t\t}\n\t\t}\n\t}\n\n\tif len(ingestSplitFiles) > 0 {\n\t\tif err := d.ingestSplit(context.TODO(), ve, updateLevelMetricsOnExcise, ingestSplitFiles, replacedFiles); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn ve, nil\n}\n\n// flush runs a compaction that copies the immutable memtables from memory to\n// disk.\n//\n// d.mu must be held when calling this, but the mutex may be dropped and\n// re-acquired during the course of this method.\nfunc (d *DB) flush1() (bytesFlushed uint64, err error) {\n\t// NB: The flushable queue can contain flushables of type ingestedFlushable.\n\t// The sstables in ingestedFlushable.files must be placed into the appropriate\n\t// level in the lsm. Let's say the flushable queue contains a prefix of\n\t// regular immutable memtables, then an ingestedFlushable, and then the\n\t// mutable memtable. When the flush of the ingestedFlushable is performed,\n\t// it needs an updated view of the lsm. That is, the prefix of immutable\n\t// memtables must have already been flushed. Similarly, if there are two\n\t// contiguous ingestedFlushables in the queue, then the first flushable must\n\t// be flushed, so that the second flushable can see an updated view of the\n\t// lsm.\n\t//\n\t// Given the above, we restrict flushes to either some prefix of regular\n\t// memtables, or a single flushable of type ingestedFlushable. The DB.flush\n\t// function will call DB.maybeScheduleFlush again, so a new flush to finish\n\t// the remaining flush work should be scheduled right away.\n\t//\n\t// NB: Large batches placed in the flushable queue share the WAL with the\n\t// previous memtable in the queue. We must ensure the property that both the\n\t// large batch and the memtable with which it shares a WAL are flushed\n\t// together. The property ensures that the minimum unflushed log number\n\t// isn't incremented incorrectly. Since a flushableBatch.readyToFlush always\n\t// returns true, and since the large batch will always be placed right after\n\t// the memtable with which it shares a WAL, the property is naturally\n\t// ensured. The large batch will always be placed after the memtable with\n\t// which it shares a WAL because we ensure it in DB.commitWrite by holding\n\t// the commitPipeline.mu and then holding DB.mu. As an extra defensive\n\t// measure, if we try to flush the memtable without also flushing the\n\t// flushable batch in the same flush, since the memtable and flushableBatch\n\t// have the same logNum, the logNum invariant check below will trigger.\n\tvar n, inputs int\n\tvar inputBytes uint64\n\tvar ingest bool\n\tfor ; n < len(d.mu.mem.queue)-1; n++ {\n\t\tif f, ok := d.mu.mem.queue[n].flushable.(*ingestedFlushable); ok {\n\t\t\tif n == 0 {\n\t\t\t\t// The first flushable is of type ingestedFlushable. Since these\n\t\t\t\t// must be flushed individually, we perform a flush for just\n\t\t\t\t// this.\n\t\t\t\tif !f.readyForFlush() {\n\t\t\t\t\t// This check is almost unnecessary, but we guard against it\n\t\t\t\t\t// just in case this invariant changes in the future.\n\t\t\t\t\tpanic(\"pebble: ingestedFlushable should always be ready to flush.\")\n\t\t\t\t}\n\t\t\t\t// By setting n = 1, we ensure that the first flushable(n == 0)\n\t\t\t\t// is scheduled for a flush. The number of tables added is equal to the\n\t\t\t\t// number of files in the ingest operation.\n\t\t\t\tn = 1\n\t\t\t\tinputs = len(f.files)\n\t\t\t\tingest = true\n\t\t\t\tbreak\n\t\t\t} else {\n\t\t\t\t// There was some prefix of flushables which weren't of type\n\t\t\t\t// ingestedFlushable. So, perform a flush for those.\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif !d.mu.mem.queue[n].readyForFlush() {\n\t\t\tbreak\n\t\t}\n\t\tinputBytes += d.mu.mem.queue[n].inuseBytes()\n\t}\n\tif n == 0 {\n\t\t// None of the immutable memtables are ready for flushing.\n\t\treturn 0, nil\n\t}\n\tif !ingest {\n\t\t// Flushes of memtables add the prefix of n memtables from the flushable\n\t\t// queue.\n\t\tinputs = n\n\t}\n\n\t// Require that every memtable being flushed has a log number less than the\n\t// new minimum unflushed log number.\n\tminUnflushedLogNum := d.mu.mem.queue[n].logNum\n\tif !d.opts.DisableWAL {\n\t\tfor i := 0; i < n; i++ {\n\t\t\tif logNum := d.mu.mem.queue[i].logNum; logNum >= minUnflushedLogNum {\n\t\t\t\tpanic(errors.AssertionFailedf(\"logNum invariant violated: flushing %d items; %d:type=%T,logNum=%d; %d:type=%T,logNum=%d\",\n\t\t\t\t\tn,\n\t\t\t\t\ti, d.mu.mem.queue[i].flushable, logNum,\n\t\t\t\t\tn, d.mu.mem.queue[n].flushable, minUnflushedLogNum))\n\t\t\t}\n\t\t}\n\t}\n\n\tc, err := newFlush(d.opts, d.mu.versions.currentVersion(),\n\t\td.mu.versions.picker.getBaseLevel(), d.mu.mem.queue[:n], d.timeNow())\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\td.addInProgressCompaction(c)\n\n\tjobID := d.newJobIDLocked()\n\td.opts.EventListener.FlushBegin(FlushInfo{\n\t\tJobID:      int(jobID),\n\t\tInput:      inputs,\n\t\tInputBytes: inputBytes,\n\t\tIngest:     ingest,\n\t})\n\tstartTime := d.timeNow()\n\n\tvar ve *manifest.VersionEdit\n\tvar stats compact.Stats\n\t// To determine the target level of the files in the ingestedFlushable, we\n\t// need to acquire the logLock, and not release it for that duration. Since,\n\t// we need to acquire the logLock below to perform the logAndApply step\n\t// anyway, we create the VersionEdit for ingestedFlushable outside of\n\t// runCompaction. For all other flush cases, we construct the VersionEdit\n\t// inside runCompaction.\n\tif c.kind != compactionKindIngestedFlushable {\n\t\tve, stats, err = d.runCompaction(jobID, c)\n\t}\n\n\t// Acquire logLock. This will be released either on an error, by way of\n\t// logUnlock, or through a call to logAndApply if there is no error.\n\td.mu.versions.logLock()\n\n\tif c.kind == compactionKindIngestedFlushable {\n\t\tve, err = d.runIngestFlush(c)\n\t}\n\n\tinfo := FlushInfo{\n\t\tJobID:      int(jobID),\n\t\tInput:      inputs,\n\t\tInputBytes: inputBytes,\n\t\tDuration:   d.timeNow().Sub(startTime),\n\t\tDone:       true,\n\t\tIngest:     ingest,\n\t\tErr:        err,\n\t}\n\tif err == nil {\n\t\tvalidateVersionEdit(ve, d.opts.Experimental.KeyValidationFunc, d.opts.Comparer.FormatKey, d.opts.Logger)\n\t\tfor i := range ve.NewFiles {\n\t\t\te := &ve.NewFiles[i]\n\t\t\tinfo.Output = append(info.Output, e.Meta.TableInfo())\n\t\t\t// Ingested tables are not necessarily flushed to L0. Record the level of\n\t\t\t// each ingested file explicitly.\n\t\t\tif ingest {\n\t\t\t\tinfo.IngestLevels = append(info.IngestLevels, e.Level)\n\t\t\t}\n\t\t}\n\t\tif len(ve.NewFiles) == 0 {\n\t\t\tinfo.Err = errEmptyTable\n\t\t}\n\n\t\t// The flush succeeded or it produced an empty sstable. In either case we\n\t\t// want to bump the minimum unflushed log number to the log number of the\n\t\t// oldest unflushed memtable.\n\t\tve.MinUnflushedLogNum = minUnflushedLogNum\n\t\tif c.kind != compactionKindIngestedFlushable {\n\t\t\tmetrics := c.metrics[0]\n\t\t\tif d.opts.DisableWAL {\n\t\t\t\t// If the WAL is disabled, every flushable has a zero [logSize],\n\t\t\t\t// resulting in zero bytes in. Instead, use the number of bytes we\n\t\t\t\t// flushed as the BytesIn. This ensures we get a reasonable w-amp\n\t\t\t\t// calculation even when the WAL is disabled.\n\t\t\t\tmetrics.BytesIn = metrics.BytesFlushed\n\t\t\t} else {\n\t\t\t\tfor i := 0; i < n; i++ {\n\t\t\t\t\tmetrics.BytesIn += d.mu.mem.queue[i].logSize\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\t// c.kind == compactionKindIngestedFlushable && we could have deleted files due\n\t\t\t// to ingest-time splits or excises.\n\t\t\tingestFlushable := c.flushing[0].flushable.(*ingestedFlushable)\n\t\t\tfor c2 := range d.mu.compact.inProgress {\n\t\t\t\t// Check if this compaction overlaps with the excise span. Note that just\n\t\t\t\t// checking if the inputs individually overlap with the excise span\n\t\t\t\t// isn't sufficient; for instance, a compaction could have [a,b] and [e,f]\n\t\t\t\t// as inputs and write it all out as [a,b,e,f] in one sstable. If we're\n\t\t\t\t// doing a [c,d) excise at the same time as this compaction, we will have\n\t\t\t\t// to error out the whole compaction as we can't guarantee it hasn't/won't\n\t\t\t\t// write a file overlapping with the excise span.\n\t\t\t\tif ingestFlushable.exciseSpan.OverlapsInternalKeyRange(d.cmp, c2.smallest, c2.largest) {\n\t\t\t\t\tc2.cancel.Store(true)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif len(ve.DeletedFiles) > 0 {\n\t\t\t\t// Iterate through all other compactions, and check if their inputs have\n\t\t\t\t// been replaced due to an ingest-time split or excise. In that case,\n\t\t\t\t// cancel the compaction.\n\t\t\t\tfor c2 := range d.mu.compact.inProgress {\n\t\t\t\t\tfor i := range c2.inputs {\n\t\t\t\t\t\titer := c2.inputs[i].files.Iter()\n\t\t\t\t\t\tfor f := iter.First(); f != nil; f = iter.Next() {\n\t\t\t\t\t\t\tif _, ok := ve.DeletedFiles[deletedFileEntry{FileNum: f.FileNum, Level: c2.inputs[i].level}]; ok {\n\t\t\t\t\t\t\t\tc2.cancel.Store(true)\n\t\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\terr = d.mu.versions.logAndApply(jobID, ve, c.metrics, false, /* forceRotation */\n\t\t\tfunc() []compactionInfo { return d.getInProgressCompactionInfoLocked(c) })\n\t\tif err != nil {\n\t\t\tinfo.Err = err\n\t\t}\n\t} else {\n\t\t// We won't be performing the logAndApply step because of the error,\n\t\t// so logUnlock.\n\t\td.mu.versions.logUnlock()\n\t}\n\n\t// If err != nil, then the flush will be retried, and we will recalculate\n\t// these metrics.\n\tif err == nil {\n\t\td.mu.snapshots.cumulativePinnedCount += stats.CumulativePinnedKeys\n\t\td.mu.snapshots.cumulativePinnedSize += stats.CumulativePinnedSize\n\t\td.mu.versions.metrics.Keys.MissizedTombstonesCount += stats.CountMissizedDels\n\t}\n\n\td.clearCompactingState(c, err != nil)\n\tdelete(d.mu.compact.inProgress, c)\n\td.mu.versions.incrementCompactions(c.kind, c.extraLevels, c.pickerMetrics)\n\n\tvar flushed flushableList\n\tif err == nil {\n\t\tflushed = d.mu.mem.queue[:n]\n\t\td.mu.mem.queue = d.mu.mem.queue[n:]\n\t\td.updateReadStateLocked(d.opts.DebugCheck)\n\t\td.updateTableStatsLocked(ve.NewFiles)\n\t\tif ingest {\n\t\t\td.mu.versions.metrics.Flush.AsIngestCount++\n\t\t\tfor _, l := range c.metrics {\n\t\t\t\td.mu.versions.metrics.Flush.AsIngestBytes += l.BytesIngested\n\t\t\t\td.mu.versions.metrics.Flush.AsIngestTableCount += l.TablesIngested\n\t\t\t}\n\t\t}\n\t\td.maybeTransitionSnapshotsToFileOnlyLocked()\n\n\t}\n\t// Signal FlushEnd after installing the new readState. This helps for unit\n\t// tests that use the callback to trigger a read using an iterator with\n\t// IterOptions.OnlyReadGuaranteedDurable.\n\tinfo.TotalDuration = d.timeNow().Sub(startTime)\n\td.opts.EventListener.FlushEnd(info)\n\n\t// The order of these operations matters here for ease of testing.\n\t// Removing the reader reference first allows tests to be guaranteed that\n\t// the memtable reservation has been released by the time a synchronous\n\t// flush returns. readerUnrefLocked may also produce obsolete files so the\n\t// call to deleteObsoleteFiles must happen after it.\n\tfor i := range flushed {\n\t\tflushed[i].readerUnrefLocked(true)\n\t}\n\n\td.deleteObsoleteFiles(jobID)\n\n\t// Mark all the memtables we flushed as flushed.\n\tfor i := range flushed {\n\t\tclose(flushed[i].flushed)\n\t}\n\n\treturn inputBytes, err\n}\n\n// maybeTransitionSnapshotsToFileOnlyLocked transitions any \"eventually\n// file-only\" snapshots to be file-only if all their visible state has been\n// flushed to sstables.\n//\n// REQUIRES: d.mu.\nfunc (d *DB) maybeTransitionSnapshotsToFileOnlyLocked() {\n\tearliestUnflushedSeqNum := d.getEarliestUnflushedSeqNumLocked()\n\tcurrentVersion := d.mu.versions.currentVersion()\n\tfor s := d.mu.snapshots.root.next; s != &d.mu.snapshots.root; {\n\t\tif s.efos == nil {\n\t\t\ts = s.next\n\t\t\tcontinue\n\t\t}\n\t\toverlapsFlushable := false\n\t\tif base.Visible(earliestUnflushedSeqNum, s.efos.seqNum, base.SeqNumMax) {\n\t\t\t// There are some unflushed keys that are still visible to the EFOS.\n\t\t\t// Check if any memtables older than the EFOS contain keys within a\n\t\t\t// protected range of the EFOS. If no, we can transition.\n\t\t\tprotectedRanges := make([]bounded, len(s.efos.protectedRanges))\n\t\t\tfor i := range s.efos.protectedRanges {\n\t\t\t\tprotectedRanges[i] = s.efos.protectedRanges[i]\n\t\t\t}\n\t\t\tfor i := range d.mu.mem.queue {\n\t\t\t\tif !base.Visible(d.mu.mem.queue[i].logSeqNum, s.efos.seqNum, base.SeqNumMax) {\n\t\t\t\t\t// All keys in this memtable are newer than the EFOS. Skip this\n\t\t\t\t\t// memtable.\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\t// NB: computePossibleOverlaps could have false positives, such as if\n\t\t\t\t// the flushable is a flushable ingest and not a memtable. In that\n\t\t\t\t// case we don't open the sstables to check; we just pessimistically\n\t\t\t\t// assume an overlap.\n\t\t\t\td.mu.mem.queue[i].computePossibleOverlaps(func(b bounded) shouldContinue {\n\t\t\t\t\toverlapsFlushable = true\n\t\t\t\t\treturn stopIteration\n\t\t\t\t}, protectedRanges...)\n\t\t\t\tif overlapsFlushable {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif overlapsFlushable {\n\t\t\ts = s.next\n\t\t\tcontinue\n\t\t}\n\t\tcurrentVersion.Ref()\n\n\t\t// NB: s.efos.transitionToFileOnlySnapshot could close s, in which\n\t\t// case s.next would be nil. Save it before calling it.\n\t\tnext := s.next\n\t\t_ = s.efos.transitionToFileOnlySnapshot(currentVersion)\n\t\ts = next\n\t}\n}\n\n// maybeScheduleCompactionAsync should be used when\n// we want to possibly schedule a compaction, but don't\n// want to eat the cost of running maybeScheduleCompaction.\n// This method should be launched in a separate goroutine.\n// d.mu must not be held when this is called.\nfunc (d *DB) maybeScheduleCompactionAsync() {\n\tdefer d.compactionSchedulers.Done()\n\n\td.mu.Lock()\n\td.maybeScheduleCompaction()\n\td.mu.Unlock()\n}\n\n// maybeScheduleCompaction schedules a compaction if necessary.\n//\n// d.mu must be held when calling this.\nfunc (d *DB) maybeScheduleCompaction() {\n\td.maybeScheduleCompactionPicker(pickAuto)\n}\n\nfunc pickAuto(picker compactionPicker, env compactionEnv) *pickedCompaction {\n\treturn picker.pickAuto(env)\n}\n\nfunc pickElisionOnly(picker compactionPicker, env compactionEnv) *pickedCompaction {\n\treturn picker.pickElisionOnlyCompaction(env)\n}\n\n// tryScheduleDownloadCompaction tries to start a download compaction.\n//\n// Returns true if we started a download compaction (or completed it\n// immediately because it is a no-op or we hit an error).\n//\n// Requires d.mu to be held. Updates d.mu.compact.downloads.\nfunc (d *DB) tryScheduleDownloadCompaction(env compactionEnv, maxConcurrentDownloads int) bool {\n\tvers := d.mu.versions.currentVersion()\n\tfor i := 0; i < len(d.mu.compact.downloads); {\n\t\tdownload := d.mu.compact.downloads[i]\n\t\tswitch d.tryLaunchDownloadCompaction(download, vers, env, maxConcurrentDownloads) {\n\t\tcase launchedCompaction:\n\t\t\treturn true\n\t\tcase didNotLaunchCompaction:\n\t\t\t// See if we can launch a compaction for another download task.\n\t\t\ti++\n\t\tcase downloadTaskCompleted:\n\t\t\t// Task is completed and must be removed.\n\t\t\td.mu.compact.downloads = slices.Delete(d.mu.compact.downloads, i, i+1)\n\t\t}\n\t}\n\treturn false\n}\n\n// maybeScheduleCompactionPicker schedules a compaction if necessary,\n// calling `pickFunc` to pick automatic compactions.\n//\n// Requires d.mu to be held.\nfunc (d *DB) maybeScheduleCompactionPicker(\n\tpickFunc func(compactionPicker, compactionEnv) *pickedCompaction,\n) {\n\tif d.closed.Load() != nil || d.opts.ReadOnly {\n\t\treturn\n\t}\n\tmaxCompactions := d.opts.MaxConcurrentCompactions()\n\tmaxDownloads := d.opts.MaxConcurrentDownloads()\n\n\tif d.mu.compact.compactingCount >= maxCompactions &&\n\t\t(len(d.mu.compact.downloads) == 0 || d.mu.compact.downloadingCount >= maxDownloads) {\n\t\tif len(d.mu.compact.manual) > 0 {\n\t\t\t// Inability to run head blocks later manual compactions.\n\t\t\td.mu.compact.manual[0].retries++\n\t\t}\n\t\treturn\n\t}\n\n\t// Compaction picking needs a coherent view of a Version. In particular, we\n\t// need to exclude concurrent ingestions from making a decision on which level\n\t// to ingest into that conflicts with our compaction\n\t// decision. versionSet.logLock provides the necessary mutual exclusion.\n\td.mu.versions.logLock()\n\tdefer d.mu.versions.logUnlock()\n\n\t// Check for the closed flag again, in case the DB was closed while we were\n\t// waiting for logLock().\n\tif d.closed.Load() != nil {\n\t\treturn\n\t}\n\n\tenv := compactionEnv{\n\t\tdiskAvailBytes:          d.diskAvailBytes.Load(),\n\t\tearliestSnapshotSeqNum:  d.mu.snapshots.earliest(),\n\t\tearliestUnflushedSeqNum: d.getEarliestUnflushedSeqNumLocked(),\n\t}\n\n\tif d.mu.compact.compactingCount < maxCompactions {\n\t\t// Check for delete-only compactions first, because they're expected to be\n\t\t// cheap and reduce future compaction work.\n\t\tif !d.opts.private.disableDeleteOnlyCompactions &&\n\t\t\t!d.opts.DisableAutomaticCompactions &&\n\t\t\tlen(d.mu.compact.deletionHints) > 0 {\n\t\t\td.tryScheduleDeleteOnlyCompaction()\n\t\t}\n\n\t\tfor len(d.mu.compact.manual) > 0 && d.mu.compact.compactingCount < maxCompactions {\n\t\t\tif manual := d.mu.compact.manual[0]; !d.tryScheduleManualCompaction(env, manual) {\n\t\t\t\t// Inability to run head blocks later manual compactions.\n\t\t\t\tmanual.retries++\n\t\t\t\tbreak\n\t\t\t}\n\t\t\td.mu.compact.manual = d.mu.compact.manual[1:]\n\t\t}\n\n\t\tfor !d.opts.DisableAutomaticCompactions && d.mu.compact.compactingCount < maxCompactions &&\n\t\t\td.tryScheduleAutoCompaction(env, pickFunc) {\n\t\t}\n\t}\n\n\tfor len(d.mu.compact.downloads) > 0 && d.mu.compact.downloadingCount < maxDownloads &&\n\t\td.tryScheduleDownloadCompaction(env, maxDownloads) {\n\t}\n}\n\n// tryScheduleDeleteOnlyCompaction tries to kick off a delete-only compaction\n// for all files that can be deleted as suggested by deletionHints.\n//\n// Requires d.mu to be held. Updates d.mu.compact.deletionHints.\nfunc (d *DB) tryScheduleDeleteOnlyCompaction() {\n\tv := d.mu.versions.currentVersion()\n\tsnapshots := d.mu.snapshots.toSlice()\n\t// We need to save the value of exciseEnabled in the compaction itself, as\n\t// it can change dynamically between now and when the compaction runs.\n\texciseEnabled := d.FormatMajorVersion() >= FormatVirtualSSTables &&\n\t\td.opts.Experimental.EnableDeleteOnlyCompactionExcises != nil && d.opts.Experimental.EnableDeleteOnlyCompactionExcises()\n\t// NB: CompactionLimiter defaults to a no-op limiter unless one is implemented\n\t// and passed-in as an option during Open.\n\tlimiter := d.opts.Experimental.CompactionLimiter\n\tvar slot base.CompactionSlot\n\t// TODO(bilal): Should we always take a slot without permission?\n\tif n := len(d.getInProgressCompactionInfoLocked(nil)); n == 0 {\n\t\t// We are not running a compaction at the moment. We should take a compaction slot\n\t\t// without permission.\n\t\tslot = limiter.TookWithoutPermission(context.TODO())\n\t} else {\n\t\tvar err error\n\t\tslot, err = limiter.RequestSlot(context.TODO())\n\t\tif err != nil {\n\t\t\td.opts.EventListener.BackgroundError(err)\n\t\t\treturn\n\t\t}\n\t\tif slot == nil {\n\t\t\t// The limiter is denying us a compaction slot. Yield to other work.\n\t\t\treturn\n\t\t}\n\t}\n\tinputs, resolvedHints, unresolvedHints := checkDeleteCompactionHints(d.cmp, v, d.mu.compact.deletionHints, snapshots, exciseEnabled)\n\td.mu.compact.deletionHints = unresolvedHints\n\n\tif len(inputs) > 0 {\n\t\tc := newDeleteOnlyCompaction(d.opts, v, inputs, d.timeNow(), resolvedHints, exciseEnabled)\n\t\tc.slot = slot\n\t\td.mu.compact.compactingCount++\n\t\td.addInProgressCompaction(c)\n\t\tgo d.compact(c, nil)\n\t} else {\n\t\tslot.Release(0 /* totalBytesWritten */)\n\t}\n}\n\n// tryScheduleManualCompaction tries to kick off the given manual compaction.\n//\n// Returns false if we are not able to run this compaction at this time.\n//\n// Requires d.mu to be held.\nfunc (d *DB) tryScheduleManualCompaction(env compactionEnv, manual *manualCompaction) bool {\n\tv := d.mu.versions.currentVersion()\n\tenv.inProgressCompactions = d.getInProgressCompactionInfoLocked(nil)\n\tpc, retryLater := pickManualCompaction(v, d.opts, env, d.mu.versions.picker.getBaseLevel(), manual)\n\tif pc == nil {\n\t\tif !retryLater {\n\t\t\t// Manual compaction is a no-op. Signal completion and exit.\n\t\t\tmanual.done <- nil\n\t\t\treturn true\n\t\t}\n\t\t// We are not able to run this manual compaction at this time.\n\t\treturn false\n\t}\n\n\tc := newCompaction(pc, d.opts, d.timeNow(), d.ObjProvider(), nil /* compactionSlot */)\n\td.mu.compact.compactingCount++\n\td.addInProgressCompaction(c)\n\tgo d.compact(c, manual.done)\n\treturn true\n}\n\n// tryScheduleAutoCompaction tries to kick off an automatic compaction.\n//\n// Returns false if no automatic compactions are necessary or able to run at\n// this time.\n//\n// Requires d.mu to be held.\nfunc (d *DB) tryScheduleAutoCompaction(\n\tenv compactionEnv, pickFunc func(compactionPicker, compactionEnv) *pickedCompaction,\n) bool {\n\tenv.inProgressCompactions = d.getInProgressCompactionInfoLocked(nil)\n\tenv.readCompactionEnv = readCompactionEnv{\n\t\treadCompactions:          &d.mu.compact.readCompactions,\n\t\tflushing:                 d.mu.compact.flushing || d.passedFlushThreshold(),\n\t\trescheduleReadCompaction: &d.mu.compact.rescheduleReadCompaction,\n\t}\n\t// NB: CompactionLimiter defaults to a no-op limiter unless one is implemented\n\t// and passed-in as an option during Open.\n\tlimiter := d.opts.Experimental.CompactionLimiter\n\tvar slot base.CompactionSlot\n\tif n := len(env.inProgressCompactions); n == 0 {\n\t\t// We are not running a compaction at the moment. We should take a compaction slot\n\t\t// without permission.\n\t\tslot = limiter.TookWithoutPermission(context.TODO())\n\t} else {\n\t\tvar err error\n\t\tslot, err = limiter.RequestSlot(context.TODO())\n\t\tif err != nil {\n\t\t\td.opts.EventListener.BackgroundError(err)\n\t\t\treturn false\n\t\t}\n\t\tif slot == nil {\n\t\t\t// The limiter is denying us a compaction slot. Yield to other work.\n\t\t\treturn false\n\t\t}\n\t}\n\tpc := pickFunc(d.mu.versions.picker, env)\n\tif pc == nil {\n\t\tslot.Release(0 /* bytesWritten */)\n\t\treturn false\n\t}\n\tvar inputSize uint64\n\tfor i := range pc.inputs {\n\t\tinputSize += pc.inputs[i].files.SizeSum()\n\t}\n\tslot.CompactionSelected(pc.startLevel.level, pc.outputLevel.level, inputSize)\n\n\t// Responsibility for releasing slot passes over to the compaction.\n\tc := newCompaction(pc, d.opts, d.timeNow(), d.ObjProvider(), slot)\n\td.mu.compact.compactingCount++\n\td.addInProgressCompaction(c)\n\tgo d.compact(c, nil)\n\treturn true\n}\n\n// deleteCompactionHintType indicates whether the deleteCompactionHint was\n// generated from a span containing a range del (point key only), a range key\n// delete (range key only), or both a point and range key.\ntype deleteCompactionHintType uint8\n\nconst (\n\t// NOTE: While these are primarily used as enumeration types, they are also\n\t// used for some bitwise operations. Care should be taken when updating.\n\tdeleteCompactionHintTypeUnknown deleteCompactionHintType = iota\n\tdeleteCompactionHintTypePointKeyOnly\n\tdeleteCompactionHintTypeRangeKeyOnly\n\tdeleteCompactionHintTypePointAndRangeKey\n)\n\n// String implements fmt.Stringer.\nfunc (h deleteCompactionHintType) String() string {\n\tswitch h {\n\tcase deleteCompactionHintTypeUnknown:\n\t\treturn \"unknown\"\n\tcase deleteCompactionHintTypePointKeyOnly:\n\t\treturn \"point-key-only\"\n\tcase deleteCompactionHintTypeRangeKeyOnly:\n\t\treturn \"range-key-only\"\n\tcase deleteCompactionHintTypePointAndRangeKey:\n\t\treturn \"point-and-range-key\"\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"unknown hint type: %d\", h))\n\t}\n}\n\n// compactionHintFromKeys returns a deleteCompactionHintType given a slice of\n// keyspan.Keys.\nfunc compactionHintFromKeys(keys []keyspan.Key) deleteCompactionHintType {\n\tvar hintType deleteCompactionHintType\n\tfor _, k := range keys {\n\t\tswitch k.Kind() {\n\t\tcase base.InternalKeyKindRangeDelete:\n\t\t\thintType |= deleteCompactionHintTypePointKeyOnly\n\t\tcase base.InternalKeyKindRangeKeyDelete:\n\t\t\thintType |= deleteCompactionHintTypeRangeKeyOnly\n\t\tdefault:\n\t\t\tpanic(fmt.Sprintf(\"unsupported key kind: %s\", k.Kind()))\n\t\t}\n\t}\n\treturn hintType\n}\n\n// A deleteCompactionHint records a user key and sequence number span that has been\n// deleted by a range tombstone. A hint is recorded if at least one sstable\n// falls completely within both the user key and sequence number spans.\n// Once the tombstones and the observed completely-contained sstables fall\n// into the same snapshot stripe, a delete-only compaction may delete any\n// sstables within the range.\ntype deleteCompactionHint struct {\n\t// The type of key span that generated this hint (point key, range key, or\n\t// both).\n\thintType deleteCompactionHintType\n\t// start and end are user keys specifying a key range [start, end) of\n\t// deleted keys.\n\tstart []byte\n\tend   []byte\n\t// The level of the file containing the range tombstone(s) when the hint\n\t// was created. Only lower levels need to be searched for files that may\n\t// be deleted.\n\ttombstoneLevel int\n\t// The file containing the range tombstone(s) that created the hint.\n\ttombstoneFile *fileMetadata\n\t// The smallest and largest sequence numbers of the abutting tombstones\n\t// merged to form this hint. All of a tables' keys must be less than the\n\t// tombstone smallest sequence number to be deleted. All of a tables'\n\t// sequence numbers must fall into the same snapshot stripe as the\n\t// tombstone largest sequence number to be deleted.\n\ttombstoneLargestSeqNum  base.SeqNum\n\ttombstoneSmallestSeqNum base.SeqNum\n\t// The smallest sequence number of a sstable that was found to be covered\n\t// by this hint. The hint cannot be resolved until this sequence number is\n\t// in the same snapshot stripe as the largest tombstone sequence number.\n\t// This is set when a hint is created, so the LSM may look different and\n\t// notably no longer contain the sstable that contained the key at this\n\t// sequence number.\n\tfileSmallestSeqNum base.SeqNum\n}\n\ntype deletionHintOverlap int8\n\nconst (\n\t// hintDoesNotApply indicates that the hint does not apply to the file.\n\thintDoesNotApply deletionHintOverlap = iota\n\t// hintExcisesFile indicates that the hint excises a portion of the file,\n\t// and the format major version of the DB supports excises.\n\thintExcisesFile\n\t// hintDeletesFile indicates that the hint deletes the entirety of the file.\n\thintDeletesFile\n)\n\nfunc (h deleteCompactionHint) String() string {\n\treturn fmt.Sprintf(\n\t\t\"L%d.%s %s-%s seqnums(tombstone=%d-%d, file-smallest=%d, type=%s)\",\n\t\th.tombstoneLevel, h.tombstoneFile.FileNum, h.start, h.end,\n\t\th.tombstoneSmallestSeqNum, h.tombstoneLargestSeqNum, h.fileSmallestSeqNum,\n\t\th.hintType,\n\t)\n}\n\nfunc (h *deleteCompactionHint) canDeleteOrExcise(\n\tcmp Compare, m *fileMetadata, snapshots compact.Snapshots, exciseEnabled bool,\n) deletionHintOverlap {\n\t// The file can only be deleted if all of its keys are older than the\n\t// earliest tombstone aggregated into the hint. Note that we use\n\t// m.LargestSeqNumAbsolute, not m.LargestSeqNum. Consider a compaction that\n\t// zeroes sequence numbers. A compaction may zero the sequence number of a\n\t// key with a sequence number > h.tombstoneSmallestSeqNum and set it to\n\t// zero. If we looked at m.LargestSeqNum, the resulting output file would\n\t// appear to not contain any keys more recent than the oldest tombstone. To\n\t// avoid this error, the largest pre-zeroing sequence number is maintained\n\t// in LargestSeqNumAbsolute and used here to make the determination whether\n\t// the file's keys are older than all of the hint's tombstones.\n\tif m.LargestSeqNumAbsolute >= h.tombstoneSmallestSeqNum || m.SmallestSeqNum < h.fileSmallestSeqNum {\n\t\treturn hintDoesNotApply\n\t}\n\n\t// The file's oldest key must  be in the same snapshot stripe as the\n\t// newest tombstone. NB: We already checked the hint's sequence numbers,\n\t// but this file's oldest sequence number might be lower than the hint's\n\t// smallest sequence number despite the file falling within the key range\n\t// if this file was constructed after the hint by a compaction.\n\tif snapshots.Index(h.tombstoneLargestSeqNum) != snapshots.Index(m.SmallestSeqNum) {\n\t\treturn hintDoesNotApply\n\t}\n\n\tswitch h.hintType {\n\tcase deleteCompactionHintTypePointKeyOnly:\n\t\t// A hint generated by a range del span cannot delete tables that contain\n\t\t// range keys.\n\t\tif m.HasRangeKeys {\n\t\t\treturn hintDoesNotApply\n\t\t}\n\tcase deleteCompactionHintTypeRangeKeyOnly:\n\t\t// A hint generated by a range key del span cannot delete tables that\n\t\t// contain point keys.\n\t\tif m.HasPointKeys {\n\t\t\treturn hintDoesNotApply\n\t\t}\n\tcase deleteCompactionHintTypePointAndRangeKey:\n\t\t// A hint from a span that contains both range dels *and* range keys can\n\t\t// only be deleted if both bounds fall within the hint. The next check takes\n\t\t// care of this.\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"pebble: unknown delete compaction hint type: %d\", h.hintType))\n\t}\n\tif cmp(h.start, m.Smallest.UserKey) <= 0 &&\n\t\tbase.UserKeyExclusive(h.end).CompareUpperBounds(cmp, m.UserKeyBounds().End) >= 0 {\n\t\treturn hintDeletesFile\n\t}\n\tif !exciseEnabled {\n\t\t// The file's keys must be completely contained within the hint range; excises\n\t\t// aren't allowed.\n\t\treturn hintDoesNotApply\n\t}\n\t// Check for any overlap. In cases of partial overlap, we can excise the part of the file\n\t// that overlaps with the deletion hint.\n\tif cmp(h.end, m.Smallest.UserKey) > 0 &&\n\t\t(m.UserKeyBounds().End.CompareUpperBounds(cmp, base.UserKeyInclusive(h.start)) >= 0) {\n\t\treturn hintExcisesFile\n\t}\n\treturn hintDoesNotApply\n}\n\n// checkDeleteCompactionHints checks the passed-in deleteCompactionHints for those that\n// can be resolved and those that cannot. A hint is considered resolved when its largest\n// tombstone sequence number and the smallest sequence number of covered files fall in\n// the same snapshot stripe. No more than maxHintsPerDeleteOnlyCompaction will be resolved\n// per method call. Resolved and unresolved hints are returned in separate return values.\n// The files that the resolved hints apply to, are returned as compactionLevels.\nfunc checkDeleteCompactionHints(\n\tcmp Compare,\n\tv *version,\n\thints []deleteCompactionHint,\n\tsnapshots compact.Snapshots,\n\texciseEnabled bool,\n) (levels []compactionLevel, resolved, unresolved []deleteCompactionHint) {\n\tvar files map[*fileMetadata]bool\n\tvar byLevel [numLevels][]*fileMetadata\n\n\t// Delete-only compactions can be quadratic (O(mn)) in terms of runtime\n\t// where m = number of files in the delete-only compaction and n = number\n\t// of resolved hints. To prevent these from growing unbounded, we cap\n\t// the number of hints we resolve for one delete-only compaction. This\n\t// cap only applies if exciseEnabled == true.\n\tconst maxHintsPerDeleteOnlyCompaction = 10\n\n\tunresolvedHints := hints[:0]\n\t// Lazily populate resolvedHints, similar to files above.\n\tresolvedHints := make([]deleteCompactionHint, 0)\n\tfor _, h := range hints {\n\t\t// Check each compaction hint to see if it's resolvable. Resolvable\n\t\t// hints are removed and trigger a delete-only compaction if any files\n\t\t// in the current LSM still meet their criteria. Unresolvable hints\n\t\t// are saved and don't trigger a delete-only compaction.\n\t\t//\n\t\t// When a compaction hint is created, the sequence numbers of the\n\t\t// range tombstones and the covered file with the oldest key are\n\t\t// recorded. The largest tombstone sequence number and the smallest\n\t\t// file sequence number must be in the same snapshot stripe for the\n\t\t// hint to be resolved. The below graphic models a compaction hint\n\t\t// covering the keyspace [b, r). The hint completely contains two\n\t\t// files, 000002 and 000003. The file 000003 contains the lowest\n\t\t// covered sequence number at #90. The tombstone b.RANGEDEL.230:h has\n\t\t// the highest tombstone sequence number incorporated into the hint.\n\t\t// The hint may be resolved only once the snapshots at #100, #180 and\n\t\t// #210 are all closed. File 000001 is not included within the hint\n\t\t// because it extends beyond the range tombstones in user key space.\n\t\t//\n\t\t// 250\n\t\t//\n\t\t//       |-b...230:h-|\n\t\t// _____________________________________________________ snapshot #210\n\t\t// 200               |--h.RANGEDEL.200:r--|\n\t\t//\n\t\t// _____________________________________________________ snapshot #180\n\t\t//\n\t\t// 150                     +--------+\n\t\t//           +---------+   | 000003 |\n\t\t//           | 000002  |   |        |\n\t\t//           +_________+   |        |\n\t\t// 100_____________________|________|___________________ snapshot #100\n\t\t//                         +--------+\n\t\t// _____________________________________________________ snapshot #70\n\t\t//                             +---------------+\n\t\t//  50                         | 000001        |\n\t\t//                             |               |\n\t\t//                             +---------------+\n\t\t// ______________________________________________________________\n\t\t//     a b c d e f g h i j k l m n o p q r s t u v w x y z\n\n\t\tif snapshots.Index(h.tombstoneLargestSeqNum) != snapshots.Index(h.fileSmallestSeqNum) ||\n\t\t\t(len(resolvedHints) >= maxHintsPerDeleteOnlyCompaction && exciseEnabled) {\n\t\t\t// Cannot resolve yet.\n\t\t\tunresolvedHints = append(unresolvedHints, h)\n\t\t\tcontinue\n\t\t}\n\n\t\t// The hint h will be resolved and dropped, if it either affects no files at all\n\t\t// or if the number of files it creates (eg. through excision) is less than or\n\t\t// equal to the number of files it deletes. First, determine how many files are\n\t\t// affected by this hint.\n\t\tfilesDeletedByCurrentHint := 0\n\t\tvar filesDeletedByLevel [7][]*fileMetadata\n\t\tfor l := h.tombstoneLevel + 1; l < numLevels; l++ {\n\t\t\toverlaps := v.Overlaps(l, base.UserKeyBoundsEndExclusive(h.start, h.end))\n\t\t\titer := overlaps.Iter()\n\n\t\t\tfor m := iter.First(); m != nil; m = iter.Next() {\n\t\t\t\tdoesHintApply := h.canDeleteOrExcise(cmp, m, snapshots, exciseEnabled)\n\t\t\t\tif m.IsCompacting() || doesHintApply == hintDoesNotApply || files[m] {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tswitch doesHintApply {\n\t\t\t\tcase hintDeletesFile:\n\t\t\t\t\tfilesDeletedByCurrentHint++\n\t\t\t\tcase hintExcisesFile:\n\t\t\t\t\t// Account for the original file being deleted.\n\t\t\t\t\tfilesDeletedByCurrentHint++\n\t\t\t\t\t// An excise could produce up to 2 new files. If the hint\n\t\t\t\t\t// leaves a fragment of the file on the left, decrement\n\t\t\t\t\t// the counter once. If the hint leaves a fragment of the\n\t\t\t\t\t// file on the right, decrement the counter once.\n\t\t\t\t\tif cmp(h.start, m.Smallest.UserKey) > 0 {\n\t\t\t\t\t\tfilesDeletedByCurrentHint--\n\t\t\t\t\t}\n\t\t\t\t\tif m.UserKeyBounds().End.IsUpperBoundFor(cmp, h.end) {\n\t\t\t\t\t\tfilesDeletedByCurrentHint--\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfilesDeletedByLevel[l] = append(filesDeletedByLevel[l], m)\n\t\t\t}\n\t\t}\n\t\tif filesDeletedByCurrentHint < 0 {\n\t\t\t// This hint does not delete a sufficient number of files to warrant\n\t\t\t// a delete-only compaction at this stage. Drop it (ie. don't add it\n\t\t\t// to either resolved or unresolved hints) so it doesn't stick around\n\t\t\t// forever.\n\t\t\tcontinue\n\t\t}\n\t\t// This hint will be resolved and dropped.\n\t\tfor l := h.tombstoneLevel + 1; l < numLevels; l++ {\n\t\t\tbyLevel[l] = append(byLevel[l], filesDeletedByLevel[l]...)\n\t\t\tfor _, m := range filesDeletedByLevel[l] {\n\t\t\t\tif files == nil {\n\t\t\t\t\t// Construct files lazily, assuming most calls will not\n\t\t\t\t\t// produce delete-only compactions.\n\t\t\t\t\tfiles = make(map[*fileMetadata]bool)\n\t\t\t\t}\n\t\t\t\tfiles[m] = true\n\t\t\t}\n\t\t}\n\t\tresolvedHints = append(resolvedHints, h)\n\t}\n\n\tvar compactLevels []compactionLevel\n\tfor l, files := range byLevel {\n\t\tif len(files) == 0 {\n\t\t\tcontinue\n\t\t}\n\t\tcompactLevels = append(compactLevels, compactionLevel{\n\t\t\tlevel: l,\n\t\t\tfiles: manifest.NewLevelSliceKeySorted(cmp, files),\n\t\t})\n\t}\n\treturn compactLevels, resolvedHints, unresolvedHints\n}\n\nfunc (d *DB) compactionPprofLabels(c *compaction) pprof.LabelSet {\n\tactivity := \"compact\"\n\tif len(c.flushing) != 0 {\n\t\tactivity = \"flush\"\n\t}\n\tlevel := \"L?\"\n\t// Delete-only compactions don't have an output level.\n\tif c.outputLevel != nil {\n\t\tlevel = fmt.Sprintf(\"L%d\", c.outputLevel.level)\n\t}\n\tif kc := d.opts.Experimental.UserKeyCategories; kc.Len() > 0 {\n\t\tcat := kc.CategorizeKeyRange(c.smallest.UserKey, c.largest.UserKey)\n\t\treturn pprof.Labels(\"pebble\", activity, \"output-level\", level, \"key-type\", cat)\n\t}\n\treturn pprof.Labels(\"pebble\", activity, \"output-level\", level)\n}\n\n// compact runs one compaction and maybe schedules another call to compact.\nfunc (d *DB) compact(c *compaction, errChannel chan error) {\n\tpprof.Do(context.Background(), d.compactionPprofLabels(c), func(context.Context) {\n\t\td.mu.Lock()\n\t\tdefer d.mu.Unlock()\n\t\tif err := d.compact1(c, errChannel); err != nil {\n\t\t\t// TODO(peter): count consecutive compaction errors and backoff.\n\t\t\td.opts.EventListener.BackgroundError(err)\n\t\t}\n\t\tif c.isDownload {\n\t\t\td.mu.compact.downloadingCount--\n\t\t} else {\n\t\t\td.mu.compact.compactingCount--\n\t\t}\n\t\tdelete(d.mu.compact.inProgress, c)\n\t\t// Add this compaction's duration to the cumulative duration. NB: This\n\t\t// must be atomic with the above removal of c from\n\t\t// d.mu.compact.InProgress to ensure Metrics.Compact.Duration does not\n\t\t// miss or double count a completing compaction's duration.\n\t\td.mu.compact.duration += d.timeNow().Sub(c.beganAt)\n\n\t\t// The previous compaction may have produced too many files in a\n\t\t// level, so reschedule another compaction if needed.\n\t\td.maybeScheduleCompaction()\n\t\td.mu.compact.cond.Broadcast()\n\t})\n}\n\n// cleanupVersionEdit cleans up any on-disk artifacts that were created\n// for the application of a versionEdit that is no longer going to be applied.\n//\n// d.mu must be held when calling this method.\nfunc (d *DB) cleanupVersionEdit(ve *versionEdit) {\n\tobsoleteFiles := make([]*fileBacking, 0, len(ve.NewFiles))\n\tdeletedFiles := make(map[base.FileNum]struct{})\n\tfor key := range ve.DeletedFiles {\n\t\tdeletedFiles[key.FileNum] = struct{}{}\n\t}\n\tfor i := range ve.NewFiles {\n\t\tif ve.NewFiles[i].Meta.Virtual {\n\t\t\t// We handle backing files separately.\n\t\t\tcontinue\n\t\t}\n\t\tif _, ok := deletedFiles[ve.NewFiles[i].Meta.FileNum]; ok {\n\t\t\t// This file is being moved in this ve to a different level.\n\t\t\t// Don't mark it as obsolete.\n\t\t\tcontinue\n\t\t}\n\t\tobsoleteFiles = append(obsoleteFiles, ve.NewFiles[i].Meta.PhysicalMeta().FileBacking)\n\t}\n\tfor i := range ve.CreatedBackingTables {\n\t\tif ve.CreatedBackingTables[i].IsUnused() {\n\t\t\tobsoleteFiles = append(obsoleteFiles, ve.CreatedBackingTables[i])\n\t\t}\n\t}\n\tfor i := range obsoleteFiles {\n\t\t// Add this file to zombie tables as well, as the versionSet\n\t\t// asserts on whether every obsolete file was at one point\n\t\t// marked zombie.\n\t\td.mu.versions.zombieTables[obsoleteFiles[i].DiskFileNum] = tableInfo{\n\t\t\tfileInfo: fileInfo{\n\t\t\t\tFileNum:  obsoleteFiles[i].DiskFileNum,\n\t\t\t\tFileSize: obsoleteFiles[i].Size,\n\t\t\t},\n\t\t\t// TODO(bilal): This is harmless if it's wrong, as it only causes\n\t\t\t// incorrect accounting for the size of it in metrics. Currently\n\t\t\t// all compactions only write to local files anyway except with\n\t\t\t// disaggregated storage; if this becomes the norm, we should do\n\t\t\t// an objprovider lookup here.\n\t\t\tisLocal: true,\n\t\t}\n\t}\n\td.mu.versions.addObsoleteLocked(obsoleteFiles)\n}\n\n// compact1 runs one compaction.\n//\n// d.mu must be held when calling this, but the mutex may be dropped and\n// re-acquired during the course of this method.\nfunc (d *DB) compact1(c *compaction, errChannel chan error) (err error) {\n\tif errChannel != nil {\n\t\tdefer func() {\n\t\t\terrChannel <- err\n\t\t}()\n\t}\n\n\tjobID := d.newJobIDLocked()\n\tinfo := c.makeInfo(jobID)\n\td.opts.EventListener.CompactionBegin(info)\n\tstartTime := d.timeNow()\n\n\tve, stats, err := d.runCompaction(jobID, c)\n\n\tinfo.Duration = d.timeNow().Sub(startTime)\n\tif err == nil {\n\t\tvalidateVersionEdit(ve, d.opts.Experimental.KeyValidationFunc, d.opts.Comparer.FormatKey, d.opts.Logger)\n\t\terr = func() error {\n\t\t\tvar err error\n\t\t\td.mu.versions.logLock()\n\t\t\t// Check if this compaction had a conflicting operation (eg. a d.excise())\n\t\t\t// that necessitates it restarting from scratch. Note that since we hold\n\t\t\t// the manifest lock, we don't expect this bool to change its value\n\t\t\t// as only the holder of the manifest lock will ever write to it.\n\t\t\tif c.cancel.Load() {\n\t\t\t\td.mu.versions.metrics.Compact.CancelledCount++\n\t\t\t\td.mu.versions.metrics.Compact.CancelledBytes += c.bytesWritten\n\n\t\t\t\terr = firstError(err, ErrCancelledCompaction)\n\t\t\t\t// This is the first time we've seen a cancellation during the\n\t\t\t\t// life of this compaction (or the original condition on err == nil\n\t\t\t\t// would not have been true). We should delete any tables already\n\t\t\t\t// created, as d.runCompaction did not do that.\n\t\t\t\td.cleanupVersionEdit(ve)\n\t\t\t\t// logAndApply calls logUnlock. If we didn't call it, we need to call\n\t\t\t\t// logUnlock ourselves.\n\t\t\t\td.mu.versions.logUnlock()\n\t\t\t\treturn err\n\t\t\t}\n\t\t\treturn d.mu.versions.logAndApply(jobID, ve, c.metrics, false /* forceRotation */, func() []compactionInfo {\n\t\t\t\treturn d.getInProgressCompactionInfoLocked(c)\n\t\t\t})\n\t\t}()\n\t}\n\n\tinfo.Done = true\n\tinfo.Err = err\n\tif err == nil {\n\t\tfor i := range ve.NewFiles {\n\t\t\te := &ve.NewFiles[i]\n\t\t\tinfo.Output.Tables = append(info.Output.Tables, e.Meta.TableInfo())\n\t\t}\n\t\td.mu.snapshots.cumulativePinnedCount += stats.CumulativePinnedKeys\n\t\td.mu.snapshots.cumulativePinnedSize += stats.CumulativePinnedSize\n\t\td.mu.versions.metrics.Keys.MissizedTombstonesCount += stats.CountMissizedDels\n\t}\n\n\t// NB: clearing compacting state must occur before updating the read state;\n\t// L0Sublevels initialization depends on it.\n\td.clearCompactingState(c, err != nil)\n\tif err != nil && errors.Is(err, ErrCancelledCompaction) {\n\t\td.mu.versions.metrics.Compact.CancelledCount++\n\t\td.mu.versions.metrics.Compact.CancelledBytes += c.bytesWritten\n\t}\n\td.mu.versions.incrementCompactions(c.kind, c.extraLevels, c.pickerMetrics)\n\td.mu.versions.incrementCompactionBytes(-c.bytesWritten)\n\n\tinfo.TotalDuration = d.timeNow().Sub(c.beganAt)\n\td.opts.EventListener.CompactionEnd(info)\n\n\t// Update the read state before deleting obsolete files because the\n\t// read-state update will cause the previous version to be unref'd and if\n\t// there are no references obsolete tables will be added to the obsolete\n\t// table list.\n\tif err == nil {\n\t\td.updateReadStateLocked(d.opts.DebugCheck)\n\t\td.updateTableStatsLocked(ve.NewFiles)\n\t}\n\td.deleteObsoleteFiles(jobID)\n\n\treturn err\n}\n\n// runCopyCompaction runs a copy compaction where a new FileNum is created that\n// is a byte-for-byte copy of the input file or span thereof in some cases. This\n// is used in lieu of a move compaction when a file is being moved across the\n// local/remote storage boundary. It could also be used in lieu of a rewrite\n// compaction as part of a Download() call, which allows copying only a span of\n// the external file, provided the file does not contain range keys or value\n// blocks (see sstable.CopySpan).\n//\n// d.mu must be held when calling this method. The mutex will be released when\n// doing IO.\nfunc (d *DB) runCopyCompaction(\n\tjobID JobID, c *compaction,\n) (ve *versionEdit, stats compact.Stats, _ error) {\n\titer := c.startLevel.files.Iter()\n\tinputMeta := iter.First()\n\tif iter.Next() != nil {\n\t\treturn nil, compact.Stats{}, base.AssertionFailedf(\"got more than one file for a move compaction\")\n\t}\n\tif c.cancel.Load() {\n\t\treturn nil, compact.Stats{}, ErrCancelledCompaction\n\t}\n\tve = &versionEdit{\n\t\tDeletedFiles: map[deletedFileEntry]*fileMetadata{\n\t\t\t{Level: c.startLevel.level, FileNum: inputMeta.FileNum}: inputMeta,\n\t\t},\n\t}\n\n\tobjMeta, err := d.objProvider.Lookup(fileTypeTable, inputMeta.FileBacking.DiskFileNum)\n\tif err != nil {\n\t\treturn nil, compact.Stats{}, err\n\t}\n\tif !objMeta.IsExternal() {\n\t\tif objMeta.IsRemote() || !remote.ShouldCreateShared(d.opts.Experimental.CreateOnShared, c.outputLevel.level) {\n\t\t\tpanic(\"pebble: scheduled a copy compaction that is not actually moving files to shared storage\")\n\t\t}\n\t\t// Note that based on logic in the compaction picker, we're guaranteed\n\t\t// inputMeta.Virtual is false.\n\t\tif inputMeta.Virtual {\n\t\t\tpanic(errors.AssertionFailedf(\"cannot do a copy compaction of a virtual sstable across local/remote storage\"))\n\t\t}\n\t}\n\n\t// We are in the relatively more complex case where we need to copy this\n\t// file to remote storage. Drop the db mutex while we do the copy\n\t//\n\t// To ease up cleanup of the local file and tracking of refs, we create\n\t// a new FileNum. This has the potential of making the block cache less\n\t// effective, however.\n\tnewMeta := &fileMetadata{\n\t\tSize:                     inputMeta.Size,\n\t\tCreationTime:             inputMeta.CreationTime,\n\t\tSmallestSeqNum:           inputMeta.SmallestSeqNum,\n\t\tLargestSeqNum:            inputMeta.LargestSeqNum,\n\t\tLargestSeqNumAbsolute:    inputMeta.LargestSeqNumAbsolute,\n\t\tStats:                    inputMeta.Stats,\n\t\tVirtual:                  inputMeta.Virtual,\n\t\tSyntheticPrefixAndSuffix: inputMeta.SyntheticPrefixAndSuffix,\n\t}\n\tif inputMeta.HasPointKeys {\n\t\tnewMeta.ExtendPointKeyBounds(c.cmp, inputMeta.SmallestPointKey, inputMeta.LargestPointKey)\n\t}\n\tif inputMeta.HasRangeKeys {\n\t\tnewMeta.ExtendRangeKeyBounds(c.cmp, inputMeta.SmallestRangeKey, inputMeta.LargestRangeKey)\n\t}\n\tnewMeta.FileNum = d.mu.versions.getNextFileNum()\n\tif objMeta.IsExternal() {\n\t\t// external -> local/shared copy. File must be virtual.\n\t\t// We will update this size later after we produce the new backing file.\n\t\tnewMeta.InitProviderBacking(base.DiskFileNum(newMeta.FileNum), inputMeta.FileBacking.Size)\n\t} else {\n\t\t// local -> shared copy. New file is guaranteed to not be virtual.\n\t\tnewMeta.InitPhysicalBacking()\n\t}\n\n\t// Before dropping the db mutex, grab a ref to the current version. This\n\t// prevents any concurrent excises from deleting files that this compaction\n\t// needs to read/maintain a reference to.\n\tvers := d.mu.versions.currentVersion()\n\tvers.Ref()\n\tdefer vers.UnrefLocked()\n\n\t// NB: The order here is reversed, lock after unlock. This is similar to\n\t// runCompaction.\n\td.mu.Unlock()\n\tdefer d.mu.Lock()\n\n\tdeleteOnExit := false\n\tdefer func() {\n\t\tif deleteOnExit {\n\t\t\t_ = d.objProvider.Remove(fileTypeTable, newMeta.FileBacking.DiskFileNum)\n\t\t}\n\t}()\n\n\t// If the src obj is external, we're doing an external to local/shared copy.\n\tif objMeta.IsExternal() {\n\t\tctx := context.TODO()\n\t\tsrc, err := d.objProvider.OpenForReading(\n\t\t\tctx, fileTypeTable, inputMeta.FileBacking.DiskFileNum, objstorage.OpenOptions{},\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, compact.Stats{}, err\n\t\t}\n\t\tdefer func() {\n\t\t\tif src != nil {\n\t\t\t\tsrc.Close()\n\t\t\t}\n\t\t}()\n\n\t\tw, _, err := d.objProvider.Create(\n\t\t\tctx, fileTypeTable, newMeta.FileBacking.DiskFileNum,\n\t\t\tobjstorage.CreateOptions{\n\t\t\t\tPreferSharedStorage: remote.ShouldCreateShared(d.opts.Experimental.CreateOnShared, c.outputLevel.level),\n\t\t\t},\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, compact.Stats{}, err\n\t\t}\n\t\tdeleteOnExit = true\n\n\t\tstart, end := newMeta.Smallest, newMeta.Largest\n\t\tif newMeta.SyntheticPrefixAndSuffix.HasPrefix() {\n\t\t\tsyntheticPrefix := newMeta.SyntheticPrefixAndSuffix.Prefix()\n\t\t\tstart.UserKey = syntheticPrefix.Invert(start.UserKey)\n\t\t\tend.UserKey = syntheticPrefix.Invert(end.UserKey)\n\t\t}\n\t\tif newMeta.SyntheticPrefixAndSuffix.HasSuffix() {\n\t\t\t// Extend the bounds as necessary so that the keys don't include suffixes.\n\t\t\tstart.UserKey = start.UserKey[:c.comparer.Split(start.UserKey)]\n\t\t\tif n := c.comparer.Split(end.UserKey); n < len(end.UserKey) {\n\t\t\t\tend = base.MakeRangeDeleteSentinelKey(c.comparer.ImmediateSuccessor(nil, end.UserKey[:n]))\n\t\t\t}\n\t\t}\n\n\t\t// NB: external files are always virtual.\n\t\tvar wrote uint64\n\t\terr = d.fileCache.withVirtualReader(inputMeta.VirtualMeta(), func(r sstable.VirtualReader) error {\n\t\t\tvar err error\n\t\t\twrote, err = sstable.CopySpan(ctx,\n\t\t\t\tsrc, r.UnsafeReader(), d.opts.MakeReaderOptions(),\n\t\t\t\tw, d.opts.MakeWriterOptions(c.outputLevel.level, d.TableFormat()),\n\t\t\t\tstart, end,\n\t\t\t)\n\t\t\treturn err\n\t\t})\n\n\t\tsrc = nil // We passed src to CopySpan; it's responsible for closing it.\n\t\tif err != nil {\n\t\t\tif errors.Is(err, sstable.ErrEmptySpan) {\n\t\t\t\t// The virtual table was empty. Just remove the backing file.\n\t\t\t\t// Note that deleteOnExit is true so we will delete the created object.\n\t\t\t\tc.metrics = map[int]*LevelMetrics{\n\t\t\t\t\tc.outputLevel.level: {\n\t\t\t\t\t\tBytesIn: inputMeta.Size,\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t\treturn ve, compact.Stats{}, nil\n\t\t\t}\n\t\t\treturn nil, compact.Stats{}, err\n\t\t}\n\t\tnewMeta.FileBacking.Size = wrote\n\t\tnewMeta.Size = wrote\n\t} else {\n\t\t_, err := d.objProvider.LinkOrCopyFromLocal(context.TODO(), d.opts.FS,\n\t\t\td.objProvider.Path(objMeta), fileTypeTable, newMeta.FileBacking.DiskFileNum,\n\t\t\tobjstorage.CreateOptions{PreferSharedStorage: true})\n\t\tif err != nil {\n\t\t\treturn nil, compact.Stats{}, err\n\t\t}\n\t\tdeleteOnExit = true\n\t}\n\tve.NewFiles = []newFileEntry{{\n\t\tLevel: c.outputLevel.level,\n\t\tMeta:  newMeta,\n\t}}\n\tif newMeta.Virtual {\n\t\tve.CreatedBackingTables = []*fileBacking{newMeta.FileBacking}\n\t}\n\tc.metrics = map[int]*LevelMetrics{\n\t\tc.outputLevel.level: {\n\t\t\tBytesIn:         inputMeta.Size,\n\t\t\tBytesCompacted:  newMeta.Size,\n\t\t\tTablesCompacted: 1,\n\t\t},\n\t}\n\n\tif err := d.objProvider.Sync(); err != nil {\n\t\treturn nil, compact.Stats{}, err\n\t}\n\tdeleteOnExit = false\n\treturn ve, compact.Stats{}, nil\n}\n\n// applyHintOnFile applies a deleteCompactionHint to a file, and updates the\n// versionEdit accordingly. It returns a list of new files that were created\n// if the hint was applied partially to a file (eg. through an excise as opposed\n// to an outright deletion). levelMetrics is kept up-to-date with the number\n// of tables deleted or excised.\nfunc (d *DB) applyHintOnFile(\n\th deleteCompactionHint,\n\tf *fileMetadata,\n\tlevel int,\n\tlevelMetrics *LevelMetrics,\n\tve *versionEdit,\n\thintOverlap deletionHintOverlap,\n) (newFiles []manifest.NewFileEntry, err error) {\n\tif hintOverlap == hintDoesNotApply {\n\t\treturn nil, nil\n\t}\n\n\t// The hint overlaps with at least part of the file.\n\tif hintOverlap == hintDeletesFile {\n\t\t// The hint deletes the entirety of this file.\n\t\tve.DeletedFiles[deletedFileEntry{\n\t\t\tLevel:   level,\n\t\t\tFileNum: f.FileNum,\n\t\t}] = f\n\t\tlevelMetrics.TablesDeleted++\n\t\treturn nil, nil\n\t}\n\t// The hint overlaps with only a part of the file, not the entirety of it. We need\n\t// to use d.excise. (hintOverlap == hintExcisesFile)\n\tif d.FormatMajorVersion() < FormatVirtualSSTables {\n\t\tpanic(\"pebble: delete-only compaction hint excising a file is not supported in this version\")\n\t}\n\n\tlevelMetrics.TablesExcised++\n\tnewFiles, err = d.excise(context.TODO(), base.UserKeyBoundsEndExclusive(h.start, h.end), f, ve, level)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"error when running excise for delete-only compaction\")\n\t}\n\tif _, ok := ve.DeletedFiles[deletedFileEntry{\n\t\tLevel:   level,\n\t\tFileNum: f.FileNum,\n\t}]; !ok {\n\t\tpanic(\"pebble: delete-only compaction hint overlapping a file did not excise that file\")\n\t}\n\treturn newFiles, nil\n}\n\nfunc (d *DB) runDeleteOnlyCompactionForLevel(\n\tcl compactionLevel,\n\tlevelMetrics *LevelMetrics,\n\tve *versionEdit,\n\tsnapshots compact.Snapshots,\n\tfragments []deleteCompactionHintFragment,\n\texciseEnabled bool,\n) error {\n\tcurFragment := 0\n\titer := cl.files.Iter()\n\tif cl.level == 0 {\n\t\tpanic(\"cannot run delete-only compaction for L0\")\n\t}\n\n\t// Outer loop loops on files. Middle loop loops on fragments. Inner loop\n\t// loops on raw fragments of hints. Number of fragments are bounded by\n\t// the number of hints this compaction was created with, which is capped\n\t// in the compaction picker to avoid very CPU-hot loops here.\n\tfor f := iter.First(); f != nil; f = iter.Next() {\n\t\t// curFile usually matches f, except if f got excised in which case\n\t\t// it maps to a virtual file that replaces f, or nil if f got removed\n\t\t// in its entirety.\n\t\tcurFile := f\n\t\tfor curFragment < len(fragments) && d.cmp(fragments[curFragment].start, f.Smallest.UserKey) <= 0 {\n\t\t\tcurFragment++\n\t\t}\n\t\tif curFragment > 0 {\n\t\t\tcurFragment--\n\t\t}\n\n\t\tfor ; curFragment < len(fragments); curFragment++ {\n\t\t\tif f.UserKeyBounds().End.CompareUpperBounds(d.cmp, base.UserKeyInclusive(fragments[curFragment].start)) < 0 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\t// Process all overlapping hints with this file. Note that applying\n\t\t\t// a hint twice is idempotent; curFile should have already been excised\n\t\t\t// the first time, resulting in no change the second time.\n\t\t\tfor _, h := range fragments[curFragment].hints {\n\t\t\t\tif h.tombstoneLevel >= cl.level {\n\t\t\t\t\t// We cannot excise out the deletion tombstone itself, or anything\n\t\t\t\t\t// above it.\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\thintOverlap := h.canDeleteOrExcise(d.cmp, curFile, snapshots, exciseEnabled)\n\t\t\t\tif hintOverlap == hintDoesNotApply {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tnewFiles, err := d.applyHintOnFile(h, curFile, cl.level, levelMetrics, ve, hintOverlap)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tif _, ok := ve.DeletedFiles[manifest.DeletedFileEntry{Level: cl.level, FileNum: curFile.FileNum}]; ok {\n\t\t\t\t\tcurFile = nil\n\t\t\t\t}\n\t\t\t\tif len(newFiles) > 0 {\n\t\t\t\t\tcurFile = newFiles[len(newFiles)-1].Meta\n\t\t\t\t} else if curFile == nil {\n\t\t\t\t\t// Nothing remains of the file.\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif curFile == nil {\n\t\t\t\t// Nothing remains of the file.\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif _, ok := ve.DeletedFiles[deletedFileEntry{\n\t\t\tLevel:   cl.level,\n\t\t\tFileNum: f.FileNum,\n\t\t}]; !ok {\n\t\t\tpanic(\"pebble: delete-only compaction scheduled with hints that did not delete or excise a file\")\n\t\t}\n\t}\n\treturn nil\n}\n\n// deleteCompactionHintFragment represents a fragment of the key space and\n// contains a set of deleteCompactionHints that apply to that fragment; a\n// fragment starts at the start field and ends where the next fragment starts.\ntype deleteCompactionHintFragment struct {\n\tstart []byte\n\thints []deleteCompactionHint\n}\n\n// Delete compaction hints can overlap with each other, and multiple fragments\n// can apply to a single file. This function takes a list of hints and fragments\n// them, to make it easier to apply them to non-overlapping files occupying a level;\n// that way, files and hint fragments can be iterated on in lockstep, while efficiently\n// being able to apply all hints overlapping with a given file.\nfunc fragmentDeleteCompactionHints(\n\tcmp Compare, hints []deleteCompactionHint,\n) []deleteCompactionHintFragment {\n\tfragments := make([]deleteCompactionHintFragment, 0, len(hints)*2)\n\tfor i := range hints {\n\t\tfragments = append(fragments, deleteCompactionHintFragment{start: hints[i].start},\n\t\t\tdeleteCompactionHintFragment{start: hints[i].end})\n\t}\n\tslices.SortFunc(fragments, func(i, j deleteCompactionHintFragment) int {\n\t\treturn cmp(i.start, j.start)\n\t})\n\tfragments = slices.CompactFunc(fragments, func(i, j deleteCompactionHintFragment) bool {\n\t\treturn bytes.Equal(i.start, j.start)\n\t})\n\tfor _, h := range hints {\n\t\tstartIdx := sort.Search(len(fragments), func(i int) bool {\n\t\t\treturn cmp(fragments[i].start, h.start) >= 0\n\t\t})\n\t\tendIdx := sort.Search(len(fragments), func(i int) bool {\n\t\t\treturn cmp(fragments[i].start, h.end) >= 0\n\t\t})\n\t\tfor i := startIdx; i < endIdx; i++ {\n\t\t\tfragments[i].hints = append(fragments[i].hints, h)\n\t\t}\n\t}\n\treturn fragments\n}\n\n// Runs a delete-only compaction.\n//\n// d.mu must *not* be held when calling this.\nfunc (d *DB) runDeleteOnlyCompaction(\n\tjobID JobID, c *compaction, snapshots compact.Snapshots,\n) (ve *versionEdit, stats compact.Stats, retErr error) {\n\tc.metrics = make(map[int]*LevelMetrics, len(c.inputs))\n\tfragments := fragmentDeleteCompactionHints(d.cmp, c.deletionHints)\n\tve = &versionEdit{\n\t\tDeletedFiles: map[deletedFileEntry]*fileMetadata{},\n\t}\n\tfor _, cl := range c.inputs {\n\t\tlevelMetrics := &LevelMetrics{}\n\t\tif err := d.runDeleteOnlyCompactionForLevel(cl, levelMetrics, ve, snapshots, fragments, c.exciseEnabled); err != nil {\n\t\t\treturn nil, stats, err\n\t\t}\n\t\tc.metrics[cl.level] = levelMetrics\n\t}\n\t// Remove any files that were added and deleted in the same versionEdit.\n\tve.NewFiles = slices.DeleteFunc(ve.NewFiles, func(e manifest.NewFileEntry) bool {\n\t\tdeletedFileEntry := manifest.DeletedFileEntry{Level: e.Level, FileNum: e.Meta.FileNum}\n\t\tif _, deleted := ve.DeletedFiles[deletedFileEntry]; deleted {\n\t\t\tdelete(ve.DeletedFiles, deletedFileEntry)\n\t\t\treturn true\n\t\t}\n\t\treturn false\n\t})\n\t// Remove any entries from CreatedBackingTables that are not used in any\n\t// NewFiles.\n\tusedBackingFiles := make(map[base.DiskFileNum]struct{})\n\tfor _, e := range ve.NewFiles {\n\t\tif e.Meta.Virtual {\n\t\t\tusedBackingFiles[e.Meta.FileBacking.DiskFileNum] = struct{}{}\n\t\t}\n\t}\n\tve.CreatedBackingTables = slices.DeleteFunc(ve.CreatedBackingTables, func(b *fileBacking) bool {\n\t\t_, used := usedBackingFiles[b.DiskFileNum]\n\t\treturn !used\n\t})\n\t// Refresh the disk available statistic whenever a compaction/flush\n\t// completes, before re-acquiring the mutex.\n\td.calculateDiskAvailableBytes()\n\treturn ve, stats, nil\n}\n\nfunc (d *DB) runMoveCompaction(\n\tjobID JobID, c *compaction,\n) (ve *versionEdit, stats compact.Stats, _ error) {\n\titer := c.startLevel.files.Iter()\n\tmeta := iter.First()\n\tif iter.Next() != nil {\n\t\treturn nil, stats, base.AssertionFailedf(\"got more than one file for a move compaction\")\n\t}\n\tif c.cancel.Load() {\n\t\treturn ve, stats, ErrCancelledCompaction\n\t}\n\tc.metrics = map[int]*LevelMetrics{\n\t\tc.outputLevel.level: {\n\t\t\tBytesMoved:  meta.Size,\n\t\t\tTablesMoved: 1,\n\t\t},\n\t}\n\tve = &versionEdit{\n\t\tDeletedFiles: map[deletedFileEntry]*fileMetadata{\n\t\t\t{Level: c.startLevel.level, FileNum: meta.FileNum}: meta,\n\t\t},\n\t\tNewFiles: []newFileEntry{\n\t\t\t{Level: c.outputLevel.level, Meta: meta},\n\t\t},\n\t}\n\n\treturn ve, stats, nil\n}\n\n// runCompaction runs a compaction that produces new on-disk tables from\n// memtables or old on-disk tables.\n//\n// runCompaction cannot be used for compactionKindIngestedFlushable.\n//\n// d.mu must be held when calling this, but the mutex may be dropped and\n// re-acquired during the course of this method.\nfunc (d *DB) runCompaction(\n\tjobID JobID, c *compaction,\n) (ve *versionEdit, stats compact.Stats, retErr error) {\n\tdefer func() {\n\t\tc.slot.Release(stats.CumulativeWrittenSize)\n\t\tc.slot = nil\n\t}()\n\tif c.cancel.Load() {\n\t\treturn ve, stats, ErrCancelledCompaction\n\t}\n\tswitch c.kind {\n\tcase compactionKindDeleteOnly:\n\t\t// Before dropping the db mutex, grab a ref to the current version. This\n\t\t// prevents any concurrent excises from deleting files that this compaction\n\t\t// needs to read/maintain a reference to.\n\t\t//\n\t\t// Note that delete-only compactions can call excise(), which needs to be able\n\t\t// to read these files.\n\t\tvers := d.mu.versions.currentVersion()\n\t\tvers.Ref()\n\t\tdefer vers.UnrefLocked()\n\t\t// Release the d.mu lock while doing I/O.\n\t\t// Note the unusual order: Unlock and then Lock.\n\t\tsnapshots := d.mu.snapshots.toSlice()\n\t\td.mu.Unlock()\n\t\tdefer d.mu.Lock()\n\t\treturn d.runDeleteOnlyCompaction(jobID, c, snapshots)\n\tcase compactionKindMove:\n\t\treturn d.runMoveCompaction(jobID, c)\n\tcase compactionKindCopy:\n\t\treturn d.runCopyCompaction(jobID, c)\n\tcase compactionKindIngestedFlushable:\n\t\tpanic(\"pebble: runCompaction cannot handle compactionKindIngestedFlushable.\")\n\t}\n\n\tsnapshots := d.mu.snapshots.toSlice()\n\n\tif c.flushing == nil {\n\t\t// Before dropping the db mutex, grab a ref to the current version. This\n\t\t// prevents any concurrent excises from deleting files that this compaction\n\t\t// needs to read/maintain a reference to.\n\t\t//\n\t\t// Note that unlike user iterators, compactionIter does not maintain a ref\n\t\t// of the version or read state.\n\t\tvers := d.mu.versions.currentVersion()\n\t\tvers.Ref()\n\t\tdefer vers.UnrefLocked()\n\t}\n\n\t// The table is typically written at the maximum allowable format implied by\n\t// the current format major version of the DB, but Options may define\n\t// additional constraints.\n\ttableFormat := d.TableFormat()\n\n\t// Release the d.mu lock while doing I/O.\n\t// Note the unusual order: Unlock and then Lock.\n\td.mu.Unlock()\n\tdefer d.mu.Lock()\n\n\tresult := d.compactAndWrite(jobID, c, snapshots, tableFormat)\n\tif result.Err == nil {\n\t\tve, result.Err = c.makeVersionEdit(result)\n\t}\n\tif result.Err != nil {\n\t\t// Delete any created tables.\n\t\tobsoleteFiles := make([]*fileBacking, 0, len(result.Tables))\n\t\td.mu.Lock()\n\t\tfor i := range result.Tables {\n\t\t\tbacking := &fileBacking{\n\t\t\t\tDiskFileNum: result.Tables[i].ObjMeta.DiskFileNum,\n\t\t\t\tSize:        result.Tables[i].WriterMeta.Size,\n\t\t\t}\n\t\t\tobsoleteFiles = append(obsoleteFiles, backing)\n\t\t\t// Add this file to zombie tables as well, as the versionSet\n\t\t\t// asserts on whether every obsolete file was at one point\n\t\t\t// marked zombie.\n\t\t\td.mu.versions.zombieTables[backing.DiskFileNum] = tableInfo{\n\t\t\t\tfileInfo: fileInfo{\n\t\t\t\t\tFileNum:  backing.DiskFileNum,\n\t\t\t\t\tFileSize: backing.Size,\n\t\t\t\t},\n\t\t\t\tisLocal: true,\n\t\t\t}\n\t\t}\n\t\td.mu.versions.addObsoleteLocked(obsoleteFiles)\n\t\td.mu.Unlock()\n\t}\n\t// Refresh the disk available statistic whenever a compaction/flush\n\t// completes, before re-acquiring the mutex.\n\td.calculateDiskAvailableBytes()\n\treturn ve, result.Stats, result.Err\n}\n\n// compactAndWrite runs the data part of a compaction, where we set up a\n// compaction iterator and use it to write output tables.\nfunc (d *DB) compactAndWrite(\n\tjobID JobID, c *compaction, snapshots compact.Snapshots, tableFormat sstable.TableFormat,\n) (result compact.Result) {\n\t// Compactions use a pool of buffers to read blocks, avoiding polluting the\n\t// block cache with blocks that will not be read again. We initialize the\n\t// buffer pool with a size 12. This initial size does not need to be\n\t// accurate, because the pool will grow to accommodate the maximum number of\n\t// blocks allocated at a given time over the course of the compaction. But\n\t// choosing a size larger than that working set avoids any additional\n\t// allocations to grow the size of the pool over the course of iteration.\n\t//\n\t// Justification for initial size 12: In a two-level compaction, at any\n\t// given moment we'll have 2 index blocks in-use and 2 data blocks in-use.\n\t// Additionally, when decoding a compressed block, we'll temporarily\n\t// allocate 1 additional block to hold the compressed buffer. In the worst\n\t// case that all input sstables have two-level index blocks (+2), value\n\t// blocks (+2), range deletion blocks (+n) and range key blocks (+n), we'll\n\t// additionally require 2n+4 blocks where n is the number of input sstables.\n\t// Range deletion and range key blocks are relatively rare, and the cost of\n\t// an additional allocation or two over the course of the compaction is\n\t// considered to be okay. A larger initial size would cause the pool to hold\n\t// on to more memory, even when it's not in-use because the pool will\n\t// recycle buffers up to the current capacity of the pool. The memory use of\n\t// a 12-buffer pool is expected to be within reason, even if all the buffers\n\t// grow to the typical size of an index block (256 KiB) which would\n\t// translate to 3 MiB per compaction.\n\tc.bufferPool.Init(12)\n\tdefer c.bufferPool.Release()\n\n\tpointIter, rangeDelIter, rangeKeyIter, err := c.newInputIters(d.newIters, d.tableNewRangeKeyIter)\n\tdefer func() {\n\t\tfor _, closer := range c.closers {\n\t\t\tcloser.FragmentIterator.Close()\n\t\t}\n\t}()\n\tif err != nil {\n\t\treturn compact.Result{Err: err}\n\t}\n\tc.allowedZeroSeqNum = c.allowZeroSeqNum()\n\tcfg := compact.IterConfig{\n\t\tComparer:         c.comparer,\n\t\tMerge:            d.merge,\n\t\tTombstoneElision: c.delElision,\n\t\tRangeKeyElision:  c.rangeKeyElision,\n\t\tSnapshots:        snapshots,\n\t\tAllowZeroSeqNum:  c.allowedZeroSeqNum,\n\t\tIneffectualSingleDeleteCallback: func(userKey []byte) {\n\t\t\td.opts.EventListener.PossibleAPIMisuse(PossibleAPIMisuseInfo{\n\t\t\t\tKind:    IneffectualSingleDelete,\n\t\t\t\tUserKey: slices.Clone(userKey),\n\t\t\t})\n\t\t},\n\t\tNondeterministicSingleDeleteCallback: func(userKey []byte) {\n\t\t\td.opts.EventListener.PossibleAPIMisuse(PossibleAPIMisuseInfo{\n\t\t\t\tKind:    NondeterministicSingleDelete,\n\t\t\t\tUserKey: slices.Clone(userKey),\n\t\t\t})\n\t\t},\n\t}\n\titer := compact.NewIter(cfg, pointIter, rangeDelIter, rangeKeyIter)\n\n\trunnerCfg := compact.RunnerConfig{\n\t\tCompactionBounds:           base.UserKeyBoundsFromInternal(c.smallest, c.largest),\n\t\tL0SplitKeys:                c.l0Limits,\n\t\tGrandparents:               c.grandparents,\n\t\tMaxGrandparentOverlapBytes: c.maxOverlapBytes,\n\t\tTargetOutputFileSize:       c.maxOutputFileSize,\n\t\tSlot:                       c.slot,\n\t\tIteratorStats:              &c.stats,\n\t}\n\trunner := compact.NewRunner(runnerCfg, iter)\n\tfor runner.MoreDataToWrite() {\n\t\tif c.cancel.Load() {\n\t\t\treturn runner.Finish().WithError(ErrCancelledCompaction)\n\t\t}\n\t\t// Create a new table.\n\t\twriterOpts := d.opts.MakeWriterOptions(c.outputLevel.level, tableFormat)\n\t\tobjMeta, tw, cpuWorkHandle, err := d.newCompactionOutput(jobID, c, writerOpts)\n\t\tif err != nil {\n\t\t\treturn runner.Finish().WithError(err)\n\t\t}\n\t\trunner.WriteTable(objMeta, tw)\n\t\td.opts.Experimental.CPUWorkPermissionGranter.CPUWorkDone(cpuWorkHandle)\n\t}\n\tresult = runner.Finish()\n\tif result.Err == nil {\n\t\tresult.Err = d.objProvider.Sync()\n\t}\n\treturn result\n}\n\n// makeVersionEdit creates the version edit for a compaction, based on the\n// tables in compact.Result.\nfunc (c *compaction) makeVersionEdit(result compact.Result) (*versionEdit, error) {\n\tve := &versionEdit{\n\t\tDeletedFiles: map[deletedFileEntry]*fileMetadata{},\n\t}\n\tfor _, cl := range c.inputs {\n\t\titer := cl.files.Iter()\n\t\tfor f := iter.First(); f != nil; f = iter.Next() {\n\t\t\tve.DeletedFiles[deletedFileEntry{\n\t\t\t\tLevel:   cl.level,\n\t\t\t\tFileNum: f.FileNum,\n\t\t\t}] = f\n\t\t}\n\t}\n\n\tstartLevelBytes := c.startLevel.files.SizeSum()\n\toutputMetrics := &LevelMetrics{\n\t\tBytesIn:   startLevelBytes,\n\t\tBytesRead: c.outputLevel.files.SizeSum(),\n\t}\n\tif len(c.extraLevels) > 0 {\n\t\toutputMetrics.BytesIn += c.extraLevels[0].files.SizeSum()\n\t}\n\toutputMetrics.BytesRead += outputMetrics.BytesIn\n\n\tc.metrics = map[int]*LevelMetrics{\n\t\tc.outputLevel.level: outputMetrics,\n\t}\n\tif len(c.flushing) == 0 && c.metrics[c.startLevel.level] == nil {\n\t\tc.metrics[c.startLevel.level] = &LevelMetrics{}\n\t}\n\tif len(c.extraLevels) > 0 {\n\t\tc.metrics[c.extraLevels[0].level] = &LevelMetrics{}\n\t\toutputMetrics.MultiLevel.BytesInTop = startLevelBytes\n\t\toutputMetrics.MultiLevel.BytesIn = outputMetrics.BytesIn\n\t\toutputMetrics.MultiLevel.BytesRead = outputMetrics.BytesRead\n\t}\n\n\tinputLargestSeqNumAbsolute := c.inputLargestSeqNumAbsolute()\n\tve.NewFiles = make([]newFileEntry, len(result.Tables))\n\tfor i := range result.Tables {\n\t\tt := &result.Tables[i]\n\n\t\tfileMeta := &fileMetadata{\n\t\t\tFileNum:        base.PhysicalTableFileNum(t.ObjMeta.DiskFileNum),\n\t\t\tCreationTime:   t.CreationTime.Unix(),\n\t\t\tSize:           t.WriterMeta.Size,\n\t\t\tSmallestSeqNum: t.WriterMeta.SmallestSeqNum,\n\t\t\tLargestSeqNum:  t.WriterMeta.LargestSeqNum,\n\t\t}\n\t\tif c.flushing == nil {\n\t\t\t// Set the file's LargestSeqNumAbsolute to be the maximum value of any\n\t\t\t// of the compaction's input sstables.\n\t\t\t// TODO(jackson): This could be narrowed to be the maximum of input\n\t\t\t// sstables that overlap the output sstable's key range.\n\t\t\tfileMeta.LargestSeqNumAbsolute = inputLargestSeqNumAbsolute\n\t\t} else {\n\t\t\tfileMeta.LargestSeqNumAbsolute = t.WriterMeta.LargestSeqNum\n\t\t}\n\t\tfileMeta.InitPhysicalBacking()\n\n\t\t// If the file didn't contain any range deletions, we can fill its\n\t\t// table stats now, avoiding unnecessarily loading the table later.\n\t\tmaybeSetStatsFromProperties(\n\t\t\tfileMeta.PhysicalMeta(), &t.WriterMeta.Properties,\n\t\t)\n\n\t\tif t.WriterMeta.HasPointKeys {\n\t\t\tfileMeta.ExtendPointKeyBounds(c.cmp, t.WriterMeta.SmallestPoint, t.WriterMeta.LargestPoint)\n\t\t}\n\t\tif t.WriterMeta.HasRangeDelKeys {\n\t\t\tfileMeta.ExtendPointKeyBounds(c.cmp, t.WriterMeta.SmallestRangeDel, t.WriterMeta.LargestRangeDel)\n\t\t}\n\t\tif t.WriterMeta.HasRangeKeys {\n\t\t\tfileMeta.ExtendRangeKeyBounds(c.cmp, t.WriterMeta.SmallestRangeKey, t.WriterMeta.LargestRangeKey)\n\t\t}\n\n\t\tve.NewFiles[i] = newFileEntry{\n\t\t\tLevel: c.outputLevel.level,\n\t\t\tMeta:  fileMeta,\n\t\t}\n\n\t\t// Update metrics.\n\t\tif c.flushing == nil {\n\t\t\toutputMetrics.TablesCompacted++\n\t\t\toutputMetrics.BytesCompacted += fileMeta.Size\n\t\t} else {\n\t\t\toutputMetrics.TablesFlushed++\n\t\t\toutputMetrics.BytesFlushed += fileMeta.Size\n\t\t}\n\t\toutputMetrics.Size += int64(fileMeta.Size)\n\t\toutputMetrics.NumFiles++\n\t\toutputMetrics.Additional.BytesWrittenDataBlocks += t.WriterMeta.Properties.DataSize\n\t\toutputMetrics.Additional.BytesWrittenValueBlocks += t.WriterMeta.Properties.ValueBlocksSize\n\t}\n\n\t// Sanity check that the tables are ordered and don't overlap.\n\tfor i := 1; i < len(ve.NewFiles); i++ {\n\t\tif ve.NewFiles[i-1].Meta.UserKeyBounds().End.IsUpperBoundFor(c.cmp, ve.NewFiles[i].Meta.Smallest.UserKey) {\n\t\t\treturn nil, base.AssertionFailedf(\"pebble: compaction output tables overlap: %s and %s\",\n\t\t\t\tve.NewFiles[i-1].Meta.DebugString(c.formatKey, true),\n\t\t\t\tve.NewFiles[i].Meta.DebugString(c.formatKey, true),\n\t\t\t)\n\t\t}\n\t}\n\n\treturn ve, nil\n}\n\n// newCompactionOutput creates an object for a new table produced by a\n// compaction or flush.\nfunc (d *DB) newCompactionOutput(\n\tjobID JobID, c *compaction, writerOpts sstable.WriterOptions,\n) (objstorage.ObjectMetadata, sstable.RawWriter, CPUWorkHandle, error) {\n\tdiskFileNum := d.mu.versions.getNextDiskFileNum()\n\n\tvar writeCategory vfs.DiskWriteCategory\n\tif d.opts.EnableSQLRowSpillMetrics {\n\t\t// In the scenario that the Pebble engine is used for SQL row spills the\n\t\t// data written to the memtable will correspond to spills to disk and\n\t\t// should be categorized as such.\n\t\twriteCategory = \"sql-row-spill\"\n\t} else if c.kind == compactionKindFlush {\n\t\twriteCategory = \"pebble-memtable-flush\"\n\t} else {\n\t\twriteCategory = \"pebble-compaction\"\n\t}\n\n\tvar reason string\n\tif c.kind == compactionKindFlush {\n\t\treason = \"flushing\"\n\t} else {\n\t\treason = \"compacting\"\n\t}\n\n\tctx := context.TODO()\n\tif objiotracing.Enabled {\n\t\tctx = objiotracing.WithLevel(ctx, c.outputLevel.level)\n\t\tif c.kind == compactionKindFlush {\n\t\t\tctx = objiotracing.WithReason(ctx, objiotracing.ForFlush)\n\t\t} else {\n\t\t\tctx = objiotracing.WithReason(ctx, objiotracing.ForCompaction)\n\t\t}\n\t}\n\n\t// Prefer shared storage if present.\n\tcreateOpts := objstorage.CreateOptions{\n\t\tPreferSharedStorage: remote.ShouldCreateShared(d.opts.Experimental.CreateOnShared, c.outputLevel.level),\n\t\tWriteCategory:       writeCategory,\n\t}\n\twritable, objMeta, err := d.objProvider.Create(ctx, fileTypeTable, diskFileNum, createOpts)\n\tif err != nil {\n\t\treturn objstorage.ObjectMetadata{}, nil, nil, err\n\t}\n\n\tif c.kind != compactionKindFlush {\n\t\twritable = &compactionWritable{\n\t\t\tWritable: writable,\n\t\t\tversions: d.mu.versions,\n\t\t\twritten:  &c.bytesWritten,\n\t\t}\n\t}\n\td.opts.EventListener.TableCreated(TableCreateInfo{\n\t\tJobID:   int(jobID),\n\t\tReason:  reason,\n\t\tPath:    d.objProvider.Path(objMeta),\n\t\tFileNum: diskFileNum,\n\t})\n\n\twriterOpts.SetInternal(sstableinternal.WriterOptions{\n\t\tCacheOpts: sstableinternal.CacheOptions{\n\t\t\tCache:   d.opts.Cache,\n\t\t\tCacheID: d.cacheID,\n\t\t\tFileNum: diskFileNum,\n\t\t},\n\t})\n\n\tconst MaxFileWriteAdditionalCPUTime = time.Millisecond * 100\n\tcpuWorkHandle := d.opts.Experimental.CPUWorkPermissionGranter.GetPermission(\n\t\tMaxFileWriteAdditionalCPUTime,\n\t)\n\twriterOpts.Parallelism =\n\t\td.opts.Experimental.MaxWriterConcurrency > 0 &&\n\t\t\t(cpuWorkHandle.Permitted() || d.opts.Experimental.ForceWriterParallelism)\n\n\t// TODO(jackson): Make the compaction body generic over the RawWriter type,\n\t// so that we don't need to pay the cost of dynamic dispatch?\n\ttw := sstable.NewRawWriter(writable, writerOpts)\n\treturn objMeta, tw, cpuWorkHandle, nil\n}\n\n// validateVersionEdit validates that start and end keys across new and deleted\n// files in a versionEdit pass the given validation function.\nfunc validateVersionEdit(\n\tve *versionEdit, validateFn func([]byte) error, format base.FormatKey, logger Logger,\n) {\n\tvalidateKey := func(f *manifest.FileMetadata, key []byte) {\n\t\tif err := validateFn(key); err != nil {\n\t\t\tlogger.Fatalf(\"pebble: version edit validation failed (key=%s file=%s): %v\", format(key), f, err)\n\t\t}\n\t}\n\n\t// Validate both new and deleted files.\n\tfor _, f := range ve.NewFiles {\n\t\tvalidateKey(f.Meta, f.Meta.Smallest.UserKey)\n\t\tvalidateKey(f.Meta, f.Meta.Largest.UserKey)\n\t}\n\tfor _, m := range ve.DeletedFiles {\n\t\tvalidateKey(m, m.Smallest.UserKey)\n\t\tvalidateKey(m, m.Largest.UserKey)\n\t}\n}\n"
        },
        {
          "name": "compaction_picker.go",
          "type": "blob",
          "size": 74.6533203125,
          "content": "// Copyright 2018 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"math\"\n\t\"sort\"\n\t\"strings\"\n\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/humanize\"\n\t\"github.com/cockroachdb/pebble/internal/invariants\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n)\n\n// The minimum count for an intra-L0 compaction. This matches the RocksDB\n// heuristic.\nconst minIntraL0Count = 4\n\ntype compactionEnv struct {\n\t// diskAvailBytes holds a statistic on the number of bytes available on\n\t// disk, as reported by the filesystem. It's used to be more restrictive in\n\t// expanding compactions if available disk space is limited.\n\t//\n\t// The cached value (d.diskAvailBytes) is updated whenever a file is deleted\n\t// and whenever a compaction or flush completes. Since file removal is the\n\t// primary means of reclaiming space, there is a rough bound on the\n\t// statistic's staleness when available bytes is growing. Compactions and\n\t// flushes are longer, slower operations and provide a much looser bound\n\t// when available bytes is decreasing.\n\tdiskAvailBytes          uint64\n\tearliestUnflushedSeqNum base.SeqNum\n\tearliestSnapshotSeqNum  base.SeqNum\n\tinProgressCompactions   []compactionInfo\n\treadCompactionEnv       readCompactionEnv\n}\n\ntype compactionPicker interface {\n\tgetScores([]compactionInfo) [numLevels]float64\n\tgetBaseLevel() int\n\testimatedCompactionDebt(l0ExtraSize uint64) uint64\n\tpickAuto(env compactionEnv) (pc *pickedCompaction)\n\tpickElisionOnlyCompaction(env compactionEnv) (pc *pickedCompaction)\n\tpickRewriteCompaction(env compactionEnv) (pc *pickedCompaction)\n\tpickReadTriggeredCompaction(env compactionEnv) (pc *pickedCompaction)\n\tforceBaseLevel1()\n}\n\n// readCompactionEnv is used to hold data required to perform read compactions\ntype readCompactionEnv struct {\n\trescheduleReadCompaction *bool\n\treadCompactions          *readCompactionQueue\n\tflushing                 bool\n}\n\n// Information about in-progress compactions provided to the compaction picker.\n// These are used to constrain the new compactions that will be picked.\ntype compactionInfo struct {\n\t// versionEditApplied is true if this compaction's version edit has already\n\t// been committed. The compaction may still be in-progress deleting newly\n\t// obsolete files.\n\tversionEditApplied bool\n\tinputs             []compactionLevel\n\toutputLevel        int\n\tsmallest           InternalKey\n\tlargest            InternalKey\n}\n\nfunc (info compactionInfo) String() string {\n\tvar buf bytes.Buffer\n\tvar largest int\n\tfor i, in := range info.inputs {\n\t\tif i > 0 {\n\t\t\tfmt.Fprintf(&buf, \" -> \")\n\t\t}\n\t\tfmt.Fprintf(&buf, \"L%d\", in.level)\n\t\tin.files.Each(func(m *fileMetadata) {\n\t\t\tfmt.Fprintf(&buf, \" %s\", m.FileNum)\n\t\t})\n\t\tif largest < in.level {\n\t\t\tlargest = in.level\n\t\t}\n\t}\n\tif largest != info.outputLevel || len(info.inputs) == 1 {\n\t\tfmt.Fprintf(&buf, \" -> L%d\", info.outputLevel)\n\t}\n\treturn buf.String()\n}\n\ntype sortCompactionLevelsByPriority []candidateLevelInfo\n\nfunc (s sortCompactionLevelsByPriority) Len() int {\n\treturn len(s)\n}\n\n// A level should be picked for compaction if the compensatedScoreRatio is >= the\n// compactionScoreThreshold.\nconst compactionScoreThreshold = 1\n\n// Less should return true if s[i] must be placed earlier than s[j] in the final\n// sorted list. The candidateLevelInfo for the level placed earlier is more likely\n// to be picked for a compaction.\nfunc (s sortCompactionLevelsByPriority) Less(i, j int) bool {\n\tiShouldCompact := s[i].compensatedScoreRatio >= compactionScoreThreshold\n\tjShouldCompact := s[j].compensatedScoreRatio >= compactionScoreThreshold\n\t// Ordering is defined as decreasing on (shouldCompact, uncompensatedScoreRatio)\n\t// where shouldCompact is 1 for true and 0 for false.\n\tif iShouldCompact && !jShouldCompact {\n\t\treturn true\n\t}\n\tif !iShouldCompact && jShouldCompact {\n\t\treturn false\n\t}\n\n\tif s[i].uncompensatedScoreRatio != s[j].uncompensatedScoreRatio {\n\t\treturn s[i].uncompensatedScoreRatio > s[j].uncompensatedScoreRatio\n\t}\n\treturn s[i].level < s[j].level\n}\n\nfunc (s sortCompactionLevelsByPriority) Swap(i, j int) {\n\ts[i], s[j] = s[j], s[i]\n}\n\n// sublevelInfo is used to tag a LevelSlice for an L0 sublevel with the\n// sublevel.\ntype sublevelInfo struct {\n\tmanifest.LevelSlice\n\tsublevel manifest.Layer\n}\n\nfunc (cl sublevelInfo) Clone() sublevelInfo {\n\treturn sublevelInfo{\n\t\tsublevel:   cl.sublevel,\n\t\tLevelSlice: cl.LevelSlice,\n\t}\n}\nfunc (cl sublevelInfo) String() string {\n\treturn fmt.Sprintf(`Sublevel %s; Levels %s`, cl.sublevel, cl.LevelSlice)\n}\n\n// generateSublevelInfo will generate the level slices for each of the sublevels\n// from the level slice for all of L0.\nfunc generateSublevelInfo(cmp base.Compare, levelFiles manifest.LevelSlice) []sublevelInfo {\n\tsublevelMap := make(map[uint64][]*fileMetadata)\n\tit := levelFiles.Iter()\n\tfor f := it.First(); f != nil; f = it.Next() {\n\t\tsublevelMap[uint64(f.SubLevel)] = append(sublevelMap[uint64(f.SubLevel)], f)\n\t}\n\n\tvar sublevels []int\n\tfor level := range sublevelMap {\n\t\tsublevels = append(sublevels, int(level))\n\t}\n\tsort.Ints(sublevels)\n\n\tvar levelSlices []sublevelInfo\n\tfor _, sublevel := range sublevels {\n\t\tmetas := sublevelMap[uint64(sublevel)]\n\t\tlevelSlices = append(\n\t\t\tlevelSlices,\n\t\t\tsublevelInfo{\n\t\t\t\tmanifest.NewLevelSliceKeySorted(cmp, metas),\n\t\t\t\tmanifest.L0Sublevel(sublevel),\n\t\t\t},\n\t\t)\n\t}\n\treturn levelSlices\n}\n\n// compactionPickerMetrics holds metrics related to the compaction picking process\ntype compactionPickerMetrics struct {\n\t// scores contains the compensatedScoreRatio from the candidateLevelInfo.\n\tscores                      []float64\n\tsingleLevelOverlappingRatio float64\n\tmultiLevelOverlappingRatio  float64\n}\n\n// pickedCompaction contains information about a compaction that has already\n// been chosen, and is being constructed. Compaction construction info lives in\n// this struct, and is copied over into the compaction struct when that's\n// created.\ntype pickedCompaction struct {\n\tcmp Compare\n\t// score of the chosen compaction. This is the same as the\n\t// compensatedScoreRatio in the candidateLevelInfo.\n\tscore float64\n\t// kind indicates the kind of compaction.\n\tkind compactionKind\n\t// startLevel is the level that is being compacted. Inputs from startLevel\n\t// and outputLevel will be merged to produce a set of outputLevel files.\n\tstartLevel *compactionLevel\n\t// outputLevel is the level that files are being produced in. outputLevel is\n\t// equal to startLevel+1 except when:\n\t//    - if startLevel is 0, the output level equals compactionPicker.baseLevel().\n\t//    - in multilevel compaction, the output level is the lowest level involved in\n\t//      the compaction\n\toutputLevel *compactionLevel\n\t// extraLevels contain additional levels in between the input and output\n\t// levels that get compacted in multi level compactions\n\textraLevels []*compactionLevel\n\tinputs      []compactionLevel\n\t// LBase at the time of compaction picking.\n\tbaseLevel int\n\t// L0-specific compaction info. Set to a non-nil value for all compactions\n\t// where startLevel == 0 that were generated by L0Sublevels.\n\tlcf *manifest.L0CompactionFiles\n\t// maxOutputFileSize is the maximum size of an individual table created\n\t// during compaction.\n\tmaxOutputFileSize uint64\n\t// maxOverlapBytes is the maximum number of bytes of overlap allowed for a\n\t// single output table with the tables in the grandparent level.\n\tmaxOverlapBytes uint64\n\t// maxReadCompactionBytes is the maximum bytes a read compaction is allowed to\n\t// overlap in its output level with. If the overlap is greater than\n\t// maxReadCompaction bytes, then we don't proceed with the compaction.\n\tmaxReadCompactionBytes uint64\n\n\t// The boundaries of the input data.\n\tsmallest      InternalKey\n\tlargest       InternalKey\n\tversion       *version\n\tpickerMetrics compactionPickerMetrics\n}\n\nfunc (pc *pickedCompaction) userKeyBounds() base.UserKeyBounds {\n\treturn base.UserKeyBoundsFromInternal(pc.smallest, pc.largest)\n}\n\nfunc defaultOutputLevel(startLevel, baseLevel int) int {\n\toutputLevel := startLevel + 1\n\tif startLevel == 0 {\n\t\toutputLevel = baseLevel\n\t}\n\tif outputLevel >= numLevels-1 {\n\t\toutputLevel = numLevels - 1\n\t}\n\treturn outputLevel\n}\n\nfunc newPickedCompaction(\n\topts *Options, cur *version, startLevel, outputLevel, baseLevel int,\n) *pickedCompaction {\n\tif startLevel > 0 && startLevel < baseLevel {\n\t\tpanic(fmt.Sprintf(\"invalid compaction: start level %d should not be empty (base level %d)\",\n\t\t\tstartLevel, baseLevel))\n\t}\n\n\tadjustedLevel := adjustedOutputLevel(outputLevel, baseLevel)\n\tpc := &pickedCompaction{\n\t\tcmp:                    opts.Comparer.Compare,\n\t\tversion:                cur,\n\t\tbaseLevel:              baseLevel,\n\t\tinputs:                 []compactionLevel{{level: startLevel}, {level: outputLevel}},\n\t\tmaxOutputFileSize:      uint64(opts.Level(adjustedLevel).TargetFileSize),\n\t\tmaxOverlapBytes:        maxGrandparentOverlapBytes(opts, adjustedLevel),\n\t\tmaxReadCompactionBytes: maxReadCompactionBytes(opts, adjustedLevel),\n\t}\n\tpc.startLevel = &pc.inputs[0]\n\tpc.outputLevel = &pc.inputs[1]\n\treturn pc\n}\n\n// adjustedOutputLevel is the output level used for the purpose of\n// determining the target output file size, overlap bytes, and expanded\n// bytes, taking into account the base level.\nfunc adjustedOutputLevel(outputLevel int, baseLevel int) int {\n\tadjustedOutputLevel := outputLevel\n\tif adjustedOutputLevel > 0 {\n\t\t// Output level is in the range [baseLevel, numLevels]. For the purpose of\n\t\t// determining the target output file size, overlap bytes, and expanded\n\t\t// bytes, we want to adjust the range to [1,numLevels].\n\t\tadjustedOutputLevel = 1 + outputLevel - baseLevel\n\t}\n\treturn adjustedOutputLevel\n}\n\nfunc newPickedCompactionFromL0(\n\tlcf *manifest.L0CompactionFiles, opts *Options, vers *version, baseLevel int, isBase bool,\n) *pickedCompaction {\n\toutputLevel := baseLevel\n\tif !isBase {\n\t\toutputLevel = 0 // Intra L0\n\t}\n\n\tpc := newPickedCompaction(opts, vers, 0, outputLevel, baseLevel)\n\tpc.lcf = lcf\n\tpc.outputLevel.level = outputLevel\n\n\t// Manually build the compaction as opposed to calling\n\t// pickAutoHelper. This is because L0Sublevels has already added\n\t// any overlapping L0 SSTables that need to be added, and\n\t// because compactions built by L0SSTables do not necessarily\n\t// pick contiguous sequences of files in pc.version.Levels[0].\n\tfiles := make([]*manifest.FileMetadata, 0, len(lcf.Files))\n\titer := vers.Levels[0].Iter()\n\tfor f := iter.First(); f != nil; f = iter.Next() {\n\t\tif lcf.FilesIncluded[f.L0Index] {\n\t\t\tfiles = append(files, f)\n\t\t}\n\t}\n\tpc.startLevel.files = manifest.NewLevelSliceSeqSorted(files)\n\treturn pc\n}\n\nfunc (pc *pickedCompaction) String() string {\n\tvar builder strings.Builder\n\tbuilder.WriteString(fmt.Sprintf(`Score=%f, `, pc.score))\n\tbuilder.WriteString(fmt.Sprintf(`Kind=%s, `, pc.kind))\n\tbuilder.WriteString(fmt.Sprintf(`AdjustedOutputLevel=%d, `, adjustedOutputLevel(pc.outputLevel.level, pc.baseLevel)))\n\tbuilder.WriteString(fmt.Sprintf(`maxOutputFileSize=%d, `, pc.maxOutputFileSize))\n\tbuilder.WriteString(fmt.Sprintf(`maxReadCompactionBytes=%d, `, pc.maxReadCompactionBytes))\n\tbuilder.WriteString(fmt.Sprintf(`smallest=%s, `, pc.smallest))\n\tbuilder.WriteString(fmt.Sprintf(`largest=%s, `, pc.largest))\n\tbuilder.WriteString(fmt.Sprintf(`version=%s, `, pc.version))\n\tbuilder.WriteString(fmt.Sprintf(`inputs=%s, `, pc.inputs))\n\tbuilder.WriteString(fmt.Sprintf(`startlevel=%s, `, pc.startLevel))\n\tbuilder.WriteString(fmt.Sprintf(`outputLevel=%s, `, pc.outputLevel))\n\tbuilder.WriteString(fmt.Sprintf(`extraLevels=%s, `, pc.extraLevels))\n\tbuilder.WriteString(fmt.Sprintf(`l0SublevelInfo=%s, `, pc.startLevel.l0SublevelInfo))\n\tbuilder.WriteString(fmt.Sprintf(`lcf=%s`, pc.lcf))\n\treturn builder.String()\n}\n\n// Clone creates a deep copy of the pickedCompaction\nfunc (pc *pickedCompaction) clone() *pickedCompaction {\n\n\t// Quickly copy over fields that do not require special deep copy care, and\n\t// set all fields that will require a deep copy to nil.\n\tnewPC := &pickedCompaction{\n\t\tcmp:                    pc.cmp,\n\t\tscore:                  pc.score,\n\t\tkind:                   pc.kind,\n\t\tbaseLevel:              pc.baseLevel,\n\t\tmaxOutputFileSize:      pc.maxOutputFileSize,\n\t\tmaxOverlapBytes:        pc.maxOverlapBytes,\n\t\tmaxReadCompactionBytes: pc.maxReadCompactionBytes,\n\t\tsmallest:               pc.smallest.Clone(),\n\t\tlargest:                pc.largest.Clone(),\n\n\t\t// TODO(msbutler): properly clone picker metrics\n\t\tpickerMetrics: pc.pickerMetrics,\n\n\t\t// Both copies see the same manifest, therefore, it's ok for them to se\n\t\t// share the same pc. version.\n\t\tversion: pc.version,\n\t}\n\n\tnewPC.inputs = make([]compactionLevel, len(pc.inputs))\n\tnewPC.extraLevels = make([]*compactionLevel, 0, len(pc.extraLevels))\n\tfor i := range pc.inputs {\n\t\tnewPC.inputs[i] = pc.inputs[i].Clone()\n\t\tif i == 0 {\n\t\t\tnewPC.startLevel = &newPC.inputs[i]\n\t\t} else if i == len(pc.inputs)-1 {\n\t\t\tnewPC.outputLevel = &newPC.inputs[i]\n\t\t} else {\n\t\t\tnewPC.extraLevels = append(newPC.extraLevels, &newPC.inputs[i])\n\t\t}\n\t}\n\n\tif len(pc.startLevel.l0SublevelInfo) > 0 {\n\t\tnewPC.startLevel.l0SublevelInfo = make([]sublevelInfo, len(pc.startLevel.l0SublevelInfo))\n\t\tfor i := range pc.startLevel.l0SublevelInfo {\n\t\t\tnewPC.startLevel.l0SublevelInfo[i] = pc.startLevel.l0SublevelInfo[i].Clone()\n\t\t}\n\t}\n\tif pc.lcf != nil {\n\t\tnewPC.lcf = pc.lcf.Clone()\n\t}\n\treturn newPC\n}\n\n// maybeExpandBounds is a helper function for setupInputs which ensures the\n// pickedCompaction's smallest and largest internal keys are updated iff\n// the candidate keys expand the key span. This avoids a bug for multi-level\n// compactions: during the second call to setupInputs, the picked compaction's\n// smallest and largest keys should not decrease the key span.\nfunc (pc *pickedCompaction) maybeExpandBounds(smallest InternalKey, largest InternalKey) {\n\tif len(smallest.UserKey) == 0 && len(largest.UserKey) == 0 {\n\t\treturn\n\t}\n\tif len(pc.smallest.UserKey) == 0 && len(pc.largest.UserKey) == 0 {\n\t\tpc.smallest = smallest\n\t\tpc.largest = largest\n\t\treturn\n\t}\n\tif base.InternalCompare(pc.cmp, pc.smallest, smallest) >= 0 {\n\t\tpc.smallest = smallest\n\t}\n\tif base.InternalCompare(pc.cmp, pc.largest, largest) <= 0 {\n\t\tpc.largest = largest\n\t}\n}\n\n// setupInputs returns true if a compaction has been set up. It returns false if\n// a concurrent compaction is occurring on the start or output level files.\nfunc (pc *pickedCompaction) setupInputs(\n\topts *Options, diskAvailBytes uint64, startLevel *compactionLevel,\n) bool {\n\t// maxExpandedBytes is the maximum size of an expanded compaction. If\n\t// growing a compaction results in a larger size, the original compaction\n\t// is used instead.\n\tmaxExpandedBytes := expandedCompactionByteSizeLimit(\n\t\topts, adjustedOutputLevel(pc.outputLevel.level, pc.baseLevel), diskAvailBytes,\n\t)\n\n\tif anyTablesCompacting(startLevel.files) {\n\t\treturn false\n\t}\n\n\tpc.maybeExpandBounds(manifest.KeyRange(pc.cmp, startLevel.files.Iter()))\n\n\t// Determine the sstables in the output level which overlap with the input\n\t// sstables. No need to do this for intra-L0 compactions; outputLevel.files is\n\t// left empty for those.\n\tif startLevel.level != pc.outputLevel.level {\n\t\tpc.outputLevel.files = pc.version.Overlaps(pc.outputLevel.level, pc.userKeyBounds())\n\t\tif anyTablesCompacting(pc.outputLevel.files) {\n\t\t\treturn false\n\t\t}\n\n\t\tpc.maybeExpandBounds(manifest.KeyRange(pc.cmp,\n\t\t\tstartLevel.files.Iter(), pc.outputLevel.files.Iter()))\n\t}\n\n\t// Grow the sstables in startLevel.level as long as it doesn't affect the number\n\t// of sstables included from pc.outputLevel.level.\n\tif pc.lcf != nil && startLevel.level == 0 && pc.outputLevel.level != 0 {\n\t\t// Call the L0-specific compaction extension method. Similar logic as\n\t\t// pc.grow. Additional L0 files are optionally added to the compaction at\n\t\t// this step. Note that the bounds passed in are not the bounds of the\n\t\t// compaction, but rather the smallest and largest internal keys that\n\t\t// the compaction cannot include from L0 without pulling in more Lbase\n\t\t// files. Consider this example:\n\t\t//\n\t\t// L0:        c-d e+f g-h\n\t\t// Lbase: a-b     e+f     i-j\n\t\t//        a b c d e f g h i j\n\t\t//\n\t\t// The e-f files have already been chosen in the compaction. As pulling\n\t\t// in more LBase files is undesirable, the logic below will pass in\n\t\t// smallest = b and largest = i to ExtendL0ForBaseCompactionTo, which\n\t\t// will expand the compaction to include c-d and g-h from L0. The\n\t\t// bounds passed in are exclusive; the compaction cannot be expanded\n\t\t// to include files that \"touch\" it.\n\t\tsmallestBaseKey := base.InvalidInternalKey\n\t\tlargestBaseKey := base.InvalidInternalKey\n\t\tif pc.outputLevel.files.Empty() {\n\t\t\tbaseIter := pc.version.Levels[pc.outputLevel.level].Iter()\n\t\t\tif sm := baseIter.SeekLT(pc.cmp, pc.smallest.UserKey); sm != nil {\n\t\t\t\tsmallestBaseKey = sm.Largest\n\t\t\t}\n\t\t\tif la := baseIter.SeekGE(pc.cmp, pc.largest.UserKey); la != nil {\n\t\t\t\tlargestBaseKey = la.Smallest\n\t\t\t}\n\t\t} else {\n\t\t\t// NB: We use Reslice to access the underlying level's files, but\n\t\t\t// we discard the returned slice. The pc.outputLevel.files slice\n\t\t\t// is not modified.\n\t\t\t_ = pc.outputLevel.files.Reslice(func(start, end *manifest.LevelIterator) {\n\t\t\t\tif sm := start.Prev(); sm != nil {\n\t\t\t\t\tsmallestBaseKey = sm.Largest\n\t\t\t\t}\n\t\t\t\tif la := end.Next(); la != nil {\n\t\t\t\t\tlargestBaseKey = la.Smallest\n\t\t\t\t}\n\t\t\t})\n\t\t}\n\t\toldLcf := pc.lcf.Clone()\n\t\tif pc.version.L0Sublevels.ExtendL0ForBaseCompactionTo(smallestBaseKey, largestBaseKey, pc.lcf) {\n\t\t\tvar newStartLevelFiles []*fileMetadata\n\t\t\titer := pc.version.Levels[0].Iter()\n\t\t\tvar sizeSum uint64\n\t\t\tfor j, f := 0, iter.First(); f != nil; j, f = j+1, iter.Next() {\n\t\t\t\tif pc.lcf.FilesIncluded[f.L0Index] {\n\t\t\t\t\tnewStartLevelFiles = append(newStartLevelFiles, f)\n\t\t\t\t\tsizeSum += f.Size\n\t\t\t\t}\n\t\t\t}\n\t\t\tif sizeSum+pc.outputLevel.files.SizeSum() < maxExpandedBytes {\n\t\t\t\tstartLevel.files = manifest.NewLevelSliceSeqSorted(newStartLevelFiles)\n\t\t\t\tpc.smallest, pc.largest = manifest.KeyRange(pc.cmp,\n\t\t\t\t\tstartLevel.files.Iter(), pc.outputLevel.files.Iter())\n\t\t\t} else {\n\t\t\t\t*pc.lcf = *oldLcf\n\t\t\t}\n\t\t}\n\t} else if pc.grow(pc.smallest, pc.largest, maxExpandedBytes, startLevel) {\n\t\tpc.maybeExpandBounds(manifest.KeyRange(pc.cmp,\n\t\t\tstartLevel.files.Iter(), pc.outputLevel.files.Iter()))\n\t}\n\n\tif pc.startLevel.level == 0 {\n\t\t// We don't change the input files for the compaction beyond this point.\n\t\tpc.startLevel.l0SublevelInfo = generateSublevelInfo(pc.cmp, pc.startLevel.files)\n\t}\n\n\treturn true\n}\n\n// grow grows the number of inputs at c.level without changing the number of\n// c.level+1 files in the compaction, and returns whether the inputs grew. sm\n// and la are the smallest and largest InternalKeys in all of the inputs.\nfunc (pc *pickedCompaction) grow(\n\tsm, la InternalKey, maxExpandedBytes uint64, startLevel *compactionLevel,\n) bool {\n\tif pc.outputLevel.files.Empty() {\n\t\treturn false\n\t}\n\tgrow0 := pc.version.Overlaps(startLevel.level, base.UserKeyBoundsFromInternal(sm, la))\n\tif anyTablesCompacting(grow0) {\n\t\treturn false\n\t}\n\tif grow0.Len() <= startLevel.files.Len() {\n\t\treturn false\n\t}\n\tif grow0.SizeSum()+pc.outputLevel.files.SizeSum() >= maxExpandedBytes {\n\t\treturn false\n\t}\n\t// We need to include the outputLevel iter because without it, in a multiLevel scenario,\n\t// sm1 and la1 could shift the output level keyspace when pc.outputLevel.files is set to grow1.\n\tsm1, la1 := manifest.KeyRange(pc.cmp, grow0.Iter(), pc.outputLevel.files.Iter())\n\tgrow1 := pc.version.Overlaps(pc.outputLevel.level, base.UserKeyBoundsFromInternal(sm1, la1))\n\tif anyTablesCompacting(grow1) {\n\t\treturn false\n\t}\n\tif grow1.Len() != pc.outputLevel.files.Len() {\n\t\treturn false\n\t}\n\tstartLevel.files = grow0\n\tpc.outputLevel.files = grow1\n\treturn true\n}\n\nfunc (pc *pickedCompaction) compactionSize() uint64 {\n\tvar bytesToCompact uint64\n\tfor i := range pc.inputs {\n\t\tbytesToCompact += pc.inputs[i].files.SizeSum()\n\t}\n\treturn bytesToCompact\n}\n\n// setupMultiLevelCandidated returns true if it successfully added another level\n// to the compaction.\nfunc (pc *pickedCompaction) setupMultiLevelCandidate(opts *Options, diskAvailBytes uint64) bool {\n\tpc.inputs = append(pc.inputs, compactionLevel{level: pc.outputLevel.level + 1})\n\n\t// Recalibrate startLevel and outputLevel:\n\t//  - startLevel and outputLevel pointers may be obsolete after appending to pc.inputs.\n\t//  - push outputLevel to extraLevels and move the new level to outputLevel\n\tpc.startLevel = &pc.inputs[0]\n\tpc.extraLevels = []*compactionLevel{&pc.inputs[1]}\n\tpc.outputLevel = &pc.inputs[2]\n\treturn pc.setupInputs(opts, diskAvailBytes, pc.extraLevels[len(pc.extraLevels)-1])\n}\n\n// anyTablesCompacting returns true if any tables in the level slice are\n// compacting.\nfunc anyTablesCompacting(inputs manifest.LevelSlice) bool {\n\tit := inputs.Iter()\n\tfor f := it.First(); f != nil; f = it.Next() {\n\t\tif f.IsCompacting() {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// newCompactionPickerByScore creates a compactionPickerByScore associated with\n// the newest version. The picker is used under logLock (until a new version is\n// installed).\nfunc newCompactionPickerByScore(\n\tv *version,\n\tvirtualBackings *manifest.VirtualBackings,\n\topts *Options,\n\tinProgressCompactions []compactionInfo,\n) *compactionPickerByScore {\n\tp := &compactionPickerByScore{\n\t\topts:            opts,\n\t\tvers:            v,\n\t\tvirtualBackings: virtualBackings,\n\t}\n\tp.initLevelMaxBytes(inProgressCompactions)\n\treturn p\n}\n\n// Information about a candidate compaction level that has been identified by\n// the compaction picker.\ntype candidateLevelInfo struct {\n\t// The compensatedScore of the level after adjusting according to the other\n\t// levels' sizes. For L0, the compensatedScoreRatio is equivalent to the\n\t// uncompensatedScoreRatio as we don't account for level size compensation in\n\t// L0.\n\tcompensatedScoreRatio float64\n\t// The score of the level after accounting for level size compensation before\n\t// adjusting according to other levels' sizes. For L0, the compensatedScore\n\t// is equivalent to the uncompensatedScore as we don't account for level\n\t// size compensation in L0.\n\tcompensatedScore float64\n\t// The score of the level to be compacted, calculated using uncompensated file\n\t// sizes and without any adjustments.\n\tuncompensatedScore float64\n\t// uncompensatedScoreRatio is the uncompensatedScore adjusted according to\n\t// the other levels' sizes.\n\tuncompensatedScoreRatio float64\n\tlevel                   int\n\t// The level to compact to.\n\toutputLevel int\n\t// The file in level that will be compacted. Additional files may be\n\t// picked by the compaction, and a pickedCompaction created for the\n\t// compaction.\n\tfile manifest.LevelFile\n}\n\nfunc (c *candidateLevelInfo) shouldCompact() bool {\n\treturn c.compensatedScoreRatio >= compactionScoreThreshold\n}\n\nfunc fileCompensation(f *fileMetadata) uint64 {\n\treturn uint64(f.Stats.PointDeletionsBytesEstimate) + f.Stats.RangeDeletionsBytesEstimate\n}\n\n// compensatedSize returns f's file size, inflated according to compaction\n// priorities.\nfunc compensatedSize(f *fileMetadata) uint64 {\n\t// Add in the estimate of disk space that may be reclaimed by compacting the\n\t// file's tombstones.\n\treturn f.Size + fileCompensation(f)\n}\n\n// compensatedSizeAnnotator is a manifest.Annotator that annotates B-Tree\n// nodes with the sum of the files' compensated sizes. Compensated sizes may\n// change once a table's stats are loaded asynchronously, so its values are\n// marked as cacheable only if a file's stats have been loaded.\nvar compensatedSizeAnnotator = manifest.SumAnnotator(func(f *fileMetadata) (uint64, bool) {\n\treturn compensatedSize(f), f.StatsValid()\n})\n\n// totalCompensatedSize computes the compensated size over a file metadata\n// iterator. Note that this function is linear in the files available to the\n// iterator. Use the compensatedSizeAnnotator if querying the total\n// compensated size of a level.\nfunc totalCompensatedSize(iter manifest.LevelIterator) uint64 {\n\tvar sz uint64\n\tfor f := iter.First(); f != nil; f = iter.Next() {\n\t\tsz += compensatedSize(f)\n\t}\n\treturn sz\n}\n\n// compactionPickerByScore holds the state and logic for picking a compaction. A\n// compaction picker is associated with a single version. A new compaction\n// picker is created and initialized every time a new version is installed.\ntype compactionPickerByScore struct {\n\topts            *Options\n\tvers            *version\n\tvirtualBackings *manifest.VirtualBackings\n\t// The level to target for L0 compactions. Levels L1 to baseLevel must be\n\t// empty.\n\tbaseLevel int\n\t// levelMaxBytes holds the dynamically adjusted max bytes setting for each\n\t// level.\n\tlevelMaxBytes [numLevels]int64\n}\n\nvar _ compactionPicker = &compactionPickerByScore{}\n\nfunc (p *compactionPickerByScore) getScores(inProgress []compactionInfo) [numLevels]float64 {\n\tvar scores [numLevels]float64\n\tfor _, info := range p.calculateLevelScores(inProgress) {\n\t\tscores[info.level] = info.compensatedScoreRatio\n\t}\n\treturn scores\n}\n\nfunc (p *compactionPickerByScore) getBaseLevel() int {\n\tif p == nil {\n\t\treturn 1\n\t}\n\treturn p.baseLevel\n}\n\n// estimatedCompactionDebt estimates the number of bytes which need to be\n// compacted before the LSM tree becomes stable.\nfunc (p *compactionPickerByScore) estimatedCompactionDebt(l0ExtraSize uint64) uint64 {\n\tif p == nil {\n\t\treturn 0\n\t}\n\n\t// We assume that all the bytes in L0 need to be compacted to Lbase. This is\n\t// unlike the RocksDB logic that figures out whether L0 needs compaction.\n\tbytesAddedToNextLevel := l0ExtraSize + p.vers.Levels[0].Size()\n\tlbaseSize := p.vers.Levels[p.baseLevel].Size()\n\n\tvar compactionDebt uint64\n\tif bytesAddedToNextLevel > 0 && lbaseSize > 0 {\n\t\t// We only incur compaction debt if both L0 and Lbase contain data. If L0\n\t\t// is empty, no compaction is necessary. If Lbase is empty, a move-based\n\t\t// compaction from L0 would occur.\n\t\tcompactionDebt += bytesAddedToNextLevel + lbaseSize\n\t}\n\n\t// loop invariant: At the beginning of the loop, bytesAddedToNextLevel is the\n\t// bytes added to `level` in the loop.\n\tfor level := p.baseLevel; level < numLevels-1; level++ {\n\t\tlevelSize := p.vers.Levels[level].Size() + bytesAddedToNextLevel\n\t\tnextLevelSize := p.vers.Levels[level+1].Size()\n\t\tif levelSize > uint64(p.levelMaxBytes[level]) {\n\t\t\tbytesAddedToNextLevel = levelSize - uint64(p.levelMaxBytes[level])\n\t\t\tif nextLevelSize > 0 {\n\t\t\t\t// We only incur compaction debt if the next level contains data. If the\n\t\t\t\t// next level is empty, a move-based compaction would be used.\n\t\t\t\tlevelRatio := float64(nextLevelSize) / float64(levelSize)\n\t\t\t\t// The current level contributes bytesAddedToNextLevel to compactions.\n\t\t\t\t// The next level contributes levelRatio * bytesAddedToNextLevel.\n\t\t\t\tcompactionDebt += uint64(float64(bytesAddedToNextLevel) * (levelRatio + 1))\n\t\t\t}\n\t\t} else {\n\t\t\t// We're not moving any bytes to the next level.\n\t\t\tbytesAddedToNextLevel = 0\n\t\t}\n\t}\n\treturn compactionDebt\n}\n\nfunc (p *compactionPickerByScore) initLevelMaxBytes(inProgressCompactions []compactionInfo) {\n\t// The levelMaxBytes calculations here differ from RocksDB in two ways:\n\t//\n\t// 1. The use of dbSize vs maxLevelSize. RocksDB uses the size of the maximum\n\t//    level in L1-L6, rather than determining the size of the bottom level\n\t//    based on the total amount of data in the dB. The RocksDB calculation is\n\t//    problematic if L0 contains a significant fraction of data, or if the\n\t//    level sizes are roughly equal and thus there is a significant fraction\n\t//    of data outside of the largest level.\n\t//\n\t// 2. Not adjusting the size of Lbase based on L0. RocksDB computes\n\t//    baseBytesMax as the maximum of the configured LBaseMaxBytes and the\n\t//    size of L0. This is problematic because baseBytesMax is used to compute\n\t//    the max size of lower levels. A very large baseBytesMax will result in\n\t//    an overly large value for the size of lower levels which will caused\n\t//    those levels not to be compacted even when they should be\n\t//    compacted. This often results in \"inverted\" LSM shapes where Ln is\n\t//    larger than Ln+1.\n\n\t// Determine the first non-empty level and the total DB size.\n\tfirstNonEmptyLevel := -1\n\tvar dbSize uint64\n\tfor level := 1; level < numLevels; level++ {\n\t\tif p.vers.Levels[level].Size() > 0 {\n\t\t\tif firstNonEmptyLevel == -1 {\n\t\t\t\tfirstNonEmptyLevel = level\n\t\t\t}\n\t\t\tdbSize += p.vers.Levels[level].Size()\n\t\t}\n\t}\n\tfor _, c := range inProgressCompactions {\n\t\tif c.outputLevel == 0 || c.outputLevel == -1 {\n\t\t\tcontinue\n\t\t}\n\t\tif c.inputs[0].level == 0 && (firstNonEmptyLevel == -1 || c.outputLevel < firstNonEmptyLevel) {\n\t\t\tfirstNonEmptyLevel = c.outputLevel\n\t\t}\n\t}\n\n\t// Initialize the max-bytes setting for each level to \"infinity\" which will\n\t// disallow compaction for that level. We'll fill in the actual value below\n\t// for levels we want to allow compactions from.\n\tfor level := 0; level < numLevels; level++ {\n\t\tp.levelMaxBytes[level] = math.MaxInt64\n\t}\n\n\tif dbSize == 0 {\n\t\t// No levels for L1 and up contain any data. Target L0 compactions for the\n\t\t// last level or to the level to which there is an ongoing L0 compaction.\n\t\tp.baseLevel = numLevels - 1\n\t\tif firstNonEmptyLevel >= 0 {\n\t\t\tp.baseLevel = firstNonEmptyLevel\n\t\t}\n\t\treturn\n\t}\n\n\tdbSize += p.vers.Levels[0].Size()\n\tbottomLevelSize := dbSize - dbSize/uint64(p.opts.Experimental.LevelMultiplier)\n\n\tcurLevelSize := bottomLevelSize\n\tfor level := numLevels - 2; level >= firstNonEmptyLevel; level-- {\n\t\tcurLevelSize = uint64(float64(curLevelSize) / float64(p.opts.Experimental.LevelMultiplier))\n\t}\n\n\t// Compute base level (where L0 data is compacted to).\n\tbaseBytesMax := uint64(p.opts.LBaseMaxBytes)\n\tp.baseLevel = firstNonEmptyLevel\n\tfor p.baseLevel > 1 && curLevelSize > baseBytesMax {\n\t\tp.baseLevel--\n\t\tcurLevelSize = uint64(float64(curLevelSize) / float64(p.opts.Experimental.LevelMultiplier))\n\t}\n\n\tsmoothedLevelMultiplier := 1.0\n\tif p.baseLevel < numLevels-1 {\n\t\tsmoothedLevelMultiplier = math.Pow(\n\t\t\tfloat64(bottomLevelSize)/float64(baseBytesMax),\n\t\t\t1.0/float64(numLevels-p.baseLevel-1))\n\t}\n\n\tlevelSize := float64(baseBytesMax)\n\tfor level := p.baseLevel; level < numLevels; level++ {\n\t\tif level > p.baseLevel && levelSize > 0 {\n\t\t\tlevelSize *= smoothedLevelMultiplier\n\t\t}\n\t\t// Round the result since test cases use small target level sizes, which\n\t\t// can be impacted by floating-point imprecision + integer truncation.\n\t\troundedLevelSize := math.Round(levelSize)\n\t\tif roundedLevelSize > float64(math.MaxInt64) {\n\t\t\tp.levelMaxBytes[level] = math.MaxInt64\n\t\t} else {\n\t\t\tp.levelMaxBytes[level] = int64(roundedLevelSize)\n\t\t}\n\t}\n}\n\ntype levelSizeAdjust struct {\n\tincomingActualBytes      uint64\n\toutgoingActualBytes      uint64\n\toutgoingCompensatedBytes uint64\n}\n\nfunc (a levelSizeAdjust) compensated() uint64 {\n\treturn a.incomingActualBytes - a.outgoingCompensatedBytes\n}\n\nfunc (a levelSizeAdjust) actual() uint64 {\n\treturn a.incomingActualBytes - a.outgoingActualBytes\n}\n\nfunc calculateSizeAdjust(inProgressCompactions []compactionInfo) [numLevels]levelSizeAdjust {\n\t// Compute size adjustments for each level based on the in-progress\n\t// compactions. We sum the file sizes of all files leaving and entering each\n\t// level in in-progress compactions. For outgoing files, we also sum a\n\t// separate sum of 'compensated file sizes', which are inflated according\n\t// to deletion estimates.\n\t//\n\t// When we adjust a level's size according to these values during score\n\t// calculation, we subtract the compensated size of start level inputs to\n\t// account for the fact that score calculation uses compensated sizes.\n\t//\n\t// Since compensated file sizes may be compensated because they reclaim\n\t// space from the output level's files, we only add the real file size to\n\t// the output level.\n\t//\n\t// This is slightly different from RocksDB's behavior, which simply elides\n\t// compacting files from the level size calculation.\n\tvar sizeAdjust [numLevels]levelSizeAdjust\n\tfor i := range inProgressCompactions {\n\t\tc := &inProgressCompactions[i]\n\t\t// If this compaction's version edit has already been applied, there's\n\t\t// no need to adjust: The LSM we'll examine will already reflect the\n\t\t// new LSM state.\n\t\tif c.versionEditApplied {\n\t\t\tcontinue\n\t\t}\n\n\t\tfor _, input := range c.inputs {\n\t\t\tactualSize := input.files.SizeSum()\n\t\t\tcompensatedSize := totalCompensatedSize(input.files.Iter())\n\n\t\t\tif input.level != c.outputLevel {\n\t\t\t\tsizeAdjust[input.level].outgoingCompensatedBytes += compensatedSize\n\t\t\t\tsizeAdjust[input.level].outgoingActualBytes += actualSize\n\t\t\t\tif c.outputLevel != -1 {\n\t\t\t\t\tsizeAdjust[c.outputLevel].incomingActualBytes += actualSize\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn sizeAdjust\n}\n\nfunc (p *compactionPickerByScore) calculateLevelScores(\n\tinProgressCompactions []compactionInfo,\n) [numLevels]candidateLevelInfo {\n\tvar scores [numLevels]candidateLevelInfo\n\tfor i := range scores {\n\t\tscores[i].level = i\n\t\tscores[i].outputLevel = i + 1\n\t}\n\tl0UncompensatedScore := calculateL0UncompensatedScore(p.vers, p.opts, inProgressCompactions)\n\tscores[0] = candidateLevelInfo{\n\t\toutputLevel:        p.baseLevel,\n\t\tuncompensatedScore: l0UncompensatedScore,\n\t\tcompensatedScore:   l0UncompensatedScore, /* No level size compensation for L0 */\n\t}\n\tsizeAdjust := calculateSizeAdjust(inProgressCompactions)\n\tfor level := 1; level < numLevels; level++ {\n\t\tcompensatedLevelSize := *compensatedSizeAnnotator.LevelAnnotation(p.vers.Levels[level]) + sizeAdjust[level].compensated()\n\t\tscores[level].compensatedScore = float64(compensatedLevelSize) / float64(p.levelMaxBytes[level])\n\t\tscores[level].uncompensatedScore = float64(p.vers.Levels[level].Size()+sizeAdjust[level].actual()) / float64(p.levelMaxBytes[level])\n\t}\n\n\t// Adjust each level's {compensated, uncompensated}Score by the uncompensatedScore\n\t// of the next level to get a {compensated, uncompensated}ScoreRatio. If the\n\t// next level has a high uncompensatedScore, and is thus a priority for compaction,\n\t// this reduces the priority for compacting the current level. If the next level\n\t// has a low uncompensatedScore (i.e. it is below its target size), this increases\n\t// the priority for compacting the current level.\n\t//\n\t// The effect of this adjustment is to help prioritize compactions in lower\n\t// levels. The following example shows the compensatedScoreRatio and the\n\t// compensatedScore. In this scenario, L0 has 68 sublevels. L3 (a.k.a. Lbase)\n\t// is significantly above its target size. The original score prioritizes\n\t// compactions from those two levels, but doing so ends up causing a future\n\t// problem: data piles up in the higher levels, starving L5->L6 compactions,\n\t// and to a lesser degree starving L4->L5 compactions.\n\t//\n\t// Note that in the example shown there is no level size compensation so the\n\t// compensatedScore and the uncompensatedScore is the same for each level.\n\t//\n\t//        compensatedScoreRatio   compensatedScore   uncompensatedScore   size   max-size\n\t//   L0                     3.2               68.0                 68.0  2.2 G          -\n\t//   L3                     3.2               21.1                 21.1  1.3 G       64 M\n\t//   L4                     3.4                6.7                  6.7  3.1 G      467 M\n\t//   L5                     3.4                2.0                  2.0  6.6 G      3.3 G\n\t//   L6                     0.6                0.6                  0.6   14 G       24 G\n\tvar prevLevel int\n\tfor level := p.baseLevel; level < numLevels; level++ {\n\t\t// The compensated scores, and uncompensated scores will be turned into\n\t\t// ratios as they're adjusted according to other levels' sizes.\n\t\tscores[prevLevel].compensatedScoreRatio = scores[prevLevel].compensatedScore\n\t\tscores[prevLevel].uncompensatedScoreRatio = scores[prevLevel].uncompensatedScore\n\n\t\t// Avoid absurdly large scores by placing a floor on the score that we'll\n\t\t// adjust a level by. The value of 0.01 was chosen somewhat arbitrarily.\n\t\tconst minScore = 0.01\n\t\tif scores[prevLevel].compensatedScoreRatio >= compactionScoreThreshold {\n\t\t\tif scores[level].uncompensatedScore >= minScore {\n\t\t\t\tscores[prevLevel].compensatedScoreRatio /= scores[level].uncompensatedScore\n\t\t\t} else {\n\t\t\t\tscores[prevLevel].compensatedScoreRatio /= minScore\n\t\t\t}\n\t\t}\n\t\tif scores[prevLevel].uncompensatedScoreRatio >= compactionScoreThreshold {\n\t\t\tif scores[level].uncompensatedScore >= minScore {\n\t\t\t\tscores[prevLevel].uncompensatedScoreRatio /= scores[level].uncompensatedScore\n\t\t\t} else {\n\t\t\t\tscores[prevLevel].uncompensatedScoreRatio /= minScore\n\t\t\t}\n\t\t}\n\t\tprevLevel = level\n\t}\n\t// Set the score ratios for the lowest level.\n\t// INVARIANT: prevLevel == numLevels-1\n\tscores[prevLevel].compensatedScoreRatio = scores[prevLevel].compensatedScore\n\tscores[prevLevel].uncompensatedScoreRatio = scores[prevLevel].uncompensatedScore\n\n\tsort.Sort(sortCompactionLevelsByPriority(scores[:]))\n\treturn scores\n}\n\n// calculateL0UncompensatedScore calculates a float score representing the\n// relative priority of compacting L0. Level L0 is special in that files within\n// L0 may overlap one another, so a different set of heuristics that take into\n// account read amplification apply.\nfunc calculateL0UncompensatedScore(\n\tvers *version, opts *Options, inProgressCompactions []compactionInfo,\n) float64 {\n\t// Use the sublevel count to calculate the score. The base vs intra-L0\n\t// compaction determination happens in pickAuto, not here.\n\tscore := float64(2*vers.L0Sublevels.MaxDepthAfterOngoingCompactions()) /\n\t\tfloat64(opts.L0CompactionThreshold)\n\n\t// Also calculate a score based on the file count but use it only if it\n\t// produces a higher score than the sublevel-based one. This heuristic is\n\t// designed to accommodate cases where L0 is accumulating non-overlapping\n\t// files in L0. Letting too many non-overlapping files accumulate in few\n\t// sublevels is undesirable, because:\n\t// 1) we can produce a massive backlog to compact once files do overlap.\n\t// 2) constructing L0 sublevels has a runtime that grows superlinearly with\n\t//    the number of files in L0 and must be done while holding D.mu.\n\tnoncompactingFiles := vers.Levels[0].Len()\n\tfor _, c := range inProgressCompactions {\n\t\tfor _, cl := range c.inputs {\n\t\t\tif cl.level == 0 {\n\t\t\t\tnoncompactingFiles -= cl.files.Len()\n\t\t\t}\n\t\t}\n\t}\n\tfileScore := float64(noncompactingFiles) / float64(opts.L0CompactionFileThreshold)\n\tif score < fileScore {\n\t\tscore = fileScore\n\t}\n\treturn score\n}\n\n// pickCompactionSeedFile picks a file from `level` in the `vers` to build a\n// compaction around. Currently, this function implements a heuristic similar to\n// RocksDB's kMinOverlappingRatio, seeking to minimize write amplification. This\n// function is linear with respect to the number of files in `level` and\n// `outputLevel`.\nfunc pickCompactionSeedFile(\n\tvers *version,\n\tvirtualBackings *manifest.VirtualBackings,\n\topts *Options,\n\tlevel, outputLevel int,\n\tearliestSnapshotSeqNum base.SeqNum,\n) (manifest.LevelFile, bool) {\n\t// Select the file within the level to compact. We want to minimize write\n\t// amplification, but also ensure that (a) deletes are propagated to the\n\t// bottom level in a timely fashion, and (b) virtual sstables that are\n\t// pinning backing sstables where most of the data is garbage are compacted\n\t// away. Doing (a) and (b) reclaims disk space. A table's smallest sequence\n\t// number provides a measure of its age. The ratio of overlapping-bytes /\n\t// table-size gives an indication of write amplification (a smaller ratio is\n\t// preferrable).\n\t//\n\t// The current heuristic is based off the RocksDB kMinOverlappingRatio\n\t// heuristic. It chooses the file with the minimum overlapping ratio with\n\t// the target level, which minimizes write amplification.\n\t//\n\t// The heuristic uses a \"compensated size\" for the denominator, which is the\n\t// file size inflated by (a) an estimate of the space that may be reclaimed\n\t// through compaction, and (b) a fraction of the amount of garbage in the\n\t// backing sstable pinned by this (virtual) sstable.\n\t//\n\t// TODO(peter): For concurrent compactions, we may want to try harder to\n\t// pick a seed file whose resulting compaction bounds do not overlap with\n\t// an in-progress compaction.\n\n\tcmp := opts.Comparer.Compare\n\tstartIter := vers.Levels[level].Iter()\n\toutputIter := vers.Levels[outputLevel].Iter()\n\n\tvar file manifest.LevelFile\n\tsmallestRatio := uint64(math.MaxUint64)\n\n\toutputFile := outputIter.First()\n\n\tfor f := startIter.First(); f != nil; f = startIter.Next() {\n\t\tvar overlappingBytes uint64\n\t\tcompacting := f.IsCompacting()\n\t\tif compacting {\n\t\t\t// Move on if this file is already being compacted. We'll likely\n\t\t\t// still need to move past the overlapping output files regardless,\n\t\t\t// but in cases where all start-level files are compacting we won't.\n\t\t\tcontinue\n\t\t}\n\n\t\t// Trim any output-level files smaller than f.\n\t\tfor outputFile != nil && sstableKeyCompare(cmp, outputFile.Largest, f.Smallest) < 0 {\n\t\t\toutputFile = outputIter.Next()\n\t\t}\n\n\t\tfor outputFile != nil && sstableKeyCompare(cmp, outputFile.Smallest, f.Largest) <= 0 && !compacting {\n\t\t\toverlappingBytes += outputFile.Size\n\t\t\tcompacting = compacting || outputFile.IsCompacting()\n\n\t\t\t// For files in the bottommost level of the LSM, the\n\t\t\t// Stats.RangeDeletionsBytesEstimate field is set to the estimate\n\t\t\t// of bytes /within/ the file itself that may be dropped by\n\t\t\t// recompacting the file. These bytes from obsolete keys would not\n\t\t\t// need to be rewritten if we compacted `f` into `outputFile`, so\n\t\t\t// they don't contribute to write amplification. Subtracting them\n\t\t\t// out of the overlapping bytes helps prioritize these compactions\n\t\t\t// that are cheaper than their file sizes suggest.\n\t\t\tif outputLevel == numLevels-1 && outputFile.LargestSeqNum < earliestSnapshotSeqNum {\n\t\t\t\toverlappingBytes -= outputFile.Stats.RangeDeletionsBytesEstimate\n\t\t\t}\n\n\t\t\t// If the file in the next level extends beyond f's largest key,\n\t\t\t// break out and don't advance outputIter because f's successor\n\t\t\t// might also overlap.\n\t\t\t//\n\t\t\t// Note, we stop as soon as we encounter an output-level file with a\n\t\t\t// largest key beyond the input-level file's largest bound. We\n\t\t\t// perform a simple user key comparison here using sstableKeyCompare\n\t\t\t// which handles the potential for exclusive largest key bounds.\n\t\t\t// There's some subtlety when the bounds are equal (eg, equal and\n\t\t\t// inclusive, or equal and exclusive). Current Pebble doesn't split\n\t\t\t// user keys across sstables within a level (and in format versions\n\t\t\t// FormatSplitUserKeysMarkedCompacted and later we guarantee no\n\t\t\t// split user keys exist within the entire LSM). In that case, we're\n\t\t\t// assured that neither the input level nor the output level's next\n\t\t\t// file shares the same user key, so compaction expansion will not\n\t\t\t// include them in any compaction compacting `f`.\n\t\t\t//\n\t\t\t// NB: If we /did/ allow split user keys, or we're running on an\n\t\t\t// old database with an earlier format major version where there are\n\t\t\t// existing split user keys, this logic would be incorrect. Consider\n\t\t\t//    L1: [a#120,a#100] [a#80,a#60]\n\t\t\t//    L2: [a#55,a#45] [a#35,a#25] [a#15,a#5]\n\t\t\t// While considering the first file in L1, [a#120,a#100], we'd skip\n\t\t\t// past all of the files in L2. When considering the second file in\n\t\t\t// L1, we'd improperly conclude that the second file overlaps\n\t\t\t// nothing in the second level and is cheap to compact, when in\n\t\t\t// reality we'd need to expand the compaction to include all 5\n\t\t\t// files.\n\t\t\tif sstableKeyCompare(cmp, outputFile.Largest, f.Largest) > 0 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\toutputFile = outputIter.Next()\n\t\t}\n\n\t\t// If the input level file or one of the overlapping files is\n\t\t// compacting, we're not going to be able to compact this file\n\t\t// anyways, so skip it.\n\t\tif compacting {\n\t\t\tcontinue\n\t\t}\n\n\t\tcompSz := compensatedSize(f) + responsibleForGarbageBytes(virtualBackings, f)\n\t\tscaledRatio := overlappingBytes * 1024 / compSz\n\t\tif scaledRatio < smallestRatio {\n\t\t\tsmallestRatio = scaledRatio\n\t\t\tfile = startIter.Take()\n\t\t}\n\t}\n\treturn file, file.FileMetadata != nil\n}\n\n// responsibleForGarbageBytes returns the amount of garbage in the backing\n// sstable that we consider the responsibility of this virtual sstable. For\n// non-virtual sstables, this is of course 0. For virtual sstables, we equally\n// distribute the responsibility of the garbage across all the virtual\n// sstables that are referencing the same backing sstable. One could\n// alternatively distribute this in proportion to the virtual sst sizes, but\n// it isn't clear that more sophisticated heuristics are worth it, given that\n// the garbage cannot be reclaimed until all the referencing virtual sstables\n// are compacted.\nfunc responsibleForGarbageBytes(virtualBackings *manifest.VirtualBackings, m *fileMetadata) uint64 {\n\tif !m.Virtual {\n\t\treturn 0\n\t}\n\tuseCount, virtualizedSize := virtualBackings.Usage(m.FileBacking.DiskFileNum)\n\t// Since virtualizedSize is the sum of the estimated size of all virtual\n\t// ssts, we allow for the possibility that virtualizedSize could exceed\n\t// m.FileBacking.Size.\n\ttotalGarbage := int64(m.FileBacking.Size) - int64(virtualizedSize)\n\tif totalGarbage <= 0 {\n\t\treturn 0\n\t}\n\tif useCount == 0 {\n\t\t// This cannot happen if m exists in the latest version. The call to\n\t\t// ResponsibleForGarbageBytes during compaction picking ensures that m\n\t\t// exists in the latest version by holding versionSet.logLock.\n\t\tpanic(errors.AssertionFailedf(\"%s has zero useCount\", m.String()))\n\t}\n\treturn uint64(totalGarbage) / uint64(useCount)\n}\n\n// pickAuto picks the best compaction, if any.\n//\n// On each call, pickAuto computes per-level size adjustments based on\n// in-progress compactions, and computes a per-level score. The levels are\n// iterated over in decreasing score order trying to find a valid compaction\n// anchored at that level.\n//\n// If a score-based compaction cannot be found, pickAuto falls back to looking\n// for an elision-only compaction to remove obsolete keys.\nfunc (p *compactionPickerByScore) pickAuto(env compactionEnv) (pc *pickedCompaction) {\n\t// Compaction concurrency is controlled by L0 read-amp. We allow one\n\t// additional compaction per L0CompactionConcurrency sublevels, as well as\n\t// one additional compaction per CompactionDebtConcurrency bytes of\n\t// compaction debt. Compaction concurrency is tied to L0 sublevels as that\n\t// signal is independent of the database size. We tack on the compaction\n\t// debt as a second signal to prevent compaction concurrency from dropping\n\t// significantly right after a base compaction finishes, and before those\n\t// bytes have been compacted further down the LSM.\n\tif n := len(env.inProgressCompactions); n > 0 {\n\t\tl0ReadAmp := p.vers.L0Sublevels.MaxDepthAfterOngoingCompactions()\n\t\tcompactionDebt := p.estimatedCompactionDebt(0)\n\t\tccSignal1 := n * p.opts.Experimental.L0CompactionConcurrency\n\t\tccSignal2 := uint64(n) * p.opts.Experimental.CompactionDebtConcurrency\n\t\tif l0ReadAmp < ccSignal1 && compactionDebt < ccSignal2 {\n\t\t\treturn nil\n\t\t}\n\t}\n\n\tscores := p.calculateLevelScores(env.inProgressCompactions)\n\n\t// TODO(bananabrick): Either remove, or change this into an event sent to the\n\t// EventListener.\n\tlogCompaction := func(pc *pickedCompaction) {\n\t\tvar buf bytes.Buffer\n\t\tfor i := 0; i < numLevels; i++ {\n\t\t\tif i != 0 && i < p.baseLevel {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tvar info *candidateLevelInfo\n\t\t\tfor j := range scores {\n\t\t\t\tif scores[j].level == i {\n\t\t\t\t\tinfo = &scores[j]\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tmarker := \" \"\n\t\t\tif pc.startLevel.level == info.level {\n\t\t\t\tmarker = \"*\"\n\t\t\t}\n\t\t\tfmt.Fprintf(&buf, \"  %sL%d: %5.1f  %5.1f  %5.1f  %5.1f %8s  %8s\",\n\t\t\t\tmarker, info.level, info.compensatedScoreRatio, info.compensatedScore,\n\t\t\t\tinfo.uncompensatedScoreRatio, info.uncompensatedScore,\n\t\t\t\thumanize.Bytes.Int64(int64(totalCompensatedSize(\n\t\t\t\t\tp.vers.Levels[info.level].Iter(),\n\t\t\t\t))),\n\t\t\t\thumanize.Bytes.Int64(p.levelMaxBytes[info.level]),\n\t\t\t)\n\n\t\t\tcount := 0\n\t\t\tfor i := range env.inProgressCompactions {\n\t\t\t\tc := &env.inProgressCompactions[i]\n\t\t\t\tif c.inputs[0].level != info.level {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tcount++\n\t\t\t\tif count == 1 {\n\t\t\t\t\tfmt.Fprintf(&buf, \"  [\")\n\t\t\t\t} else {\n\t\t\t\t\tfmt.Fprintf(&buf, \" \")\n\t\t\t\t}\n\t\t\t\tfmt.Fprintf(&buf, \"L%d->L%d\", c.inputs[0].level, c.outputLevel)\n\t\t\t}\n\t\t\tif count > 0 {\n\t\t\t\tfmt.Fprintf(&buf, \"]\")\n\t\t\t}\n\t\t\tfmt.Fprintf(&buf, \"\\n\")\n\t\t}\n\t\tp.opts.Logger.Infof(\"pickAuto: L%d->L%d\\n%s\",\n\t\t\tpc.startLevel.level, pc.outputLevel.level, buf.String())\n\t}\n\n\t// Check for a score-based compaction. candidateLevelInfos are first sorted\n\t// by whether they should be compacted, so if we find a level which shouldn't\n\t// be compacted, we can break early.\n\tfor i := range scores {\n\t\tinfo := &scores[i]\n\t\tif !info.shouldCompact() {\n\t\t\tbreak\n\t\t}\n\t\tif info.level == numLevels-1 {\n\t\t\tcontinue\n\t\t}\n\n\t\tif info.level == 0 {\n\t\t\tpc = pickL0(env, p.opts, p.vers, p.baseLevel)\n\t\t\t// Fail-safe to protect against compacting the same sstable\n\t\t\t// concurrently.\n\t\t\tif pc != nil && !inputRangeAlreadyCompacting(env, pc) {\n\t\t\t\tp.addScoresToPickedCompactionMetrics(pc, scores)\n\t\t\t\tpc.score = info.compensatedScoreRatio\n\t\t\t\t// TODO(bananabrick): Create an EventListener for logCompaction.\n\t\t\t\tif false {\n\t\t\t\t\tlogCompaction(pc)\n\t\t\t\t}\n\t\t\t\treturn pc\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\t// info.level > 0\n\t\tvar ok bool\n\t\tinfo.file, ok = pickCompactionSeedFile(p.vers, p.virtualBackings, p.opts, info.level, info.outputLevel, env.earliestSnapshotSeqNum)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tpc := pickAutoLPositive(env, p.opts, p.vers, *info, p.baseLevel)\n\t\t// Fail-safe to protect against compacting the same sstable concurrently.\n\t\tif pc != nil && !inputRangeAlreadyCompacting(env, pc) {\n\t\t\tp.addScoresToPickedCompactionMetrics(pc, scores)\n\t\t\tpc.score = info.compensatedScoreRatio\n\t\t\t// TODO(bananabrick): Create an EventListener for logCompaction.\n\t\t\tif false {\n\t\t\t\tlogCompaction(pc)\n\t\t\t}\n\t\t\treturn pc\n\t\t}\n\t}\n\n\t// Check for files which contain excessive point tombstones that could slow\n\t// down reads. Unlike elision-only compactions, these compactions may select\n\t// a file at any level rather than only the lowest level.\n\tif pc := p.pickTombstoneDensityCompaction(env); pc != nil {\n\t\treturn pc\n\t}\n\n\t// Check for L6 files with tombstones that may be elided. These files may\n\t// exist if a snapshot prevented the elision of a tombstone or because of\n\t// a move compaction. These are low-priority compactions because they\n\t// don't help us keep up with writes, just reclaim disk space.\n\tif pc := p.pickElisionOnlyCompaction(env); pc != nil {\n\t\treturn pc\n\t}\n\n\tif pc := p.pickReadTriggeredCompaction(env); pc != nil {\n\t\treturn pc\n\t}\n\n\t// NB: This should only be run if a read compaction wasn't\n\t// scheduled.\n\t//\n\t// We won't be scheduling a read compaction right now, and in\n\t// read heavy workloads, compactions won't be scheduled frequently\n\t// because flushes aren't frequent. So we need to signal to the\n\t// iterator to schedule a compaction when it adds compactions to\n\t// the read compaction queue.\n\t//\n\t// We need the nil check here because without it, we have some\n\t// tests which don't set that variable fail. Since there's a\n\t// chance that one of those tests wouldn't want extra compactions\n\t// to be scheduled, I added this check here, instead of\n\t// setting rescheduleReadCompaction in those tests.\n\tif env.readCompactionEnv.rescheduleReadCompaction != nil {\n\t\t*env.readCompactionEnv.rescheduleReadCompaction = true\n\t}\n\n\t// At the lowest possible compaction-picking priority, look for files marked\n\t// for compaction. Pebble will mark files for compaction if they have atomic\n\t// compaction units that span multiple files. While current Pebble code does\n\t// not construct such sstables, RocksDB and earlier versions of Pebble may\n\t// have created them. These split user keys form sets of files that must be\n\t// compacted together for correctness (referred to as \"atomic compaction\n\t// units\" within the code). Rewrite them in-place.\n\t//\n\t// It's also possible that a file may have been marked for compaction by\n\t// even earlier versions of Pebble code, since FileMetadata's\n\t// MarkedForCompaction field is persisted in the manifest. That's okay. We\n\t// previously would've ignored the designation, whereas now we'll re-compact\n\t// the file in place.\n\tif p.vers.Stats.MarkedForCompaction > 0 {\n\t\tif pc := p.pickRewriteCompaction(env); pc != nil {\n\t\t\treturn pc\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (p *compactionPickerByScore) addScoresToPickedCompactionMetrics(\n\tpc *pickedCompaction, candInfo [numLevels]candidateLevelInfo,\n) {\n\n\t// candInfo is sorted by score, not by compaction level.\n\tinfoByLevel := [numLevels]candidateLevelInfo{}\n\tfor i := range candInfo {\n\t\tlevel := candInfo[i].level\n\t\tinfoByLevel[level] = candInfo[i]\n\t}\n\t// Gather the compaction scores for the levels participating in the compaction.\n\tpc.pickerMetrics.scores = make([]float64, len(pc.inputs))\n\tinputIdx := 0\n\tfor i := range infoByLevel {\n\t\tif pc.inputs[inputIdx].level == infoByLevel[i].level {\n\t\t\tpc.pickerMetrics.scores[inputIdx] = infoByLevel[i].compensatedScoreRatio\n\t\t\tinputIdx++\n\t\t}\n\t\tif inputIdx == len(pc.inputs) {\n\t\t\tbreak\n\t\t}\n\t}\n}\n\n// elisionOnlyAnnotator is a manifest.Annotator that annotates B-Tree\n// nodes with the *fileMetadata of a file meeting the obsolete keys criteria\n// for an elision-only compaction within the subtree. If multiple files meet\n// the criteria, it chooses whichever file has the lowest LargestSeqNum. The\n// lowest LargestSeqNum file will be the first eligible for an elision-only\n// compaction once snapshots less than or equal to its LargestSeqNum are closed.\nvar elisionOnlyAnnotator = &manifest.Annotator[fileMetadata]{\n\tAggregator: manifest.PickFileAggregator{\n\t\tFilter: func(f *fileMetadata) (eligible bool, cacheOK bool) {\n\t\t\tif f.IsCompacting() {\n\t\t\t\treturn false, true\n\t\t\t}\n\t\t\tif !f.StatsValid() {\n\t\t\t\treturn false, false\n\t\t\t}\n\t\t\t// Bottommost files are large and not worthwhile to compact just\n\t\t\t// to remove a few tombstones. Consider a file eligible only if\n\t\t\t// either its own range deletions delete at least 10% of its data or\n\t\t\t// its deletion tombstones make at least 10% of its entries.\n\t\t\t//\n\t\t\t// TODO(jackson): This does not account for duplicate user keys\n\t\t\t// which may be collapsed. Ideally, we would have 'obsolete keys'\n\t\t\t// statistics that would include tombstones, the keys that are\n\t\t\t// dropped by tombstones and duplicated user keys. See #847.\n\t\t\t//\n\t\t\t// Note that tables that contain exclusively range keys (i.e. no point keys,\n\t\t\t// `NumEntries` and `RangeDeletionsBytesEstimate` are both zero) are excluded\n\t\t\t// from elision-only compactions.\n\t\t\t// TODO(travers): Consider an alternative heuristic for elision of range-keys.\n\t\t\treturn f.Stats.RangeDeletionsBytesEstimate*10 >= f.Size || f.Stats.NumDeletions*10 > f.Stats.NumEntries, true\n\t\t},\n\t\tCompare: func(f1 *fileMetadata, f2 *fileMetadata) bool {\n\t\t\treturn f1.LargestSeqNum < f2.LargestSeqNum\n\t\t},\n\t},\n}\n\n// markedForCompactionAnnotator is a manifest.Annotator that annotates B-Tree\n// nodes with the *fileMetadata of a file that is marked for compaction\n// within the subtree. If multiple files meet the criteria, it chooses\n// whichever file has the lowest LargestSeqNum.\nvar markedForCompactionAnnotator = &manifest.Annotator[fileMetadata]{\n\tAggregator: manifest.PickFileAggregator{\n\t\tFilter: func(f *fileMetadata) (eligible bool, cacheOK bool) {\n\t\t\treturn f.MarkedForCompaction, true\n\t\t},\n\t\tCompare: func(f1 *fileMetadata, f2 *fileMetadata) bool {\n\t\t\treturn f1.LargestSeqNum < f2.LargestSeqNum\n\t\t},\n\t},\n}\n\n// pickedCompactionFromCandidateFile creates a pickedCompaction from a *fileMetadata\n// with various checks to ensure that the file still exists in the expected level\n// and isn't already being compacted.\nfunc (p *compactionPickerByScore) pickedCompactionFromCandidateFile(\n\tcandidate *fileMetadata, env compactionEnv, startLevel int, outputLevel int, kind compactionKind,\n) *pickedCompaction {\n\tif candidate == nil || candidate.IsCompacting() {\n\t\treturn nil\n\t}\n\n\tvar inputs manifest.LevelSlice\n\tif startLevel == 0 {\n\t\t// Overlapping L0 files must also be compacted alongside the candidate.\n\t\tinputs = p.vers.Overlaps(0, candidate.UserKeyBounds())\n\t} else {\n\t\tinputs = p.vers.Levels[startLevel].Find(p.opts.Comparer.Compare, candidate)\n\t}\n\tif invariants.Enabled {\n\t\tfound := false\n\t\tinputs.Each(func(f *fileMetadata) {\n\t\t\tif f.FileNum == candidate.FileNum {\n\t\t\t\tfound = true\n\t\t\t}\n\t\t})\n\t\tif !found {\n\t\t\tpanic(fmt.Sprintf(\"file %s not found in level %d as expected\", candidate.FileNum, startLevel))\n\t\t}\n\t}\n\n\tpc := newPickedCompaction(p.opts, p.vers, startLevel, outputLevel, p.baseLevel)\n\tpc.kind = kind\n\tpc.startLevel.files = inputs\n\tpc.smallest, pc.largest = manifest.KeyRange(pc.cmp, pc.startLevel.files.Iter())\n\n\t// Fail-safe to protect against compacting the same sstable concurrently.\n\tif inputRangeAlreadyCompacting(env, pc) {\n\t\treturn nil\n\t}\n\n\tif !pc.setupInputs(p.opts, env.diskAvailBytes, pc.startLevel) {\n\t\treturn nil\n\t}\n\n\treturn pc\n}\n\n// pickElisionOnlyCompaction looks for compactions of sstables in the\n// bottommost level containing obsolete records that may now be dropped.\nfunc (p *compactionPickerByScore) pickElisionOnlyCompaction(\n\tenv compactionEnv,\n) (pc *pickedCompaction) {\n\tif p.opts.private.disableElisionOnlyCompactions {\n\t\treturn nil\n\t}\n\tcandidate := elisionOnlyAnnotator.LevelAnnotation(p.vers.Levels[numLevels-1])\n\tif candidate == nil {\n\t\treturn nil\n\t}\n\tif candidate.LargestSeqNum >= env.earliestSnapshotSeqNum {\n\t\treturn nil\n\t}\n\treturn p.pickedCompactionFromCandidateFile(candidate, env, numLevels-1, numLevels-1, compactionKindElisionOnly)\n}\n\n// pickRewriteCompaction attempts to construct a compaction that\n// rewrites a file marked for compaction. pickRewriteCompaction will\n// pull in adjacent files in the file's atomic compaction unit if\n// necessary. A rewrite compaction outputs files to the same level as\n// the input level.\nfunc (p *compactionPickerByScore) pickRewriteCompaction(env compactionEnv) (pc *pickedCompaction) {\n\tfor l := numLevels - 1; l >= 0; l-- {\n\t\tcandidate := markedForCompactionAnnotator.LevelAnnotation(p.vers.Levels[l])\n\t\tif candidate == nil {\n\t\t\t// Try the next level.\n\t\t\tcontinue\n\t\t}\n\t\tpc := p.pickedCompactionFromCandidateFile(candidate, env, l, l, compactionKindRewrite)\n\t\tif pc != nil {\n\t\t\treturn pc\n\t\t}\n\t}\n\treturn nil\n}\n\n// pickTombstoneDensityCompaction looks for a compaction that eliminates\n// regions of extremely high point tombstone density. For each level, it picks\n// a file where the ratio of tombstone-dense blocks is at least\n// options.Experimental.MinTombstoneDenseRatio, prioritizing compaction of\n// files with higher ratios of tombstone-dense blocks.\nfunc (p *compactionPickerByScore) pickTombstoneDensityCompaction(\n\tenv compactionEnv,\n) (pc *pickedCompaction) {\n\tif p.opts.Experimental.TombstoneDenseCompactionThreshold <= 0 {\n\t\t// Tombstone density compactions are disabled.\n\t\treturn nil\n\t}\n\n\tvar candidate *fileMetadata\n\tvar level int\n\t// If a candidate file has a very high overlapping ratio, point tombstones\n\t// in it are likely sparse in keyspace even if the sstable itself is tombstone\n\t// dense. These tombstones likely wouldn't be slow to iterate over, so we exclude\n\t// these files from tombstone density compactions. The threshold of 40.0 is\n\t// chosen somewhat arbitrarily, after some observations around excessively large\n\t// tombstone density compactions.\n\tconst maxOverlappingRatio = 40.0\n\t// NB: we don't consider the lowest level because elision-only compactions\n\t// handle that case.\n\tlastNonEmptyLevel := numLevels - 1\n\tfor l := numLevels - 2; l >= 0; l-- {\n\t\titer := p.vers.Levels[l].Iter()\n\t\tfor f := iter.First(); f != nil; f = iter.Next() {\n\t\t\tif f.IsCompacting() || !f.StatsValid() || f.Size == 0 {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif f.Stats.TombstoneDenseBlocksRatio < p.opts.Experimental.TombstoneDenseCompactionThreshold {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\toverlaps := p.vers.Overlaps(lastNonEmptyLevel, f.UserKeyBounds())\n\t\t\tif float64(overlaps.SizeSum())/float64(f.Size) > maxOverlappingRatio {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif candidate == nil || candidate.Stats.TombstoneDenseBlocksRatio < f.Stats.TombstoneDenseBlocksRatio {\n\t\t\t\tcandidate = f\n\t\t\t\tlevel = l\n\t\t\t}\n\t\t}\n\t\t// We prefer lower level (ie. L5) candidates over higher level (ie. L4) ones.\n\t\tif candidate != nil {\n\t\t\tbreak\n\t\t}\n\t\tif !p.vers.Levels[l].Empty() {\n\t\t\tlastNonEmptyLevel = l\n\t\t}\n\t}\n\n\treturn p.pickedCompactionFromCandidateFile(candidate, env, level, defaultOutputLevel(level, p.baseLevel), compactionKindTombstoneDensity)\n}\n\n// pickAutoLPositive picks an automatic compaction for the candidate\n// file in a positive-numbered level. This function must not be used for\n// L0.\nfunc pickAutoLPositive(\n\tenv compactionEnv, opts *Options, vers *version, cInfo candidateLevelInfo, baseLevel int,\n) (pc *pickedCompaction) {\n\tif cInfo.level == 0 {\n\t\tpanic(\"pebble: pickAutoLPositive called for L0\")\n\t}\n\n\tpc = newPickedCompaction(opts, vers, cInfo.level, defaultOutputLevel(cInfo.level, baseLevel), baseLevel)\n\tif pc.outputLevel.level != cInfo.outputLevel {\n\t\tpanic(\"pebble: compaction picked unexpected output level\")\n\t}\n\tpc.startLevel.files = cInfo.file.Slice()\n\t// Files in level 0 may overlap each other, so pick up all overlapping ones.\n\tif pc.startLevel.level == 0 {\n\t\tcmp := opts.Comparer.Compare\n\t\tsmallest, largest := manifest.KeyRange(cmp, pc.startLevel.files.Iter())\n\t\tpc.startLevel.files = vers.Overlaps(0, base.UserKeyBoundsFromInternal(smallest, largest))\n\t\tif pc.startLevel.files.Empty() {\n\t\t\tpanic(\"pebble: empty compaction\")\n\t\t}\n\t}\n\n\tif !pc.setupInputs(opts, env.diskAvailBytes, pc.startLevel) {\n\t\treturn nil\n\t}\n\treturn pc.maybeAddLevel(opts, env.diskAvailBytes)\n}\n\n// maybeAddLevel maybe adds a level to the picked compaction.\nfunc (pc *pickedCompaction) maybeAddLevel(opts *Options, diskAvailBytes uint64) *pickedCompaction {\n\tpc.pickerMetrics.singleLevelOverlappingRatio = pc.overlappingRatio()\n\tif pc.outputLevel.level == numLevels-1 {\n\t\t// Don't add a level if the current output level is in L6\n\t\treturn pc\n\t}\n\tif !opts.Experimental.MultiLevelCompactionHeuristic.allowL0() && pc.startLevel.level == 0 {\n\t\treturn pc\n\t}\n\tif pc.compactionSize() > expandedCompactionByteSizeLimit(\n\t\topts, adjustedOutputLevel(pc.outputLevel.level, pc.baseLevel), diskAvailBytes) {\n\t\t// Don't add a level if the current compaction exceeds the compaction size limit\n\t\treturn pc\n\t}\n\treturn opts.Experimental.MultiLevelCompactionHeuristic.pick(pc, opts, diskAvailBytes)\n}\n\n// MultiLevelHeuristic evaluates whether to add files from the next level into the compaction.\ntype MultiLevelHeuristic interface {\n\t// Evaluate returns the preferred compaction.\n\tpick(pc *pickedCompaction, opts *Options, diskAvailBytes uint64) *pickedCompaction\n\n\t// Returns if the heuristic allows L0 to be involved in ML compaction\n\tallowL0() bool\n\n\t// String implements fmt.Stringer.\n\tString() string\n}\n\n// NoMultiLevel will never add an additional level to the compaction.\ntype NoMultiLevel struct{}\n\nvar _ MultiLevelHeuristic = (*NoMultiLevel)(nil)\n\nfunc (nml NoMultiLevel) pick(\n\tpc *pickedCompaction, opts *Options, diskAvailBytes uint64,\n) *pickedCompaction {\n\treturn pc\n}\n\nfunc (nml NoMultiLevel) allowL0() bool  { return false }\nfunc (nml NoMultiLevel) String() string { return \"none\" }\n\nfunc (pc *pickedCompaction) predictedWriteAmp() float64 {\n\tvar bytesToCompact uint64\n\tvar higherLevelBytes uint64\n\tfor i := range pc.inputs {\n\t\tlevelSize := pc.inputs[i].files.SizeSum()\n\t\tbytesToCompact += levelSize\n\t\tif i != len(pc.inputs)-1 {\n\t\t\thigherLevelBytes += levelSize\n\t\t}\n\t}\n\treturn float64(bytesToCompact) / float64(higherLevelBytes)\n}\n\nfunc (pc *pickedCompaction) overlappingRatio() float64 {\n\tvar higherLevelBytes uint64\n\tvar lowestLevelBytes uint64\n\tfor i := range pc.inputs {\n\t\tlevelSize := pc.inputs[i].files.SizeSum()\n\t\tif i == len(pc.inputs)-1 {\n\t\t\tlowestLevelBytes += levelSize\n\t\t\tcontinue\n\t\t}\n\t\thigherLevelBytes += levelSize\n\t}\n\treturn float64(lowestLevelBytes) / float64(higherLevelBytes)\n}\n\n// WriteAmpHeuristic defines a multi level compaction heuristic which will add\n// an additional level to the picked compaction if it reduces predicted write\n// amp of the compaction + the addPropensity constant.\ntype WriteAmpHeuristic struct {\n\t// addPropensity is a constant that affects the propensity to conduct multilevel\n\t// compactions. If positive, a multilevel compaction may get picked even if\n\t// the single level compaction has lower write amp, and vice versa.\n\tAddPropensity float64\n\n\t// AllowL0 if true, allow l0 to be involved in a ML compaction.\n\tAllowL0 bool\n}\n\nvar _ MultiLevelHeuristic = (*WriteAmpHeuristic)(nil)\n\n// TODO(msbutler): microbenchmark the extent to which multilevel compaction\n// picking slows down the compaction picking process.  This should be as fast as\n// possible since Compaction-picking holds d.mu, which prevents WAL rotations,\n// in-progress flushes and compactions from completing, etc. Consider ways to\n// deduplicate work, given that setupInputs has already been called.\nfunc (wa WriteAmpHeuristic) pick(\n\tpcOrig *pickedCompaction, opts *Options, diskAvailBytes uint64,\n) *pickedCompaction {\n\tpcMulti := pcOrig.clone()\n\tif !pcMulti.setupMultiLevelCandidate(opts, diskAvailBytes) {\n\t\treturn pcOrig\n\t}\n\t// We consider the addition of a level as an \"expansion\" of the compaction.\n\t// If pcMulti is past the expanded compaction byte size limit already,\n\t// we don't consider it.\n\tif pcMulti.compactionSize() >= expandedCompactionByteSizeLimit(\n\t\topts, adjustedOutputLevel(pcMulti.outputLevel.level, pcMulti.baseLevel), diskAvailBytes) {\n\t\treturn pcOrig\n\t}\n\tpicked := pcOrig\n\tif pcMulti.predictedWriteAmp() <= pcOrig.predictedWriteAmp()+wa.AddPropensity {\n\t\tpicked = pcMulti\n\t}\n\t// Regardless of what compaction was picked, log the multilevelOverlapping ratio.\n\tpicked.pickerMetrics.multiLevelOverlappingRatio = pcMulti.overlappingRatio()\n\treturn picked\n}\n\nfunc (wa WriteAmpHeuristic) allowL0() bool {\n\treturn wa.AllowL0\n}\n\n// String implements fmt.Stringer.\nfunc (wa WriteAmpHeuristic) String() string {\n\treturn fmt.Sprintf(\"wamp(%.2f, %t)\", wa.AddPropensity, wa.AllowL0)\n}\n\n// Helper method to pick compactions originating from L0. Uses information about\n// sublevels to generate a compaction.\nfunc pickL0(env compactionEnv, opts *Options, vers *version, baseLevel int) (pc *pickedCompaction) {\n\t// It is important to pass information about Lbase files to L0Sublevels\n\t// so it can pick a compaction that does not conflict with an Lbase => Lbase+1\n\t// compaction. Without this, we observed reduced concurrency of L0=>Lbase\n\t// compactions, and increasing read amplification in L0.\n\t//\n\t// TODO(bilal) Remove the minCompactionDepth parameter once fixing it at 1\n\t// has been shown to not cause a performance regression.\n\tlcf, err := vers.L0Sublevels.PickBaseCompaction(1, vers.Levels[baseLevel].Slice())\n\tif err != nil {\n\t\topts.Logger.Errorf(\"error when picking base compaction: %s\", err)\n\t\treturn\n\t}\n\tif lcf != nil {\n\t\tpc = newPickedCompactionFromL0(lcf, opts, vers, baseLevel, true)\n\t\tpc.setupInputs(opts, env.diskAvailBytes, pc.startLevel)\n\t\tif pc.startLevel.files.Empty() {\n\t\t\topts.Logger.Fatalf(\"empty compaction chosen\")\n\t\t}\n\t\treturn pc.maybeAddLevel(opts, env.diskAvailBytes)\n\t}\n\n\t// Couldn't choose a base compaction. Try choosing an intra-L0\n\t// compaction. Note that we pass in L0CompactionThreshold here as opposed to\n\t// 1, since choosing a single sublevel intra-L0 compaction is\n\t// counterproductive.\n\tlcf, err = vers.L0Sublevels.PickIntraL0Compaction(env.earliestUnflushedSeqNum, minIntraL0Count)\n\tif err != nil {\n\t\topts.Logger.Errorf(\"error when picking intra-L0 compaction: %s\", err)\n\t\treturn\n\t}\n\tif lcf != nil {\n\t\tpc = newPickedCompactionFromL0(lcf, opts, vers, 0, false)\n\t\tif !pc.setupInputs(opts, env.diskAvailBytes, pc.startLevel) {\n\t\t\treturn nil\n\t\t}\n\t\tif pc.startLevel.files.Empty() {\n\t\t\topts.Logger.Fatalf(\"empty compaction chosen\")\n\t\t}\n\t\t{\n\t\t\titer := pc.startLevel.files.Iter()\n\t\t\tif iter.First() == nil || iter.Next() == nil {\n\t\t\t\t// A single-file intra-L0 compaction is unproductive.\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\n\t\tpc.smallest, pc.largest = manifest.KeyRange(pc.cmp, pc.startLevel.files.Iter())\n\t}\n\treturn pc\n}\n\nfunc pickManualCompaction(\n\tvers *version, opts *Options, env compactionEnv, baseLevel int, manual *manualCompaction,\n) (pc *pickedCompaction, retryLater bool) {\n\toutputLevel := manual.level + 1\n\tif manual.level == 0 {\n\t\toutputLevel = baseLevel\n\t} else if manual.level < baseLevel {\n\t\t// The start level for a compaction must be >= Lbase. A manual\n\t\t// compaction could have been created adhering to that condition, and\n\t\t// then an automatic compaction came in and compacted all of the\n\t\t// sstables in Lbase to Lbase+1 which caused Lbase to change. Simply\n\t\t// ignore this manual compaction as there is nothing to do (manual.level\n\t\t// points to an empty level).\n\t\treturn nil, false\n\t}\n\t// This conflictsWithInProgress call is necessary for the manual compaction to\n\t// be retried when it conflicts with an ongoing automatic compaction. Without\n\t// it, the compaction is dropped due to pc.setupInputs returning false since\n\t// the input/output range is already being compacted, and the manual\n\t// compaction ends with a non-compacted LSM.\n\tif conflictsWithInProgress(manual, outputLevel, env.inProgressCompactions, opts.Comparer.Compare) {\n\t\treturn nil, true\n\t}\n\tpc = newPickedCompaction(opts, vers, manual.level, defaultOutputLevel(manual.level, baseLevel), baseLevel)\n\tmanual.outputLevel = pc.outputLevel.level\n\tpc.startLevel.files = vers.Overlaps(manual.level, base.UserKeyBoundsInclusive(manual.start, manual.end))\n\tif pc.startLevel.files.Empty() {\n\t\t// Nothing to do\n\t\treturn nil, false\n\t}\n\tif !pc.setupInputs(opts, env.diskAvailBytes, pc.startLevel) {\n\t\t// setupInputs returned false indicating there's a conflicting\n\t\t// concurrent compaction.\n\t\treturn nil, true\n\t}\n\tif pc = pc.maybeAddLevel(opts, env.diskAvailBytes); pc == nil {\n\t\treturn nil, false\n\t}\n\tif pc.outputLevel.level != outputLevel {\n\t\tif len(pc.extraLevels) > 0 {\n\t\t\t// multilevel compactions relax this invariant\n\t\t} else {\n\t\t\tpanic(\"pebble: compaction picked unexpected output level\")\n\t\t}\n\t}\n\t// Fail-safe to protect against compacting the same sstable concurrently.\n\tif inputRangeAlreadyCompacting(env, pc) {\n\t\treturn nil, true\n\t}\n\treturn pc, false\n}\n\n// pickDownloadCompaction picks a download compaction for the downloadSpan,\n// which could be specified as being performed either by a copy compaction of\n// the backing file or a rewrite compaction.\nfunc pickDownloadCompaction(\n\tvers *version,\n\topts *Options,\n\tenv compactionEnv,\n\tbaseLevel int,\n\tkind compactionKind,\n\tlevel int,\n\tfile *fileMetadata,\n) (pc *pickedCompaction) {\n\t// Check if the file is compacting already.\n\tif file.CompactionState == manifest.CompactionStateCompacting {\n\t\treturn nil\n\t}\n\tif kind != compactionKindCopy && kind != compactionKindRewrite {\n\t\tpanic(\"invalid download/rewrite compaction kind\")\n\t}\n\tpc = newPickedCompaction(opts, vers, level, level, baseLevel)\n\tpc.kind = kind\n\tpc.startLevel.files = manifest.NewLevelSliceKeySorted(opts.Comparer.Compare, []*fileMetadata{file})\n\tif !pc.setupInputs(opts, env.diskAvailBytes, pc.startLevel) {\n\t\t// setupInputs returned false indicating there's a conflicting\n\t\t// concurrent compaction.\n\t\treturn nil\n\t}\n\tif pc.outputLevel.level != level {\n\t\tpanic(\"pebble: download compaction picked unexpected output level\")\n\t}\n\t// Fail-safe to protect against compacting the same sstable concurrently.\n\tif inputRangeAlreadyCompacting(env, pc) {\n\t\treturn nil\n\t}\n\treturn pc\n}\n\nfunc (p *compactionPickerByScore) pickReadTriggeredCompaction(\n\tenv compactionEnv,\n) (pc *pickedCompaction) {\n\t// If a flush is in-progress or expected to happen soon, it means more writes are taking place. We would\n\t// soon be scheduling more write focussed compactions. In this case, skip read compactions as they are\n\t// lower priority.\n\tif env.readCompactionEnv.flushing || env.readCompactionEnv.readCompactions == nil {\n\t\treturn nil\n\t}\n\tfor env.readCompactionEnv.readCompactions.size > 0 {\n\t\trc := env.readCompactionEnv.readCompactions.remove()\n\t\tif pc = pickReadTriggeredCompactionHelper(p, rc, env); pc != nil {\n\t\t\tbreak\n\t\t}\n\t}\n\treturn pc\n}\n\nfunc pickReadTriggeredCompactionHelper(\n\tp *compactionPickerByScore, rc *readCompaction, env compactionEnv,\n) (pc *pickedCompaction) {\n\toverlapSlice := p.vers.Overlaps(rc.level, base.UserKeyBoundsInclusive(rc.start, rc.end))\n\tif overlapSlice.Empty() {\n\t\t// If there is no overlap, then the file with the key range\n\t\t// must have been compacted away. So, we don't proceed to\n\t\t// compact the same key range again.\n\t\treturn nil\n\t}\n\n\titer := overlapSlice.Iter()\n\tvar fileMatches bool\n\tfor f := iter.First(); f != nil; f = iter.Next() {\n\t\tif f.FileNum == rc.fileNum {\n\t\t\tfileMatches = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif !fileMatches {\n\t\treturn nil\n\t}\n\n\tpc = newPickedCompaction(p.opts, p.vers, rc.level, defaultOutputLevel(rc.level, p.baseLevel), p.baseLevel)\n\n\tpc.startLevel.files = overlapSlice\n\tif !pc.setupInputs(p.opts, env.diskAvailBytes, pc.startLevel) {\n\t\treturn nil\n\t}\n\tif inputRangeAlreadyCompacting(env, pc) {\n\t\treturn nil\n\t}\n\tpc.kind = compactionKindRead\n\n\t// Prevent read compactions which are too wide.\n\toutputOverlaps := pc.version.Overlaps(pc.outputLevel.level, pc.userKeyBounds())\n\tif outputOverlaps.SizeSum() > pc.maxReadCompactionBytes {\n\t\treturn nil\n\t}\n\n\t// Prevent compactions which start with a small seed file X, but overlap\n\t// with over allowedCompactionWidth * X file sizes in the output layer.\n\tconst allowedCompactionWidth = 35\n\tif outputOverlaps.SizeSum() > overlapSlice.SizeSum()*allowedCompactionWidth {\n\t\treturn nil\n\t}\n\n\treturn pc\n}\n\nfunc (p *compactionPickerByScore) forceBaseLevel1() {\n\tp.baseLevel = 1\n}\n\nfunc inputRangeAlreadyCompacting(env compactionEnv, pc *pickedCompaction) bool {\n\tfor _, cl := range pc.inputs {\n\t\titer := cl.files.Iter()\n\t\tfor f := iter.First(); f != nil; f = iter.Next() {\n\t\t\tif f.IsCompacting() {\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t}\n\n\t// Look for active compactions outputting to the same region of the key\n\t// space in the same output level. Two potential compactions may conflict\n\t// without sharing input files if there are no files in the output level\n\t// that overlap with the intersection of the compactions' key spaces.\n\t//\n\t// Consider an active L0->Lbase compaction compacting two L0 files one\n\t// [a-f] and the other [t-z] into Lbase.\n\t//\n\t// L0\n\t//     ↦ 000100  ↤                           ↦  000101   ↤\n\t// L1\n\t//     ↦ 000004  ↤\n\t//     a b c d e f g h i j k l m n o p q r s t u v w x y z\n\t//\n\t// If a new file 000102 [j-p] is flushed while the existing compaction is\n\t// still ongoing, new file would not be in any compacting sublevel\n\t// intervals and would not overlap with any Lbase files that are also\n\t// compacting. However, this compaction cannot be picked because the\n\t// compaction's output key space [j-p] would overlap the existing\n\t// compaction's output key space [a-z].\n\t//\n\t// L0\n\t//     ↦ 000100* ↤       ↦   000102  ↤       ↦  000101*  ↤\n\t// L1\n\t//     ↦ 000004* ↤\n\t//     a b c d e f g h i j k l m n o p q r s t u v w x y z\n\t//\n\t// * - currently compacting\n\tif pc.outputLevel != nil && pc.outputLevel.level != 0 {\n\t\tfor _, c := range env.inProgressCompactions {\n\t\t\tif pc.outputLevel.level != c.outputLevel {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif base.InternalCompare(pc.cmp, c.largest, pc.smallest) < 0 ||\n\t\t\t\tbase.InternalCompare(pc.cmp, c.smallest, pc.largest) > 0 {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// The picked compaction and the in-progress compaction c are\n\t\t\t// outputting to the same region of the key space of the same\n\t\t\t// level.\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// conflictsWithInProgress checks if there are any in-progress compactions with overlapping keyspace.\nfunc conflictsWithInProgress(\n\tmanual *manualCompaction, outputLevel int, inProgressCompactions []compactionInfo, cmp Compare,\n) bool {\n\tfor _, c := range inProgressCompactions {\n\t\tif (c.outputLevel == manual.level || c.outputLevel == outputLevel) &&\n\t\t\tisUserKeysOverlapping(manual.start, manual.end, c.smallest.UserKey, c.largest.UserKey, cmp) {\n\t\t\treturn true\n\t\t}\n\t\tfor _, in := range c.inputs {\n\t\t\tif in.files.Empty() {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\titer := in.files.Iter()\n\t\t\tsmallest := iter.First().Smallest.UserKey\n\t\t\tlargest := iter.Last().Largest.UserKey\n\t\t\tif (in.level == manual.level || in.level == outputLevel) &&\n\t\t\t\tisUserKeysOverlapping(manual.start, manual.end, smallest, largest, cmp) {\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t}\n\treturn false\n}\n\nfunc isUserKeysOverlapping(x1, x2, y1, y2 []byte, cmp Compare) bool {\n\treturn cmp(x1, y2) <= 0 && cmp(y1, x2) <= 0\n}\n"
        },
        {
          "name": "compaction_picker_test.go",
          "type": "blob",
          "size": 46.2353515625,
          "content": "// Copyright 2018 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"math\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/crlib/crstrings\"\n\t\"github.com/cockroachdb/datadriven\"\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/humanize\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n\t\"github.com/cockroachdb/pebble/internal/testkeys\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc loadVersion(t *testing.T, d *datadriven.TestData) (*version, *Options, string) {\n\tvar sizes [numLevels]int64\n\topts := &Options{}\n\topts.testingRandomized(t)\n\topts.EnsureDefaults()\n\n\tif len(d.CmdArgs) != 1 {\n\t\treturn nil, nil, fmt.Sprintf(\"%s expects 1 argument\", d.Cmd)\n\t}\n\tvar err error\n\topts.LBaseMaxBytes, err = strconv.ParseInt(d.CmdArgs[0].Key, 10, 64)\n\tif err != nil {\n\t\treturn nil, nil, err.Error()\n\t}\n\n\tvar files [numLevels][]*fileMetadata\n\tif len(d.Input) > 0 {\n\t\t// Parse each line as\n\t\t//\n\t\t// <level>: <size> [compensation]\n\t\t//\n\t\t// Creating sstables within the level whose file sizes total to `size`\n\t\t// and whose compensated file sizes total to `size`+`compensation`. If\n\t\t// size is sufficiently large, only one single file is created. See\n\t\t// the TODO below.\n\t\tfor _, data := range strings.Split(d.Input, \"\\n\") {\n\t\t\tparts := strings.Split(data, \" \")\n\t\t\tparts[0] = strings.TrimSuffix(strings.TrimSpace(parts[0]), \":\")\n\t\t\tif len(parts) < 2 {\n\t\t\t\treturn nil, nil, fmt.Sprintf(\"malformed test:\\n%s\", d.Input)\n\t\t\t}\n\t\t\tlevel, err := strconv.Atoi(parts[0])\n\t\t\tif err != nil {\n\t\t\t\treturn nil, nil, err.Error()\n\t\t\t}\n\t\t\tif files[level] != nil {\n\t\t\t\treturn nil, nil, fmt.Sprintf(\"level %d already filled\", level)\n\t\t\t}\n\t\t\tsize, err := strconv.ParseUint(strings.TrimSpace(parts[1]), 10, 64)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, nil, err.Error()\n\t\t\t}\n\t\t\tvar compensation uint64\n\t\t\tif len(parts) == 3 {\n\t\t\t\tcompensation, err = strconv.ParseUint(strings.TrimSpace(parts[2]), 10, 64)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, nil, err.Error()\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tvar lastFile *fileMetadata\n\t\t\tfor i := uint64(1); sizes[level] < int64(size); i++ {\n\t\t\t\tvar key InternalKey\n\t\t\t\tif level == 0 {\n\t\t\t\t\t// For L0, make `size` overlapping files.\n\t\t\t\t\tkey = base.MakeInternalKey([]byte(fmt.Sprintf(\"%04d\", 1)), base.SeqNum(i), InternalKeyKindSet)\n\t\t\t\t} else {\n\t\t\t\t\tkey = base.MakeInternalKey([]byte(fmt.Sprintf(\"%04d\", i)), base.SeqNum(i), InternalKeyKindSet)\n\t\t\t\t}\n\t\t\t\tm := (&fileMetadata{\n\t\t\t\t\tFileNum:               base.FileNum(uint64(level)*100_000 + i),\n\t\t\t\t\tSmallestSeqNum:        key.SeqNum(),\n\t\t\t\t\tLargestSeqNum:         key.SeqNum(),\n\t\t\t\t\tLargestSeqNumAbsolute: key.SeqNum(),\n\t\t\t\t\tSize:                  1,\n\t\t\t\t\tStats: manifest.TableStats{\n\t\t\t\t\t\tRangeDeletionsBytesEstimate: 0,\n\t\t\t\t\t},\n\t\t\t\t}).ExtendPointKeyBounds(opts.Comparer.Compare, key, key)\n\t\t\t\tm.InitPhysicalBacking()\n\t\t\t\tm.StatsMarkValid()\n\t\t\t\tlastFile = m\n\t\t\t\tif size >= 100 {\n\t\t\t\t\t// If the requested size of the level is very large only add a single\n\t\t\t\t\t// file in order to avoid massive blow-up in the number of files in\n\t\t\t\t\t// the Version.\n\t\t\t\t\t//\n\t\t\t\t\t// TODO(peter): There is tension between the testing in\n\t\t\t\t\t// TestCompactionPickerLevelMaxBytes and\n\t\t\t\t\t// TestCompactionPickerTargetLevel. Clean this up somehow.\n\t\t\t\t\tm.Size = size\n\t\t\t\t\tif level != 0 {\n\t\t\t\t\t\tendKey := base.MakeInternalKey([]byte(fmt.Sprintf(\"%04d\", size)), base.SeqNum(i), InternalKeyKindSet)\n\t\t\t\t\t\tm.ExtendPointKeyBounds(opts.Comparer.Compare, key, endKey)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfiles[level] = append(files[level], m)\n\t\t\t\tsizes[level] += int64(m.Size)\n\t\t\t}\n\t\t\t// Let all the compensation be due to the last file.\n\t\t\tif lastFile != nil && compensation > 0 {\n\t\t\t\tlastFile.Stats.RangeDeletionsBytesEstimate = compensation\n\t\t\t}\n\t\t}\n\t}\n\n\tvers := newVersion(opts, files)\n\treturn vers, opts, \"\"\n}\n\nfunc TestCompactionPickerByScoreLevelMaxBytes(t *testing.T) {\n\tdatadriven.RunTest(t, \"testdata/compaction_picker_level_max_bytes\",\n\t\tfunc(t *testing.T, d *datadriven.TestData) string {\n\t\t\tswitch d.Cmd {\n\t\t\tcase \"init\":\n\t\t\t\tvers, opts, errMsg := loadVersion(t, d)\n\t\t\t\tif errMsg != \"\" {\n\t\t\t\t\treturn errMsg\n\t\t\t\t}\n\n\t\t\t\tvb := manifest.MakeVirtualBackings()\n\t\t\t\tp := newCompactionPickerByScore(vers, &vb, opts, nil)\n\t\t\t\tvar buf bytes.Buffer\n\t\t\t\tfor level := p.getBaseLevel(); level < numLevels; level++ {\n\t\t\t\t\tfmt.Fprintf(&buf, \"%d: %d\\n\", level, p.levelMaxBytes[level])\n\t\t\t\t}\n\t\t\t\treturn buf.String()\n\n\t\t\tdefault:\n\t\t\t\treturn fmt.Sprintf(\"unknown command: %s\", d.Cmd)\n\t\t\t}\n\t\t})\n}\n\nfunc TestCompactionPickerTargetLevel(t *testing.T) {\n\tvar vers *version\n\tvar opts *Options\n\tvar pickerByScore *compactionPickerByScore\n\n\tparseInProgress := func(vals []string) ([]compactionInfo, error) {\n\t\tvar levels []int\n\t\tfor _, s := range vals {\n\t\t\tl, err := strconv.ParseInt(s, 10, 8)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tlevels = append(levels, int(l))\n\t\t}\n\t\tif len(levels)%2 != 0 {\n\t\t\treturn nil, errors.New(\"odd number of levels with ongoing compactions\")\n\t\t}\n\t\tvar inProgress []compactionInfo\n\t\tfor i := 0; i < len(levels); i += 2 {\n\t\t\tinProgress = append(inProgress, compactionInfo{\n\t\t\t\tinputs: []compactionLevel{\n\t\t\t\t\t{level: levels[i]},\n\t\t\t\t\t{level: levels[i+1]},\n\t\t\t\t},\n\t\t\t\toutputLevel: levels[i+1],\n\t\t\t})\n\t\t}\n\t\treturn inProgress, nil\n\t}\n\n\tresetCompacting := func() {\n\t\tfor _, files := range vers.Levels {\n\t\t\tfiles.Slice().Each(func(f *fileMetadata) {\n\t\t\t\tf.CompactionState = manifest.CompactionStateNotCompacting\n\t\t\t})\n\t\t}\n\t}\n\n\tdatadriven.RunTest(t, \"testdata/compaction_picker_target_level\",\n\t\tfunc(t *testing.T, d *datadriven.TestData) string {\n\t\t\tswitch d.Cmd {\n\t\t\tcase \"init\":\n\t\t\t\t// loadVersion expects a single datadriven argument that it\n\t\t\t\t// sets as Options.LBaseMaxBytes. It parses the input as\n\t\t\t\t// newline-separated levels, specifying the level's file size\n\t\t\t\t// and optionally additional compensation to be added during\n\t\t\t\t// compensated file size calculations. Eg:\n\t\t\t\t//\n\t\t\t\t// init <LBaseMaxBytes>\n\t\t\t\t// <level>: <size> [compensation]\n\t\t\t\t// <level>: <size> [compensation]\n\t\t\t\tvar errMsg string\n\t\t\t\tvers, opts, errMsg = loadVersion(t, d)\n\t\t\t\tif errMsg != \"\" {\n\t\t\t\t\treturn errMsg\n\t\t\t\t}\n\t\t\t\treturn runVersionFileSizes(vers)\n\t\t\tcase \"init_cp\":\n\t\t\t\tresetCompacting()\n\n\t\t\t\tvar inProgress []compactionInfo\n\t\t\t\tif arg, ok := d.Arg(\"ongoing\"); ok {\n\t\t\t\t\tvar err error\n\t\t\t\t\tinProgress, err = parseInProgress(arg.Vals)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tvb := manifest.MakeVirtualBackings()\n\t\t\t\tpickerByScore = newCompactionPickerByScore(vers, &vb, opts, inProgress)\n\t\t\t\treturn fmt.Sprintf(\"base: %d\", pickerByScore.baseLevel)\n\t\t\tcase \"queue\":\n\t\t\t\tvar b strings.Builder\n\t\t\t\tvar inProgress []compactionInfo\n\t\t\t\tfor {\n\t\t\t\t\tenv := compactionEnv{\n\t\t\t\t\t\tdiskAvailBytes:          math.MaxUint64,\n\t\t\t\t\t\tearliestUnflushedSeqNum: base.SeqNumMax,\n\t\t\t\t\t\tinProgressCompactions:   inProgress,\n\t\t\t\t\t}\n\t\t\t\t\tpc := pickerByScore.pickAuto(env)\n\t\t\t\t\tif pc == nil {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t\tfmt.Fprintf(&b, \"L%d->L%d: %.1f\\n\", pc.startLevel.level, pc.outputLevel.level, pc.score)\n\t\t\t\t\tinProgress = append(inProgress, compactionInfo{\n\t\t\t\t\t\tinputs:      pc.inputs,\n\t\t\t\t\t\toutputLevel: pc.outputLevel.level,\n\t\t\t\t\t\tsmallest:    pc.smallest,\n\t\t\t\t\t\tlargest:     pc.largest,\n\t\t\t\t\t})\n\t\t\t\t\tif pc.outputLevel.level == 0 {\n\t\t\t\t\t\t// Once we pick one L0->L0 compaction, we'll keep on doing so\n\t\t\t\t\t\t// because the test isn't marking files as Compacting.\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t\tfor _, cl := range pc.inputs {\n\t\t\t\t\t\tcl.files.Each(func(f *fileMetadata) {\n\t\t\t\t\t\t\tf.CompactionState = manifest.CompactionStateCompacting\n\t\t\t\t\t\t\tfmt.Fprintf(&b, \"  %s marked as compacting\\n\", f)\n\t\t\t\t\t\t})\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tresetCompacting()\n\t\t\t\treturn b.String()\n\t\t\tcase \"pick\":\n\t\t\t\tresetCompacting()\n\n\t\t\t\tvar inProgress []compactionInfo\n\t\t\t\tif len(d.CmdArgs) == 1 {\n\t\t\t\t\targ := d.CmdArgs[0]\n\t\t\t\t\tif arg.Key != \"ongoing\" {\n\t\t\t\t\t\treturn \"unknown arg: \" + arg.Key\n\t\t\t\t\t}\n\t\t\t\t\tvar err error\n\t\t\t\t\tinProgress, err = parseInProgress(arg.Vals)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\t// Mark files as compacting for each in-progress compaction.\n\t\t\t\tfor i := range inProgress {\n\t\t\t\t\tc := &inProgress[i]\n\t\t\t\t\tfor j, cl := range c.inputs {\n\t\t\t\t\t\titer := vers.Levels[cl.level].Iter()\n\t\t\t\t\t\tfor f := iter.First(); f != nil; f = iter.Next() {\n\t\t\t\t\t\t\tif !f.IsCompacting() {\n\t\t\t\t\t\t\t\tf.CompactionState = manifest.CompactionStateCompacting\n\t\t\t\t\t\t\t\tc.inputs[j].files = iter.Take().Slice()\n\t\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tif c.inputs[0].level == 0 && c.outputLevel != 0 {\n\t\t\t\t\t\t// L0->Lbase: mark all of Lbase as compacting.\n\t\t\t\t\t\tc.inputs[1].files = vers.Levels[c.outputLevel].Slice()\n\t\t\t\t\t\tfor _, in := range c.inputs {\n\t\t\t\t\t\t\tin.files.Each(func(f *fileMetadata) {\n\t\t\t\t\t\t\t\tf.CompactionState = manifest.CompactionStateCompacting\n\t\t\t\t\t\t\t})\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tvar b strings.Builder\n\t\t\t\tfmt.Fprintf(&b, \"Initial state before pick:\\n%s\", runVersionFileSizes(vers))\n\t\t\t\tpc := pickerByScore.pickAuto(compactionEnv{\n\t\t\t\t\tearliestUnflushedSeqNum: base.SeqNumMax,\n\t\t\t\t\tinProgressCompactions:   inProgress,\n\t\t\t\t})\n\t\t\t\tif pc != nil {\n\t\t\t\t\tfmt.Fprintf(&b, \"Picked: L%d->L%d: %0.1f\\n\", pc.startLevel.level, pc.outputLevel.level, pc.score)\n\t\t\t\t}\n\t\t\t\tif pc == nil {\n\t\t\t\t\tfmt.Fprintln(&b, \"Picked: no compaction\")\n\t\t\t\t}\n\t\t\t\treturn b.String()\n\t\t\tcase \"pick_manual\":\n\t\t\t\tvar startLevel int\n\t\t\t\tvar start, end string\n\t\t\t\td.MaybeScanArgs(t, \"level\", &startLevel)\n\t\t\t\td.MaybeScanArgs(t, \"start\", &start)\n\t\t\t\td.MaybeScanArgs(t, \"end\", &end)\n\n\t\t\t\tiStart := base.MakeInternalKey([]byte(start), base.SeqNumMax, InternalKeyKindMax)\n\t\t\t\tiEnd := base.MakeInternalKey([]byte(end), 0, 0)\n\t\t\t\tmanual := &manualCompaction{\n\t\t\t\t\tdone:  make(chan error, 1),\n\t\t\t\t\tlevel: startLevel,\n\t\t\t\t\tstart: iStart.UserKey,\n\t\t\t\t\tend:   iEnd.UserKey,\n\t\t\t\t}\n\n\t\t\t\tpc, retryLater := pickManualCompaction(\n\t\t\t\t\tpickerByScore.vers,\n\t\t\t\t\topts,\n\t\t\t\t\tcompactionEnv{\n\t\t\t\t\t\tearliestUnflushedSeqNum: base.SeqNumMax,\n\t\t\t\t\t},\n\t\t\t\t\tpickerByScore.getBaseLevel(),\n\t\t\t\t\tmanual)\n\t\t\t\tif pc == nil {\n\t\t\t\t\treturn fmt.Sprintf(\"nil, retryLater = %v\", retryLater)\n\t\t\t\t}\n\n\t\t\t\treturn fmt.Sprintf(\"L%d->L%d, retryLater = %v\", pc.startLevel.level, pc.outputLevel.level, retryLater)\n\t\t\tdefault:\n\t\t\t\treturn fmt.Sprintf(\"unknown command: %s\", d.Cmd)\n\t\t\t}\n\t\t})\n}\n\nfunc TestCompactionPickerEstimatedCompactionDebt(t *testing.T) {\n\tdatadriven.RunTest(t, \"testdata/compaction_picker_estimated_debt\",\n\t\tfunc(t *testing.T, d *datadriven.TestData) string {\n\t\t\tswitch d.Cmd {\n\t\t\tcase \"init\":\n\t\t\t\tvers, opts, errMsg := loadVersion(t, d)\n\t\t\t\tif errMsg != \"\" {\n\t\t\t\t\treturn errMsg\n\t\t\t\t}\n\t\t\t\topts.MemTableSize = 1000\n\n\t\t\t\tvb := manifest.MakeVirtualBackings()\n\t\t\t\tp := newCompactionPickerByScore(vers, &vb, opts, nil)\n\t\t\t\treturn fmt.Sprintf(\"%d\\n\", p.estimatedCompactionDebt(0))\n\n\t\t\tdefault:\n\t\t\t\treturn fmt.Sprintf(\"unknown command: %s\", d.Cmd)\n\t\t\t}\n\t\t})\n}\n\nfunc TestCompactionPickerL0(t *testing.T) {\n\topts := (*Options)(nil).EnsureDefaults()\n\topts.Experimental.L0CompactionConcurrency = 1\n\n\tparseMeta := func(s string) (*fileMetadata, error) {\n\t\tparts := strings.Split(s, \":\")\n\t\tfileNum, err := strconv.Atoi(parts[0])\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tfields := strings.Fields(parts[1])\n\t\tparts = strings.Split(fields[0], \"-\")\n\t\tif len(parts) != 2 {\n\t\t\treturn nil, errors.Errorf(\"malformed table spec: %s\", s)\n\t\t}\n\t\tm := (&fileMetadata{\n\t\t\tFileNum: base.FileNum(fileNum),\n\t\t}).ExtendPointKeyBounds(\n\t\t\topts.Comparer.Compare,\n\t\t\tbase.ParseInternalKey(strings.TrimSpace(parts[0])),\n\t\t\tbase.ParseInternalKey(strings.TrimSpace(parts[1])),\n\t\t)\n\t\tm.SmallestSeqNum = m.Smallest.SeqNum()\n\t\tm.LargestSeqNum = m.Largest.SeqNum()\n\t\tif m.SmallestSeqNum > m.LargestSeqNum {\n\t\t\tm.SmallestSeqNum, m.LargestSeqNum = m.LargestSeqNum, m.SmallestSeqNum\n\t\t}\n\t\tm.LargestSeqNumAbsolute = m.LargestSeqNum\n\t\tm.InitPhysicalBacking()\n\t\treturn m, nil\n\t}\n\n\tvar picker *compactionPickerByScore\n\tvar inProgressCompactions []compactionInfo\n\tvar pc *pickedCompaction\n\n\tdatadriven.RunTest(t, \"testdata/compaction_picker_L0\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"define\":\n\t\t\tfileMetas := [manifest.NumLevels][]*fileMetadata{}\n\t\t\tbaseLevel := manifest.NumLevels - 1\n\t\t\tlevel := 0\n\t\t\tvar err error\n\t\t\tlines := strings.Split(td.Input, \"\\n\")\n\t\t\tvar compactionLines []string\n\n\t\t\tfor len(lines) > 0 {\n\t\t\t\tdata := strings.TrimSpace(lines[0])\n\t\t\t\tlines = lines[1:]\n\t\t\t\tswitch data {\n\t\t\t\tcase \"L0\", \"L1\", \"L2\", \"L3\", \"L4\", \"L5\", \"L6\":\n\t\t\t\t\tlevel, err = strconv.Atoi(data[1:])\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\tcase \"compactions\":\n\t\t\t\t\tcompactionLines, lines = lines, nil\n\t\t\t\tdefault:\n\t\t\t\t\tmeta, err := parseMeta(data)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t\tif level != 0 && level < baseLevel {\n\t\t\t\t\t\tbaseLevel = level\n\t\t\t\t\t}\n\t\t\t\t\tfileMetas[level] = append(fileMetas[level], meta)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Parse in-progress compactions in the form of:\n\t\t\t//   L0 000001 -> L2 000005\n\t\t\tinProgressCompactions = nil\n\t\t\tfor len(compactionLines) > 0 {\n\t\t\t\tparts := strings.Fields(compactionLines[0])\n\t\t\t\tcompactionLines = compactionLines[1:]\n\n\t\t\t\tvar level int\n\t\t\t\tvar info compactionInfo\n\t\t\t\tfirst := true\n\t\t\t\tcompactionFiles := map[int][]*fileMetadata{}\n\t\t\t\tfor _, p := range parts {\n\t\t\t\t\tswitch p {\n\t\t\t\t\tcase \"L0\", \"L1\", \"L2\", \"L3\", \"L4\", \"L5\", \"L6\":\n\t\t\t\t\t\tvar err error\n\t\t\t\t\t\tlevel, err = strconv.Atoi(p[1:])\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif len(info.inputs) > 0 && info.inputs[len(info.inputs)-1].level == level {\n\t\t\t\t\t\t\t// eg, L0 -> L0 compaction or L6 -> L6 compaction\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif info.outputLevel < level {\n\t\t\t\t\t\t\tinfo.outputLevel = level\n\t\t\t\t\t\t}\n\t\t\t\t\t\tinfo.inputs = append(info.inputs, compactionLevel{level: level})\n\t\t\t\t\tcase \"->\":\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tfileNum, err := strconv.Atoi(p)\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t\t}\n\t\t\t\t\t\tvar compactFile *fileMetadata\n\t\t\t\t\t\tfor _, m := range fileMetas[level] {\n\t\t\t\t\t\t\tif m.FileNum == FileNum(fileNum) {\n\t\t\t\t\t\t\t\tcompactFile = m\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif compactFile == nil {\n\t\t\t\t\t\t\treturn fmt.Sprintf(\"cannot find compaction file %s\", FileNum(fileNum))\n\t\t\t\t\t\t}\n\t\t\t\t\t\tcompactFile.CompactionState = manifest.CompactionStateCompacting\n\t\t\t\t\t\tif first || base.InternalCompare(DefaultComparer.Compare, info.largest, compactFile.Largest) < 0 {\n\t\t\t\t\t\t\tinfo.largest = compactFile.Largest\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif first || base.InternalCompare(DefaultComparer.Compare, info.smallest, compactFile.Smallest) > 0 {\n\t\t\t\t\t\t\tinfo.smallest = compactFile.Smallest\n\t\t\t\t\t\t}\n\t\t\t\t\t\tfirst = false\n\t\t\t\t\t\tcompactionFiles[level] = append(compactionFiles[level], compactFile)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfor i, cl := range info.inputs {\n\t\t\t\t\tfiles := compactionFiles[cl.level]\n\t\t\t\t\tinfo.inputs[i].files = manifest.NewLevelSliceSeqSorted(files)\n\t\t\t\t\t// Mark as intra-L0 compacting if the compaction is\n\t\t\t\t\t// L0 -> L0.\n\t\t\t\t\tif info.outputLevel == 0 {\n\t\t\t\t\t\tfor _, f := range files {\n\t\t\t\t\t\t\tf.IsIntraL0Compacting = true\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tinProgressCompactions = append(inProgressCompactions, info)\n\t\t\t}\n\n\t\t\tversion := newVersion(opts, fileMetas)\n\t\t\tversion.L0Sublevels.InitCompactingFileInfo(inProgressL0Compactions(inProgressCompactions))\n\t\t\tvs := &versionSet{\n\t\t\t\topts: opts,\n\t\t\t\tcmp:  DefaultComparer,\n\t\t\t}\n\t\t\tvs.versions.Init(nil)\n\t\t\tvs.append(version)\n\t\t\tpicker = &compactionPickerByScore{\n\t\t\t\topts:      opts,\n\t\t\t\tvers:      version,\n\t\t\t\tbaseLevel: baseLevel,\n\t\t\t}\n\t\t\tvs.picker = picker\n\t\t\tpicker.initLevelMaxBytes(inProgressCompactions)\n\n\t\t\tvar buf bytes.Buffer\n\t\t\tfmt.Fprint(&buf, version.String())\n\t\t\tif len(inProgressCompactions) > 0 {\n\t\t\t\tfmt.Fprintln(&buf, \"compactions\")\n\t\t\t\tfor _, c := range inProgressCompactions {\n\t\t\t\t\tfmt.Fprintf(&buf, \"  %s\\n\", c.String())\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn buf.String()\n\t\tcase \"pick-auto\":\n\t\t\ttd.MaybeScanArgs(t, \"l0_compaction_threshold\", &opts.L0CompactionThreshold)\n\t\t\ttd.MaybeScanArgs(t, \"l0_compaction_file_threshold\", &opts.L0CompactionFileThreshold)\n\n\t\t\tpc = picker.pickAuto(compactionEnv{\n\t\t\t\tdiskAvailBytes:          math.MaxUint64,\n\t\t\t\tearliestUnflushedSeqNum: math.MaxUint64,\n\t\t\t\tinProgressCompactions:   inProgressCompactions,\n\t\t\t})\n\t\t\tvar result strings.Builder\n\t\t\tif pc != nil {\n\t\t\t\tcheckClone(t, pc)\n\t\t\t\tc := newCompaction(pc, opts, time.Now(), nil /* provider */, nil /* slot */)\n\t\t\t\tfmt.Fprintf(&result, \"L%d -> L%d\\n\", pc.startLevel.level, pc.outputLevel.level)\n\t\t\t\tfmt.Fprintf(&result, \"L%d: %s\\n\", pc.startLevel.level, fileNums(pc.startLevel.files))\n\t\t\t\tif !pc.outputLevel.files.Empty() {\n\t\t\t\t\tfmt.Fprintf(&result, \"L%d: %s\\n\", pc.outputLevel.level, fileNums(pc.outputLevel.files))\n\t\t\t\t}\n\t\t\t\tif !c.grandparents.Empty() {\n\t\t\t\t\tfmt.Fprintf(&result, \"grandparents: %s\\n\", fileNums(c.grandparents))\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\treturn \"nil\"\n\t\t\t}\n\t\t\treturn result.String()\n\t\tcase \"mark-for-compaction\":\n\t\t\tvar fileNum uint64\n\t\t\ttd.ScanArgs(t, \"file\", &fileNum)\n\t\t\tfor l, lm := range picker.vers.Levels {\n\t\t\t\titer := lm.Iter()\n\t\t\t\tfor f := iter.First(); f != nil; f = iter.Next() {\n\t\t\t\t\tif f.FileNum != base.FileNum(fileNum) {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tf.MarkedForCompaction = true\n\t\t\t\t\tpicker.vers.Stats.MarkedForCompaction++\n\t\t\t\t\tmarkedForCompactionAnnotator.InvalidateLevelAnnotation(picker.vers.Levels[l])\n\t\t\t\t\treturn fmt.Sprintf(\"marked L%d.%s\", l, f.FileNum)\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn \"not-found\"\n\t\tcase \"max-output-file-size\":\n\t\t\tif pc == nil {\n\t\t\t\treturn \"no compaction\"\n\t\t\t}\n\t\t\treturn fmt.Sprintf(\"%d\", pc.maxOutputFileSize)\n\t\tcase \"max-overlap-bytes\":\n\t\t\tif pc == nil {\n\t\t\t\treturn \"no compaction\"\n\t\t\t}\n\t\t\treturn fmt.Sprintf(\"%d\", pc.maxOverlapBytes)\n\t\t}\n\t\treturn fmt.Sprintf(\"unrecognized command: %s\", td.Cmd)\n\t})\n}\n\nfunc TestCompactionPickerConcurrency(t *testing.T) {\n\topts := (*Options)(nil).EnsureDefaults()\n\topts.Experimental.L0CompactionConcurrency = 1\n\n\tparseMeta := func(s string) (*fileMetadata, error) {\n\t\tparts := strings.Split(s, \":\")\n\t\tfileNum, err := strconv.Atoi(parts[0])\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tfields := strings.Fields(parts[1])\n\t\tparts = strings.Split(fields[0], \"-\")\n\t\tif len(parts) != 2 {\n\t\t\treturn nil, errors.Errorf(\"malformed table spec: %s\", s)\n\t\t}\n\t\tm := (&fileMetadata{\n\t\t\tFileNum: base.FileNum(fileNum),\n\t\t\tSize:    1028,\n\t\t}).ExtendPointKeyBounds(\n\t\t\topts.Comparer.Compare,\n\t\t\tbase.ParseInternalKey(strings.TrimSpace(parts[0])),\n\t\t\tbase.ParseInternalKey(strings.TrimSpace(parts[1])),\n\t\t)\n\t\tm.InitPhysicalBacking()\n\t\tfor _, p := range fields[1:] {\n\t\t\tif strings.HasPrefix(p, \"size=\") {\n\t\t\t\tv, err := strconv.Atoi(strings.TrimPrefix(p, \"size=\"))\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\tm.Size = uint64(v)\n\t\t\t}\n\t\t}\n\t\tm.SmallestSeqNum = m.Smallest.SeqNum()\n\t\tm.LargestSeqNum = m.Largest.SeqNum()\n\t\tm.LargestSeqNumAbsolute = m.Largest.SeqNum()\n\t\treturn m, nil\n\t}\n\n\tvar picker *compactionPickerByScore\n\tvar inProgressCompactions []compactionInfo\n\n\tdatadriven.RunTest(t, \"testdata/compaction_picker_concurrency\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"define\":\n\t\t\tfileMetas := [manifest.NumLevels][]*fileMetadata{}\n\t\t\tlevel := 0\n\t\t\tvar err error\n\t\t\tlines := strings.Split(td.Input, \"\\n\")\n\t\t\tvar compactionLines []string\n\n\t\t\tfor len(lines) > 0 {\n\t\t\t\tdata := strings.TrimSpace(lines[0])\n\t\t\t\tlines = lines[1:]\n\t\t\t\tswitch data {\n\t\t\t\tcase \"L0\", \"L1\", \"L2\", \"L3\", \"L4\", \"L5\", \"L6\":\n\t\t\t\t\tlevel, err = strconv.Atoi(data[1:])\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\tcase \"compactions\":\n\t\t\t\t\tcompactionLines, lines = lines, nil\n\t\t\t\tdefault:\n\t\t\t\t\tmeta, err := parseMeta(data)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t\tfileMetas[level] = append(fileMetas[level], meta)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Parse in-progress compactions in the form of:\n\t\t\t//   L0 000001 -> L2 000005\n\t\t\tinProgressCompactions = nil\n\t\t\tfor len(compactionLines) > 0 {\n\t\t\t\tparts := strings.Fields(compactionLines[0])\n\t\t\t\tcompactionLines = compactionLines[1:]\n\n\t\t\t\tvar level int\n\t\t\t\tvar info compactionInfo\n\t\t\t\tfirst := true\n\t\t\t\tcompactionFiles := map[int][]*fileMetadata{}\n\t\t\t\tfor _, p := range parts {\n\t\t\t\t\tswitch p {\n\t\t\t\t\tcase \"L0\", \"L1\", \"L2\", \"L3\", \"L4\", \"L5\", \"L6\":\n\t\t\t\t\t\tvar err error\n\t\t\t\t\t\tlevel, err = strconv.Atoi(p[1:])\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif len(info.inputs) > 0 && info.inputs[len(info.inputs)-1].level == level {\n\t\t\t\t\t\t\t// eg, L0 -> L0 compaction or L6 -> L6 compaction\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif info.outputLevel < level {\n\t\t\t\t\t\t\tinfo.outputLevel = level\n\t\t\t\t\t\t}\n\t\t\t\t\t\tinfo.inputs = append(info.inputs, compactionLevel{level: level})\n\t\t\t\t\tcase \"->\":\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tfileNum, err := strconv.Atoi(p)\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t\t}\n\t\t\t\t\t\tvar compactFile *fileMetadata\n\t\t\t\t\t\tfor _, m := range fileMetas[level] {\n\t\t\t\t\t\t\tif m.FileNum == FileNum(fileNum) {\n\t\t\t\t\t\t\t\tcompactFile = m\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif compactFile == nil {\n\t\t\t\t\t\t\treturn fmt.Sprintf(\"cannot find compaction file %s\", FileNum(fileNum))\n\t\t\t\t\t\t}\n\t\t\t\t\t\tcompactFile.CompactionState = manifest.CompactionStateCompacting\n\t\t\t\t\t\tif first || base.InternalCompare(DefaultComparer.Compare, info.largest, compactFile.Largest) < 0 {\n\t\t\t\t\t\t\tinfo.largest = compactFile.Largest\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif first || base.InternalCompare(DefaultComparer.Compare, info.smallest, compactFile.Smallest) > 0 {\n\t\t\t\t\t\t\tinfo.smallest = compactFile.Smallest\n\t\t\t\t\t\t}\n\t\t\t\t\t\tfirst = false\n\t\t\t\t\t\tcompactionFiles[level] = append(compactionFiles[level], compactFile)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfor i, cl := range info.inputs {\n\t\t\t\t\tfiles := compactionFiles[cl.level]\n\t\t\t\t\tif cl.level == 0 {\n\t\t\t\t\t\tinfo.inputs[i].files = manifest.NewLevelSliceSeqSorted(files)\n\t\t\t\t\t} else {\n\t\t\t\t\t\tinfo.inputs[i].files = manifest.NewLevelSliceKeySorted(DefaultComparer.Compare, files)\n\t\t\t\t\t}\n\t\t\t\t\t// Mark as intra-L0 compacting if the compaction is\n\t\t\t\t\t// L0 -> L0.\n\t\t\t\t\tif info.outputLevel == 0 {\n\t\t\t\t\t\tfor _, f := range files {\n\t\t\t\t\t\t\tf.IsIntraL0Compacting = true\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tinProgressCompactions = append(inProgressCompactions, info)\n\t\t\t}\n\n\t\t\tversion := newVersion(opts, fileMetas)\n\t\t\tversion.L0Sublevels.InitCompactingFileInfo(inProgressL0Compactions(inProgressCompactions))\n\t\t\tvs := &versionSet{\n\t\t\t\topts: opts,\n\t\t\t\tcmp:  DefaultComparer,\n\t\t\t}\n\t\t\tvs.versions.Init(nil)\n\t\t\tvs.append(version)\n\n\t\t\tvb := manifest.MakeVirtualBackings()\n\t\t\tpicker = newCompactionPickerByScore(version, &vb, opts, inProgressCompactions)\n\t\t\tvs.picker = picker\n\n\t\t\tvar buf bytes.Buffer\n\t\t\tfmt.Fprint(&buf, version.String())\n\t\t\tif len(inProgressCompactions) > 0 {\n\t\t\t\tfmt.Fprintln(&buf, \"compactions\")\n\t\t\t\tfor _, c := range inProgressCompactions {\n\t\t\t\t\tfmt.Fprintf(&buf, \"  %s\\n\", c.String())\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn buf.String()\n\n\t\tcase \"pick-auto\":\n\t\t\ttd.MaybeScanArgs(t, \"l0_compaction_threshold\", &opts.L0CompactionThreshold)\n\t\t\ttd.MaybeScanArgs(t, \"l0_compaction_concurrency\", &opts.Experimental.L0CompactionConcurrency)\n\t\t\ttd.MaybeScanArgs(t, \"compaction_debt_concurrency\", &opts.Experimental.CompactionDebtConcurrency)\n\n\t\t\tpc := picker.pickAuto(compactionEnv{\n\t\t\t\tearliestUnflushedSeqNum: math.MaxUint64,\n\t\t\t\tinProgressCompactions:   inProgressCompactions,\n\t\t\t})\n\t\t\tvar result strings.Builder\n\t\t\tif pc != nil {\n\t\t\t\tc := newCompaction(pc, opts, time.Now(), nil /* provider */, nil /* slot */)\n\t\t\t\tfmt.Fprintf(&result, \"L%d -> L%d\\n\", pc.startLevel.level, pc.outputLevel.level)\n\t\t\t\tfmt.Fprintf(&result, \"L%d: %s\\n\", pc.startLevel.level, fileNums(pc.startLevel.files))\n\t\t\t\tif !pc.outputLevel.files.Empty() {\n\t\t\t\t\tfmt.Fprintf(&result, \"L%d: %s\\n\", pc.outputLevel.level, fileNums(pc.outputLevel.files))\n\t\t\t\t}\n\t\t\t\tif !c.grandparents.Empty() {\n\t\t\t\t\tfmt.Fprintf(&result, \"grandparents: %s\\n\", fileNums(c.grandparents))\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\treturn \"nil\"\n\t\t\t}\n\t\t\treturn result.String()\n\t\t}\n\t\treturn fmt.Sprintf(\"unrecognized command: %s\", td.Cmd)\n\t})\n}\n\nfunc TestCompactionPickerPickReadTriggered(t *testing.T) {\n\topts := (*Options)(nil).EnsureDefaults()\n\tvar picker *compactionPickerByScore\n\tvar rcList readCompactionQueue\n\tvar vers *version\n\n\tparseMeta := func(s string) (*fileMetadata, error) {\n\t\tparts := strings.Split(s, \":\")\n\t\tfileNum, err := strconv.Atoi(parts[0])\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tfields := strings.Fields(parts[1])\n\t\tparts = strings.Split(fields[0], \"-\")\n\t\tif len(parts) != 2 {\n\t\t\treturn nil, errors.Errorf(\"malformed table spec: %s. usage: <file-num>:start.SET.1-end.SET.2\", s)\n\t\t}\n\t\tm := (&fileMetadata{\n\t\t\tFileNum: base.FileNum(fileNum),\n\t\t\tSize:    1028,\n\t\t}).ExtendPointKeyBounds(\n\t\t\topts.Comparer.Compare,\n\t\t\tbase.ParseInternalKey(strings.TrimSpace(parts[0])),\n\t\t\tbase.ParseInternalKey(strings.TrimSpace(parts[1])),\n\t\t)\n\t\tm.InitPhysicalBacking()\n\t\tfor _, p := range fields[1:] {\n\t\t\tif strings.HasPrefix(p, \"size=\") {\n\t\t\t\tv, err := strconv.Atoi(strings.TrimPrefix(p, \"size=\"))\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\tm.Size = uint64(v)\n\t\t\t}\n\t\t}\n\t\tm.SmallestSeqNum = m.Smallest.SeqNum()\n\t\tm.LargestSeqNum = m.Largest.SeqNum()\n\t\tm.LargestSeqNumAbsolute = m.Largest.SeqNum()\n\t\treturn m, nil\n\t}\n\n\tdatadriven.RunTest(t, \"testdata/compaction_picker_read_triggered\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"define\":\n\t\t\trcList = readCompactionQueue{}\n\t\t\tfileMetas := [manifest.NumLevels][]*fileMetadata{}\n\t\t\tlevel := 0\n\t\t\tvar err error\n\t\t\tlines := strings.Split(td.Input, \"\\n\")\n\n\t\t\tfor len(lines) > 0 {\n\t\t\t\tdata := strings.TrimSpace(lines[0])\n\t\t\t\tlines = lines[1:]\n\t\t\t\tswitch data {\n\t\t\t\tcase \"L0\", \"L1\", \"L2\", \"L3\", \"L4\", \"L5\", \"L6\":\n\t\t\t\t\tlevel, err = strconv.Atoi(data[1:])\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\tdefault:\n\t\t\t\t\tmeta, err := parseMeta(data)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t\tfileMetas[level] = append(fileMetas[level], meta)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tvers = newVersion(opts, fileMetas)\n\t\t\tvs := &versionSet{\n\t\t\t\topts: opts,\n\t\t\t\tcmp:  DefaultComparer,\n\t\t\t}\n\t\t\tvs.versions.Init(nil)\n\t\t\tvs.append(vers)\n\t\t\tvar inProgressCompactions []compactionInfo\n\t\t\tvb := manifest.MakeVirtualBackings()\n\t\t\tpicker = newCompactionPickerByScore(vers, &vb, opts, inProgressCompactions)\n\t\t\tvs.picker = picker\n\n\t\t\tvar buf bytes.Buffer\n\t\t\tfmt.Fprint(&buf, vers.String())\n\t\t\treturn buf.String()\n\n\t\tcase \"add-read-compaction\":\n\t\t\tfor _, line := range crstrings.Lines(td.Input) {\n\t\t\t\tparts := strings.Split(line, \" \")\n\t\t\t\tif len(parts) != 3 {\n\t\t\t\t\treturn \"error: malformed data for add-read-compaction. usage: <level>: <start>-<end> <filenum>\"\n\t\t\t\t}\n\t\t\t\tif l, err := strconv.Atoi(parts[0][:1]); err == nil {\n\t\t\t\t\tkeys := strings.Split(parts[1], \"-\")\n\t\t\t\t\tfileNum, _ := strconv.Atoi(parts[2])\n\n\t\t\t\t\trc := readCompaction{\n\t\t\t\t\t\tlevel:   l,\n\t\t\t\t\t\tstart:   []byte(keys[0]),\n\t\t\t\t\t\tend:     []byte(keys[1]),\n\t\t\t\t\t\tfileNum: base.FileNum(fileNum),\n\t\t\t\t\t}\n\t\t\t\t\trcList.add(&rc, DefaultComparer.Compare)\n\t\t\t\t} else {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"show-read-compactions\":\n\t\t\tvar sb strings.Builder\n\t\t\tif rcList.size == 0 {\n\t\t\t\tsb.WriteString(\"(none)\")\n\t\t\t}\n\t\t\tfor i := 0; i < rcList.size; i++ {\n\t\t\t\trc := rcList.at(i)\n\t\t\t\tsb.WriteString(fmt.Sprintf(\"(level: %d, start: %s, end: %s)\\n\", rc.level, string(rc.start), string(rc.end)))\n\t\t\t}\n\t\t\treturn sb.String()\n\n\t\tcase \"pick-auto\":\n\t\t\tpc := picker.pickAuto(compactionEnv{\n\t\t\t\tearliestUnflushedSeqNum: math.MaxUint64,\n\t\t\t\treadCompactionEnv: readCompactionEnv{\n\t\t\t\t\treadCompactions: &rcList,\n\t\t\t\t\tflushing:        false,\n\t\t\t\t},\n\t\t\t})\n\t\t\tvar result strings.Builder\n\t\t\tif pc != nil {\n\t\t\t\tfmt.Fprintf(&result, \"L%d -> L%d\\n\", pc.startLevel.level, pc.outputLevel.level)\n\t\t\t\tfmt.Fprintf(&result, \"L%d: %s\\n\", pc.startLevel.level, fileNums(pc.startLevel.files))\n\t\t\t\tif !pc.outputLevel.files.Empty() {\n\t\t\t\t\tfmt.Fprintf(&result, \"L%d: %s\\n\", pc.outputLevel.level, fileNums(pc.outputLevel.files))\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\treturn \"nil\"\n\t\t\t}\n\t\t\treturn result.String()\n\t\t}\n\t\treturn fmt.Sprintf(\"unrecognized command: %s\", td.Cmd)\n\t})\n}\n\ntype alwaysMultiLevel struct{}\n\nfunc (d alwaysMultiLevel) pick(\n\tpcOrig *pickedCompaction, opts *Options, diskAvailBytes uint64,\n) *pickedCompaction {\n\tpcMulti := pcOrig.clone()\n\tif !pcMulti.setupMultiLevelCandidate(opts, diskAvailBytes) {\n\t\treturn pcOrig\n\t}\n\treturn pcMulti\n}\n\nfunc (d alwaysMultiLevel) allowL0() bool  { return false }\nfunc (d alwaysMultiLevel) String() string { return \"always\" }\n\nfunc TestPickedCompactionSetupInputs(t *testing.T) {\n\topts := &Options{}\n\topts.EnsureDefaults()\n\n\tparseMeta := func(s string) *fileMetadata {\n\t\tparts := strings.Split(strings.TrimSpace(s), \" \")\n\t\tvar fileSize uint64\n\t\tvar compacting bool\n\t\tfor _, part := range parts {\n\t\t\tswitch {\n\t\t\tcase part == \"compacting\":\n\t\t\t\tcompacting = true\n\t\t\tcase strings.HasPrefix(part, \"size=\"):\n\t\t\t\tv, err := strconv.ParseUint(strings.TrimPrefix(part, \"size=\"), 10, 64)\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\tfileSize = v\n\t\t\t}\n\t\t}\n\t\ttableParts := strings.Split(parts[0], \"-\")\n\t\tif len(tableParts) != 2 {\n\t\t\tt.Fatalf(\"malformed table spec: %s\", s)\n\t\t}\n\t\tstate := manifest.CompactionStateNotCompacting\n\t\tif compacting {\n\t\t\tstate = manifest.CompactionStateCompacting\n\t\t}\n\t\tm := (&fileMetadata{\n\t\t\tCompactionState: state,\n\t\t\tSize:            fileSize,\n\t\t}).ExtendPointKeyBounds(\n\t\t\topts.Comparer.Compare,\n\t\t\tbase.ParseInternalKey(strings.TrimSpace(tableParts[0])),\n\t\t\tbase.ParseInternalKey(strings.TrimSpace(tableParts[1])),\n\t\t)\n\t\tm.SmallestSeqNum = m.Smallest.SeqNum()\n\t\tm.LargestSeqNum = m.Largest.SeqNum()\n\t\tif m.SmallestSeqNum > m.LargestSeqNum {\n\t\t\tm.SmallestSeqNum, m.LargestSeqNum = m.LargestSeqNum, m.SmallestSeqNum\n\t\t}\n\t\tm.LargestSeqNumAbsolute = m.LargestSeqNum\n\t\tm.InitPhysicalBacking()\n\t\treturn m\n\t}\n\n\tsetupInputTest := func(t *testing.T, d *datadriven.TestData) string {\n\t\tswitch d.Cmd {\n\t\tcase \"setup-inputs\":\n\t\t\tvar availBytes uint64 = math.MaxUint64\n\t\t\tvar maxLevelBytes [7]int64\n\t\t\targs := d.CmdArgs\n\n\t\t\tif len(args) > 0 && args[0].Key == \"avail-bytes\" {\n\t\t\t\trequire.Equal(t, 1, len(args[0].Vals))\n\t\t\t\tvar err error\n\t\t\t\tavailBytes, err = strconv.ParseUint(args[0].Vals[0], 10, 64)\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\targs = args[1:]\n\t\t\t}\n\n\t\t\tif len(args) != 2 {\n\t\t\t\treturn \"setup-inputs [avail-bytes=XXX] <start> <end>\"\n\t\t\t}\n\n\t\t\tpc := &pickedCompaction{\n\t\t\t\tcmp:    DefaultComparer.Compare,\n\t\t\t\tinputs: []compactionLevel{{level: -1}, {level: -1}},\n\t\t\t}\n\t\t\tpc.startLevel, pc.outputLevel = &pc.inputs[0], &pc.inputs[1]\n\t\t\tvar currentLevel int\n\t\t\tvar files [numLevels][]*fileMetadata\n\t\t\tfileNum := FileNum(1)\n\n\t\t\tfor _, data := range strings.Split(d.Input, \"\\n\") {\n\t\t\t\tswitch data[:2] {\n\t\t\t\tcase \"L0\", \"L1\", \"L2\", \"L3\", \"L4\", \"L5\", \"L6\":\n\t\t\t\t\tlevelArgs := strings.Fields(data)\n\t\t\t\t\tlevel, err := strconv.Atoi(levelArgs[0][1:])\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t\tcurrentLevel = level\n\t\t\t\t\tif len(levelArgs) > 1 {\n\t\t\t\t\t\tmaxSizeArg := strings.Replace(levelArgs[1], \"max-size=\", \"\", 1)\n\t\t\t\t\t\tmaxSize, err := strconv.ParseInt(maxSizeArg, 10, 64)\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t\t}\n\t\t\t\t\t\tmaxLevelBytes[level] = maxSize\n\t\t\t\t\t} else {\n\t\t\t\t\t\tmaxLevelBytes[level] = math.MaxInt64\n\t\t\t\t\t}\n\t\t\t\t\tif pc.startLevel.level == -1 {\n\t\t\t\t\t\tpc.startLevel.level = level\n\n\t\t\t\t\t} else if pc.outputLevel.level == -1 {\n\t\t\t\t\t\tif pc.startLevel.level >= level {\n\t\t\t\t\t\t\treturn fmt.Sprintf(\"startLevel=%d >= outputLevel=%d\\n\", pc.startLevel.level, level)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tpc.outputLevel.level = level\n\t\t\t\t\t}\n\t\t\t\tdefault:\n\t\t\t\t\tmeta := parseMeta(data)\n\t\t\t\t\tmeta.FileNum = fileNum\n\t\t\t\t\tfileNum++\n\t\t\t\t\tfiles[currentLevel] = append(files[currentLevel], meta)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfor _, levelFiles := range files {\n\t\t\t\tfor i := 1; i < len(levelFiles); i++ {\n\t\t\t\t\tif !checkTableBoundary(levelFiles[i-1], levelFiles[i], opts.Comparer.Compare) {\n\t\t\t\t\t\td.Fatalf(t, \"overlapping tables: %s and %s\", levelFiles[i-1], levelFiles[i])\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif pc.outputLevel.level == -1 {\n\t\t\t\tpc.outputLevel.level = pc.startLevel.level + 1\n\t\t\t}\n\t\t\tpc.version = newVersion(opts, files)\n\t\t\tpc.startLevel.files = pc.version.Overlaps(\n\t\t\t\tpc.startLevel.level,\n\t\t\t\tbase.UserKeyBoundsInclusive([]byte(args[0].String()), []byte(args[1].String())),\n\t\t\t)\n\n\t\t\tvar isCompacting bool\n\t\t\tif !pc.setupInputs(opts, availBytes, pc.startLevel) {\n\t\t\t\tisCompacting = true\n\t\t\t}\n\t\t\torigPC := pc\n\t\t\tpc = pc.maybeAddLevel(opts, availBytes)\n\t\t\t// If pc points to a new pickedCompaction, a new multi level compaction\n\t\t\t// was initialized.\n\t\t\tinitMultiLevel := pc != origPC\n\t\t\tcheckClone(t, pc)\n\t\t\tvar buf bytes.Buffer\n\t\t\tfor _, cl := range pc.inputs {\n\t\t\t\tif cl.files.Empty() {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tfmt.Fprintf(&buf, \"L%d\\n\", cl.level)\n\t\t\t\tcl.files.Each(func(f *fileMetadata) {\n\t\t\t\t\tfmt.Fprintf(&buf, \"  %s\\n\", f)\n\t\t\t\t})\n\t\t\t}\n\t\t\tif isCompacting {\n\t\t\t\tfmt.Fprintf(&buf, \"is-compacting\\n\")\n\t\t\t}\n\n\t\t\tif initMultiLevel {\n\t\t\t\textraLevel := pc.extraLevels[0].level\n\t\t\t\tfmt.Fprintf(&buf, \"init-multi-level(%d,%d,%d)\\n\", pc.startLevel.level, extraLevel,\n\t\t\t\t\tpc.outputLevel.level)\n\t\t\t\tfmt.Fprintf(&buf, \"Original WriteAmp %.2f; ML WriteAmp %.2f\\n\", origPC.predictedWriteAmp(), pc.predictedWriteAmp())\n\t\t\t\tfmt.Fprintf(&buf, \"Original OverlappingRatio %.2f; ML OverlappingRatio %.2f\\n\", origPC.overlappingRatio(), pc.overlappingRatio())\n\t\t\t}\n\t\t\treturn buf.String()\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", d.Cmd)\n\t\t}\n\t}\n\n\tt.Logf(\"Test basic setup inputs behavior without multi level compactions\")\n\topts.Experimental.MultiLevelCompactionHeuristic = NoMultiLevel{}\n\tdatadriven.RunTest(t, \"testdata/compaction_setup_inputs\",\n\t\tsetupInputTest)\n\n\tt.Logf(\"Turning multi level compaction on\")\n\topts.Experimental.MultiLevelCompactionHeuristic = alwaysMultiLevel{}\n\tdatadriven.RunTest(t, \"testdata/compaction_setup_inputs_multilevel_dummy\",\n\t\tsetupInputTest)\n\n\tt.Logf(\"Try Write-Amp Heuristic\")\n\topts.Experimental.MultiLevelCompactionHeuristic = WriteAmpHeuristic{}\n\tdatadriven.RunTest(t, \"testdata/compaction_setup_inputs_multilevel_write_amp\",\n\t\tsetupInputTest)\n}\n\nfunc TestPickedCompactionExpandInputs(t *testing.T) {\n\topts := &Options{}\n\topts.EnsureDefaults()\n\tcmp := DefaultComparer.Compare\n\tvar files []*fileMetadata\n\n\tparseMeta := func(s string) *fileMetadata {\n\t\tparts := strings.Split(s, \"-\")\n\t\tif len(parts) != 2 {\n\t\t\tt.Fatalf(\"malformed table spec: %s\", s)\n\t\t}\n\t\tm := (&fileMetadata{}).ExtendPointKeyBounds(\n\t\t\topts.Comparer.Compare,\n\t\t\tbase.ParseInternalKey(parts[0]),\n\t\t\tbase.ParseInternalKey(parts[1]),\n\t\t)\n\t\tm.InitPhysicalBacking()\n\t\treturn m\n\t}\n\n\tdatadriven.RunTest(t, \"testdata/compaction_expand_inputs\",\n\t\tfunc(t *testing.T, d *datadriven.TestData) string {\n\t\t\tswitch d.Cmd {\n\t\t\tcase \"define\":\n\t\t\t\tfiles = nil\n\t\t\t\tif len(d.Input) == 0 {\n\t\t\t\t\treturn \"\"\n\t\t\t\t}\n\t\t\t\tfor _, data := range strings.Split(d.Input, \"\\n\") {\n\t\t\t\t\tmeta := parseMeta(data)\n\t\t\t\t\tmeta.FileNum = FileNum(len(files))\n\t\t\t\t\tfiles = append(files, meta)\n\t\t\t\t}\n\t\t\t\tmanifest.SortBySmallest(files, cmp)\n\t\t\t\t// Verify that the tables have no user key overlap.\n\t\t\t\tfor i := 1; i < len(files); i++ {\n\t\t\t\t\tif !checkTableBoundary(files[i-1], files[i], cmp) {\n\t\t\t\t\t\td.Fatalf(t, \"overlapping tables: %s and %s\", files[i-1], files[i])\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn \"\"\n\n\t\t\tcase \"expand-inputs\":\n\t\t\t\tpc := &pickedCompaction{\n\t\t\t\t\tcmp:    cmp,\n\t\t\t\t\tinputs: []compactionLevel{{level: 1}},\n\t\t\t\t}\n\t\t\t\tpc.startLevel = &pc.inputs[0]\n\n\t\t\t\tvar filesLevelled [numLevels][]*fileMetadata\n\t\t\t\tfilesLevelled[pc.startLevel.level] = files\n\t\t\t\tpc.version = newVersion(opts, filesLevelled)\n\n\t\t\t\tif len(d.CmdArgs) != 1 {\n\t\t\t\t\treturn fmt.Sprintf(\"%s expects 1 argument\", d.Cmd)\n\t\t\t\t}\n\t\t\t\tindex, err := strconv.ParseInt(d.CmdArgs[0].String(), 10, 64)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\n\t\t\t\t// Advance the iterator to position `index`.\n\t\t\t\titer := pc.version.Levels[pc.startLevel.level].Iter()\n\t\t\t\t_ = iter.First()\n\t\t\t\tfor i := int64(0); i < index; i++ {\n\t\t\t\t\t_ = iter.Next()\n\t\t\t\t}\n\n\t\t\t\tvar buf bytes.Buffer\n\t\t\t\titer.Take().Slice().Each(func(f *fileMetadata) {\n\t\t\t\t\tfmt.Fprintf(&buf, \"%d: %s-%s\\n\", f.FileNum, f.Smallest, f.Largest)\n\t\t\t\t})\n\t\t\t\treturn buf.String()\n\n\t\t\tdefault:\n\t\t\t\treturn fmt.Sprintf(\"unknown command: %s\", d.Cmd)\n\t\t\t}\n\t\t})\n}\n\nfunc TestCompactionOutputFileSize(t *testing.T) {\n\topts := (*Options)(nil).EnsureDefaults()\n\tvar picker *compactionPickerByScore\n\tvar vers *version\n\n\tparseMeta := func(s string) (*fileMetadata, error) {\n\t\tparts := strings.Split(s, \":\")\n\t\tfileNum, err := strconv.Atoi(parts[0])\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tfields := strings.Fields(parts[1])\n\t\tparts = strings.Split(fields[0], \"-\")\n\t\tif len(parts) != 2 {\n\t\t\treturn nil, errors.Errorf(\"malformed table spec: %s. usage: <file-num>:start.SET.1-end.SET.2\", s)\n\t\t}\n\t\tm := (&fileMetadata{\n\t\t\tFileNum: base.FileNum(fileNum),\n\t\t\tSize:    1028,\n\t\t}).ExtendPointKeyBounds(\n\t\t\topts.Comparer.Compare,\n\t\t\tbase.ParseInternalKey(strings.TrimSpace(parts[0])),\n\t\t\tbase.ParseInternalKey(strings.TrimSpace(parts[1])),\n\t\t)\n\t\tm.InitPhysicalBacking()\n\t\tfor _, p := range fields[1:] {\n\t\t\tif strings.HasPrefix(p, \"size=\") {\n\t\t\t\tv, err := strconv.Atoi(strings.TrimPrefix(p, \"size=\"))\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\tm.Size = uint64(v)\n\t\t\t}\n\t\t\tif strings.HasPrefix(p, \"range-deletions-bytes-estimate=\") {\n\t\t\t\tv, err := strconv.Atoi(strings.TrimPrefix(p, \"range-deletions-bytes-estimate=\"))\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\tm.Stats.RangeDeletionsBytesEstimate = uint64(v)\n\t\t\t\tm.Stats.NumDeletions = 1 // At least one range del responsible for the deletion bytes.\n\t\t\t\tm.StatsMarkValid()\n\t\t\t}\n\t\t}\n\t\tm.SmallestSeqNum = m.Smallest.SeqNum()\n\t\tm.LargestSeqNum = m.Largest.SeqNum()\n\t\tm.LargestSeqNumAbsolute = m.LargestSeqNum\n\t\treturn m, nil\n\t}\n\n\tdatadriven.RunTest(t, \"testdata/compaction_output_file_size\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"define\":\n\t\t\tfileMetas := [manifest.NumLevels][]*fileMetadata{}\n\t\t\tlevel := 0\n\t\t\tvar err error\n\t\t\tlines := strings.Split(td.Input, \"\\n\")\n\n\t\t\tfor len(lines) > 0 {\n\t\t\t\tdata := strings.TrimSpace(lines[0])\n\t\t\t\tlines = lines[1:]\n\t\t\t\tswitch data {\n\t\t\t\tcase \"L0\", \"L1\", \"L2\", \"L3\", \"L4\", \"L5\", \"L6\":\n\t\t\t\t\tlevel, err = strconv.Atoi(data[1:])\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\tdefault:\n\t\t\t\t\tmeta, err := parseMeta(data)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t\tfileMetas[level] = append(fileMetas[level], meta)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tvers = newVersion(opts, fileMetas)\n\t\t\tvs := &versionSet{\n\t\t\t\topts: opts,\n\t\t\t\tcmp:  DefaultComparer,\n\t\t\t}\n\t\t\tvs.versions.Init(nil)\n\t\t\tvs.append(vers)\n\t\t\tvar inProgressCompactions []compactionInfo\n\t\t\tvb := manifest.MakeVirtualBackings()\n\t\t\tpicker = newCompactionPickerByScore(vers, &vb, opts, inProgressCompactions)\n\t\t\tvs.picker = picker\n\n\t\t\tvar buf bytes.Buffer\n\t\t\tfmt.Fprint(&buf, vers.String())\n\t\t\treturn buf.String()\n\n\t\tcase \"pick-auto\":\n\t\t\tpc := picker.pickAuto(compactionEnv{\n\t\t\t\tearliestUnflushedSeqNum: math.MaxUint64,\n\t\t\t\tearliestSnapshotSeqNum:  math.MaxUint64,\n\t\t\t})\n\t\t\tvar buf bytes.Buffer\n\t\t\tif pc != nil {\n\t\t\t\tfmt.Fprintf(&buf, \"L%d -> L%d\\n\", pc.startLevel.level, pc.outputLevel.level)\n\t\t\t\tfmt.Fprintf(&buf, \"L%d: %s\\n\", pc.startLevel.level, fileNums(pc.startLevel.files))\n\t\t\t\tfmt.Fprintf(&buf, \"maxOutputFileSize: %d\\n\", pc.maxOutputFileSize)\n\t\t\t} else {\n\t\t\t\treturn \"nil\"\n\t\t\t}\n\t\t\treturn buf.String()\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unrecognized command: %s\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc TestCompactionPickerCompensatedSize(t *testing.T) {\n\ttestCases := []struct {\n\t\tsize                  uint64\n\t\tpointDelEstimateBytes uint64\n\t\trangeDelEstimateBytes uint64\n\t\twantBytes             uint64\n\t}{\n\t\t{\n\t\t\tsize:                  100,\n\t\t\tpointDelEstimateBytes: 0,\n\t\t\trangeDelEstimateBytes: 0,\n\t\t\twantBytes:             100,\n\t\t},\n\t\t{\n\t\t\tsize:                  100,\n\t\t\tpointDelEstimateBytes: 10,\n\t\t\trangeDelEstimateBytes: 0,\n\t\t\twantBytes:             100 + 10,\n\t\t},\n\t\t{\n\t\t\tsize:                  100,\n\t\t\tpointDelEstimateBytes: 10,\n\t\t\trangeDelEstimateBytes: 5,\n\t\t\twantBytes:             100 + 10 + 5,\n\t\t},\n\t}\n\n\tfor _, tc := range testCases {\n\t\tt.Run(\"\", func(t *testing.T) {\n\t\t\tf := &fileMetadata{Size: tc.size}\n\t\t\tf.InitPhysicalBacking()\n\t\t\tf.Stats.PointDeletionsBytesEstimate = tc.pointDelEstimateBytes\n\t\t\tf.Stats.RangeDeletionsBytesEstimate = tc.rangeDelEstimateBytes\n\t\t\tgotBytes := compensatedSize(f)\n\t\t\trequire.Equal(t, tc.wantBytes, gotBytes)\n\t\t})\n\t}\n}\n\nfunc TestCompactionPickerPickFile(t *testing.T) {\n\tfs := vfs.NewMem()\n\topts := &Options{\n\t\tComparer:           testkeys.Comparer,\n\t\tFormatMajorVersion: FormatNewest,\n\t\tFS:                 fs,\n\t}\n\topts.Experimental.EnableColumnarBlocks = func() bool { return true }\n\n\td, err := Open(\"\", opts)\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\tif d != nil {\n\t\t\trequire.NoError(t, d.Close())\n\t\t}\n\t}()\n\n\tdatadriven.RunTest(t, \"testdata/compaction_picker_pick_file\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"define\":\n\t\t\trequire.NoError(t, d.Close())\n\n\t\t\td, err = runDBDefineCmd(td, opts)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\td.mu.Lock()\n\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\td.mu.Unlock()\n\t\t\treturn s\n\n\t\tcase \"file-sizes\":\n\t\t\treturn runTableFileSizesCmd(td, d)\n\n\t\tcase \"build\":\n\t\t\tif err := runBuildCmd(td, d, d.opts.FS); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"ingest-and-excise\":\n\t\t\tif err := runIngestAndExciseCmd(td, d, d.opts.FS); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"lsm\":\n\t\t\td.mu.Lock()\n\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\td.mu.Unlock()\n\t\t\treturn s\n\n\t\tcase \"pick-file\":\n\t\t\ts := strings.TrimPrefix(td.CmdArgs[0].String(), \"L\")\n\t\t\tlevel, err := strconv.Atoi(s)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Sprintf(\"unable to parse arg %q as level\", td.CmdArgs[0].String())\n\t\t\t}\n\t\t\tif level == 0 {\n\t\t\t\tpanic(\"L0 picking unimplemented\")\n\t\t\t}\n\t\t\td.mu.Lock()\n\t\t\tdefer d.mu.Unlock()\n\n\t\t\t// Use maybeScheduleCompactionPicker to take care of all of the\n\t\t\t// initialization of the compaction-picking environment, but never\n\t\t\t// pick a compaction; just call pickFile using the user-provided\n\t\t\t// level.\n\t\t\tvar lf manifest.LevelFile\n\t\t\tvar ok bool\n\t\t\td.maybeScheduleCompactionPicker(func(untypedPicker compactionPicker, env compactionEnv) *pickedCompaction {\n\t\t\t\tp := untypedPicker.(*compactionPickerByScore)\n\t\t\t\tlf, ok = pickCompactionSeedFile(p.vers, p.virtualBackings, opts, level, level+1, env.earliestSnapshotSeqNum)\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\tif !ok {\n\t\t\t\treturn \"(none)\"\n\t\t\t}\n\t\t\treturn lf.FileMetadata.String()\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t}\n\t})\n}\n\ntype pausableCleaner struct {\n\tmu      sync.Mutex\n\tcond    sync.Cond\n\tpaused  bool\n\tcleaner Cleaner\n}\n\nfunc (c *pausableCleaner) Clean(fs vfs.FS, fileType base.FileType, path string) error {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\tfor c.paused {\n\t\tc.cond.Wait()\n\t}\n\treturn c.cleaner.Clean(fs, fileType, path)\n}\n\nfunc (c *pausableCleaner) pause() {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\tc.paused = true\n}\n\nfunc (c *pausableCleaner) resume() {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\tc.paused = false\n\tc.cond.Broadcast()\n}\n\nfunc TestCompactionPickerScores(t *testing.T) {\n\tfs := vfs.NewMem()\n\tcleaner := pausableCleaner{cleaner: DeleteCleaner{}}\n\tcleaner.cond.L = &cleaner.mu\n\topts := &Options{\n\t\tCleaner:                     &cleaner,\n\t\tComparer:                    testkeys.Comparer,\n\t\tDisableAutomaticCompactions: true,\n\t\tFormatMajorVersion:          FormatNewest,\n\t\tFS:                          fs,\n\t}\n\topts.Experimental.EnableColumnarBlocks = func() bool { return true }\n\n\td, err := Open(\"\", opts)\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\tif d != nil {\n\t\t\tcleaner.resume()\n\t\t\trequire.NoError(t, closeAllSnapshots(d))\n\t\t\trequire.NoError(t, d.Close())\n\t\t}\n\t}()\n\n\tvar buf bytes.Buffer\n\tdatadriven.RunTest(t, \"testdata/compaction_picker_scores\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"define\":\n\t\t\trequire.NoError(t, closeAllSnapshots(d))\n\t\t\trequire.NoError(t, d.Close())\n\n\t\t\tif td.HasArg(\"pause-cleaning\") {\n\t\t\t\tcleaner.pause()\n\t\t\t}\n\n\t\t\td, err = runDBDefineCmd(td, opts)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\td.mu.Lock()\n\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\td.mu.Unlock()\n\t\t\treturn s\n\n\t\tcase \"disable-table-stats\":\n\t\t\td.mu.Lock()\n\t\t\td.opts.DisableTableStats = true\n\t\t\td.mu.Unlock()\n\t\t\treturn \"\"\n\n\t\tcase \"enable-table-stats\":\n\t\t\td.mu.Lock()\n\t\t\td.opts.DisableTableStats = false\n\t\t\td.maybeCollectTableStatsLocked()\n\t\t\td.mu.Unlock()\n\t\t\treturn \"\"\n\n\t\tcase \"resume-cleaning\":\n\t\t\tcleaner.resume()\n\t\t\treturn \"\"\n\n\t\tcase \"ingest\":\n\t\t\tif err = runBuildCmd(td, d, d.opts.FS); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tif err = runIngestCmd(td, d, d.opts.FS); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\td.mu.Lock()\n\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\td.mu.Unlock()\n\t\t\treturn s\n\n\t\tcase \"lsm\":\n\t\t\treturn runLSMCmd(td, d)\n\n\t\tcase \"maybe-compact\":\n\t\t\tbuf.Reset()\n\t\t\td.mu.Lock()\n\t\t\td.opts.DisableAutomaticCompactions = false\n\t\t\td.maybeScheduleCompaction()\n\t\t\tfmt.Fprintf(&buf, \"%d compactions in progress:\", d.mu.compact.compactingCount)\n\t\t\tfor c := range d.mu.compact.inProgress {\n\t\t\t\tfmt.Fprintf(&buf, \"\\n%s\", c)\n\t\t\t}\n\t\t\td.opts.DisableAutomaticCompactions = true\n\t\t\td.mu.Unlock()\n\t\t\treturn buf.String()\n\n\t\tcase \"scores\":\n\t\t\twaitFor := \"completion\"\n\t\t\ttd.MaybeScanArgs(t, \"wait-for-compaction\", &waitFor)\n\n\t\t\t// Wait for any running compactions to complete before calculating\n\t\t\t// scores. Otherwise, the output of this command is\n\t\t\t// nondeterministic.\n\t\t\tswitch waitFor {\n\t\t\tcase \"completion\":\n\t\t\t\td.mu.Lock()\n\t\t\t\tfor d.mu.compact.compactingCount > 0 {\n\t\t\t\t\td.mu.compact.cond.Wait()\n\t\t\t\t}\n\t\t\t\td.mu.Unlock()\n\t\t\tcase \"version-edit\":\n\t\t\t\tfunc() {\n\t\t\t\t\tfor {\n\t\t\t\t\t\td.mu.Lock()\n\t\t\t\t\t\twait := len(d.mu.compact.inProgress) > 0\n\t\t\t\t\t\tfor c := range d.mu.compact.inProgress {\n\t\t\t\t\t\t\twait = wait && !c.versionEditApplied\n\t\t\t\t\t\t}\n\t\t\t\t\t\td.mu.Unlock()\n\t\t\t\t\t\tif !wait {\n\t\t\t\t\t\t\treturn\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// d.mu.compact.cond isn't notified until the compaction\n\t\t\t\t\t\t// is removed from inProgress, so we need to just sleep\n\t\t\t\t\t\t// and check again soon.\n\t\t\t\t\t\ttime.Sleep(10 * time.Millisecond)\n\t\t\t\t\t}\n\t\t\t\t}()\n\t\t\tdefault:\n\t\t\t\tpanic(fmt.Sprintf(\"unrecognized `wait-for-compaction` value: %q\", waitFor))\n\t\t\t}\n\n\t\t\tbuf.Reset()\n\t\t\tfmt.Fprintf(&buf, \"L       Size   Score\\n\")\n\t\t\tfor l, lm := range d.Metrics().Levels {\n\t\t\t\tif l < numLevels-1 {\n\t\t\t\t\tfmt.Fprintf(&buf, \"L%-3d\\t%-7s%.1f\\n\", l, humanize.Bytes.Int64(lm.Size), lm.Score)\n\t\t\t\t} else {\n\t\t\t\t\tfmt.Fprintf(&buf, \"L%-3d\\t%-7s-\\n\", l, humanize.Bytes.Int64(lm.Size))\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn buf.String()\n\n\t\tcase \"wait-pending-table-stats\":\n\t\t\treturn runTableStatsCmd(td, d)\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc fileNums(files manifest.LevelSlice) string {\n\tvar ss []string\n\tfiles.Each(func(f *fileMetadata) {\n\t\tss = append(ss, f.FileNum.String())\n\t})\n\tsort.Strings(ss)\n\treturn strings.Join(ss, \",\")\n}\n\nfunc checkClone(t *testing.T, pc *pickedCompaction) {\n\tpcClone := pc.clone()\n\trequire.Equal(t, pc.String(), pcClone.String())\n\n\t// ensure all input files are in new address\n\tfor i := range pc.inputs {\n\t\t// Len could be zero if setup inputs rejected a level\n\t\tif pc.inputs[i].files.Len() > 0 {\n\t\t\trequire.NotEqual(t, &pc.inputs[i], &pcClone.inputs[i])\n\t\t}\n\t}\n\tfor i := range pc.startLevel.l0SublevelInfo {\n\t\tif pc.startLevel.l0SublevelInfo[i].Len() > 0 {\n\t\t\trequire.NotEqual(t, &pc.startLevel.l0SublevelInfo[i], &pcClone.startLevel.l0SublevelInfo[i])\n\t\t}\n\t}\n}\n\nfunc checkTableBoundary(a, b *fileMetadata, cmp base.Compare) (ok bool) {\n\tc := cmp(a.LargestPointKey.UserKey, b.SmallestPointKey.UserKey)\n\treturn c < 0 || (c == 0 && a.LargestPointKey.IsExclusiveSentinel())\n}\n"
        },
        {
          "name": "compaction_test.go",
          "type": "blob",
          "size": 73.630859375,
          "content": "// Copyright 2013 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\tcrand \"crypto/rand\"\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand/v2\"\n\t\"path/filepath\"\n\t\"regexp\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync/atomic\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/crlib/crstrings\"\n\t\"github.com/cockroachdb/datadriven\"\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/errors/oserror\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/compact\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n\t\"github.com/cockroachdb/pebble/internal/testkeys\"\n\t\"github.com/cockroachdb/pebble/internal/testutils\"\n\t\"github.com/cockroachdb/pebble/objstorage\"\n\t\"github.com/cockroachdb/pebble/objstorage/objstorageprovider\"\n\t\"github.com/cockroachdb/pebble/objstorage/remote\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/cockroachdb/pebble/vfs/errorfs\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc newVersion(opts *Options, files [numLevels][]*fileMetadata) *version {\n\tv := manifest.NewVersion(\n\t\topts.Comparer,\n\t\topts.FlushSplitBytes,\n\t\tfiles)\n\tif err := v.CheckOrdering(); err != nil {\n\t\tpanic(err)\n\t}\n\treturn v\n}\n\ntype compactionPickerForTesting struct {\n\tscore     float64\n\tlevel     int\n\tbaseLevel int\n\topts      *Options\n\tvers      *manifest.Version\n}\n\nvar _ compactionPicker = &compactionPickerForTesting{}\n\nfunc (p *compactionPickerForTesting) getScores([]compactionInfo) [numLevels]float64 {\n\treturn [numLevels]float64{}\n}\n\nfunc (p *compactionPickerForTesting) getBaseLevel() int {\n\treturn p.baseLevel\n}\n\nfunc (p *compactionPickerForTesting) estimatedCompactionDebt(l0ExtraSize uint64) uint64 {\n\treturn 0\n}\n\nfunc (p *compactionPickerForTesting) forceBaseLevel1() {}\n\nfunc (p *compactionPickerForTesting) pickAuto(env compactionEnv) (pc *pickedCompaction) {\n\tif p.score < 1 {\n\t\treturn nil\n\t}\n\toutputLevel := p.level + 1\n\tif p.level == 0 {\n\t\toutputLevel = p.baseLevel\n\t}\n\titer := p.vers.Levels[p.level].Iter()\n\titer.First()\n\tcInfo := candidateLevelInfo{\n\t\tlevel:       p.level,\n\t\toutputLevel: outputLevel,\n\t\tfile:        iter.Take(),\n\t}\n\tif cInfo.level == 0 {\n\t\treturn pickL0(env, p.opts, p.vers, p.baseLevel)\n\t}\n\treturn pickAutoLPositive(env, p.opts, p.vers, cInfo, p.baseLevel)\n}\n\nfunc (p *compactionPickerForTesting) pickElisionOnlyCompaction(\n\tenv compactionEnv,\n) (pc *pickedCompaction) {\n\treturn nil\n}\n\nfunc (p *compactionPickerForTesting) pickRewriteCompaction(\n\tenv compactionEnv,\n) (pc *pickedCompaction) {\n\treturn nil\n}\n\nfunc (p *compactionPickerForTesting) pickReadTriggeredCompaction(\n\tenv compactionEnv,\n) (pc *pickedCompaction) {\n\treturn nil\n}\n\nfunc TestPickCompaction(t *testing.T) {\n\tfileNums := func(files manifest.LevelSlice) string {\n\t\tvar ss []string\n\t\tfiles.Each(func(meta *fileMetadata) {\n\t\t\tss = append(ss, strconv.Itoa(int(meta.FileNum)))\n\t\t})\n\t\tsort.Strings(ss)\n\t\treturn strings.Join(ss, \",\")\n\t}\n\n\topts := (*Options)(nil).EnsureDefaults()\n\tnewFileMeta := func(fileNum FileNum, size uint64, smallest, largest base.InternalKey) *fileMetadata {\n\t\tm := (&fileMetadata{\n\t\t\tFileNum: fileNum,\n\t\t\tSize:    size,\n\t\t}).ExtendPointKeyBounds(opts.Comparer.Compare, smallest, largest)\n\t\tm.InitPhysicalBacking()\n\t\treturn m\n\t}\n\n\ttestCases := []struct {\n\t\tdesc      string\n\t\tversion   *version\n\t\tpicker    compactionPickerForTesting\n\t\twant      string\n\t\twantMulti bool\n\t}{\n\t\t{\n\t\t\tdesc: \"no compaction\",\n\t\t\tversion: newVersion(opts, [numLevels][]*fileMetadata{\n\t\t\t\t0: {\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t100,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"i.SET.101\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"j.SET.102\"),\n\t\t\t\t\t),\n\t\t\t\t},\n\t\t\t}),\n\t\t\twant: \"\",\n\t\t},\n\n\t\t{\n\t\t\tdesc: \"1 L0 file\",\n\t\t\tversion: newVersion(opts, [numLevels][]*fileMetadata{\n\t\t\t\t0: {\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t100,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"i.SET.101\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"j.SET.102\"),\n\t\t\t\t\t),\n\t\t\t\t},\n\t\t\t}),\n\t\t\tpicker: compactionPickerForTesting{\n\t\t\t\tscore:     99,\n\t\t\t\tlevel:     0,\n\t\t\t\tbaseLevel: 1,\n\t\t\t},\n\t\t\twant: \"100  \",\n\t\t},\n\n\t\t{\n\t\t\tdesc: \"2 L0 files (0 overlaps)\",\n\t\t\tversion: newVersion(opts, [numLevels][]*fileMetadata{\n\t\t\t\t0: {\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t100,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"i.SET.101\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"j.SET.102\"),\n\t\t\t\t\t),\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t110,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"k.SET.111\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"l.SET.112\"),\n\t\t\t\t\t),\n\t\t\t\t},\n\t\t\t}),\n\t\t\tpicker: compactionPickerForTesting{\n\t\t\t\tscore:     99,\n\t\t\t\tlevel:     0,\n\t\t\t\tbaseLevel: 1,\n\t\t\t},\n\t\t\twant: \"100,110  \",\n\t\t},\n\n\t\t{\n\t\t\tdesc: \"2 L0 files, with ikey overlap\",\n\t\t\tversion: newVersion(opts, [numLevels][]*fileMetadata{\n\t\t\t\t0: {\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t100,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"i.SET.101\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"p.SET.102\"),\n\t\t\t\t\t),\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t110,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"j.SET.111\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"q.SET.112\"),\n\t\t\t\t\t),\n\t\t\t\t},\n\t\t\t}),\n\t\t\tpicker: compactionPickerForTesting{\n\t\t\t\tscore:     99,\n\t\t\t\tlevel:     0,\n\t\t\t\tbaseLevel: 1,\n\t\t\t},\n\t\t\twant: \"100,110  \",\n\t\t},\n\n\t\t{\n\t\t\tdesc: \"2 L0 files, with ukey overlap\",\n\t\t\tversion: newVersion(opts, [numLevels][]*fileMetadata{\n\t\t\t\t0: {\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t100,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"i.SET.102\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"i.SET.101\"),\n\t\t\t\t\t),\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t110,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"i.SET.112\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"i.SET.111\"),\n\t\t\t\t\t),\n\t\t\t\t},\n\t\t\t}),\n\t\t\tpicker: compactionPickerForTesting{\n\t\t\t\tscore:     99,\n\t\t\t\tlevel:     0,\n\t\t\t\tbaseLevel: 1,\n\t\t\t},\n\t\t\twant: \"100,110  \",\n\t\t},\n\n\t\t{\n\t\t\tdesc: \"1 L0 file, 2 L1 files (0 overlaps)\",\n\t\t\tversion: newVersion(opts, [numLevels][]*fileMetadata{\n\t\t\t\t0: {\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t100,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"i.SET.102\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"i.SET.101\"),\n\t\t\t\t\t),\n\t\t\t\t},\n\t\t\t\t1: {\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t200,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"a.SET.201\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"b.SET.202\"),\n\t\t\t\t\t),\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t210,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"y.SET.211\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"z.SET.212\"),\n\t\t\t\t\t),\n\t\t\t\t},\n\t\t\t}),\n\t\t\tpicker: compactionPickerForTesting{\n\t\t\t\tscore:     99,\n\t\t\t\tlevel:     0,\n\t\t\t\tbaseLevel: 1,\n\t\t\t},\n\t\t\twant: \"100  \",\n\t\t},\n\n\t\t{\n\t\t\tdesc: \"1 L0 file, 2 L1 files (1 overlap), 4 L2 files (3 overlaps)\",\n\t\t\tversion: newVersion(opts, [numLevels][]*fileMetadata{\n\t\t\t\t0: {\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t100,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"i.SET.102\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"t.SET.101\"),\n\t\t\t\t\t),\n\t\t\t\t},\n\t\t\t\t1: {\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t200,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"a.SET.201\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"e.SET.202\"),\n\t\t\t\t\t),\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t210,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"f.SET.211\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"j.SET.212\"),\n\t\t\t\t\t),\n\t\t\t\t},\n\t\t\t\t2: {\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t300,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"a.SET.301\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"b.SET.302\"),\n\t\t\t\t\t),\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t310,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"c.SET.311\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"g.SET.312\"),\n\t\t\t\t\t),\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t320,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"h.SET.321\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"m.SET.322\"),\n\t\t\t\t\t),\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t330,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"n.SET.331\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"z.SET.332\"),\n\t\t\t\t\t),\n\t\t\t\t},\n\t\t\t}),\n\t\t\tpicker: compactionPickerForTesting{\n\t\t\t\tscore:     99,\n\t\t\t\tlevel:     0,\n\t\t\t\tbaseLevel: 1,\n\t\t\t},\n\t\t\twant: \"100 210 310,320,330\",\n\t\t},\n\n\t\t{\n\t\t\tdesc: \"4 L1 files, 2 L2 files, can grow\",\n\t\t\tversion: newVersion(opts, [numLevels][]*fileMetadata{\n\t\t\t\t1: {\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t200,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"i1.SET.201\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"i2.SET.202\"),\n\t\t\t\t\t),\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t210,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"j1.SET.211\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"j2.SET.212\"),\n\t\t\t\t\t),\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t220,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"k1.SET.221\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"k2.SET.222\"),\n\t\t\t\t\t),\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t230,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"l1.SET.231\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"l2.SET.232\"),\n\t\t\t\t\t),\n\t\t\t\t},\n\t\t\t\t2: {\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t300,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"a0.SET.301\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"l0.SET.302\"),\n\t\t\t\t\t),\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t310,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"l2.SET.311\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"z2.SET.312\"),\n\t\t\t\t\t),\n\t\t\t\t},\n\t\t\t}),\n\t\t\tpicker: compactionPickerForTesting{\n\t\t\t\tscore:     99,\n\t\t\t\tlevel:     1,\n\t\t\t\tbaseLevel: 1,\n\t\t\t},\n\t\t\twant:      \"200,210,220 300  \",\n\t\t\twantMulti: true,\n\t\t},\n\n\t\t{\n\t\t\tdesc: \"4 L1 files, 2 L2 files, can't grow (range)\",\n\t\t\tversion: newVersion(opts, [numLevels][]*fileMetadata{\n\t\t\t\t1: {\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t200,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"i1.SET.201\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"i2.SET.202\"),\n\t\t\t\t\t),\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t210,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"j1.SET.211\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"j2.SET.212\"),\n\t\t\t\t\t),\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t220,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"k1.SET.221\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"k2.SET.222\"),\n\t\t\t\t\t),\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t230,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"l1.SET.231\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"l2.SET.232\"),\n\t\t\t\t\t),\n\t\t\t\t},\n\t\t\t\t2: {\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t300,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"a0.SET.301\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"j0.SET.302\"),\n\t\t\t\t\t),\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t310,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"j2.SET.311\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"z2.SET.312\"),\n\t\t\t\t\t),\n\t\t\t\t},\n\t\t\t}),\n\t\t\tpicker: compactionPickerForTesting{\n\t\t\t\tscore:     99,\n\t\t\t\tlevel:     1,\n\t\t\t\tbaseLevel: 1,\n\t\t\t},\n\t\t\twant:      \"200 300  \",\n\t\t\twantMulti: true,\n\t\t},\n\n\t\t{\n\t\t\tdesc: \"4 L1 files, 2 L2 files, can't grow (size)\",\n\t\t\tversion: newVersion(opts, [numLevels][]*fileMetadata{\n\t\t\t\t1: {\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t200,\n\t\t\t\t\t\texpandedCompactionByteSizeLimit(opts, 1, math.MaxUint64)-1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"i1.SET.201\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"i2.SET.202\"),\n\t\t\t\t\t),\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t210,\n\t\t\t\t\t\texpandedCompactionByteSizeLimit(opts, 1, math.MaxUint64)-1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"j1.SET.211\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"j2.SET.212\"),\n\t\t\t\t\t),\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t220,\n\t\t\t\t\t\texpandedCompactionByteSizeLimit(opts, 1, math.MaxUint64)-1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"k1.SET.221\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"k2.SET.222\"),\n\t\t\t\t\t),\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t230,\n\t\t\t\t\t\texpandedCompactionByteSizeLimit(opts, 1, math.MaxUint64)-1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"l1.SET.231\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"l2.SET.232\"),\n\t\t\t\t\t),\n\t\t\t\t},\n\t\t\t\t2: {\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t300,\n\t\t\t\t\t\texpandedCompactionByteSizeLimit(opts, 2, math.MaxUint64)-1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"a0.SET.301\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"l0.SET.302\"),\n\t\t\t\t\t),\n\t\t\t\t\tnewFileMeta(\n\t\t\t\t\t\t310,\n\t\t\t\t\t\texpandedCompactionByteSizeLimit(opts, 2, math.MaxUint64)-1,\n\t\t\t\t\t\tbase.ParseInternalKey(\"l2.SET.311\"),\n\t\t\t\t\t\tbase.ParseInternalKey(\"z2.SET.312\"),\n\t\t\t\t\t),\n\t\t\t\t},\n\t\t\t}),\n\t\t\tpicker: compactionPickerForTesting{\n\t\t\t\tscore:     99,\n\t\t\t\tlevel:     1,\n\t\t\t\tbaseLevel: 1,\n\t\t\t},\n\t\t\twant: \"200 300 \",\n\t\t},\n\t}\n\n\tfor _, tc := range testCases {\n\t\tvs := &versionSet{\n\t\t\topts: opts,\n\t\t\tcmp:  DefaultComparer,\n\t\t}\n\t\tvs.versions.Init(nil)\n\t\tvs.append(tc.version)\n\t\ttc.picker.opts = opts\n\t\ttc.picker.vers = tc.version\n\t\tvs.picker = &tc.picker\n\t\tpc, got := vs.picker.pickAuto(compactionEnv{diskAvailBytes: math.MaxUint64}), \"\"\n\t\tif pc != nil {\n\t\t\tc := newCompaction(pc, opts, time.Now(), nil /* provider */, nil /* slot */)\n\n\t\t\tgotStart := fileNums(c.startLevel.files)\n\t\t\tgotML := \"\"\n\t\t\tobservedMulti := len(c.extraLevels) > 0\n\t\t\tif observedMulti {\n\t\t\t\tgotML = \" \" + fileNums(c.extraLevels[0].files)\n\t\t\t}\n\t\t\tgotOutput := \" \" + fileNums(c.outputLevel.files)\n\t\t\tgotGrandparents := \" \" + fileNums(c.grandparents)\n\t\t\tgot = gotStart + gotML + gotOutput + gotGrandparents\n\t\t\tif tc.wantMulti != observedMulti {\n\t\t\t\tt.Fatalf(\"Expected Multi %t; Observed Multi %t, for %s\", tc.wantMulti, observedMulti, got)\n\t\t\t}\n\n\t\t}\n\t\tif got != tc.want {\n\t\t\tt.Fatalf(\"%s:\\ngot  %q\\nwant %q\", tc.desc, got, tc.want)\n\t\t}\n\t}\n}\n\ntype cpuPermissionGranter struct {\n\t// requestCount is used to confirm that every GetPermission function call\n\t// has a corresponding CPUWorkDone function call.\n\trequestCount int\n\tused         bool\n\tpermit       bool\n}\n\ntype cpuWorkHandle struct {\n\tpermit bool\n}\n\nfunc (c cpuWorkHandle) Permitted() bool {\n\treturn c.permit\n}\n\nfunc (t *cpuPermissionGranter) GetPermission(dur time.Duration) CPUWorkHandle {\n\tt.requestCount++\n\tt.used = true\n\treturn cpuWorkHandle{t.permit}\n}\n\nfunc (t *cpuPermissionGranter) CPUWorkDone(_ CPUWorkHandle) {\n\tt.requestCount--\n}\n\n// Simple test to check if compactions are using the granter, and if exactly\n// the acquired handles are returned.\nfunc TestCompactionCPUGranter(t *testing.T) {\n\tmem := vfs.NewMem()\n\topts := (&Options{FS: mem}).WithFSDefaults()\n\tg := &cpuPermissionGranter{permit: true}\n\topts.Experimental.CPUWorkPermissionGranter = g\n\td, err := Open(\"\", opts)\n\tif err != nil {\n\t\tt.Fatalf(\"Open: %v\", err)\n\t}\n\tdefer d.Close()\n\n\td.Set([]byte{'a'}, []byte{'a'}, nil)\n\terr = d.Compact([]byte{'a'}, []byte{'b'}, true)\n\tif err != nil {\n\t\tt.Fatalf(\"Compact: %v\", err)\n\t}\n\trequire.True(t, g.used)\n\trequire.Equal(t, g.requestCount, 0)\n}\n\n// Tests that there's no errors or panics when the default CPU granter is used.\nfunc TestCompactionCPUGranterDefault(t *testing.T) {\n\tmem := vfs.NewMem()\n\topts := (&Options{FS: mem}).WithFSDefaults()\n\td, err := Open(\"\", opts)\n\tif err != nil {\n\t\tt.Fatalf(\"Open: %v\", err)\n\t}\n\tdefer d.Close()\n\n\td.Set([]byte{'a'}, []byte{'a'}, nil)\n\terr = d.Compact([]byte{'a'}, []byte{'b'}, true)\n\tif err != nil {\n\t\tt.Fatalf(\"Compact: %v\", err)\n\t}\n}\n\nfunc TestCompaction(t *testing.T) {\n\tconst memTableSize = 10000\n\t// Tuned so that 2 values can reside in the memtable before a flush, but a\n\t// 3rd value will cause a flush. Needs to account for the max skiplist node\n\t// size.\n\tconst valueSize = 3500\n\n\tmem := vfs.NewMem()\n\topts := &Options{\n\t\tComparer:              testkeys.Comparer,\n\t\tDebugCheck:            DebugCheckLevels,\n\t\tFS:                    mem,\n\t\tL0CompactionThreshold: 8,\n\t\tMemTableSize:          memTableSize,\n\t}\n\topts.testingRandomized(t).WithFSDefaults()\n\td, err := Open(\"\", opts)\n\tif err != nil {\n\t\tt.Fatalf(\"Open: %v\", err)\n\t}\n\n\tget1 := func(iter internalIterator) (ret string) {\n\t\tb := &bytes.Buffer{}\n\t\tfor kv := iter.First(); kv != nil; kv = iter.Next() {\n\t\t\tb.Write(kv.K.UserKey)\n\t\t}\n\t\tif err := iter.Close(); err != nil {\n\t\t\tt.Fatalf(\"iterator Close: %v\", err)\n\t\t}\n\t\treturn b.String()\n\t}\n\tgetAll := func() (gotMem, gotDisk string, err error) {\n\t\td.mu.Lock()\n\t\tdefer d.mu.Unlock()\n\n\t\tif d.mu.mem.mutable != nil {\n\t\t\tgotMem = get1(d.mu.mem.mutable.newIter(nil))\n\t\t}\n\t\tss := []string(nil)\n\t\tv := d.mu.versions.currentVersion()\n\t\tprovider, err := objstorageprovider.Open(objstorageprovider.DefaultSettings(mem, \"\" /* dirName */))\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"%v\", err)\n\t\t}\n\t\tdefer provider.Close()\n\t\tfor _, levelMetadata := range v.Levels {\n\t\t\titer := levelMetadata.Iter()\n\t\t\tfor meta := iter.First(); meta != nil; meta = iter.Next() {\n\t\t\t\tif meta.Virtual {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tf, err := provider.OpenForReading(context.Background(), base.FileTypeTable, meta.FileBacking.DiskFileNum, objstorage.OpenOptions{})\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn \"\", \"\", errors.WithStack(err)\n\t\t\t\t}\n\t\t\t\tr, err := sstable.NewReader(context.Background(), f, opts.MakeReaderOptions())\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn \"\", \"\", errors.WithStack(err)\n\t\t\t\t}\n\t\t\t\tdefer r.Close()\n\t\t\t\titer, err := r.NewIter(sstable.NoTransforms, nil /* lower */, nil /* upper */)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn \"\", \"\", errors.WithStack(err)\n\t\t\t\t}\n\t\t\t\tss = append(ss, get1(iter)+\".\")\n\t\t\t}\n\t\t}\n\t\tsort.Strings(ss)\n\t\treturn gotMem, strings.Join(ss, \"\"), nil\n\t}\n\n\tvalue := bytes.Repeat([]byte(\"x\"), valueSize)\n\ttestCases := []struct {\n\t\tkey, wantMem, wantDisk string\n\t}{\n\t\t{\"+A\", \"A\", \"\"},\n\t\t{\"+a\", \"Aa\", \"\"},\n\t\t{\"+B\", \"B\", \"Aa.\"},\n\t\t{\"+b\", \"Bb\", \"Aa.\"},\n\t\t// The next level-0 table overwrites the B key.\n\t\t{\"+C\", \"C\", \"Aa.Bb.\"},\n\t\t{\"+B\", \"BC\", \"Aa.Bb.\"},\n\t\t// The next level-0 table deletes the a key.\n\t\t{\"+D\", \"D\", \"Aa.BC.Bb.\"},\n\t\t{\"-a\", \"Da\", \"Aa.BC.Bb.\"},\n\t\t{\"+d\", \"Dad\", \"Aa.BC.Bb.\"},\n\t\t{\"+E\", \"E\", \"Aa.BC.Bb.Dad.\"},\n\t\t{\"+e\", \"Ee\", \"Aa.BC.Bb.Dad.\"},\n\t\t// The next addition creates the fourth level-0 table, and l0CompactionTrigger == 8,\n\t\t// but since the sublevel count is doubled when comparing with l0CompactionTrigger,\n\t\t// the addition of the 4th sublevel triggers a non-trivial compaction into one level-1 table.\n\t\t// Note that the keys in this one larger table are interleaved from the four smaller ones.\n\t\t{\"+F\", \"F\", \"ABCDEbde.\"},\n\t}\n\tfor _, tc := range testCases {\n\t\tif key := tc.key[1:]; tc.key[0] == '+' {\n\t\t\tif err := d.Set([]byte(key), value, nil); err != nil {\n\t\t\t\tt.Errorf(\"%q: Set: %v\", key, err)\n\t\t\t\tbreak\n\t\t\t}\n\t\t} else {\n\t\t\tif err := d.Delete([]byte(key), nil); err != nil {\n\t\t\t\tt.Errorf(\"%q: Delete: %v\", key, err)\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\n\t\t// try backs off to allow any writes to the memfs to complete.\n\t\terr := try(100*time.Microsecond, 20*time.Second, func() error {\n\t\t\tgotMem, gotDisk, err := getAll()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif testing.Verbose() {\n\t\t\t\tfmt.Printf(\"mem=%s (%s) disk=%s (%s)\\n\", gotMem, tc.wantMem, gotDisk, tc.wantDisk)\n\t\t\t}\n\n\t\t\tif gotMem != tc.wantMem {\n\t\t\t\treturn errors.Errorf(\"mem: got %q, want %q\", gotMem, tc.wantMem)\n\t\t\t}\n\t\t\tif gotDisk != tc.wantDisk {\n\t\t\t\treturn errors.Errorf(\"ldb: got %q, want %q\", gotDisk, tc.wantDisk)\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\t\tif err != nil {\n\t\t\tt.Errorf(\"%q: %v\", tc.key, err)\n\t\t}\n\t}\n\tif err := d.Close(); err != nil {\n\t\tt.Fatalf(\"db Close: %v\", err)\n\t}\n}\n\nfunc TestValidateVersionEdit(t *testing.T) {\n\tconst badKey = \"malformed-key\"\n\n\terrValidationFailed := errors.New(\"validation failed\")\n\tvalidateFn := func(key []byte) error {\n\t\tif string(key) == badKey {\n\t\t\treturn errValidationFailed\n\t\t}\n\t\treturn nil\n\t}\n\n\tcmp := DefaultComparer.Compare\n\tnewFileMeta := func(smallest, largest base.InternalKey) *fileMetadata {\n\t\tm := (&fileMetadata{}).ExtendPointKeyBounds(cmp, smallest, largest)\n\t\tm.InitPhysicalBacking()\n\t\treturn m\n\t}\n\n\ttestCases := []struct {\n\t\tdesc    string\n\t\tve      *versionEdit\n\t\tvFunc   func([]byte) error\n\t\twantErr error\n\t}{\n\t\t{\n\t\t\tdesc: \"single new file; start key\",\n\t\t\tve: &versionEdit{\n\t\t\t\tNewFiles: []manifest.NewFileEntry{\n\t\t\t\t\t{\n\t\t\t\t\t\tMeta: newFileMeta(\n\t\t\t\t\t\t\tmanifest.InternalKey{UserKey: []byte(badKey)},\n\t\t\t\t\t\t\tmanifest.InternalKey{UserKey: []byte(\"z\")},\n\t\t\t\t\t\t),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\tvFunc:   validateFn,\n\t\t\twantErr: errValidationFailed,\n\t\t},\n\t\t{\n\t\t\tdesc: \"single new file; end key\",\n\t\t\tve: &versionEdit{\n\t\t\t\tNewFiles: []manifest.NewFileEntry{\n\t\t\t\t\t{\n\t\t\t\t\t\tMeta: newFileMeta(\n\t\t\t\t\t\t\tmanifest.InternalKey{UserKey: []byte(\"a\")},\n\t\t\t\t\t\t\tmanifest.InternalKey{UserKey: []byte(badKey)},\n\t\t\t\t\t\t),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\tvFunc:   validateFn,\n\t\t\twantErr: errValidationFailed,\n\t\t},\n\t\t{\n\t\t\tdesc: \"multiple new files\",\n\t\t\tve: &versionEdit{\n\t\t\t\tNewFiles: []manifest.NewFileEntry{\n\t\t\t\t\t{\n\t\t\t\t\t\tMeta: newFileMeta(\n\t\t\t\t\t\t\tmanifest.InternalKey{UserKey: []byte(\"a\")},\n\t\t\t\t\t\t\tmanifest.InternalKey{UserKey: []byte(\"c\")},\n\t\t\t\t\t\t),\n\t\t\t\t\t},\n\t\t\t\t\t{\n\t\t\t\t\t\tMeta: newFileMeta(\n\t\t\t\t\t\t\tmanifest.InternalKey{UserKey: []byte(badKey)},\n\t\t\t\t\t\t\tmanifest.InternalKey{UserKey: []byte(\"z\")},\n\t\t\t\t\t\t),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\tvFunc:   validateFn,\n\t\t\twantErr: errValidationFailed,\n\t\t},\n\t\t{\n\t\t\tdesc: \"single deleted file; start key\",\n\t\t\tve: &versionEdit{\n\t\t\t\tDeletedFiles: map[manifest.DeletedFileEntry]*manifest.FileMetadata{\n\t\t\t\t\tdeletedFileEntry{Level: 0, FileNum: 0}: newFileMeta(\n\t\t\t\t\t\tmanifest.InternalKey{UserKey: []byte(badKey)},\n\t\t\t\t\t\tmanifest.InternalKey{UserKey: []byte(\"z\")},\n\t\t\t\t\t),\n\t\t\t\t},\n\t\t\t},\n\t\t\tvFunc:   validateFn,\n\t\t\twantErr: errValidationFailed,\n\t\t},\n\t\t{\n\t\t\tdesc: \"single deleted file; end key\",\n\t\t\tve: &versionEdit{\n\t\t\t\tDeletedFiles: map[manifest.DeletedFileEntry]*manifest.FileMetadata{\n\t\t\t\t\tdeletedFileEntry{Level: 0, FileNum: 0}: newFileMeta(\n\t\t\t\t\t\tmanifest.InternalKey{UserKey: []byte(\"a\")},\n\t\t\t\t\t\tmanifest.InternalKey{UserKey: []byte(badKey)},\n\t\t\t\t\t),\n\t\t\t\t},\n\t\t\t},\n\t\t\tvFunc:   validateFn,\n\t\t\twantErr: errValidationFailed,\n\t\t},\n\t\t{\n\t\t\tdesc: \"multiple deleted files\",\n\t\t\tve: &versionEdit{\n\t\t\t\tDeletedFiles: map[manifest.DeletedFileEntry]*manifest.FileMetadata{\n\t\t\t\t\tdeletedFileEntry{Level: 0, FileNum: 0}: newFileMeta(\n\t\t\t\t\t\tmanifest.InternalKey{UserKey: []byte(\"a\")},\n\t\t\t\t\t\tmanifest.InternalKey{UserKey: []byte(\"c\")},\n\t\t\t\t\t),\n\t\t\t\t\tdeletedFileEntry{Level: 0, FileNum: 1}: newFileMeta(\n\t\t\t\t\t\tmanifest.InternalKey{UserKey: []byte(badKey)},\n\t\t\t\t\t\tmanifest.InternalKey{UserKey: []byte(\"z\")},\n\t\t\t\t\t),\n\t\t\t\t},\n\t\t\t},\n\t\t\tvFunc:   validateFn,\n\t\t\twantErr: errValidationFailed,\n\t\t},\n\t\t{\n\t\t\tdesc: \"no errors\",\n\t\t\tve: &versionEdit{\n\t\t\t\tNewFiles: []manifest.NewFileEntry{\n\t\t\t\t\t{\n\t\t\t\t\t\tLevel: 0,\n\t\t\t\t\t\tMeta: newFileMeta(\n\t\t\t\t\t\t\tmanifest.InternalKey{UserKey: []byte(\"b\")},\n\t\t\t\t\t\t\tmanifest.InternalKey{UserKey: []byte(\"c\")},\n\t\t\t\t\t\t),\n\t\t\t\t\t},\n\t\t\t\t\t{\n\t\t\t\t\t\tLevel: 0,\n\t\t\t\t\t\tMeta: newFileMeta(\n\t\t\t\t\t\t\tmanifest.InternalKey{UserKey: []byte(\"d\")},\n\t\t\t\t\t\t\tmanifest.InternalKey{UserKey: []byte(\"g\")},\n\t\t\t\t\t\t),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\tDeletedFiles: map[manifest.DeletedFileEntry]*manifest.FileMetadata{\n\t\t\t\t\tdeletedFileEntry{Level: 6, FileNum: 0}: newFileMeta(\n\t\t\t\t\t\tmanifest.InternalKey{UserKey: []byte(\"a\")},\n\t\t\t\t\t\tmanifest.InternalKey{UserKey: []byte(\"d\")},\n\t\t\t\t\t),\n\t\t\t\t\tdeletedFileEntry{Level: 6, FileNum: 1}: newFileMeta(\n\t\t\t\t\t\tmanifest.InternalKey{UserKey: []byte(\"x\")},\n\t\t\t\t\t\tmanifest.InternalKey{UserKey: []byte(\"z\")},\n\t\t\t\t\t),\n\t\t\t\t},\n\t\t\t},\n\t\t\tvFunc: validateFn,\n\t\t},\n\t}\n\n\tfor _, tc := range testCases {\n\t\tt.Run(tc.desc, func(t *testing.T) {\n\t\t\tlogger := &base.InMemLogger{}\n\t\t\tvalidateVersionEdit(tc.ve, tc.vFunc, base.DefaultFormatter, logger)\n\t\t\tmsg := strings.TrimSpace(logger.String())\n\t\t\tif tc.wantErr != nil {\n\t\t\t\tif !strings.Contains(msg, tc.wantErr.Error()) {\n\t\t\t\t\tt.Fatalf(\"got: %q; want: %s\", msg, tc.wantErr)\n\t\t\t\t}\n\t\t\t} else if msg != \"\" {\n\t\t\t\tt.Fatalf(\"got %s; wanted no error\", msg)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestManualCompaction(t *testing.T) {\n\tvar mem vfs.FS\n\tvar d *DB\n\tdefer func() {\n\t\tif d != nil {\n\t\t\trequire.NoError(t, closeAllSnapshots(d))\n\t\t\trequire.NoError(t, d.Close())\n\t\t}\n\t}()\n\n\tseed := uint64(time.Now().UnixNano())\n\trng := rand.New(rand.NewPCG(0, seed))\n\tt.Logf(\"seed: %d\", seed)\n\n\trandVersion := func(min, max FormatMajorVersion) FormatMajorVersion {\n\t\treturn FormatMajorVersion(int(min) + rng.IntN(int(max)-int(min)+1))\n\t}\n\n\tvar compactionLog bytes.Buffer\n\tcompactionLogEventListener := &EventListener{\n\t\tCompactionEnd: func(info CompactionInfo) {\n\t\t\t// Ensure determinism.\n\t\t\tinfo.JobID = 1\n\t\t\tinfo.Duration = time.Second\n\t\t\tinfo.TotalDuration = time.Second\n\t\t\tfmt.Fprintln(&compactionLog, info.String())\n\t\t},\n\t}\n\treset := func(minVersion, maxVersion FormatMajorVersion) {\n\t\tcompactionLog.Reset()\n\t\tif d != nil {\n\t\t\trequire.NoError(t, closeAllSnapshots(d))\n\t\t\trequire.NoError(t, d.Close())\n\t\t}\n\t\tmem = vfs.NewMem()\n\t\trequire.NoError(t, mem.MkdirAll(\"ext\", 0755))\n\n\t\topts := (&Options{\n\t\t\tFS:                          mem,\n\t\t\tDebugCheck:                  DebugCheckLevels,\n\t\t\tDisableAutomaticCompactions: true,\n\t\t\tEventListener:               compactionLogEventListener,\n\t\t\tFormatMajorVersion:          randVersion(minVersion, maxVersion),\n\t\t}).WithFSDefaults()\n\t\topts.Experimental.EnableColumnarBlocks = func() bool { return true }\n\n\t\tvar err error\n\t\td, err = Open(\"\", opts)\n\t\trequire.NoError(t, err)\n\t}\n\n\t// d.mu must be held when calling.\n\tcreateOngoingCompaction := func(start, end []byte, startLevel, outputLevel int) (ongoingCompaction *compaction) {\n\t\tongoingCompaction = &compaction{\n\t\t\tinputs:   []compactionLevel{{level: startLevel}, {level: outputLevel}},\n\t\t\tsmallest: InternalKey{UserKey: start},\n\t\t\tlargest:  InternalKey{UserKey: end},\n\t\t}\n\t\tongoingCompaction.startLevel = &ongoingCompaction.inputs[0]\n\t\tongoingCompaction.outputLevel = &ongoingCompaction.inputs[1]\n\t\t// Mark files as compacting.\n\t\tcurr := d.mu.versions.currentVersion()\n\t\tongoingCompaction.startLevel.files = curr.Overlaps(startLevel, base.UserKeyBoundsInclusive(start, end))\n\t\tongoingCompaction.outputLevel.files = curr.Overlaps(outputLevel, base.UserKeyBoundsInclusive(start, end))\n\t\tfor _, cl := range ongoingCompaction.inputs {\n\t\t\titer := cl.files.Iter()\n\t\t\tfor f := iter.First(); f != nil; f = iter.Next() {\n\t\t\t\tf.CompactionState = manifest.CompactionStateCompacting\n\t\t\t}\n\t\t}\n\t\td.mu.compact.inProgress[ongoingCompaction] = struct{}{}\n\t\td.mu.compact.compactingCount++\n\t\treturn\n\t}\n\n\t// d.mu must be held when calling.\n\tdeleteOngoingCompaction := func(ongoingCompaction *compaction) {\n\t\tfor _, cl := range ongoingCompaction.inputs {\n\t\t\titer := cl.files.Iter()\n\t\t\tfor f := iter.First(); f != nil; f = iter.Next() {\n\t\t\t\tf.CompactionState = manifest.CompactionStateNotCompacting\n\t\t\t}\n\t\t}\n\t\tdelete(d.mu.compact.inProgress, ongoingCompaction)\n\t\td.mu.compact.compactingCount--\n\t}\n\n\trunTest := func(t *testing.T, testData string, minVersion, maxVersion FormatMajorVersion, verbose bool) {\n\t\treset(minVersion, maxVersion)\n\t\tvar ongoingCompaction *compaction\n\t\tdatadriven.RunTest(t, testData, func(t *testing.T, td *datadriven.TestData) string {\n\t\t\tswitch td.Cmd {\n\t\t\tcase \"reset\":\n\t\t\t\treset(minVersion, maxVersion)\n\t\t\t\treturn \"\"\n\n\t\t\tcase \"batch\":\n\t\t\t\tb := d.NewIndexedBatch()\n\t\t\t\tif err := runBatchDefineCmd(td, b); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\trequire.NoError(t, b.Commit(nil))\n\t\t\t\treturn \"\"\n\n\t\t\tcase \"build\":\n\t\t\t\tif err := runBuildCmd(td, d, mem); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\treturn \"\"\n\n\t\t\tcase \"compact\":\n\t\t\t\tif err := runCompactCmd(td, d); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\td.mu.Lock()\n\t\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\t\tif verbose {\n\t\t\t\t\ts = d.mu.versions.currentVersion().DebugString()\n\t\t\t\t}\n\t\t\t\td.mu.Unlock()\n\t\t\t\tif td.HasArg(\"hide-file-num\") {\n\t\t\t\t\tre := regexp.MustCompile(`([0-9]*):\\[`)\n\t\t\t\t\ts = re.ReplaceAllString(s, \"[\")\n\t\t\t\t}\n\t\t\t\tif td.HasArg(\"hide-size\") {\n\t\t\t\t\tre := regexp.MustCompile(` size:([0-9]*)`)\n\t\t\t\t\ts = re.ReplaceAllString(s, \"\")\n\t\t\t\t}\n\t\t\t\treturn s\n\n\t\t\tcase \"define\":\n\t\t\t\tif d != nil {\n\t\t\t\t\tif err := closeAllSnapshots(d); err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t\tif err := d.Close(); err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tmem = vfs.NewMem()\n\t\t\t\topts := (&Options{\n\t\t\t\t\tFS:                          mem,\n\t\t\t\t\tDebugCheck:                  DebugCheckLevels,\n\t\t\t\t\tEventListener:               compactionLogEventListener,\n\t\t\t\t\tFormatMajorVersion:          randVersion(minVersion, maxVersion),\n\t\t\t\t\tDisableAutomaticCompactions: true,\n\t\t\t\t}).WithFSDefaults()\n\t\t\t\topts.Experimental.EnableColumnarBlocks = func() bool { return true }\n\n\t\t\t\tvar err error\n\t\t\t\tif d, err = runDBDefineCmd(td, opts); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\n\t\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\t\tif verbose {\n\t\t\t\t\ts = d.mu.versions.currentVersion().DebugString()\n\t\t\t\t}\n\t\t\t\tif td.HasArg(\"hide-size\") {\n\t\t\t\t\tre := regexp.MustCompile(` size:([0-9]*)`)\n\t\t\t\t\ts = re.ReplaceAllString(s, \"\")\n\t\t\t\t}\n\t\t\t\treturn s\n\n\t\t\tcase \"file-sizes\":\n\t\t\t\treturn runTableFileSizesCmd(td, d)\n\n\t\t\tcase \"flush\":\n\t\t\t\tif err := d.Flush(); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\td.mu.Lock()\n\t\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\t\tif verbose {\n\t\t\t\t\ts = d.mu.versions.currentVersion().DebugString()\n\t\t\t\t}\n\t\t\t\td.mu.Unlock()\n\t\t\t\treturn s\n\n\t\t\tcase \"ingest\":\n\t\t\t\tif err := runIngestCmd(td, d, mem); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\td.mu.Lock()\n\t\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\t\tif verbose {\n\t\t\t\t\ts = d.mu.versions.currentVersion().DebugString()\n\t\t\t\t}\n\t\t\t\td.mu.Unlock()\n\t\t\t\treturn s\n\n\t\t\tcase \"iter\":\n\t\t\t\t// TODO(peter): runDBDefineCmd doesn't properly update the visible\n\t\t\t\t// sequence number. So we have to use a snapshot with a very large\n\t\t\t\t// sequence number, otherwise the DB appears empty.\n\t\t\t\tsnap := Snapshot{\n\t\t\t\t\tdb:     d,\n\t\t\t\t\tseqNum: base.SeqNumMax,\n\t\t\t\t}\n\t\t\t\titer, _ := snap.NewIter(nil)\n\t\t\t\treturn runIterCmd(td, iter, true)\n\n\t\t\tcase \"lsm\":\n\t\t\t\treturn runLSMCmd(td, d)\n\n\t\t\tcase \"populate\":\n\t\t\t\tb := d.NewBatch()\n\t\t\t\trunPopulateCmd(t, td, b)\n\t\t\t\tcount := b.Count()\n\t\t\t\trequire.NoError(t, b.Commit(nil))\n\t\t\t\treturn fmt.Sprintf(\"wrote %d keys\\n\", count)\n\n\t\t\tcase \"async-compact\":\n\t\t\t\tvar s string\n\t\t\t\tch := make(chan error, 1)\n\t\t\t\tgo func() {\n\t\t\t\t\tif err := runCompactCmd(td, d); err != nil {\n\t\t\t\t\t\tch <- err\n\t\t\t\t\t\tclose(ch)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\td.mu.Lock()\n\t\t\t\t\ts = d.mu.versions.currentVersion().String()\n\t\t\t\t\td.mu.Unlock()\n\t\t\t\t\tclose(ch)\n\t\t\t\t}()\n\n\t\t\t\tmanualDone := func() bool {\n\t\t\t\t\tselect {\n\t\t\t\t\tcase <-ch:\n\t\t\t\t\t\treturn true\n\t\t\t\t\tdefault:\n\t\t\t\t\t\treturn false\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\terr := try(100*time.Microsecond, 20*time.Second, func() error {\n\t\t\t\t\tif manualDone() {\n\t\t\t\t\t\treturn nil\n\t\t\t\t\t}\n\n\t\t\t\t\td.mu.Lock()\n\t\t\t\t\tdefer d.mu.Unlock()\n\t\t\t\t\tif len(d.mu.compact.manual) == 0 {\n\t\t\t\t\t\treturn errors.New(\"no manual compaction queued\")\n\t\t\t\t\t}\n\t\t\t\t\tmanual := d.mu.compact.manual[0]\n\t\t\t\t\tif manual.retries == 0 {\n\t\t\t\t\t\treturn errors.New(\"manual compaction has not been retried\")\n\t\t\t\t\t}\n\t\t\t\t\treturn nil\n\t\t\t\t})\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\n\t\t\t\tif manualDone() {\n\t\t\t\t\treturn \"manual compaction did not block for ongoing\\n\" + s\n\t\t\t\t}\n\n\t\t\t\td.mu.Lock()\n\t\t\t\tdeleteOngoingCompaction(ongoingCompaction)\n\t\t\t\tongoingCompaction = nil\n\t\t\t\td.maybeScheduleCompaction()\n\t\t\t\td.mu.Unlock()\n\t\t\t\tif err := <-ch; err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\treturn \"manual compaction blocked until ongoing finished\\n\" + s\n\n\t\t\tcase \"add-ongoing-compaction\":\n\t\t\t\tvar startLevel int\n\t\t\t\tvar outputLevel int\n\t\t\t\tvar start string\n\t\t\t\tvar end string\n\t\t\t\ttd.ScanArgs(t, \"startLevel\", &startLevel)\n\t\t\t\ttd.ScanArgs(t, \"outputLevel\", &outputLevel)\n\t\t\t\ttd.ScanArgs(t, \"start\", &start)\n\t\t\t\ttd.ScanArgs(t, \"end\", &end)\n\t\t\t\td.mu.Lock()\n\t\t\t\tongoingCompaction = createOngoingCompaction([]byte(start), []byte(end), startLevel, outputLevel)\n\t\t\t\td.mu.Unlock()\n\t\t\t\treturn \"\"\n\n\t\t\tcase \"remove-ongoing-compaction\":\n\t\t\t\td.mu.Lock()\n\t\t\t\tdeleteOngoingCompaction(ongoingCompaction)\n\t\t\t\tongoingCompaction = nil\n\t\t\t\td.mu.Unlock()\n\t\t\t\treturn \"\"\n\n\t\t\tcase \"set-concurrent-compactions\":\n\t\t\t\tvar concurrentCompactions int\n\t\t\t\ttd.ScanArgs(t, \"num\", &concurrentCompactions)\n\t\t\t\td.opts.MaxConcurrentCompactions = func() int {\n\t\t\t\t\treturn concurrentCompactions\n\t\t\t\t}\n\t\t\t\treturn \"\"\n\n\t\t\tcase \"sstable-properties\":\n\t\t\t\treturn runSSTablePropertiesCmd(t, td, d)\n\n\t\t\tcase \"wait-pending-table-stats\":\n\t\t\t\treturn runTableStatsCmd(td, d)\n\n\t\t\tcase \"close-snapshots\":\n\t\t\t\td.mu.Lock()\n\t\t\t\t// Re-enable automatic compactions if they were disabled so that\n\t\t\t\t// closing snapshots can trigger elision-only compactions if\n\t\t\t\t// necessary.\n\t\t\t\td.opts.DisableAutomaticCompactions = false\n\n\t\t\t\tvar ss []*Snapshot\n\t\t\t\tl := &d.mu.snapshots\n\t\t\t\tfor i := l.root.next; i != &l.root; i = i.next {\n\t\t\t\t\tss = append(ss, i)\n\t\t\t\t}\n\t\t\t\td.mu.Unlock()\n\t\t\t\tfor i := range ss {\n\t\t\t\t\tif err := ss[i].Close(); err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn \"\"\n\n\t\t\tcase \"compaction-log\":\n\t\t\t\tdefer compactionLog.Reset()\n\t\t\t\treturn compactionLog.String()\n\n\t\t\tdefault:\n\t\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t\t}\n\t\t})\n\t}\n\n\ttestCases := []struct {\n\t\ttestData   string\n\t\tminVersion FormatMajorVersion // inclusive, FormatMinSupported if unspecified.\n\t\tmaxVersion FormatMajorVersion // inclusive, internalFormatNewest if unspecified.\n\t\tverbose    bool\n\t}{\n\t\t{\n\t\t\ttestData: \"testdata/singledel_manual_compaction_set_with_del\",\n\t\t},\n\t\t{\n\t\t\ttestData: \"testdata/manual_compaction_range_keys\",\n\t\t\tverbose:  true,\n\t\t},\n\t\t{\n\t\t\ttestData:   \"testdata/manual_compaction_file_boundaries_delsized\",\n\t\t\tminVersion: FormatDeleteSizedAndObsolete,\n\t\t\tmaxVersion: FormatFlushableIngestExcises,\n\t\t},\n\t\t{\n\t\t\ttestData:   \"testdata/manual_compaction_set_with_del_sstable_Pebblev4\",\n\t\t\tminVersion: FormatDeleteSizedAndObsolete,\n\t\t\tmaxVersion: FormatFlushableIngestExcises,\n\t\t},\n\t\t{\n\t\t\ttestData: \"testdata/manual_compaction_multilevel\",\n\t\t},\n\t\t{\n\t\t\ttestData:   \"testdata/manual_compaction_set_with_del_sstable_Pebblev5\",\n\t\t\tminVersion: FormatColumnarBlocks,\n\t\t},\n\t}\n\n\tfor _, tc := range testCases {\n\t\tt.Run(tc.testData, func(t *testing.T) {\n\t\t\tminVersion, maxVersion := tc.minVersion, tc.maxVersion\n\t\t\tif minVersion == 0 {\n\t\t\t\tminVersion = FormatMinSupported\n\t\t\t}\n\t\t\tif maxVersion == 0 {\n\t\t\t\tmaxVersion = internalFormatNewest\n\t\t\t}\n\t\t\trunTest(t, tc.testData, minVersion, maxVersion, tc.verbose)\n\t\t})\n\t}\n}\n\nfunc TestCompactionOutputLevel(t *testing.T) {\n\topts := (*Options)(nil).EnsureDefaults()\n\tversion := manifest.TestingNewVersion(opts.Comparer)\n\n\tdatadriven.RunTest(t, \"testdata/compaction_output_level\",\n\t\tfunc(t *testing.T, d *datadriven.TestData) (res string) {\n\t\t\tdefer func() {\n\t\t\t\tif r := recover(); r != nil {\n\t\t\t\t\tres = fmt.Sprintln(r)\n\t\t\t\t}\n\t\t\t}()\n\n\t\t\tswitch d.Cmd {\n\t\t\tcase \"compact\":\n\t\t\t\tvar start, base int\n\t\t\t\td.ScanArgs(t, \"start\", &start)\n\t\t\t\td.ScanArgs(t, \"base\", &base)\n\t\t\t\tpc := newPickedCompaction(opts, version, start, defaultOutputLevel(start, base), base)\n\t\t\t\tc := newCompaction(pc, opts, time.Now(), nil /* provider */, nil /* slot */)\n\t\t\t\treturn fmt.Sprintf(\"output=%d\\nmax-output-file-size=%d\\n\",\n\t\t\t\t\tc.outputLevel.level, c.maxOutputFileSize)\n\n\t\t\tdefault:\n\t\t\t\treturn fmt.Sprintf(\"unknown command: %s\", d.Cmd)\n\t\t\t}\n\t\t})\n}\n\nfunc TestCompactionDeleteOnlyHints(t *testing.T) {\n\tparseUint64 := func(s string) uint64 {\n\t\tv, err := strconv.ParseUint(s, 10, 64)\n\t\trequire.NoError(t, err)\n\t\treturn v\n\t}\n\tvar d *DB\n\tdefer func() {\n\t\tif d != nil {\n\t\t\trequire.NoError(t, closeAllSnapshots(d))\n\t\t\trequire.NoError(t, d.Close())\n\t\t}\n\t}()\n\n\tvar compactInfo *CompactionInfo // protected by d.mu\n\treset := func() (*Options, error) {\n\t\tif d != nil {\n\t\t\tcompactInfo = nil\n\t\t\tif err := closeAllSnapshots(d); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tif err := d.Close(); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t\topts := (&Options{\n\t\t\tFS:         vfs.NewMem(),\n\t\t\tDebugCheck: DebugCheckLevels,\n\t\t\t// Collection of table stats can trigger compactions. As we want full\n\t\t\t// control over when compactions are run, disable stats by default.\n\t\t\tDisableTableStats: true,\n\t\t\tEventListener: &EventListener{\n\t\t\t\tCompactionEnd: func(info CompactionInfo) {\n\t\t\t\t\tif compactInfo != nil {\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\tcompactInfo = &info\n\t\t\t\t},\n\t\t\t},\n\t\t\tFormatMajorVersion: internalFormatNewest,\n\t\t}).WithFSDefaults()\n\t\topts.Experimental.EnableDeleteOnlyCompactionExcises = func() bool { return true }\n\t\topts.Experimental.EnableColumnarBlocks = func() bool { return true }\n\t\treturn opts, nil\n\t}\n\n\tcompactionString := func() string {\n\t\tfor d.mu.compact.compactingCount > 0 {\n\t\t\td.mu.compact.cond.Wait()\n\t\t}\n\n\t\ts := \"(none)\"\n\t\tif compactInfo != nil {\n\t\t\t// Fix the job ID and durations for determinism.\n\t\t\tcompactInfo.JobID = 100\n\t\t\tcompactInfo.Duration = time.Second\n\t\t\tcompactInfo.TotalDuration = 2 * time.Second\n\t\t\ts = compactInfo.String()\n\t\t\tcompactInfo = nil\n\t\t}\n\t\treturn s\n\t}\n\n\tvar err error\n\tvar opts *Options\n\tdatadriven.RunTest(t, \"testdata/compaction_delete_only_hints\",\n\t\tfunc(t *testing.T, td *datadriven.TestData) string {\n\t\t\tswitch td.Cmd {\n\t\t\tcase \"define\":\n\t\t\t\topts, err = reset()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\td, err = runDBDefineCmd(td, opts)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\td.mu.Lock()\n\t\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\t\td.mu.Unlock()\n\t\t\t\treturn s\n\n\t\t\tcase \"force-set-hints\":\n\t\t\t\td.mu.Lock()\n\t\t\t\tdefer d.mu.Unlock()\n\t\t\t\td.mu.compact.deletionHints = d.mu.compact.deletionHints[:0]\n\t\t\t\tvar buf bytes.Buffer\n\t\t\t\tfor _, data := range strings.Split(td.Input, \"\\n\") {\n\t\t\t\t\tparts := strings.FieldsFunc(strings.TrimSpace(data),\n\t\t\t\t\t\tfunc(r rune) bool { return r == '-' || r == ' ' || r == '.' })\n\n\t\t\t\t\tstart, end := []byte(parts[2]), []byte(parts[3])\n\n\t\t\t\t\tvar tombstoneFile *fileMetadata\n\t\t\t\t\ttombstoneLevel := int(parseUint64(parts[0][1:]))\n\n\t\t\t\t\t// Set file number to the value provided in the input.\n\t\t\t\t\ttombstoneFile = &fileMetadata{\n\t\t\t\t\t\tFileNum: base.FileNum(parseUint64(parts[1])),\n\t\t\t\t\t}\n\n\t\t\t\t\tvar hintType deleteCompactionHintType\n\t\t\t\t\tswitch typ := parts[7]; typ {\n\t\t\t\t\tcase \"point_key_only\":\n\t\t\t\t\t\thintType = deleteCompactionHintTypePointKeyOnly\n\t\t\t\t\tcase \"range_key_only\":\n\t\t\t\t\t\thintType = deleteCompactionHintTypeRangeKeyOnly\n\t\t\t\t\tcase \"point_and_range_key\":\n\t\t\t\t\t\thintType = deleteCompactionHintTypePointAndRangeKey\n\t\t\t\t\tdefault:\n\t\t\t\t\t\treturn fmt.Sprintf(\"unknown hint type: %s\", typ)\n\t\t\t\t\t}\n\n\t\t\t\t\th := deleteCompactionHint{\n\t\t\t\t\t\thintType:                hintType,\n\t\t\t\t\t\tstart:                   start,\n\t\t\t\t\t\tend:                     end,\n\t\t\t\t\t\tfileSmallestSeqNum:      base.SeqNum(parseUint64(parts[4])),\n\t\t\t\t\t\ttombstoneLevel:          tombstoneLevel,\n\t\t\t\t\t\ttombstoneFile:           tombstoneFile,\n\t\t\t\t\t\ttombstoneSmallestSeqNum: base.SeqNum(parseUint64(parts[5])),\n\t\t\t\t\t\ttombstoneLargestSeqNum:  base.SeqNum(parseUint64(parts[6])),\n\t\t\t\t\t}\n\t\t\t\t\td.mu.compact.deletionHints = append(d.mu.compact.deletionHints, h)\n\t\t\t\t\tfmt.Fprintln(&buf, h.String())\n\t\t\t\t}\n\t\t\t\treturn buf.String()\n\n\t\t\tcase \"get-hints\":\n\t\t\t\td.mu.Lock()\n\t\t\t\tdefer d.mu.Unlock()\n\n\t\t\t\t// Force collection of table stats. This requires re-enabling the\n\t\t\t\t// collection flag. We also do not want compactions to run as part of\n\t\t\t\t// the stats collection job, so we disable it temporarily.\n\t\t\t\td.opts.DisableTableStats = false\n\t\t\t\td.opts.DisableAutomaticCompactions = true\n\t\t\t\tdefer func() {\n\t\t\t\t\td.opts.DisableTableStats = true\n\t\t\t\t\td.opts.DisableAutomaticCompactions = false\n\t\t\t\t}()\n\n\t\t\t\t// NB: collectTableStats attempts to acquire the lock. Temporarily\n\t\t\t\t// unlock here to avoid a deadlock.\n\t\t\t\td.mu.Unlock()\n\t\t\t\tdidRun := d.collectTableStats()\n\t\t\t\td.mu.Lock()\n\n\t\t\t\tif !didRun {\n\t\t\t\t\t// If a job was already running, wait for the results.\n\t\t\t\t\td.waitTableStats()\n\t\t\t\t}\n\n\t\t\t\thints := d.mu.compact.deletionHints\n\t\t\t\tif len(hints) == 0 {\n\t\t\t\t\treturn \"(none)\"\n\t\t\t\t}\n\t\t\t\tvar buf bytes.Buffer\n\t\t\t\tfor _, h := range hints {\n\t\t\t\t\tbuf.WriteString(h.String() + \"\\n\")\n\t\t\t\t}\n\t\t\t\treturn buf.String()\n\n\t\t\tcase \"maybe-compact\":\n\t\t\t\td.mu.Lock()\n\t\t\t\td.maybeScheduleCompaction()\n\n\t\t\t\tvar buf bytes.Buffer\n\t\t\t\tfmt.Fprintf(&buf, \"Deletion hints:\\n\")\n\t\t\t\tfor _, h := range d.mu.compact.deletionHints {\n\t\t\t\t\tfmt.Fprintf(&buf, \"  %s\\n\", h.String())\n\t\t\t\t}\n\t\t\t\tif len(d.mu.compact.deletionHints) == 0 {\n\t\t\t\t\tfmt.Fprintf(&buf, \"  (none)\\n\")\n\t\t\t\t}\n\t\t\t\tfmt.Fprintf(&buf, \"Compactions:\\n\")\n\t\t\t\tfmt.Fprintf(&buf, \"  %s\", compactionString())\n\t\t\t\td.mu.Unlock()\n\t\t\t\treturn buf.String()\n\n\t\t\tcase \"compact\":\n\t\t\t\tif err := runCompactCmd(td, d); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\td.mu.Lock()\n\t\t\t\tcompactInfo = nil\n\t\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\t\td.mu.Unlock()\n\t\t\t\treturn s\n\n\t\t\tcase \"close-snapshot\":\n\t\t\t\tseqNum := base.ParseSeqNum(strings.TrimSpace(td.Input))\n\t\t\t\td.mu.Lock()\n\t\t\t\tvar s *Snapshot\n\t\t\t\tl := &d.mu.snapshots\n\t\t\t\tfor i := l.root.next; i != &l.root; i = i.next {\n\t\t\t\t\tif i.seqNum == seqNum {\n\t\t\t\t\t\ts = i\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\td.mu.Unlock()\n\t\t\t\tif s == nil {\n\t\t\t\t\treturn \"(not found)\"\n\t\t\t\t} else if err := s.Close(); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\n\t\t\t\td.mu.Lock()\n\t\t\t\t// Closing the snapshot may have triggered a compaction.\n\t\t\t\tstr := compactionString()\n\t\t\t\td.mu.Unlock()\n\t\t\t\treturn str\n\n\t\t\tcase \"iter\":\n\t\t\t\tsnap := Snapshot{\n\t\t\t\t\tdb:     d,\n\t\t\t\t\tseqNum: base.SeqNumMax,\n\t\t\t\t}\n\t\t\t\titer, _ := snap.NewIter(nil)\n\t\t\t\treturn runIterCmd(td, iter, true)\n\n\t\t\tcase \"reset\":\n\t\t\t\topts, err = reset()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\td, err = Open(\"\", opts)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\treturn \"\"\n\n\t\t\tcase \"ingest\":\n\t\t\t\tif err = runBuildCmd(td, d, d.opts.FS); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\tif err = runIngestCmd(td, d, d.opts.FS); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\treturn \"OK\"\n\n\t\t\tcase \"describe-lsm\":\n\t\t\t\td.mu.Lock()\n\t\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\t\td.mu.Unlock()\n\t\t\t\treturn s\n\n\t\t\tdefault:\n\t\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t\t}\n\t\t})\n}\n\nfunc TestCompactionTombstones(t *testing.T) {\n\tvar d *DB\n\tdefer func() {\n\t\tif d != nil {\n\t\t\trequire.NoError(t, closeAllSnapshots(d))\n\t\t\trequire.NoError(t, d.Close())\n\t\t}\n\t}()\n\n\tvar compactInfo *CompactionInfo // protected by d.mu\n\n\tcompactionString := func() string {\n\t\tfor d.mu.compact.compactingCount > 0 {\n\t\t\td.mu.compact.cond.Wait()\n\t\t}\n\n\t\ts := \"(none)\"\n\t\tif compactInfo != nil {\n\t\t\t// Fix the job ID and durations for determinism.\n\t\t\tcompactInfo.JobID = 100\n\t\t\tcompactInfo.Duration = time.Second\n\t\t\tcompactInfo.TotalDuration = 2 * time.Second\n\t\t\ts = compactInfo.String()\n\t\t\tcompactInfo = nil\n\t\t}\n\t\treturn s\n\t}\n\n\tdatadriven.RunTest(t, \"testdata/compaction_tombstones\",\n\t\tfunc(t *testing.T, td *datadriven.TestData) string {\n\t\t\tswitch td.Cmd {\n\t\t\tcase \"define\":\n\t\t\t\tif d != nil {\n\t\t\t\t\tcompactInfo = nil\n\t\t\t\t\trequire.NoError(t, closeAllSnapshots(d))\n\t\t\t\t\tif err := d.Close(); err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\topts := (&Options{\n\t\t\t\t\tFS:         vfs.NewMem(),\n\t\t\t\t\tDebugCheck: DebugCheckLevels,\n\t\t\t\t\tEventListener: &EventListener{\n\t\t\t\t\t\tCompactionEnd: func(info CompactionInfo) {\n\t\t\t\t\t\t\tcompactInfo = &info\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\tFormatMajorVersion: internalFormatNewest,\n\t\t\t\t}).WithFSDefaults()\n\t\t\t\topts.Experimental.EnableDeleteOnlyCompactionExcises = func() bool { return true }\n\t\t\t\topts.Experimental.EnableColumnarBlocks = func() bool { return true }\n\t\t\t\tvar err error\n\t\t\t\td, err = runDBDefineCmd(td, opts)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\td.mu.Lock()\n\t\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\t\td.mu.Unlock()\n\t\t\t\treturn s\n\n\t\t\tcase \"maybe-compact\":\n\t\t\t\td.mu.Lock()\n\t\t\t\td.opts.DisableAutomaticCompactions = false\n\t\t\t\td.maybeScheduleCompaction()\n\t\t\t\ts := compactionString()\n\t\t\t\td.mu.Unlock()\n\t\t\t\treturn s\n\n\t\t\tcase \"wait-pending-table-stats\":\n\t\t\t\treturn runTableStatsCmd(td, d)\n\n\t\t\tcase \"close-snapshot\":\n\t\t\t\tseqNum := base.ParseSeqNum(strings.TrimSpace(td.Input))\n\t\t\t\td.mu.Lock()\n\t\t\t\tvar s *Snapshot\n\t\t\t\tl := &d.mu.snapshots\n\t\t\t\tfor i := l.root.next; i != &l.root; i = i.next {\n\t\t\t\t\tif i.seqNum == seqNum {\n\t\t\t\t\t\ts = i\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\td.mu.Unlock()\n\t\t\t\tif s == nil {\n\t\t\t\t\treturn \"(not found)\"\n\t\t\t\t} else if err := s.Close(); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\n\t\t\t\td.mu.Lock()\n\t\t\t\t// Closing the snapshot may have triggered a compaction.\n\t\t\t\tstr := compactionString()\n\t\t\t\td.mu.Unlock()\n\t\t\t\treturn str\n\n\t\t\tcase \"close\":\n\t\t\t\tif err := d.Close(); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\td = nil\n\t\t\t\treturn \"\"\n\n\t\t\tcase \"version\":\n\t\t\t\td.mu.Lock()\n\t\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\t\td.mu.Unlock()\n\t\t\t\treturn s\n\n\t\t\tdefault:\n\t\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t\t}\n\t\t})\n}\n\nfunc closeAllSnapshots(d *DB) error {\n\td.mu.Lock()\n\tvar ss []*Snapshot\n\tl := &d.mu.snapshots\n\tfor i := l.root.next; i != &l.root; i = i.next {\n\t\tss = append(ss, i)\n\t}\n\td.mu.Unlock()\n\tfor i := range ss {\n\t\tif err := ss[i].Close(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc TestCompactionReadTriggeredQueue(t *testing.T) {\n\n\t// Convert a read compaction to a string which this test\n\t// understands.\n\tshowRC := func(rc *readCompaction) string {\n\t\treturn fmt.Sprintf(\n\t\t\t\"L%d: %s-%s %d\\n\", rc.level, string(rc.start), string(rc.end), rc.fileNum,\n\t\t)\n\t}\n\n\tvar queue *readCompactionQueue\n\n\tdatadriven.RunTest(t, \"testdata/read_compaction_queue\",\n\t\tfunc(t *testing.T, td *datadriven.TestData) string {\n\t\t\tswitch td.Cmd {\n\t\t\tcase \"create\":\n\t\t\t\tqueue = &readCompactionQueue{}\n\t\t\t\treturn \"(success)\"\n\t\t\tcase \"add-compaction\":\n\t\t\t\tfor _, line := range crstrings.Lines(td.Input) {\n\t\t\t\t\tparts := strings.Split(line, \" \")\n\n\t\t\t\t\tif len(parts) != 3 {\n\t\t\t\t\t\treturn \"error: malformed data for add-compaction. usage: <level>: <start>-<end> <filenum>\"\n\t\t\t\t\t}\n\t\t\t\t\tif l, err := strconv.Atoi(parts[0][1:2]); err == nil {\n\t\t\t\t\t\tkeys := strings.Split(parts[1], \"-\")\n\t\t\t\t\t\tfileNum, _ := strconv.Atoi(parts[2])\n\t\t\t\t\t\trc := readCompaction{\n\t\t\t\t\t\t\tlevel:   l,\n\t\t\t\t\t\t\tstart:   []byte(keys[0]),\n\t\t\t\t\t\t\tend:     []byte(keys[1]),\n\t\t\t\t\t\t\tfileNum: base.FileNum(fileNum),\n\t\t\t\t\t\t}\n\t\t\t\t\t\tqueue.add(&rc, DefaultComparer.Compare)\n\t\t\t\t\t} else {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn \"\"\n\t\t\tcase \"remove-compaction\":\n\t\t\t\trc := queue.remove()\n\t\t\t\tif rc == nil {\n\t\t\t\t\treturn \"(nil)\"\n\t\t\t\t}\n\t\t\t\treturn showRC(rc)\n\t\t\tcase \"print-size\":\n\t\t\t\t// Print the size of the queue.\n\t\t\t\treturn fmt.Sprintf(\"%d\", queue.size)\n\t\t\tcase \"print-queue\":\n\t\t\t\t// Print each element of the queue on a separate line.\n\t\t\t\tvar sb strings.Builder\n\t\t\t\tif queue.size == 0 {\n\t\t\t\t\tsb.WriteString(\"(empty)\")\n\t\t\t\t}\n\n\t\t\t\tfor i := 0; i < queue.size; i++ {\n\t\t\t\t\trc := queue.at(i)\n\t\t\t\t\tsb.WriteString(showRC(rc))\n\t\t\t\t}\n\t\t\t\treturn sb.String()\n\t\t\tdefault:\n\t\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t\t}\n\t\t},\n\t)\n}\n\nfunc (qu *readCompactionQueue) at(i int) *readCompaction {\n\tif i >= qu.size {\n\t\treturn nil\n\t}\n\n\treturn qu.queue[i]\n}\n\nfunc TestCompactionReadTriggered(t *testing.T) {\n\tvar d *DB\n\tdefer func() {\n\t\tif d != nil {\n\t\t\trequire.NoError(t, d.Close())\n\t\t}\n\t}()\n\n\tvar compactInfo *CompactionInfo // protected by d.mu\n\n\tcompactionString := func() string {\n\t\tfor d.mu.compact.compactingCount > 0 {\n\t\t\td.mu.compact.cond.Wait()\n\t\t}\n\n\t\ts := \"(none)\"\n\t\tif compactInfo != nil {\n\t\t\t// Fix the job ID and durations for determinism.\n\t\t\tcompactInfo.JobID = 100\n\t\t\tcompactInfo.Duration = time.Second\n\t\t\tcompactInfo.TotalDuration = 2 * time.Second\n\t\t\ts = compactInfo.String()\n\t\t\tcompactInfo = nil\n\t\t}\n\t\treturn s\n\t}\n\n\tdatadriven.RunTest(t, \"testdata/compaction_read_triggered\",\n\t\tfunc(t *testing.T, td *datadriven.TestData) string {\n\t\t\tswitch td.Cmd {\n\t\t\tcase \"define\":\n\t\t\t\tif d != nil {\n\t\t\t\t\tcompactInfo = nil\n\t\t\t\t\tif err := d.Close(); err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\topts := (&Options{\n\t\t\t\t\tFS:         vfs.NewMem(),\n\t\t\t\t\tDebugCheck: DebugCheckLevels,\n\t\t\t\t\tEventListener: &EventListener{\n\t\t\t\t\t\tCompactionEnd: func(info CompactionInfo) {\n\t\t\t\t\t\t\tcompactInfo = &info\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t}).WithFSDefaults()\n\t\t\t\tvar err error\n\t\t\t\td, err = runDBDefineCmd(td, opts)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\td.mu.Lock()\n\t\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\t\td.mu.Unlock()\n\t\t\t\treturn s\n\n\t\t\tcase \"add-read-compaction\":\n\t\t\t\td.mu.Lock()\n\t\t\t\ttd.MaybeScanArgs(t, \"flushing\", &d.mu.compact.flushing)\n\t\t\t\tfor _, line := range crstrings.Lines(td.Input) {\n\t\t\t\t\tparts := strings.Split(line, \" \")\n\t\t\t\t\tif len(parts) != 3 {\n\t\t\t\t\t\treturn \"error: malformed data for add-read-compaction. usage: <level>: <start>-<end> <filenum>\"\n\t\t\t\t\t}\n\t\t\t\t\tif l, err := strconv.Atoi(parts[0][:1]); err == nil {\n\t\t\t\t\t\tkeys := strings.Split(parts[1], \"-\")\n\t\t\t\t\t\tfileNum, _ := strconv.Atoi(parts[2])\n\t\t\t\t\t\trc := readCompaction{\n\t\t\t\t\t\t\tlevel:   l,\n\t\t\t\t\t\t\tstart:   []byte(keys[0]),\n\t\t\t\t\t\t\tend:     []byte(keys[1]),\n\t\t\t\t\t\t\tfileNum: base.FileNum(fileNum),\n\t\t\t\t\t\t}\n\t\t\t\t\t\td.mu.compact.readCompactions.add(&rc, DefaultComparer.Compare)\n\t\t\t\t\t} else {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\td.mu.Unlock()\n\t\t\t\treturn \"\"\n\n\t\t\tcase \"show-read-compactions\":\n\t\t\t\td.mu.Lock()\n\t\t\t\tvar sb strings.Builder\n\t\t\t\tif d.mu.compact.readCompactions.size == 0 {\n\t\t\t\t\tsb.WriteString(\"(none)\")\n\t\t\t\t}\n\t\t\t\tfor i := 0; i < d.mu.compact.readCompactions.size; i++ {\n\t\t\t\t\trc := d.mu.compact.readCompactions.at(i)\n\t\t\t\t\tsb.WriteString(fmt.Sprintf(\"(level: %d, start: %s, end: %s)\\n\", rc.level, string(rc.start), string(rc.end)))\n\t\t\t\t}\n\t\t\t\td.mu.Unlock()\n\t\t\t\treturn sb.String()\n\n\t\t\tcase \"maybe-compact\":\n\t\t\t\td.mu.Lock()\n\t\t\t\td.opts.DisableAutomaticCompactions = false\n\t\t\t\td.maybeScheduleCompaction()\n\t\t\t\ts := compactionString()\n\t\t\t\td.mu.Unlock()\n\t\t\t\treturn s\n\n\t\t\tcase \"version\":\n\t\t\t\td.mu.Lock()\n\t\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\t\td.mu.Unlock()\n\t\t\t\treturn s\n\n\t\t\tdefault:\n\t\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t\t}\n\t\t})\n}\n\nfunc TestCompactionAllowZeroSeqNum(t *testing.T) {\n\tvar d *DB\n\tdefer func() {\n\t\tif d != nil {\n\t\t\trequire.NoError(t, closeAllSnapshots(d))\n\t\t\trequire.NoError(t, d.Close())\n\t\t}\n\t}()\n\n\tmetaRE := regexp.MustCompile(`^L([0-9]+):([^-]+)-(.+)$`)\n\tvar fileNum base.FileNum\n\tparseMeta := func(s string) (level int, meta *fileMetadata) {\n\t\tmatch := metaRE.FindStringSubmatch(s)\n\t\tif match == nil {\n\t\t\tt.Fatalf(\"malformed table spec: %s\", s)\n\t\t}\n\t\tlevel, err := strconv.Atoi(match[1])\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"malformed table spec: %s: %s\", s, err)\n\t\t}\n\t\tfileNum++\n\t\tmeta = (&fileMetadata{\n\t\t\tFileNum: fileNum,\n\t\t}).ExtendPointKeyBounds(\n\t\t\td.cmp,\n\t\t\tInternalKey{UserKey: []byte(match[2])},\n\t\t\tInternalKey{UserKey: []byte(match[3])},\n\t\t)\n\t\tmeta.InitPhysicalBacking()\n\t\treturn level, meta\n\t}\n\n\tdatadriven.RunTest(t, \"testdata/compaction_allow_zero_seqnum\",\n\t\tfunc(t *testing.T, td *datadriven.TestData) string {\n\t\t\tswitch td.Cmd {\n\t\t\tcase \"define\":\n\t\t\t\tif d != nil {\n\t\t\t\t\trequire.NoError(t, closeAllSnapshots(d))\n\t\t\t\t\tif err := d.Close(); err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tvar err error\n\t\t\t\tif d, err = runDBDefineCmd(td, nil /* options */); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\n\t\t\t\td.mu.Lock()\n\t\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\t\td.mu.Unlock()\n\t\t\t\treturn s\n\n\t\t\tcase \"allow-zero-seqnum\":\n\t\t\t\td.mu.Lock()\n\t\t\t\tc := &compaction{\n\t\t\t\t\tcmp:      d.cmp,\n\t\t\t\t\tcomparer: d.opts.Comparer,\n\t\t\t\t\tversion:  d.mu.versions.currentVersion(),\n\t\t\t\t\tinputs:   []compactionLevel{{}, {}},\n\t\t\t\t}\n\t\t\t\tc.startLevel, c.outputLevel = &c.inputs[0], &c.inputs[1]\n\t\t\t\td.mu.Unlock()\n\n\t\t\t\tvar buf bytes.Buffer\n\t\t\t\tfor _, line := range crstrings.Lines(td.Input) {\n\t\t\t\t\tparts := strings.Fields(line)\n\t\t\t\t\tc.flushing = nil\n\t\t\t\t\tc.startLevel.level = -1\n\n\t\t\t\t\tvar startFiles, outputFiles []*fileMetadata\n\n\t\t\t\t\tswitch {\n\t\t\t\t\tcase len(parts) == 1 && parts[0] == \"flush\":\n\t\t\t\t\t\tc.outputLevel.level = 0\n\t\t\t\t\t\td.mu.Lock()\n\t\t\t\t\t\tc.flushing = d.mu.mem.queue\n\t\t\t\t\t\td.mu.Unlock()\n\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tfor _, p := range parts {\n\t\t\t\t\t\t\tlevel, meta := parseMeta(p)\n\t\t\t\t\t\t\tif c.startLevel.level == -1 {\n\t\t\t\t\t\t\t\tc.startLevel.level = level\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tswitch level {\n\t\t\t\t\t\t\tcase c.startLevel.level:\n\t\t\t\t\t\t\t\tstartFiles = append(startFiles, meta)\n\t\t\t\t\t\t\tcase c.startLevel.level + 1:\n\t\t\t\t\t\t\t\toutputFiles = append(outputFiles, meta)\n\t\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t\t\treturn fmt.Sprintf(\"invalid level %d: expected %d or %d\",\n\t\t\t\t\t\t\t\t\tlevel, c.startLevel.level, c.startLevel.level+1)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\tc.outputLevel.level = c.startLevel.level + 1\n\t\t\t\t\t\tc.startLevel.files = manifest.NewLevelSliceSpecificOrder(startFiles)\n\t\t\t\t\t\tc.outputLevel.files = manifest.NewLevelSliceKeySorted(c.cmp, outputFiles)\n\t\t\t\t\t}\n\n\t\t\t\t\tc.smallest, c.largest = manifest.KeyRange(c.cmp,\n\t\t\t\t\t\tc.startLevel.files.Iter(),\n\t\t\t\t\t\tc.outputLevel.files.Iter())\n\n\t\t\t\t\tc.delElision, c.rangeKeyElision = compact.SetupTombstoneElision(\n\t\t\t\t\t\tc.cmp, c.version, c.outputLevel.level, base.UserKeyBoundsFromInternal(c.smallest, c.largest),\n\t\t\t\t\t)\n\t\t\t\t\tfmt.Fprintf(&buf, \"%t\\n\", c.allowZeroSeqNum())\n\t\t\t\t}\n\t\t\t\treturn buf.String()\n\n\t\t\tdefault:\n\t\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t\t}\n\t\t})\n}\n\nfunc TestCompactionErrorOnUserKeyOverlap(t *testing.T) {\n\tcmp := DefaultComparer.Compare\n\tparseMeta := func(s string) *fileMetadata {\n\t\tparts := strings.Split(s, \"-\")\n\t\tif len(parts) != 2 {\n\t\t\tt.Fatalf(\"malformed table spec: %s\", s)\n\t\t}\n\t\tm := (&fileMetadata{}).ExtendPointKeyBounds(\n\t\t\tcmp,\n\t\t\tbase.ParseInternalKey(strings.TrimSpace(parts[0])),\n\t\t\tbase.ParseInternalKey(strings.TrimSpace(parts[1])),\n\t\t)\n\t\tm.SmallestSeqNum = m.Smallest.SeqNum()\n\t\tm.LargestSeqNum = m.Largest.SeqNum()\n\t\tm.LargestSeqNumAbsolute = m.LargestSeqNum\n\t\tm.InitPhysicalBacking()\n\t\treturn m\n\t}\n\n\tdatadriven.RunTest(t, \"testdata/compaction_error_on_user_key_overlap\",\n\t\tfunc(t *testing.T, d *datadriven.TestData) string {\n\t\t\tswitch d.Cmd {\n\t\t\tcase \"error-on-user-key-overlap\":\n\t\t\t\tc := &compaction{\n\t\t\t\t\tcmp:       DefaultComparer.Compare,\n\t\t\t\t\tcomparer:  DefaultComparer,\n\t\t\t\t\tformatKey: DefaultComparer.FormatKey,\n\t\t\t\t}\n\t\t\t\tvar files []manifest.NewFileEntry\n\t\t\t\tfileNum := FileNum(1)\n\n\t\t\t\tfor _, data := range strings.Split(d.Input, \"\\n\") {\n\t\t\t\t\tmeta := parseMeta(data)\n\t\t\t\t\tmeta.FileNum = fileNum\n\t\t\t\t\tfileNum++\n\t\t\t\t\tfiles = append(files, manifest.NewFileEntry{Level: 1, Meta: meta})\n\t\t\t\t}\n\n\t\t\t\tresult := \"OK\"\n\t\t\t\tve := &versionEdit{\n\t\t\t\t\tNewFiles: files,\n\t\t\t\t}\n\t\t\t\tif err := c.errorOnUserKeyOverlap(ve); err != nil {\n\t\t\t\t\tresult = fmt.Sprint(err)\n\t\t\t\t}\n\t\t\t\treturn result\n\n\t\t\tdefault:\n\t\t\t\treturn fmt.Sprintf(\"unknown command: %s\", d.Cmd)\n\t\t\t}\n\t\t})\n}\n\n// TestCompactionErrorCleanup tests an error encountered during a compaction\n// after some output tables have been created. It ensures that the pending\n// output tables are removed from the filesystem.\nfunc TestCompactionErrorCleanup(t *testing.T) {\n\t// protected by d.mu\n\tvar (\n\t\tinitialSetupDone bool\n\t\ttablesCreated    []base.DiskFileNum\n\t)\n\n\tmem := vfs.NewMem()\n\tii := errorfs.OnIndex(math.MaxInt32) // start disabled\n\topts := (&Options{\n\t\tFS:     errorfs.Wrap(mem, errorfs.ErrInjected.If(ii)),\n\t\tLevels: make([]LevelOptions, numLevels),\n\t\tEventListener: &EventListener{\n\t\t\tTableCreated: func(info TableCreateInfo) {\n\t\t\t\tt.Log(info)\n\n\t\t\t\t// If the initial setup is over, record tables created and\n\t\t\t\t// inject an error immediately after the second table is\n\t\t\t\t// created.\n\t\t\t\tif initialSetupDone {\n\t\t\t\t\ttablesCreated = append(tablesCreated, info.FileNum)\n\t\t\t\t\tif len(tablesCreated) >= 2 {\n\t\t\t\t\t\tii.Store(0)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\t}).WithFSDefaults()\n\tfor i := range opts.Levels {\n\t\topts.Levels[i].TargetFileSize = 1\n\t}\n\topts.testingRandomized(t)\n\td, err := Open(\"\", opts)\n\trequire.NoError(t, err)\n\n\tingest := func(keys ...string) {\n\t\tt.Helper()\n\t\tf, err := mem.Create(\"ext\", vfs.WriteCategoryUnspecified)\n\t\trequire.NoError(t, err)\n\n\t\tw := sstable.NewWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{\n\t\t\tTableFormat: d.TableFormat(),\n\t\t})\n\t\tfor _, k := range keys {\n\t\t\trequire.NoError(t, w.Set([]byte(k), nil))\n\t\t}\n\t\trequire.NoError(t, w.Close())\n\t\trequire.NoError(t, d.Ingest(context.Background(), []string{\"ext\"}))\n\t}\n\tingest(\"a\", \"c\")\n\tingest(\"b\")\n\n\t// Trigger a manual compaction, which will encounter an injected error\n\t// after the second table is created.\n\td.mu.Lock()\n\tinitialSetupDone = true\n\td.mu.Unlock()\n\terr = d.Compact([]byte(\"a\"), []byte(\"d\"), false)\n\trequire.Error(t, err, \"injected error\")\n\n\td.mu.Lock()\n\tif len(tablesCreated) < 2 {\n\t\tt.Fatalf(\"expected 2 output tables created by compaction: found %d\", len(tablesCreated))\n\t}\n\td.mu.Unlock()\n\n\trequire.NoError(t, d.Close())\n\tfor _, fileNum := range tablesCreated {\n\t\tfilename := fmt.Sprintf(\"%s.sst\", fileNum)\n\t\tif _, err = mem.Stat(filename); err == nil || !oserror.IsNotExist(err) {\n\t\t\tt.Errorf(\"expected %q to not exist: %s\", filename, err)\n\t\t}\n\t}\n}\n\nfunc TestCompactionCheckOrdering(t *testing.T) {\n\tcmp := DefaultComparer.Compare\n\tparseMeta := func(s string) *fileMetadata {\n\t\tparts := strings.Split(s, \"-\")\n\t\tif len(parts) != 2 {\n\t\t\tt.Fatalf(\"malformed table spec: %s\", s)\n\t\t}\n\t\tm := (&fileMetadata{}).ExtendPointKeyBounds(\n\t\t\tcmp,\n\t\t\tbase.ParseInternalKey(strings.TrimSpace(parts[0])),\n\t\t\tbase.ParseInternalKey(strings.TrimSpace(parts[1])),\n\t\t)\n\t\tm.SmallestSeqNum = m.Smallest.SeqNum()\n\t\tm.LargestSeqNum = m.Largest.SeqNum()\n\t\tm.LargestSeqNumAbsolute = m.LargestSeqNum\n\t\tm.InitPhysicalBacking()\n\t\treturn m\n\t}\n\n\tdatadriven.RunTest(t, \"testdata/compaction_check_ordering\",\n\t\tfunc(t *testing.T, d *datadriven.TestData) string {\n\t\t\tswitch d.Cmd {\n\t\t\tcase \"check-ordering\":\n\t\t\t\tc := &compaction{\n\t\t\t\t\tcmp:       DefaultComparer.Compare,\n\t\t\t\t\tcomparer:  DefaultComparer,\n\t\t\t\t\tformatKey: DefaultComparer.FormatKey,\n\t\t\t\t\tlogger:    panicLogger{},\n\t\t\t\t\tinputs:    []compactionLevel{{level: -1}, {level: -1}},\n\t\t\t\t}\n\t\t\t\tc.startLevel, c.outputLevel = &c.inputs[0], &c.inputs[1]\n\t\t\t\tvar startFiles, outputFiles []*fileMetadata\n\t\t\t\tvar sublevels []manifest.LevelSlice\n\t\t\t\tvar files *[]*fileMetadata\n\t\t\t\tvar sublevel []*fileMetadata\n\t\t\t\tvar sublevelNum int\n\t\t\t\tvar parsingSublevel bool\n\t\t\t\tfileNum := FileNum(1)\n\n\t\t\t\tswitchSublevel := func() {\n\t\t\t\t\tif sublevel != nil {\n\t\t\t\t\t\tsublevels = append(\n\t\t\t\t\t\t\tsublevels, manifest.NewLevelSliceSpecificOrder(sublevel),\n\t\t\t\t\t\t)\n\t\t\t\t\t\tsublevel = nil\n\t\t\t\t\t}\n\t\t\t\t\tparsingSublevel = false\n\t\t\t\t}\n\n\t\t\t\tfor _, data := range strings.Split(d.Input, \"\\n\") {\n\t\t\t\t\tif data[0] == 'L' && len(data) == 4 {\n\t\t\t\t\t\t// Format L0.{sublevel}.\n\t\t\t\t\t\tswitchSublevel()\n\t\t\t\t\t\tlevel, err := strconv.Atoi(data[1:2])\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t\t}\n\t\t\t\t\t\tsublevelNum, err = strconv.Atoi(data[3:])\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif c.startLevel.level == -1 {\n\t\t\t\t\t\t\tc.startLevel.level = level\n\t\t\t\t\t\t\tfiles = &startFiles\n\t\t\t\t\t\t}\n\t\t\t\t\t\tparsingSublevel = true\n\t\t\t\t\t} else if data[0] == 'L' {\n\t\t\t\t\t\tswitchSublevel()\n\t\t\t\t\t\tlevel, err := strconv.Atoi(data[1:])\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif c.startLevel.level == -1 {\n\t\t\t\t\t\t\tc.startLevel.level = level\n\t\t\t\t\t\t\tfiles = &startFiles\n\t\t\t\t\t\t} else if c.outputLevel.level == -1 {\n\t\t\t\t\t\t\tif c.startLevel.level >= level {\n\t\t\t\t\t\t\t\treturn fmt.Sprintf(\"startLevel=%d >= outputLevel=%d\\n\", c.startLevel.level, level)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tc.outputLevel.level = level\n\t\t\t\t\t\t\tfiles = &outputFiles\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\treturn \"outputLevel already set\\n\"\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\tmeta := parseMeta(data)\n\t\t\t\t\t\tmeta.FileNum = fileNum\n\t\t\t\t\t\tfileNum++\n\t\t\t\t\t\t*files = append(*files, meta)\n\t\t\t\t\t\tif parsingSublevel {\n\t\t\t\t\t\t\tmeta.SubLevel = sublevelNum\n\t\t\t\t\t\t\tsublevel = append(sublevel, meta)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tswitchSublevel()\n\t\t\t\tc.startLevel.files = manifest.NewLevelSliceSpecificOrder(startFiles)\n\t\t\t\tc.outputLevel.files = manifest.NewLevelSliceSpecificOrder(outputFiles)\n\t\t\t\tif c.outputLevel.level == -1 {\n\t\t\t\t\tc.outputLevel.level = 0\n\t\t\t\t}\n\t\t\t\tif c.startLevel.level == 0 {\n\t\t\t\t\t// We don't change the input files for the compaction beyond this point.\n\t\t\t\t\tc.startLevel.l0SublevelInfo = generateSublevelInfo(c.cmp, c.startLevel.files)\n\t\t\t\t}\n\n\t\t\t\tnewIters := func(\n\t\t\t\t\t_ context.Context, _ *manifest.FileMetadata, _ *IterOptions, _ internalIterOpts, _ iterKinds,\n\t\t\t\t) (iterSet, error) {\n\t\t\t\t\treturn iterSet{point: &errorIter{}}, nil\n\t\t\t\t}\n\t\t\t\tresult := \"OK\"\n\t\t\t\t_, _, _, err := c.newInputIters(newIters, nil)\n\t\t\t\tif err != nil {\n\t\t\t\t\tresult = fmt.Sprint(err)\n\t\t\t\t}\n\t\t\t\treturn result\n\n\t\t\tdefault:\n\t\t\t\treturn fmt.Sprintf(\"unknown command: %s\", d.Cmd)\n\t\t\t}\n\t\t})\n}\n\nfunc TestCompactFlushQueuedMemTableAndFlushMetrics(t *testing.T) {\n\tt.Run(\"\", func(t *testing.T) {\n\t\t// Verify that manual compaction forces a flush of a queued memtable.\n\n\t\tmem := vfs.NewMem()\n\t\td, err := Open(\"\", testingRandomized(t, &Options{\n\t\t\tFS: mem,\n\t\t}).WithFSDefaults())\n\t\trequire.NoError(t, err)\n\n\t\t// Add the key \"a\" to the memtable, then fill up the memtable with the key\n\t\t// prefix \"b\". The compaction will only overlap with the queued memtable,\n\t\t// not the mutable memtable.\n\t\t// NB: The initial memtable size is 256KB, which is filled up with random\n\t\t// values which typically don't compress well. The test also appends the\n\t\t// random value to the \"b\" key to limit overwriting of the same key, which\n\t\t// would get collapsed at flush time since there are no open snapshots.\n\t\tvalue := make([]byte, 50)\n\t\t_, err = crand.Read(value)\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, d.Set([]byte(\"a\"), value, nil))\n\t\tfor {\n\t\t\t_, err = crand.Read(value)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.NoError(t, d.Set(append([]byte(\"b\"), value...), value, nil))\n\t\t\td.mu.Lock()\n\t\t\tdone := len(d.mu.mem.queue) == 2\n\t\t\td.mu.Unlock()\n\t\t\tif done {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\n\t\trequire.NoError(t, d.Compact([]byte(\"a\"), []byte(\"a\\x00\"), false))\n\t\td.mu.Lock()\n\t\trequire.Equal(t, 1, len(d.mu.mem.queue))\n\t\td.mu.Unlock()\n\t\t// Flush metrics are updated after and non-atomically with the memtable\n\t\t// being removed from the queue.\n\t\tfor begin := time.Now(); ; {\n\t\t\tmetrics := d.Metrics()\n\t\t\trequire.NotNil(t, metrics)\n\t\t\tif metrics.Flush.WriteThroughput.Bytes >= 50*1024 {\n\t\t\t\t// The writes (during which the flush is idle) and the flush work\n\t\t\t\t// should not be so fast as to be unrealistic. If these turn out to be\n\t\t\t\t// flaky we could instead inject a clock.\n\t\t\t\ttinyInterval := 50 * time.Microsecond\n\t\t\t\ttestutils.DurationIsAtLeast(t, metrics.Flush.WriteThroughput.WorkDuration, tinyInterval)\n\t\t\t\ttestutils.DurationIsAtLeast(t, metrics.Flush.WriteThroughput.IdleDuration, tinyInterval)\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tif time.Since(begin) > 2*time.Second {\n\t\t\t\tt.Fatal(\"flush did not happen\")\n\t\t\t}\n\t\t\ttime.Sleep(time.Millisecond)\n\t\t}\n\t\trequire.NoError(t, d.Close())\n\t})\n}\n\nfunc TestCompactFlushQueuedLargeBatch(t *testing.T) {\n\t// Verify that compaction forces a flush of a queued large batch.\n\n\tmem := vfs.NewMem()\n\td, err := Open(\"\", testingRandomized(t, &Options{\n\t\tFS: mem,\n\t}).WithFSDefaults())\n\trequire.NoError(t, err)\n\n\t// The default large batch threshold is slightly less than 1/2 of the\n\t// memtable size which makes triggering a problem with flushing queued large\n\t// batches irritating. Manually adjust the threshold to 1/8 of the memtable\n\t// size in order to more easily create a situation where a large batch is\n\t// queued but not automatically flushed.\n\td.mu.Lock()\n\td.largeBatchThreshold = d.opts.MemTableSize / 8\n\trequire.Equal(t, 1, len(d.mu.mem.queue))\n\td.mu.Unlock()\n\n\t// Set a record with a large value. This will be transformed into a large\n\t// batch and placed in the flushable queue.\n\trequire.NoError(t, d.Set([]byte(\"a\"), bytes.Repeat([]byte(\"v\"), int(d.largeBatchThreshold)), nil))\n\td.mu.Lock()\n\trequire.Greater(t, len(d.mu.mem.queue), 1)\n\td.mu.Unlock()\n\n\trequire.NoError(t, d.Compact([]byte(\"a\"), []byte(\"a\\x00\"), false))\n\td.mu.Lock()\n\trequire.Equal(t, 1, len(d.mu.mem.queue))\n\td.mu.Unlock()\n\n\trequire.NoError(t, d.Close())\n}\n\nfunc TestFlushError(t *testing.T) {\n\t// Error the first five times we try to write a sstable.\n\tvar errorOps atomic.Int32\n\terrorOps.Store(3)\n\tfs := errorfs.Wrap(vfs.NewMem(), errorfs.InjectorFunc(func(op errorfs.Op) error {\n\t\tif op.Kind == errorfs.OpCreate && filepath.Ext(op.Path) == \".sst\" && errorOps.Add(-1) >= 0 {\n\t\t\treturn errorfs.ErrInjected\n\t\t}\n\t\treturn nil\n\t}))\n\td, err := Open(\"\", testingRandomized(t, &Options{\n\t\tFS: fs,\n\t\tEventListener: &EventListener{\n\t\t\tBackgroundError: func(err error) {\n\t\t\t\tt.Log(err)\n\t\t\t},\n\t\t},\n\t}).WithFSDefaults())\n\trequire.NoError(t, err)\n\trequire.NoError(t, d.Set([]byte(\"a\"), []byte(\"foo\"), NoSync))\n\trequire.NoError(t, d.Flush())\n\trequire.NoError(t, d.Close())\n}\n\nfunc TestAdjustGrandparentOverlapBytesForFlush(t *testing.T) {\n\t// 500MB in Lbase\n\tvar lbaseFiles []*manifest.FileMetadata\n\tconst lbaseSize = 5 << 20\n\tfor i := 0; i < 100; i++ {\n\t\tm := &manifest.FileMetadata{Size: lbaseSize, FileNum: FileNum(i)}\n\t\tm.InitPhysicalBacking()\n\t\tlbaseFiles =\n\t\t\tappend(lbaseFiles, m)\n\t}\n\tconst maxOutputFileSize = 2 << 20\n\t// 20MB max overlap, so flush split into 25 files.\n\tconst maxOverlapBytes = 20 << 20\n\tls := manifest.NewLevelSliceSpecificOrder(lbaseFiles)\n\ttestCases := []struct {\n\t\tflushingBytes        uint64\n\t\tadjustedOverlapBytes uint64\n\t}{\n\t\t// Flushes large enough that 25 files is acceptable.\n\t\t{flushingBytes: 128 << 20, adjustedOverlapBytes: 20971520},\n\t\t{flushingBytes: 64 << 20, adjustedOverlapBytes: 20971520},\n\t\t// Small increase in adjustedOverlapBytes.\n\t\t{flushingBytes: 32 << 20, adjustedOverlapBytes: 32768000},\n\t\t// Large increase in adjusterOverlapBytes, to limit to 4 files.\n\t\t{flushingBytes: 1 << 20, adjustedOverlapBytes: 131072000},\n\t}\n\tfor _, tc := range testCases {\n\t\tt.Run(\"\", func(t *testing.T) {\n\t\t\tc := compaction{\n\t\t\t\tgrandparents:      ls,\n\t\t\t\tmaxOverlapBytes:   maxOverlapBytes,\n\t\t\t\tmaxOutputFileSize: maxOutputFileSize,\n\t\t\t}\n\t\t\tadjustGrandparentOverlapBytesForFlush(&c, tc.flushingBytes)\n\t\t\trequire.Equal(t, tc.adjustedOverlapBytes, c.maxOverlapBytes)\n\t\t})\n\t}\n}\n\nfunc TestCompactionInvalidBounds(t *testing.T) {\n\tdb, err := Open(\"\", testingRandomized(t, &Options{\n\t\tFS: vfs.NewMem(),\n\t}).WithFSDefaults())\n\trequire.NoError(t, err)\n\tdefer db.Close()\n\trequire.NoError(t, db.Compact([]byte(\"a\"), []byte(\"b\"), false))\n\trequire.Error(t, db.Compact([]byte(\"a\"), []byte(\"a\"), false))\n\trequire.Error(t, db.Compact([]byte(\"b\"), []byte(\"a\"), false))\n}\n\nfunc TestMarkedForCompaction(t *testing.T) {\n\tvar mem vfs.FS = vfs.NewMem()\n\tvar d *DB\n\tdefer func() {\n\t\tif d != nil {\n\t\t\trequire.NoError(t, d.Close())\n\t\t}\n\t}()\n\n\tvar buf bytes.Buffer\n\topts := (&Options{\n\t\tFS:                          mem,\n\t\tDebugCheck:                  DebugCheckLevels,\n\t\tDisableAutomaticCompactions: true,\n\t\tFormatMajorVersion:          internalFormatNewest,\n\t\tEventListener: &EventListener{\n\t\t\tCompactionEnd: func(info CompactionInfo) {\n\t\t\t\t// Fix the job ID and durations for determinism.\n\t\t\t\tinfo.JobID = 100\n\t\t\t\tinfo.Duration = time.Second\n\t\t\t\tinfo.TotalDuration = 2 * time.Second\n\t\t\t\tfmt.Fprintln(&buf, info)\n\t\t\t},\n\t\t},\n\t}).WithFSDefaults()\n\topts.Experimental.EnableColumnarBlocks = func() bool { return true }\n\n\treset := func() {\n\t\tif d != nil {\n\t\t\trequire.NoError(t, d.Close())\n\t\t}\n\t\tmem = vfs.NewMem()\n\t\trequire.NoError(t, mem.MkdirAll(\"ext\", 0755))\n\n\t\tvar err error\n\t\td, err = Open(\"\", opts)\n\t\trequire.NoError(t, err)\n\t}\n\tdatadriven.RunTest(t, \"testdata/marked_for_compaction\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"reset\":\n\t\t\treset()\n\t\t\treturn \"\"\n\n\t\tcase \"define\":\n\t\t\tif d != nil {\n\t\t\t\tif err := d.Close(); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t}\n\t\t\tvar err error\n\t\t\tif d, err = runDBDefineCmd(td, opts); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\td.mu.Lock()\n\t\t\tdefer d.mu.Unlock()\n\t\t\tt := time.Now()\n\t\t\td.timeNow = func() time.Time {\n\t\t\t\tt = t.Add(time.Second)\n\t\t\t\treturn t\n\t\t\t}\n\t\t\ts := d.mu.versions.currentVersion().DebugString()\n\t\t\treturn s\n\n\t\tcase \"mark-for-compaction\":\n\t\t\td.mu.Lock()\n\t\t\tdefer d.mu.Unlock()\n\t\t\tvers := d.mu.versions.currentVersion()\n\t\t\tvar fileNum uint64\n\t\t\ttd.ScanArgs(t, \"file\", &fileNum)\n\t\t\tfor l, lm := range vers.Levels {\n\t\t\t\titer := lm.Iter()\n\t\t\t\tfor f := iter.First(); f != nil; f = iter.Next() {\n\t\t\t\t\tif f.FileNum != base.FileNum(fileNum) {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tf.MarkedForCompaction = true\n\t\t\t\t\tvers.Stats.MarkedForCompaction++\n\t\t\t\t\tmarkedForCompactionAnnotator.InvalidateLevelAnnotation(vers.Levels[l])\n\t\t\t\t\treturn fmt.Sprintf(\"marked L%d.%s\", l, f.FileNum)\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn \"not-found\"\n\n\t\tcase \"maybe-compact\":\n\t\t\td.mu.Lock()\n\t\t\tdefer d.mu.Unlock()\n\t\t\td.opts.DisableAutomaticCompactions = false\n\t\t\td.maybeScheduleCompaction()\n\t\t\tfor d.mu.compact.compactingCount > 0 {\n\t\t\t\td.mu.compact.cond.Wait()\n\t\t\t}\n\n\t\t\tfmt.Fprintln(&buf, d.mu.versions.currentVersion().DebugString())\n\t\t\ts := strings.TrimSpace(buf.String())\n\t\t\tbuf.Reset()\n\t\t\topts.DisableAutomaticCompactions = true\n\t\t\treturn s\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t}\n\t})\n}\n\n// createManifestErrorInjector injects errors (when enabled) into vfs.FS calls\n// to create MANIFEST files.\ntype createManifestErrorInjector struct {\n\tenabled atomic.Bool\n}\n\n// TODO(jackson): Replace the createManifestErrorInjector with the composition\n// of primitives defined in errorfs. This may require additional primitives.\n\nfunc (i *createManifestErrorInjector) String() string { return \"MANIFEST-Creates\" }\n\n// enable enables error injection for the vfs.FS.\nfunc (i *createManifestErrorInjector) enable() {\n\ti.enabled.Store(true)\n}\n\n// MaybeError implements errorfs.Injector.\nfunc (i *createManifestErrorInjector) MaybeError(op errorfs.Op) error {\n\tif !i.enabled.Load() {\n\t\treturn nil\n\t}\n\t// This necessitates having a MaxManifestSize of 1, to reliably induce\n\t// logAndApply errors.\n\tif strings.Contains(op.Path, \"MANIFEST\") && op.Kind == errorfs.OpCreate {\n\t\treturn errorfs.ErrInjected\n\t}\n\treturn nil\n}\n\nvar _ errorfs.Injector = &createManifestErrorInjector{}\n\n// TestCompaction_LogAndApplyFails exercises a flush or ingest encountering an\n// unrecoverable error during logAndApply.\n//\n// Regression test for #1669.\nfunc TestCompaction_LogAndApplyFails(t *testing.T) {\n\t// flushKeys writes the given keys to the DB, flushing the resulting memtable.\n\tvar key = []byte(\"foo\")\n\tflushErrC := make(chan error)\n\tflushKeys := func(db *DB) error {\n\t\tb := db.NewBatch()\n\t\terr := b.Set(key, nil, nil)\n\t\trequire.NoError(t, err)\n\t\terr = b.Commit(nil)\n\t\trequire.NoError(t, err)\n\t\t// An error from a failing flush is returned asynchronously.\n\t\tgo func() { _ = db.Flush() }()\n\t\treturn <-flushErrC\n\t}\n\n\t// ingestKeys adds the given keys to the DB via an ingestion.\n\tingestKeys := func(db *DB) error {\n\t\t// Create an SST for ingestion.\n\t\tconst fName = \"ext\"\n\t\tf, err := db.opts.FS.Create(fName, vfs.WriteCategoryUnspecified)\n\t\trequire.NoError(t, err)\n\t\tw := sstable.NewWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{})\n\t\trequire.NoError(t, w.Set(key, nil))\n\t\trequire.NoError(t, w.Close())\n\t\t// Ingest the SST.\n\t\treturn db.Ingest(context.Background(), []string{fName})\n\t}\n\n\ttestCases := []struct {\n\t\tname              string\n\t\taddFn             func(db *DB) error\n\t\tbackgroundErrorFn func(*DB, error)\n\t}{\n\t\t{\n\t\t\tname:  \"flush\",\n\t\t\taddFn: flushKeys,\n\t\t\tbackgroundErrorFn: func(db *DB, err error) {\n\t\t\t\trequire.True(t, errors.Is(err, errorfs.ErrInjected))\n\t\t\t\tflushErrC <- err\n\t\t\t\t// A flush will attempt to retry in the background. For the purposes of\n\t\t\t\t// testing this particular scenario, where we would have crashed anyway,\n\t\t\t\t// drop the memtable on the floor to short circuit the retry loop.\n\t\t\t\t// NB: we hold db.mu here.\n\t\t\t\tvar cur *flushableEntry\n\t\t\t\tcur, db.mu.mem.queue = db.mu.mem.queue[0], db.mu.mem.queue[1:]\n\t\t\t\tcur.readerUnrefLocked(true)\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:  \"ingest\",\n\t\t\taddFn: ingestKeys,\n\t\t},\n\t}\n\n\trunTest := func(t *testing.T, addFn func(db *DB) error, bgFn func(*DB, error)) {\n\t\tvar db *DB\n\t\tinj := &createManifestErrorInjector{}\n\t\tlogger := &fatalCapturingLogger{t: t}\n\t\topts := (&Options{\n\t\t\tFS: errorfs.Wrap(vfs.NewMem(), inj),\n\t\t\t// Rotate the manifest after each write. This is required to trigger a\n\t\t\t// file creation, into which errors can be injected.\n\t\t\tMaxManifestFileSize: 1,\n\t\t\tLogger:              logger,\n\t\t\tEventListener: &EventListener{\n\t\t\t\tBackgroundError: func(err error) {\n\t\t\t\t\tif bgFn != nil {\n\t\t\t\t\t\tbgFn(db, err)\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t},\n\t\t\tDisableAutomaticCompactions: true,\n\t\t}).WithFSDefaults()\n\n\t\tdb, err := Open(\"\", opts)\n\t\trequire.NoError(t, err)\n\t\tdefer func() { _ = db.Close() }()\n\n\t\tinj.enable()\n\t\terr = addFn(db)\n\t\trequire.True(t, errors.Is(err, errorfs.ErrInjected))\n\n\t\t// Under normal circumstances, such an error in logAndApply would panic and\n\t\t// cause the DB to terminate here. Assert that we captured the fatal error.\n\t\trequire.True(t, errors.Is(logger.err, errorfs.ErrInjected))\n\t}\n\tfor _, tc := range testCases {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\trunTest(t, tc.addFn, tc.backgroundErrorFn)\n\t\t})\n\t}\n}\n\n// TestSharedObjectDeletePacing tests that we don't throttle shared object\n// deletes (see the TargetBytesDeletionRate option).\nfunc TestSharedObjectDeletePacing(t *testing.T) {\n\tvar opts Options\n\topts.FS = vfs.NewMem()\n\topts.Experimental.RemoteStorage = remote.MakeSimpleFactory(map[remote.Locator]remote.Storage{\n\t\t\"\": remote.NewInMem(),\n\t})\n\topts.Experimental.CreateOnShared = remote.CreateOnSharedAll\n\topts.TargetByteDeletionRate = 1\n\topts.Logger = testLogger{t}\n\n\td, err := Open(\"\", &opts)\n\trequire.NoError(t, err)\n\trequire.NoError(t, d.SetCreatorID(1))\n\n\trandVal := func() []byte {\n\t\tres := make([]byte, 1024)\n\t\t_, err := crand.Read(res)\n\t\trequire.NoError(t, err)\n\t\treturn res\n\t}\n\n\t// We must set up things so that we will have more live bytes than obsolete\n\t// bytes, otherwise delete pacing will be disabled anyway.\n\tkey := func(i int) string {\n\t\treturn fmt.Sprintf(\"k%02d\", i)\n\t}\n\tconst numKeys = 20\n\tfor i := 1; i <= numKeys; i++ {\n\t\trequire.NoError(t, d.Set([]byte(key(i)), randVal(), nil))\n\t\trequire.NoError(t, d.Compact([]byte(key(i)), []byte(key(i)+\"1\"), false))\n\t}\n\n\tdone := make(chan struct{})\n\tgo func() {\n\t\terr = d.DeleteRange([]byte(key(5)), []byte(key(9)), nil)\n\t\tif err == nil {\n\t\t\terr = d.Compact([]byte(key(5)), []byte(key(9)), false)\n\t\t}\n\t\t// Wait for objects to be deleted.\n\t\tfor {\n\t\t\ttime.Sleep(10 * time.Millisecond)\n\t\t\tif len(d.objProvider.List()) < numKeys-2 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tclose(done)\n\t}()\n\n\tselect {\n\tcase <-time.After(60 * time.Second):\n\t\t// Don't close the DB in this case (the goroutine above might panic).\n\t\tt.Fatalf(\"compaction timed out, possibly due to incorrect deletion pacing\")\n\tcase <-done:\n\t}\n\trequire.NoError(t, err)\n\td.Close()\n}\n\ntype WriteErrorInjector struct {\n\tenabled atomic.Bool\n}\n\n// TODO(jackson): Replace WriteErrorInjector with use of primitives in errorfs,\n// adding new primitives as necessary.\n\nfunc (i *WriteErrorInjector) String() string { return \"FileWrites(ErrInjected)\" }\n\n// enable enables error injection for the vfs.FS.\nfunc (i *WriteErrorInjector) enable() {\n\ti.enabled.Store(true)\n}\n\n// disable disabled error injection for the vfs.FS.\nfunc (i *WriteErrorInjector) disable() {\n\ti.enabled.Store(false)\n}\n\n// MaybeError implements errorfs.Injector.\nfunc (i *WriteErrorInjector) MaybeError(op errorfs.Op) error {\n\tif !i.enabled.Load() {\n\t\treturn nil\n\t}\n\t// Fail any future write.\n\tif op.Kind == errorfs.OpFileWrite {\n\t\treturn errorfs.ErrInjected\n\t}\n\treturn nil\n}\n\nvar _ errorfs.Injector = &WriteErrorInjector{}\n\n// Cumulative compaction stats shouldn't be updated on compaction error.\nfunc TestCompactionErrorStats(t *testing.T) {\n\t// protected by d.mu\n\tvar (\n\t\tuseInjector   bool\n\t\ttablesCreated []base.DiskFileNum\n\t)\n\n\tmem := vfs.NewMem()\n\tinjector := &WriteErrorInjector{}\n\topts := (&Options{\n\t\tFS:     errorfs.Wrap(mem, injector),\n\t\tLevels: make([]LevelOptions, numLevels),\n\t\tEventListener: &EventListener{\n\t\t\tTableCreated: func(info TableCreateInfo) {\n\t\t\t\tt.Log(info)\n\n\t\t\t\tif useInjector {\n\t\t\t\t\t// We'll write 3 tables during compaction, and we only need\n\t\t\t\t\t// the writes to error on the third file write, so only enable\n\t\t\t\t\t// the injector after the first two files have been written to.\n\t\t\t\t\ttablesCreated = append(tablesCreated, info.FileNum)\n\t\t\t\t\tif len(tablesCreated) >= 2 {\n\t\t\t\t\t\tinjector.enable()\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\t}).WithFSDefaults()\n\tfor i := range opts.Levels {\n\t\topts.Levels[i].TargetFileSize = 1\n\t}\n\topts.testingRandomized(t)\n\td, err := Open(\"\", opts)\n\trequire.NoError(t, err)\n\n\tingest := func(keys ...string) {\n\t\tt.Helper()\n\t\tf, err := mem.Create(\"ext\", vfs.WriteCategoryUnspecified)\n\t\trequire.NoError(t, err)\n\n\t\tw := sstable.NewWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{\n\t\t\tTableFormat: d.TableFormat(),\n\t\t})\n\t\tfor _, k := range keys {\n\t\t\trequire.NoError(t, w.Set([]byte(k), nil))\n\t\t}\n\t\trequire.NoError(t, w.Close())\n\t\trequire.NoError(t, d.Ingest(context.Background(), []string{\"ext\"}))\n\t}\n\tingest(\"a\", \"c\")\n\t// Snapshot will preserve the older \"a\" key during compaction.\n\tsnap := d.NewSnapshot()\n\tingest(\"a\", \"b\")\n\n\t// Trigger a manual compaction, which will encounter an injected error\n\t// after the second table is created.\n\td.mu.Lock()\n\tuseInjector = true\n\td.mu.Unlock()\n\n\terr = d.Compact([]byte(\"a\"), []byte(\"d\"), false)\n\trequire.Error(t, err, \"injected error\")\n\n\t// Due to the error, stats shouldn't have been updated.\n\td.mu.Lock()\n\trequire.Equal(t, 0, int(d.mu.snapshots.cumulativePinnedCount))\n\trequire.Equal(t, 0, int(d.mu.snapshots.cumulativePinnedSize))\n\tuseInjector = false\n\td.mu.Unlock()\n\n\tinjector.disable()\n\n\t// The following compaction won't error, but snapshot is open, so snapshot\n\t// pinned stats should update.\n\trequire.NoError(t, d.Compact([]byte(\"a\"), []byte(\"d\"), false))\n\trequire.NoError(t, snap.Close())\n\n\td.mu.Lock()\n\trequire.Equal(t, 1, int(d.mu.snapshots.cumulativePinnedCount))\n\trequire.Equal(t, 9, int(d.mu.snapshots.cumulativePinnedSize))\n\td.mu.Unlock()\n\trequire.NoError(t, d.Close())\n}\n"
        },
        {
          "name": "comparer.go",
          "type": "blob",
          "size": 1.1787109375,
          "content": "// Copyright 2011 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport \"github.com/cockroachdb/pebble/internal/base\"\n\n// Compare exports the base.Compare type.\ntype Compare = base.Compare\n\n// Equal exports the base.Equal type.\ntype Equal = base.Equal\n\n// AbbreviatedKey exports the base.AbbreviatedKey type.\ntype AbbreviatedKey = base.AbbreviatedKey\n\n// Separator exports the base.Separator type.\ntype Separator = base.Separator\n\n// Successor exports the base.Successor type.\ntype Successor = base.Successor\n\n// Split exports the base.Split type.\ntype Split = base.Split\n\n// Comparer exports the base.Comparer type.\ntype Comparer = base.Comparer\n\n// DefaultComparer exports the base.DefaultComparer variable.\nvar DefaultComparer = base.DefaultComparer\n\n// CheckComparer is a mini test suite that verifies a comparer implementation.\n//\n// It takes strictly ordered (according to the comparator) lists of prefixes and\n// suffixes. Both lists must contain the empty slice. It is recommended that\n// both lists have at least three elements.\nvar CheckComparer = base.CheckComparer\n"
        },
        {
          "name": "data_test.go",
          "type": "blob",
          "size": 40.724609375,
          "content": "// Copyright 2018 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\tcrand \"crypto/rand\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\t\"math/rand/v2\"\n\t\"regexp\"\n\t\"slices\"\n\t\"strconv\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/datadriven\"\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/bloom\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/humanize\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan\"\n\t\"github.com/cockroachdb/pebble/internal/rangekey\"\n\t\"github.com/cockroachdb/pebble/internal/sstableinternal\"\n\t\"github.com/cockroachdb/pebble/internal/testkeys\"\n\t\"github.com/cockroachdb/pebble/objstorage/objstorageprovider\"\n\t\"github.com/cockroachdb/pebble/objstorage/remote\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/cockroachdb/pebble/vfs/errorfs\"\n\t\"github.com/cockroachdb/pebble/wal\"\n\t\"github.com/ghemawat/stream\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc runGetCmd(t testing.TB, td *datadriven.TestData, d *DB) string {\n\tsnap := Snapshot{\n\t\tdb:     d,\n\t\tseqNum: base.SeqNumMax,\n\t}\n\tif td.HasArg(\"seq\") {\n\t\tvar n uint64\n\t\ttd.ScanArgs(t, \"seq\", &n)\n\t\tsnap.seqNum = base.SeqNum(n)\n\t}\n\n\tvar buf bytes.Buffer\n\tfor _, data := range strings.Split(td.Input, \"\\n\") {\n\t\tv, closer, err := snap.Get([]byte(data))\n\t\tif err != nil {\n\t\t\tfmt.Fprintf(&buf, \"%s: %s\\n\", data, err)\n\t\t} else {\n\t\t\tfmt.Fprintf(&buf, \"%s:%s\\n\", data, v)\n\t\t\tcloser.Close()\n\t\t}\n\t}\n\treturn buf.String()\n}\n\nfunc runIterCmd(d *datadriven.TestData, iter *Iterator, closeIter bool) string {\n\tif closeIter {\n\t\tdefer func() {\n\t\t\tif iter != nil {\n\t\t\t\titer.Close()\n\t\t\t}\n\t\t}()\n\t}\n\tvar b bytes.Buffer\n\tfor _, line := range strings.Split(d.Input, \"\\n\") {\n\t\tparts := strings.Fields(line)\n\t\tif len(parts) == 0 {\n\t\t\tcontinue\n\t\t}\n\t\tprintValidityState := false\n\t\tvar valid bool\n\t\tvar validityState IterValidityState\n\t\tswitch parts[0] {\n\t\tcase \"seek-ge\":\n\t\t\tif len(parts) != 2 {\n\t\t\t\treturn \"seek-ge <key>\\n\"\n\t\t\t}\n\t\t\tvalid = iter.SeekGE([]byte(parts[1]))\n\t\tcase \"seek-prefix-ge\":\n\t\t\tif len(parts) != 2 {\n\t\t\t\treturn \"seek-prefix-ge <key>\\n\"\n\t\t\t}\n\t\t\tvalid = iter.SeekPrefixGE([]byte(parts[1]))\n\t\tcase \"seek-lt\":\n\t\t\tif len(parts) != 2 {\n\t\t\t\treturn \"seek-lt <key>\\n\"\n\t\t\t}\n\t\t\tvalid = iter.SeekLT([]byte(parts[1]))\n\t\tcase \"seek-ge-limit\":\n\t\t\tif len(parts) != 3 {\n\t\t\t\treturn \"seek-ge-limit <key> <limit>\\n\"\n\t\t\t}\n\t\t\tvalidityState = iter.SeekGEWithLimit(\n\t\t\t\t[]byte(parts[1]), []byte(parts[2]))\n\t\t\tprintValidityState = true\n\t\tcase \"seek-lt-limit\":\n\t\t\tif len(parts) != 3 {\n\t\t\t\treturn \"seek-lt-limit <key> <limit>\\n\"\n\t\t\t}\n\t\t\tvalidityState = iter.SeekLTWithLimit(\n\t\t\t\t[]byte(parts[1]), []byte(parts[2]))\n\t\t\tprintValidityState = true\n\t\tcase \"inspect\":\n\t\t\tif len(parts) != 2 {\n\t\t\t\treturn \"inspect <field>\\n\"\n\t\t\t}\n\t\t\tfield := parts[1]\n\t\t\tswitch field {\n\t\t\tcase \"lastPositioningOp\":\n\t\t\t\top := \"?\"\n\t\t\t\tswitch iter.lastPositioningOp {\n\t\t\t\tcase unknownLastPositionOp:\n\t\t\t\t\top = \"unknown\"\n\t\t\t\tcase seekPrefixGELastPositioningOp:\n\t\t\t\t\top = \"seekprefixge\"\n\t\t\t\tcase seekGELastPositioningOp:\n\t\t\t\t\top = \"seekge\"\n\t\t\t\tcase seekLTLastPositioningOp:\n\t\t\t\t\top = \"seeklt\"\n\t\t\t\t}\n\t\t\t\tfmt.Fprintf(&b, \"%s=%q\\n\", field, op)\n\t\t\tdefault:\n\t\t\t\treturn fmt.Sprintf(\"unrecognized inspect field %q\\n\", field)\n\t\t\t}\n\t\t\tcontinue\n\t\tcase \"next-limit\":\n\t\t\tif len(parts) != 2 {\n\t\t\t\treturn \"next-limit <limit>\\n\"\n\t\t\t}\n\t\t\tvalidityState = iter.NextWithLimit([]byte(parts[1]))\n\t\t\tprintValidityState = true\n\t\tcase \"internal-next\":\n\t\t\tvalidity, keyKind := iter.internalNext()\n\t\t\tswitch validity {\n\t\t\tcase internalNextError:\n\t\t\t\tfmt.Fprintf(&b, \"err: %s\\n\", iter.Error())\n\t\t\tcase internalNextExhausted:\n\t\t\t\tfmt.Fprint(&b, \".\\n\")\n\t\t\tcase internalNextValid:\n\t\t\t\tfmt.Fprintf(&b, \"%s\\n\", keyKind)\n\t\t\tdefault:\n\t\t\t\tpanic(\"unreachable\")\n\t\t\t}\n\t\t\tcontinue\n\t\tcase \"can-deterministically-single-delete\":\n\t\t\tok, err := CanDeterministicallySingleDelete(iter)\n\t\t\tif err != nil {\n\t\t\t\tfmt.Fprintf(&b, \"err: %s\\n\", err)\n\t\t\t} else {\n\t\t\t\tfmt.Fprintf(&b, \"%t\\n\", ok)\n\t\t\t}\n\t\t\tcontinue\n\t\tcase \"prev-limit\":\n\t\t\tif len(parts) != 2 {\n\t\t\t\treturn \"prev-limit <limit>\\n\"\n\t\t\t}\n\t\t\tvalidityState = iter.PrevWithLimit([]byte(parts[1]))\n\t\t\tprintValidityState = true\n\t\tcase \"first\":\n\t\t\tvalid = iter.First()\n\t\tcase \"last\":\n\t\t\tvalid = iter.Last()\n\t\tcase \"next\":\n\t\t\tvalid = iter.Next()\n\t\tcase \"next-prefix\":\n\t\t\tvalid = iter.NextPrefix()\n\t\tcase \"prev\":\n\t\t\tvalid = iter.Prev()\n\t\tcase \"set-bounds\":\n\t\t\tif len(parts) <= 1 || len(parts) > 3 {\n\t\t\t\treturn \"set-bounds lower=<lower> upper=<upper>\\n\"\n\t\t\t}\n\t\t\tvar lower []byte\n\t\t\tvar upper []byte\n\t\t\tfor _, part := range parts[1:] {\n\t\t\t\targ := strings.Split(part, \"=\")\n\t\t\t\tswitch arg[0] {\n\t\t\t\tcase \"lower\":\n\t\t\t\t\tlower = []byte(arg[1])\n\t\t\t\tcase \"upper\":\n\t\t\t\t\tupper = []byte(arg[1])\n\t\t\t\tdefault:\n\t\t\t\t\treturn fmt.Sprintf(\"set-bounds: unknown arg: %s\", arg)\n\t\t\t\t}\n\t\t\t}\n\t\t\titer.SetBounds(lower, upper)\n\t\t\tvalid = iter.Valid()\n\t\tcase \"set-options\":\n\t\t\topts := iter.opts\n\t\t\tif _, err := parseIterOptions(&opts, &iter.opts, parts[1:]); err != nil {\n\t\t\t\treturn fmt.Sprintf(\"set-options: %s\", err.Error())\n\t\t\t}\n\t\t\titer.SetOptions(&opts)\n\t\t\tvalid = iter.Valid()\n\t\tcase \"stats\":\n\t\t\tstats := iter.Stats()\n\t\t\t// The timing is non-deterministic, so set to 0.\n\t\t\tstats.InternalStats.BlockReadDuration = 0\n\t\t\tfmt.Fprintf(&b, \"stats: %s\\n\", stats.String())\n\t\t\tcontinue\n\t\tcase \"clone\":\n\t\t\tvar opts CloneOptions\n\t\t\tif len(parts) > 1 {\n\t\t\t\tvar iterOpts IterOptions\n\t\t\t\tif foundAny, err := parseIterOptions(&iterOpts, &iter.opts, parts[1:]); err != nil {\n\t\t\t\t\treturn fmt.Sprintf(\"clone: %s\", err.Error())\n\t\t\t\t} else if foundAny {\n\t\t\t\t\topts.IterOptions = &iterOpts\n\t\t\t\t}\n\t\t\t\tfor _, part := range parts[1:] {\n\t\t\t\t\tif arg := strings.Split(part, \"=\"); len(arg) == 2 && arg[0] == \"refresh-batch\" {\n\t\t\t\t\t\tvar err error\n\t\t\t\t\t\topts.RefreshBatchView, err = strconv.ParseBool(arg[1])\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\treturn fmt.Sprintf(\"clone: refresh-batch: %s\", err.Error())\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tclonedIter, err := iter.Clone(opts)\n\t\t\tif err != nil {\n\t\t\t\tfmt.Fprintf(&b, \"error in clone, skipping rest of input: err=%v\\n\", err)\n\t\t\t\treturn b.String()\n\t\t\t}\n\t\t\tif err = iter.Close(); err != nil {\n\t\t\t\tfmt.Fprintf(&b, \"err=%v\\n\", err)\n\t\t\t}\n\t\t\titer = clonedIter\n\t\tcase \"is-using-combined\":\n\t\t\tif iter.opts.KeyTypes != IterKeyTypePointsAndRanges {\n\t\t\t\tfmt.Fprintln(&b, \"not configured for combined iteration\")\n\t\t\t} else if iter.lazyCombinedIter.combinedIterState.initialized {\n\t\t\t\tfmt.Fprintln(&b, \"using combined (non-lazy) iterator\")\n\t\t\t} else {\n\t\t\t\tfmt.Fprintln(&b, \"using lazy iterator\")\n\t\t\t}\n\t\t\tcontinue\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown op: %s\", parts[0])\n\t\t}\n\n\t\tvalid = valid || validityState == IterValid\n\t\tif valid != iter.Valid() {\n\t\t\tfmt.Fprintf(&b, \"mismatched valid states: %t vs %t\\n\", valid, iter.Valid())\n\t\t}\n\t\thasPoint, hasRange := iter.HasPointAndRange()\n\t\thasEither := hasPoint || hasRange\n\t\tif hasEither != valid {\n\t\t\tfmt.Fprintf(&b, \"mismatched valid/HasPointAndRange states: valid=%t HasPointAndRange=(%t,%t)\\n\", valid, hasPoint, hasRange)\n\t\t}\n\n\t\tif valid {\n\t\t\tvalidityState = IterValid\n\t\t}\n\t\tprintIterState(&b, iter, validityState, printValidityState)\n\t}\n\treturn b.String()\n}\n\nfunc parseIterOptions(\n\topts *IterOptions, ref *IterOptions, parts []string,\n) (foundAny bool, err error) {\n\tconst usageString = \"[lower=<lower>] [upper=<upper>] [key-types=point|range|both] [mask-suffix=<suffix>] [mask-filter=<bool>] [only-durable=<bool>] point-filters=reuse|none]\\n\"\n\tfor _, part := range parts {\n\t\targ := strings.SplitN(part, \"=\", 2)\n\t\tif len(arg) != 2 {\n\t\t\treturn false, errors.Newf(usageString)\n\t\t}\n\t\tswitch arg[0] {\n\t\tcase \"point-filters\":\n\t\t\tswitch arg[1] {\n\t\t\tcase \"reuse\":\n\t\t\t\topts.PointKeyFilters = ref.PointKeyFilters\n\t\t\tcase \"none\":\n\t\t\t\topts.PointKeyFilters = nil\n\t\t\tdefault:\n\t\t\t\treturn false, errors.Newf(\"unknown arg point-filter=%q:\\n%s\", arg[1], usageString)\n\t\t\t}\n\t\tcase \"lower\":\n\t\t\topts.LowerBound = []byte(arg[1])\n\t\tcase \"upper\":\n\t\t\topts.UpperBound = []byte(arg[1])\n\t\tcase \"key-types\":\n\t\t\tswitch arg[1] {\n\t\t\tcase \"point\":\n\t\t\t\topts.KeyTypes = IterKeyTypePointsOnly\n\t\t\tcase \"range\":\n\t\t\t\topts.KeyTypes = IterKeyTypeRangesOnly\n\t\t\tcase \"both\":\n\t\t\t\topts.KeyTypes = IterKeyTypePointsAndRanges\n\t\t\tdefault:\n\t\t\t\treturn false, errors.Newf(\"unknown key-type %q:\\n%s\", arg[1], usageString)\n\t\t\t}\n\t\tcase \"mask-suffix\":\n\t\t\topts.RangeKeyMasking.Suffix = []byte(arg[1])\n\t\tcase \"mask-filter\":\n\t\t\topts.RangeKeyMasking.Filter = func() BlockPropertyFilterMask {\n\t\t\t\treturn sstable.NewTestKeysMaskingFilter()\n\t\t\t}\n\t\tcase \"only-durable\":\n\t\t\tvar err error\n\t\t\topts.OnlyReadGuaranteedDurable, err = strconv.ParseBool(arg[1])\n\t\t\tif err != nil {\n\t\t\t\treturn false, errors.Newf(\"cannot parse only-durable=%q: %s\", arg[1], err)\n\t\t\t}\n\t\tdefault:\n\t\t\tcontinue\n\t\t}\n\t\tfoundAny = true\n\t}\n\treturn foundAny, nil\n}\n\nfunc printIterState(\n\tb io.Writer, iter *Iterator, validity IterValidityState, printValidityState bool,\n) {\n\tvar validityStateStr string\n\tif printValidityState {\n\t\tswitch validity {\n\t\tcase IterExhausted:\n\t\t\tvalidityStateStr = \" exhausted\"\n\t\tcase IterValid:\n\t\t\tvalidityStateStr = \" valid\"\n\t\tcase IterAtLimit:\n\t\t\tvalidityStateStr = \" at-limit\"\n\t\t}\n\t}\n\tif validity == IterValid {\n\t\tswitch {\n\t\tcase iter.opts.pointKeys():\n\t\t\thasPoint, hasRange := iter.HasPointAndRange()\n\t\t\tfmt.Fprintf(b, \"%s:%s (\", iter.Key(), validityStateStr)\n\t\t\tif hasPoint {\n\t\t\t\tfmt.Fprintf(b, \"%s, \", formatASCIIValue(iter.Value()))\n\t\t\t} else {\n\t\t\t\tfmt.Fprint(b, \"., \")\n\t\t\t}\n\t\t\tif hasRange {\n\t\t\t\tstart, end := iter.RangeBounds()\n\t\t\t\tfmt.Fprintf(b, \"[%s-%s)\", formatASCIIKey(start), formatASCIIKey(end))\n\t\t\t\twriteRangeKeys(b, iter)\n\t\t\t} else {\n\t\t\t\tfmt.Fprint(b, \".\")\n\t\t\t}\n\t\t\tif iter.RangeKeyChanged() {\n\t\t\t\tfmt.Fprint(b, \" UPDATED\")\n\t\t\t}\n\t\t\tfmt.Fprint(b, \")\")\n\t\tdefault:\n\t\t\tif iter.Valid() {\n\t\t\t\thasPoint, hasRange := iter.HasPointAndRange()\n\t\t\t\tif hasPoint || !hasRange {\n\t\t\t\t\tpanic(fmt.Sprintf(\"pebble: unexpected HasPointAndRange (%t, %t)\", hasPoint, hasRange))\n\t\t\t\t}\n\t\t\t\tstart, end := iter.RangeBounds()\n\t\t\t\tfmt.Fprintf(b, \"%s [%s-%s)\", iter.Key(), formatASCIIKey(start), formatASCIIKey(end))\n\t\t\t\twriteRangeKeys(b, iter)\n\t\t\t} else {\n\t\t\t\tfmt.Fprint(b, \".\")\n\t\t\t}\n\t\t\tif iter.RangeKeyChanged() {\n\t\t\t\tfmt.Fprint(b, \" UPDATED\")\n\t\t\t}\n\t\t}\n\t\tfmt.Fprintln(b)\n\t} else {\n\t\tif err := iter.Error(); err != nil {\n\t\t\tfmt.Fprintf(b, \"err=%v\\n\", err)\n\t\t} else {\n\t\t\tfmt.Fprintf(b, \".%s\\n\", validityStateStr)\n\t\t}\n\t}\n}\n\nfunc formatASCIIKey(b []byte) string {\n\tif bytes.IndexFunc(b, func(r rune) bool { return r < 'A' || r > 'z' }) != -1 {\n\t\t// This key is not just ASCII letters. Quote it.\n\t\treturn fmt.Sprintf(\"%q\", b)\n\t}\n\treturn string(b)\n}\n\nfunc formatASCIIValue(b []byte) string {\n\tif len(b) > 1<<10 {\n\t\treturn fmt.Sprintf(\"[LARGE VALUE len=%d]\", len(b))\n\t}\n\tif bytes.IndexFunc(b, func(r rune) bool { return r < '!' || r > 'z' }) != -1 {\n\t\t// This key is not just legible ASCII characters. Quote it.\n\t\treturn fmt.Sprintf(\"%q\", b)\n\t}\n\treturn string(b)\n}\n\nfunc writeRangeKeys(b io.Writer, iter *Iterator) {\n\trangeKeys := iter.RangeKeys()\n\tfor j := 0; j < len(rangeKeys); j++ {\n\t\tif j > 0 {\n\t\t\tfmt.Fprint(b, \",\")\n\t\t}\n\t\tfmt.Fprintf(b, \" %s=%s\", rangeKeys[j].Suffix, formatASCIIValue(rangeKeys[j].Value))\n\t}\n}\n\nfunc parseValue(s string) []byte {\n\tif strings.HasPrefix(s, \"<rand-bytes=\") {\n\t\ts = strings.TrimPrefix(s, \"<rand-bytes=\")\n\t\ts = strings.TrimSuffix(s, \">\")\n\t\tn, err := strconv.Atoi(s)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\tb := make([]byte, n)\n\t\trnd := rand.New(rand.NewPCG(0, uint64(n)))\n\t\tfor i := range b {\n\t\t\tb[i] = byte(rnd.Uint32())\n\t\t}\n\t\treturn b\n\t}\n\treturn []byte(s)\n}\n\nfunc runBatchDefineCmd(d *datadriven.TestData, b *Batch) error {\n\tfor _, line := range strings.Split(d.Input, \"\\n\") {\n\t\tparts := strings.Fields(line)\n\t\tif len(parts) == 0 {\n\t\t\tcontinue\n\t\t}\n\t\tif parts[1] == `<nil>` {\n\t\t\tparts[1] = \"\"\n\t\t}\n\t\tvar err error\n\t\tswitch parts[0] {\n\t\tcase \"set\":\n\t\t\tif len(parts) != 3 {\n\t\t\t\treturn errors.Errorf(\"%s expects 2 arguments\", parts[0])\n\t\t\t}\n\t\t\terr = b.Set([]byte(parts[1]), parseValue(parts[2]), nil)\n\n\t\tcase \"set-multiple\":\n\t\t\tif len(parts) != 3 {\n\t\t\t\treturn errors.Errorf(\"%s expects 2 arguments (n and prefix)\", parts[0])\n\t\t\t}\n\t\t\tn, err := strconv.ParseUint(parts[1], 10, 32)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tfor i := uint64(0); i < n; i++ {\n\t\t\t\tkey := fmt.Sprintf(\"%s-%05d\", parts[2], i)\n\t\t\t\tval := fmt.Sprintf(\"val-%05d\", i)\n\t\t\t\tif err := b.Set([]byte(key), []byte(val), nil); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\n\t\tcase \"del\":\n\t\t\tif len(parts) != 2 {\n\t\t\t\treturn errors.Errorf(\"%s expects 1 argument\", parts[0])\n\t\t\t}\n\t\t\terr = b.Delete([]byte(parts[1]), nil)\n\t\tcase \"del-sized\":\n\t\t\tif len(parts) != 3 {\n\t\t\t\treturn errors.Errorf(\"%s expects 2 arguments\", parts[0])\n\t\t\t}\n\t\t\tvar valSize uint64\n\t\t\tvalSize, err = strconv.ParseUint(parts[2], 10, 32)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\terr = b.DeleteSized([]byte(parts[1]), uint32(valSize), nil)\n\t\tcase \"singledel\":\n\t\t\tif len(parts) != 2 {\n\t\t\t\treturn errors.Errorf(\"%s expects 1 argument\", parts[0])\n\t\t\t}\n\t\t\terr = b.SingleDelete([]byte(parts[1]), nil)\n\t\tcase \"del-range\":\n\t\t\tif len(parts) != 3 {\n\t\t\t\treturn errors.Errorf(\"%s expects 2 arguments\", parts[0])\n\t\t\t}\n\t\t\terr = b.DeleteRange([]byte(parts[1]), []byte(parts[2]), nil)\n\t\tcase \"merge\":\n\t\t\tif len(parts) != 3 {\n\t\t\t\treturn errors.Errorf(\"%s expects 2 arguments\", parts[0])\n\t\t\t}\n\t\t\terr = b.Merge([]byte(parts[1]), parseValue(parts[2]), nil)\n\t\tcase \"range-key-set\":\n\t\t\tif len(parts) < 4 || len(parts) > 5 {\n\t\t\t\treturn errors.Errorf(\"%s expects 3 or 4 arguments\", parts[0])\n\t\t\t}\n\t\t\tvar val []byte\n\t\t\tif len(parts) == 5 {\n\t\t\t\tval = parseValue(parts[4])\n\t\t\t}\n\t\t\terr = b.RangeKeySet(\n\t\t\t\t[]byte(parts[1]),\n\t\t\t\t[]byte(parts[2]),\n\t\t\t\t[]byte(parts[3]),\n\t\t\t\tval,\n\t\t\t\tnil)\n\t\tcase \"range-key-unset\":\n\t\t\tif len(parts) != 4 {\n\t\t\t\treturn errors.Errorf(\"%s expects 3 arguments\", parts[0])\n\t\t\t}\n\t\t\terr = b.RangeKeyUnset(\n\t\t\t\t[]byte(parts[1]),\n\t\t\t\t[]byte(parts[2]),\n\t\t\t\t[]byte(parts[3]),\n\t\t\t\tnil)\n\t\tcase \"range-key-del\":\n\t\t\tif len(parts) != 3 {\n\t\t\t\treturn errors.Errorf(\"%s expects 2 arguments\", parts[0])\n\t\t\t}\n\t\t\terr = b.RangeKeyDelete(\n\t\t\t\t[]byte(parts[1]),\n\t\t\t\t[]byte(parts[2]),\n\t\t\t\tnil)\n\t\tdefault:\n\t\t\treturn errors.Errorf(\"unknown op: %s\", parts[0])\n\t\t}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc runBuildRemoteCmd(td *datadriven.TestData, d *DB, storage remote.Storage) error {\n\tb := d.NewIndexedBatch()\n\tif err := runBatchDefineCmd(td, b); err != nil {\n\t\treturn err\n\t}\n\n\tif len(td.CmdArgs) < 1 {\n\t\treturn errors.New(\"build <path>: argument missing\")\n\t}\n\tpath := td.CmdArgs[0].String()\n\n\t// Override table format, if provided.\n\ttableFormat := d.TableFormat()\n\tvar blockSize int64\n\tfor _, cmdArg := range td.CmdArgs[1:] {\n\t\tswitch cmdArg.Key {\n\t\tcase \"format\":\n\t\t\tswitch cmdArg.Vals[0] {\n\t\t\tcase \"pebblev1\":\n\t\t\t\ttableFormat = sstable.TableFormatPebblev1\n\t\t\tcase \"pebblev2\":\n\t\t\t\ttableFormat = sstable.TableFormatPebblev2\n\t\t\tcase \"pebblev3\":\n\t\t\t\ttableFormat = sstable.TableFormatPebblev3\n\t\t\tcase \"pebblev4\":\n\t\t\t\ttableFormat = sstable.TableFormatPebblev4\n\t\t\tdefault:\n\t\t\t\treturn errors.Errorf(\"unknown format string %s\", cmdArg.Vals[0])\n\t\t\t}\n\t\tcase \"block-size\":\n\t\t\tvar err error\n\t\t\tblockSize, err = strconv.ParseInt(cmdArg.Vals[0], 10, 64)\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Wrap(err, td.Pos)\n\t\t\t}\n\t\t}\n\t}\n\n\twriteOpts := d.opts.MakeWriterOptions(0 /* level */, tableFormat)\n\tif blockSize == 0 && rand.IntN(4) == 0 {\n\t\t// Force two-level indexes if not already forced on or off.\n\t\tblockSize = 5\n\t}\n\twriteOpts.BlockSize = int(blockSize)\n\twriteOpts.IndexBlockSize = int(blockSize)\n\n\tf, err := storage.CreateObject(path)\n\tif err != nil {\n\t\treturn err\n\t}\n\tw := sstable.NewWriter(objstorageprovider.NewRemoteWritable(f), writeOpts)\n\titer := b.newInternalIter(nil)\n\tfor kv := iter.First(); kv != nil; kv = iter.Next() {\n\t\ttmp := kv.K\n\t\ttmp.SetSeqNum(0)\n\t\tif err := w.Raw().AddWithForceObsolete(tmp, kv.InPlaceValue(), false); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif err := iter.Close(); err != nil {\n\t\treturn err\n\t}\n\n\tif rdi := b.newRangeDelIter(nil, math.MaxUint64); rdi != nil {\n\t\ts, err := rdi.First()\n\t\tfor ; s != nil && err == nil; s, err = rdi.Next() {\n\t\t\tif err = w.DeleteRange(s.Start, s.End); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif rki := b.newRangeKeyIter(nil, math.MaxUint64); rki != nil {\n\t\ts, err := rki.First()\n\t\tfor ; s != nil; s, err = rki.Next() {\n\t\t\tfor _, k := range s.Keys {\n\t\t\t\tvar err error\n\t\t\t\tswitch k.Kind() {\n\t\t\t\tcase base.InternalKeyKindRangeKeySet:\n\t\t\t\t\terr = w.RangeKeySet(s.Start, s.End, k.Suffix, k.Value)\n\t\t\t\tcase base.InternalKeyKindRangeKeyUnset:\n\t\t\t\t\terr = w.RangeKeyUnset(s.Start, s.End, k.Suffix)\n\t\t\t\tcase base.InternalKeyKindRangeKeyDelete:\n\t\t\t\t\terr = w.RangeKeyDelete(s.Start, s.End)\n\t\t\t\tdefault:\n\t\t\t\t\tpanic(\"not a range key\")\n\t\t\t\t}\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn w.Close()\n}\n\nfunc runBuildCmd(td *datadriven.TestData, d *DB, fs vfs.FS) error {\n\tb := newIndexedBatch(nil, d.opts.Comparer)\n\tif err := runBatchDefineCmd(td, b); err != nil {\n\t\treturn err\n\t}\n\n\tif len(td.CmdArgs) < 1 {\n\t\treturn errors.New(\"build <path>: argument missing\")\n\t}\n\tpath := td.CmdArgs[0].String()\n\n\t// Override table format, if provided.\n\ttableFormat := d.TableFormat()\n\tfor _, cmdArg := range td.CmdArgs[1:] {\n\t\tswitch cmdArg.Key {\n\t\tcase \"format\":\n\t\t\tswitch cmdArg.Vals[0] {\n\t\t\tcase \"pebblev1\":\n\t\t\t\ttableFormat = sstable.TableFormatPebblev1\n\t\t\tcase \"pebblev2\":\n\t\t\t\ttableFormat = sstable.TableFormatPebblev2\n\t\t\tcase \"pebblev3\":\n\t\t\t\ttableFormat = sstable.TableFormatPebblev3\n\t\t\tcase \"pebblev4\":\n\t\t\t\ttableFormat = sstable.TableFormatPebblev4\n\t\t\tdefault:\n\t\t\t\treturn errors.Errorf(\"unknown format string %s\", cmdArg.Vals[0])\n\t\t\t}\n\t\t}\n\t}\n\n\twriteOpts := d.opts.MakeWriterOptions(0 /* level */, tableFormat)\n\n\tf, err := fs.Create(path, vfs.WriteCategoryUnspecified)\n\tif err != nil {\n\t\treturn err\n\t}\n\tw := sstable.NewWriter(objstorageprovider.NewFileWritable(f), writeOpts)\n\titer := b.newInternalIter(nil)\n\tfor kv := iter.First(); kv != nil; kv = iter.Next() {\n\t\ttmp := kv.K\n\t\ttmp.SetSeqNum(0)\n\t\tif err := w.Raw().AddWithForceObsolete(tmp, kv.InPlaceValue(), false); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif err := iter.Close(); err != nil {\n\t\treturn err\n\t}\n\n\tif rdi := b.newRangeDelIter(nil, math.MaxUint64); rdi != nil {\n\t\ts, err := rdi.First()\n\t\tfor ; s != nil && err == nil; s, err = rdi.Next() {\n\t\t\terr = w.DeleteRange(s.Start, s.End)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif rki := b.newRangeKeyIter(nil, math.MaxUint64); rki != nil {\n\t\ts, err := rki.First()\n\t\tfor ; s != nil; s, err = rki.Next() {\n\t\t\tfor _, k := range s.Keys {\n\t\t\t\tvar err error\n\t\t\t\tswitch k.Kind() {\n\t\t\t\tcase base.InternalKeyKindRangeKeySet:\n\t\t\t\t\terr = w.RangeKeySet(s.Start, s.End, k.Suffix, k.Value)\n\t\t\t\tcase base.InternalKeyKindRangeKeyUnset:\n\t\t\t\t\terr = w.RangeKeyUnset(s.Start, s.End, k.Suffix)\n\t\t\t\tcase base.InternalKeyKindRangeKeyDelete:\n\t\t\t\t\terr = w.RangeKeyDelete(s.Start, s.End)\n\t\t\t\tdefault:\n\t\t\t\t\tpanic(\"not a range key\")\n\t\t\t\t}\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn w.Close()\n}\n\nfunc runCompactCmd(td *datadriven.TestData, d *DB) error {\n\tif len(td.CmdArgs) > 4 {\n\t\treturn errors.Errorf(\"%s expects at most four arguments\", td.Cmd)\n\t}\n\tparts := strings.Split(td.CmdArgs[0].Key, \"-\")\n\tif len(parts) != 2 {\n\t\treturn errors.Errorf(\"expected <begin>-<end>: %s\", td.Input)\n\t}\n\tparallelize := td.HasArg(\"parallel\")\n\tif len(td.CmdArgs) >= 2 && strings.HasPrefix(td.CmdArgs[1].Key, \"L\") {\n\t\tlevelString := td.CmdArgs[1].String()\n\t\tiStart := base.MakeInternalKey([]byte(parts[0]), base.SeqNumMax, InternalKeyKindMax)\n\t\tiEnd := base.MakeInternalKey([]byte(parts[1]), 0, 0)\n\t\tif levelString[0] != 'L' {\n\t\t\treturn errors.Errorf(\"expected L<n>: %s\", levelString)\n\t\t}\n\t\tlevel, err := strconv.Atoi(levelString[1:])\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn d.manualCompact(iStart.UserKey, iEnd.UserKey, level, parallelize)\n\t}\n\treturn d.Compact([]byte(parts[0]), []byte(parts[1]), parallelize)\n}\n\n// runDBDefineCmd prepares a database state, returning the opened\n// database with the initialized state.\n//\n// The command accepts input describing memtables and sstables to\n// construct. Each new table is indicated by a line containing the\n// level of the next table to build (eg, \"L6\"), or \"mem\" to build\n// a memtable. Each subsequent line contains a new key-value pair.\n//\n// Point keys and range deletions should be encoded as the\n// InternalKey's string representation, as understood by\n// ParseInternalKey, followed a colon and the corresponding value.\n//\n//\tb.SET.50:foo\n//\tc.DEL.20\n//\n// Range keys may be encoded by prefixing the line with `rangekey:`,\n// followed by the keyspan.Span string representation, as understood\n// by keyspan.ParseSpan.\n//\n//\trangekey:b-d:{(#5,RANGEKEYSET,@2,foo)}\n//\n// # Mechanics\n//\n// runDBDefineCmd works by simulating a flush for every file written.\n// Keys are written to a memtable. When a file is complete, the table\n// is flushed to physical files through manually invoking runCompaction.\n// The resulting version edit is then manipulated to write the files\n// to the indicated level.\n//\n// Because of it's low-level manipulation, runDBDefineCmd does allow the\n// creation of invalid database states. If opts.DebugCheck is set, the\n// level checker should detect the invalid state.\nfunc runDBDefineCmd(td *datadriven.TestData, opts *Options) (*DB, error) {\n\topts = opts.EnsureDefaults()\n\topts.FS = vfs.NewMem()\n\treturn runDBDefineCmdReuseFS(td, opts)\n}\n\n// runDBDefineCmdReuseFS is like runDBDefineCmd, but does not set opts.FS, expecting\n// the caller to have set an appropriate FS already.\nfunc runDBDefineCmdReuseFS(td *datadriven.TestData, opts *Options) (*DB, error) {\n\topts = opts.EnsureDefaults()\n\tif err := parseDBOptionsArgs(opts, td.CmdArgs); err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar snapshots []base.SeqNum\n\tvar levelMaxBytes map[int]int64\n\tfor _, arg := range td.CmdArgs {\n\t\tswitch arg.Key {\n\t\tcase \"snapshots\":\n\t\t\tsnapshots = make([]base.SeqNum, len(arg.Vals))\n\t\t\tfor i := range arg.Vals {\n\t\t\t\tsnapshots[i] = base.ParseSeqNum(arg.Vals[i])\n\t\t\t\tif i > 0 && snapshots[i] < snapshots[i-1] {\n\t\t\t\t\treturn nil, errors.New(\"Snapshots must be in ascending order\")\n\t\t\t\t}\n\t\t\t}\n\t\tcase \"level-max-bytes\":\n\t\t\tlevelMaxBytes = map[int]int64{}\n\t\t\tfor i := range arg.Vals {\n\t\t\t\tj := strings.Index(arg.Vals[i], \":\")\n\t\t\t\tlevelStr := strings.TrimSpace(arg.Vals[i][:j])\n\t\t\t\tlevel, err := strconv.Atoi(levelStr[1:])\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\tsize, err := strconv.ParseInt(strings.TrimSpace(arg.Vals[i][j+1:]), 10, 64)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\tlevelMaxBytes[level] = size\n\t\t\t}\n\t\t}\n\t}\n\n\td, err := Open(\"\", opts)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\tfor i := range snapshots {\n\t\ts := &Snapshot{db: d}\n\t\ts.seqNum = snapshots[i]\n\t\td.mu.snapshots.pushBack(s)\n\t}\n\t// Set the level max bytes only right before we exit; the body of this\n\t// function expects it to be unset.\n\tdefer func() {\n\t\tfor l, maxBytes := range levelMaxBytes {\n\t\t\td.mu.versions.picker.(*compactionPickerByScore).levelMaxBytes[l] = maxBytes\n\t\t}\n\t}()\n\tif td.Input == \"\" {\n\t\t// Empty LSM.\n\t\treturn d, nil\n\t}\n\td.mu.versions.dynamicBaseLevel = false\n\n\tvar mem *memTable\n\tvar start, end *base.InternalKey\n\tve := &versionEdit{}\n\tlevel := -1\n\n\tmaybeFlush := func() error {\n\t\tif level < 0 {\n\t\t\treturn nil\n\t\t}\n\n\t\ttoFlush := flushableList{{\n\t\t\tflushable: mem,\n\t\t\tflushed:   make(chan struct{}),\n\t\t}}\n\t\tc, err := newFlush(d.opts, d.mu.versions.currentVersion(),\n\t\t\td.mu.versions.picker.getBaseLevel(), toFlush, time.Now())\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// NB: define allows the test to exactly specify which keys go\n\t\t// into which sstables. If the test has a small target file\n\t\t// size to test grandparent limits, etc, the maxOutputFileSize\n\t\t// can cause splitting /within/ the bounds specified to the\n\t\t// test. Ignore the target size here, and split only according\n\t\t// to the user-defined boundaries.\n\t\tc.maxOutputFileSize = math.MaxUint64\n\n\t\tnewVE, _, err := d.runCompaction(0, c)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlargestSeqNum := d.mu.versions.logSeqNum.Load()\n\t\tfor _, f := range newVE.NewFiles {\n\t\t\tif start != nil {\n\t\t\t\tf.Meta.SmallestPointKey = *start\n\t\t\t\tf.Meta.Smallest = *start\n\t\t\t}\n\t\t\tif end != nil {\n\t\t\t\tf.Meta.LargestPointKey = *end\n\t\t\t\tf.Meta.Largest = *end\n\t\t\t}\n\t\t\tif largestSeqNum <= f.Meta.LargestSeqNum {\n\t\t\t\tlargestSeqNum = f.Meta.LargestSeqNum + 1\n\t\t\t}\n\t\t\tve.NewFiles = append(ve.NewFiles, newFileEntry{\n\t\t\t\tLevel: level,\n\t\t\t\tMeta:  f.Meta,\n\t\t\t})\n\t\t}\n\t\t// The committed keys were never written to the WAL, so neither\n\t\t// the logSeqNum nor the commit pipeline's visibleSeqNum have\n\t\t// been ratcheted. Manually ratchet them to the largest sequence\n\t\t// number committed to ensure iterators opened from the database\n\t\t// correctly observe the committed keys.\n\t\tif d.mu.versions.logSeqNum.Load() < largestSeqNum {\n\t\t\td.mu.versions.logSeqNum.Store(largestSeqNum)\n\t\t}\n\t\tif d.mu.versions.visibleSeqNum.Load() < largestSeqNum {\n\t\t\td.mu.versions.visibleSeqNum.Store(largestSeqNum)\n\t\t}\n\t\tlevel = -1\n\t\treturn nil\n\t}\n\n\t// Example, a-c.\n\tparseMeta := func(s string) (*fileMetadata, error) {\n\t\tparts := strings.Split(s, \"-\")\n\t\tif len(parts) != 2 {\n\t\t\treturn nil, errors.Errorf(\"malformed table spec: %s\", s)\n\t\t}\n\t\tm := (&fileMetadata{}).ExtendPointKeyBounds(\n\t\t\topts.Comparer.Compare,\n\t\t\tInternalKey{UserKey: []byte(parts[0])},\n\t\t\tInternalKey{UserKey: []byte(parts[1])},\n\t\t)\n\t\tm.InitPhysicalBacking()\n\t\treturn m, nil\n\t}\n\n\t// Example, compact: a-c.\n\tparseCompaction := func(outputLevel int, s string) (*compaction, error) {\n\t\tm, err := parseMeta(s[len(\"compact:\"):])\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tc := &compaction{\n\t\t\tinputs:   []compactionLevel{{}, {level: outputLevel}},\n\t\t\tsmallest: m.Smallest,\n\t\t\tlargest:  m.Largest,\n\t\t}\n\t\tc.startLevel, c.outputLevel = &c.inputs[0], &c.inputs[1]\n\t\treturn c, nil\n\t}\n\n\tfor _, line := range strings.Split(td.Input, \"\\n\") {\n\t\tfields := strings.Fields(line)\n\t\tif len(fields) > 0 {\n\t\t\tswitch fields[0] {\n\t\t\tcase \"mem\":\n\t\t\t\tif err := maybeFlush(); err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\t// Add a memtable layer.\n\t\t\t\tif !d.mu.mem.mutable.empty() {\n\t\t\t\t\td.mu.mem.mutable = newMemTable(memTableOptions{Options: d.opts})\n\t\t\t\t\tentry := d.newFlushableEntry(d.mu.mem.mutable, 0, 0)\n\t\t\t\t\tentry.readerRefs.Add(1)\n\t\t\t\t\td.mu.mem.queue = append(d.mu.mem.queue, entry)\n\t\t\t\t\td.updateReadStateLocked(nil)\n\t\t\t\t}\n\t\t\t\tmem = d.mu.mem.mutable\n\t\t\t\tstart, end = nil, nil\n\t\t\t\tfields = fields[1:]\n\t\t\tcase \"L0\", \"L1\", \"L2\", \"L3\", \"L4\", \"L5\", \"L6\":\n\t\t\t\tif err := maybeFlush(); err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\tvar err error\n\t\t\t\tif level, err = strconv.Atoi(fields[0][1:]); err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\tfields = fields[1:]\n\t\t\t\tstart, end = nil, nil\n\t\t\t\tboundFields := 0\n\t\t\t\tfor _, field := range fields {\n\t\t\t\t\ttoBreak := false\n\t\t\t\t\tswitch {\n\t\t\t\t\tcase strings.HasPrefix(field, \"start=\"):\n\t\t\t\t\t\tikey := base.ParseInternalKey(strings.TrimPrefix(field, \"start=\"))\n\t\t\t\t\t\tstart = &ikey\n\t\t\t\t\t\tboundFields++\n\t\t\t\t\tcase strings.HasPrefix(field, \"end=\"):\n\t\t\t\t\t\tikey := base.ParseInternalKey(strings.TrimPrefix(field, \"end=\"))\n\t\t\t\t\t\tend = &ikey\n\t\t\t\t\t\tboundFields++\n\t\t\t\t\tdefault:\n\t\t\t\t\t\ttoBreak = true\n\t\t\t\t\t}\n\t\t\t\t\tif toBreak {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfields = fields[boundFields:]\n\t\t\t\tmem = newMemTable(memTableOptions{Options: d.opts})\n\t\t\t}\n\t\t}\n\n\t\tfor _, data := range fields {\n\t\t\ti := strings.Index(data, \":\")\n\t\t\t// Define in-progress compactions.\n\t\t\tif data[:i] == \"compact\" {\n\t\t\t\tc, err := parseCompaction(level, data)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\td.mu.compact.inProgress[c] = struct{}{}\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif data[:i] == \"rangekey\" {\n\t\t\t\tspan := keyspan.ParseSpan(data[i:])\n\t\t\t\terr := rangekey.Encode(span, func(k base.InternalKey, v []byte) error {\n\t\t\t\t\treturn mem.set(k, v)\n\t\t\t\t})\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tkey := base.ParseInternalKey(data[:i])\n\t\t\tvalueStr := data[i+1:]\n\t\t\tvalue := []byte(valueStr)\n\t\t\tvar randBytes int\n\t\t\tif n, err := fmt.Sscanf(valueStr, \"<rand-bytes=%d>\", &randBytes); err == nil && n == 1 {\n\t\t\t\tvalue = make([]byte, randBytes)\n\t\t\t\trnd := rand.New(rand.NewPCG(0, uint64(key.SeqNum())))\n\t\t\t\tfor j := range value {\n\t\t\t\t\tvalue[j] = byte(rnd.Uint32())\n\t\t\t\t}\n\t\t\t}\n\t\t\tif err := mem.set(key, value); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t}\n\n\tif err := maybeFlush(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif len(ve.NewFiles) > 0 {\n\t\tjobID := d.newJobIDLocked()\n\t\td.mu.versions.logLock()\n\t\tif err := d.mu.versions.logAndApply(jobID, ve, newFileMetrics(ve.NewFiles), false, func() []compactionInfo {\n\t\t\treturn nil\n\t\t}); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\td.updateReadStateLocked(nil)\n\t\td.updateTableStatsLocked(ve.NewFiles)\n\t}\n\n\treturn d, nil\n}\n\nfunc runTableStatsCmd(td *datadriven.TestData, d *DB) string {\n\tu, err := strconv.ParseUint(strings.TrimSpace(td.Input), 10, 64)\n\tif err != nil {\n\t\treturn err.Error()\n\t}\n\tfileNum := base.FileNum(u)\n\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\tv := d.mu.versions.currentVersion()\n\tfor _, levelMetadata := range v.Levels {\n\t\titer := levelMetadata.Iter()\n\t\tfor f := iter.First(); f != nil; f = iter.Next() {\n\t\t\tif f.FileNum != fileNum {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tif !f.StatsValid() {\n\t\t\t\td.waitTableStats()\n\t\t\t}\n\n\t\t\tvar b bytes.Buffer\n\t\t\tfmt.Fprintf(&b, \"num-entries: %d\\n\", f.Stats.NumEntries)\n\t\t\tfmt.Fprintf(&b, \"num-deletions: %d\\n\", f.Stats.NumDeletions)\n\t\t\tfmt.Fprintf(&b, \"num-range-key-sets: %d\\n\", f.Stats.NumRangeKeySets)\n\t\t\tfmt.Fprintf(&b, \"point-deletions-bytes-estimate: %d\\n\", f.Stats.PointDeletionsBytesEstimate)\n\t\t\tfmt.Fprintf(&b, \"range-deletions-bytes-estimate: %d\\n\", f.Stats.RangeDeletionsBytesEstimate)\n\t\t\treturn b.String()\n\t\t}\n\t}\n\treturn \"(not found)\"\n}\n\nfunc runTableFileSizesCmd(td *datadriven.TestData, d *DB) string {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\treturn runVersionFileSizes(d.mu.versions.currentVersion())\n}\n\nfunc runVersionFileSizes(v *version) string {\n\tvar buf bytes.Buffer\n\tfor l, levelMetadata := range v.Levels {\n\t\tif levelMetadata.Empty() {\n\t\t\tcontinue\n\t\t}\n\t\tfmt.Fprintf(&buf, \"L%d:\\n\", l)\n\t\titer := levelMetadata.Iter()\n\t\tfor f := iter.First(); f != nil; f = iter.Next() {\n\t\t\tfmt.Fprintf(&buf, \"  %s: %d bytes (%s)\", f, f.Size, humanize.Bytes.Uint64(f.Size))\n\t\t\tif f.IsCompacting() {\n\t\t\t\tfmt.Fprintf(&buf, \" (IsCompacting)\")\n\t\t\t}\n\t\t\tfmt.Fprintln(&buf)\n\t\t}\n\t}\n\treturn buf.String()\n}\n\n// Prints some metadata about some sstable which is currently in the latest\n// version.\nfunc runMetadataCommand(t *testing.T, td *datadriven.TestData, d *DB) string {\n\tvar file int\n\ttd.ScanArgs(t, \"file\", &file)\n\tvar m *fileMetadata\n\td.mu.Lock()\n\tcurrVersion := d.mu.versions.currentVersion()\n\tfor _, level := range currVersion.Levels {\n\t\tlIter := level.Iter()\n\t\tfor f := lIter.First(); f != nil; f = lIter.Next() {\n\t\t\tif f.FileNum == base.FileNum(uint64(file)) {\n\t\t\t\tm = f\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\td.mu.Unlock()\n\tvar buf bytes.Buffer\n\t// Add more metadata as needed.\n\tfmt.Fprintf(&buf, \"size: %d\\n\", m.Size)\n\treturn buf.String()\n}\n\nfunc runSSTablePropertiesCmd(t *testing.T, td *datadriven.TestData, d *DB) string {\n\tvar file int\n\ttd.ScanArgs(t, \"file\", &file)\n\n\t// See if we can grab the FileMetadata associated with the file. This is needed\n\t// to easily construct virtual sstable properties.\n\tvar m *fileMetadata\n\td.mu.Lock()\n\tcurrVersion := d.mu.versions.currentVersion()\n\tfor _, level := range currVersion.Levels {\n\t\tlIter := level.Iter()\n\t\tfor f := lIter.First(); f != nil; f = lIter.Next() {\n\t\t\tif f.FileNum == base.FileNum(uint64(file)) {\n\t\t\t\tm = f\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\td.mu.Unlock()\n\n\t// Note that m can be nil here if the sstable exists in the file system, but\n\t// not in the lsm. If m is nil just assume that file is not virtual.\n\n\tbackingFileNum := base.DiskFileNum(file)\n\tif m != nil {\n\t\tbackingFileNum = m.FileBacking.DiskFileNum\n\t}\n\tfileName := base.MakeFilename(fileTypeTable, backingFileNum)\n\tf, err := d.opts.FS.Open(fileName)\n\tif err != nil {\n\t\treturn err.Error()\n\t}\n\treadable, err := sstable.NewSimpleReadable(f)\n\tif err != nil {\n\t\treturn err.Error()\n\t}\n\treaderOpts := d.opts.MakeReaderOptions()\n\t// TODO(bananabrick): cacheOpts is used to set the file number on a Reader,\n\t// and virtual sstables expect this file number to be set. Split out the\n\t// opts into fileNum opts, and cache opts.\n\treaderOpts.SetInternalCacheOpts(sstableinternal.CacheOptions{\n\t\tCache:   d.opts.Cache,\n\t\tCacheID: 0,\n\t\tFileNum: backingFileNum,\n\t})\n\tr, err := sstable.NewReader(context.Background(), readable, readerOpts)\n\tif err != nil {\n\t\treturn err.Error()\n\t}\n\tdefer r.Close()\n\n\tvar v sstable.VirtualReader\n\tprops := r.Properties.String()\n\tif m != nil && m.Virtual {\n\t\tv = sstable.MakeVirtualReader(r, m.VirtualMeta().VirtualReaderParams(false /* isShared */))\n\t\tprops = v.Properties.String()\n\t}\n\tif len(td.Input) == 0 {\n\t\treturn props\n\t}\n\tvar buf bytes.Buffer\n\tpropsSlice := strings.Split(props, \"\\n\")\n\tfor _, requestedProp := range strings.Split(td.Input, \"\\n\") {\n\t\tfmt.Fprintf(&buf, \"%s:\\n\", requestedProp)\n\t\tfor _, prop := range propsSlice {\n\t\t\tif strings.Contains(prop, requestedProp) {\n\t\t\t\tfmt.Fprintf(&buf, \"  %s\\n\", prop)\n\t\t\t}\n\t\t}\n\t}\n\treturn buf.String()\n}\n\nfunc runPopulateCmd(t *testing.T, td *datadriven.TestData, b *Batch) {\n\tvar maxKeyLength, valLength int\n\tvar timestamps []int\n\ttd.ScanArgs(t, \"keylen\", &maxKeyLength)\n\ttd.MaybeScanArgs(t, \"timestamps\", &timestamps)\n\ttd.MaybeScanArgs(t, \"vallen\", &valLength)\n\t// Default to writing timestamps @1.\n\tif len(timestamps) == 0 {\n\t\ttimestamps = append(timestamps, 1)\n\t}\n\n\tks := testkeys.Alpha(maxKeyLength)\n\tbuf := make([]byte, ks.MaxLen()+testkeys.MaxSuffixLen)\n\tvbuf := make([]byte, valLength)\n\tfor i := int64(0); i < ks.Count(); i++ {\n\t\tfor _, ts := range timestamps {\n\t\t\tn := testkeys.WriteKeyAt(buf, ks, i, int64(ts))\n\n\t\t\t// Default to using the key as the value, but if the user provided\n\t\t\t// the vallen argument, generate a random value of the specified\n\t\t\t// length.\n\t\t\tvalue := buf[:n]\n\t\t\tif valLength > 0 {\n\t\t\t\t_, err := crand.Read(vbuf)\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\tvalue = vbuf\n\t\t\t}\n\t\t\trequire.NoError(t, b.Set(buf[:n], value, nil))\n\t\t}\n\t}\n}\n\n// waitTableStats waits until all new files' statistics have been loaded. It's\n// used in tests. The d.mu mutex must be locked while calling this method.\nfunc (d *DB) waitTableStats() {\n\tfor d.mu.tableStats.loading || len(d.mu.tableStats.pending) > 0 {\n\t\td.mu.tableStats.cond.Wait()\n\t}\n}\n\nfunc runIngestAndExciseCmd(td *datadriven.TestData, d *DB, fs vfs.FS) error {\n\tvar exciseSpan KeyRange\n\tpaths := make([]string, 0, len(td.CmdArgs))\n\tfor i, arg := range td.CmdArgs {\n\t\tswitch td.CmdArgs[i].Key {\n\t\tcase \"excise\":\n\t\t\tif len(td.CmdArgs[i].Vals) != 1 {\n\t\t\t\treturn errors.New(\"expected 2 values for excise separated by -, eg. ingest-and-excise foo1 excise=\\\"start-end\\\"\")\n\t\t\t}\n\t\t\tfields := strings.Split(td.CmdArgs[i].Vals[0], \"-\")\n\t\t\tif len(fields) != 2 {\n\t\t\t\treturn errors.New(\"expected 2 values for excise separated by -, eg. ingest-and-excise foo1 excise=\\\"start-end\\\"\")\n\t\t\t}\n\t\t\texciseSpan.Start = []byte(fields[0])\n\t\t\texciseSpan.End = []byte(fields[1])\n\t\tcase \"no-wait\":\n\t\t\t// Handled by callers.\n\t\tdefault:\n\t\t\tpaths = append(paths, arg.String())\n\t\t}\n\t}\n\n\tif _, err := d.IngestAndExcise(context.Background(), paths, nil /* shared */, nil /* external */, exciseSpan); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc runIngestCmd(td *datadriven.TestData, d *DB, fs vfs.FS) error {\n\tpaths := make([]string, 0, len(td.CmdArgs))\n\tfor _, arg := range td.CmdArgs {\n\t\tif arg.Key == \"no-wait\" {\n\t\t\t// Handled by callers.\n\t\t\tcontinue\n\t\t}\n\t\tpaths = append(paths, arg.String())\n\t}\n\n\tif err := d.Ingest(context.Background(), paths); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc runIngestExternalCmd(\n\tt testing.TB, td *datadriven.TestData, d *DB, st remote.Storage, locator string,\n) error {\n\tvar external []ExternalFile\n\tfor _, line := range strings.Split(td.Input, \"\\n\") {\n\t\tusageErr := func(info interface{}) {\n\t\t\tt.Helper()\n\t\t\ttd.Fatalf(t, \"error parsing %q: %v; \"+\n\t\t\t\t\"usage: obj bounds=(smallest,largest) [size=x] [synthetic-prefix=prefix] [synthetic-suffix=suffix] [no-point-keys] [has-range-keys]\",\n\t\t\t\tline, info,\n\t\t\t)\n\t\t}\n\t\tobjName, args, err := datadriven.ParseLine(line)\n\t\tif err != nil {\n\t\t\tusageErr(err)\n\t\t}\n\t\tsz, err := st.Size(objName)\n\t\tif err != nil {\n\t\t\treturn errors.Wrapf(err, \"sizeof %s\", objName)\n\t\t}\n\t\tef := ExternalFile{\n\t\t\tLocator:     remote.Locator(locator),\n\t\t\tObjName:     objName,\n\t\t\tHasPointKey: true,\n\t\t\tSize:        uint64(sz),\n\t\t}\n\t\tfor _, arg := range args {\n\t\t\tnArgs := func(n int) {\n\t\t\t\tif len(arg.Vals) != n {\n\t\t\t\t\tusageErr(fmt.Sprintf(\"%s must have %d arguments\", arg.Key, n))\n\t\t\t\t}\n\t\t\t}\n\t\t\tswitch arg.Key {\n\t\t\tcase \"bounds\":\n\t\t\t\tnArgs(2)\n\t\t\t\tef.StartKey = []byte(arg.Vals[0])\n\t\t\t\tef.EndKey = []byte(arg.Vals[1])\n\t\t\tcase \"bounds-are-inclusive\":\n\t\t\t\tnArgs(1)\n\t\t\t\tb, err := strconv.ParseBool(arg.Vals[0])\n\t\t\t\tif err != nil {\n\t\t\t\t\tusageErr(fmt.Sprintf(\"%s should have boolean argument: %v\",\n\t\t\t\t\t\targ.Key, err))\n\t\t\t\t}\n\t\t\t\tef.EndKeyIsInclusive = b\n\t\t\tcase \"size\":\n\t\t\t\tnArgs(1)\n\t\t\t\targ.Scan(t, 0, &ef.Size)\n\n\t\t\tcase \"synthetic-prefix\":\n\t\t\t\tnArgs(1)\n\t\t\t\tef.SyntheticPrefix = []byte(arg.Vals[0])\n\n\t\t\tcase \"synthetic-suffix\":\n\t\t\t\tnArgs(1)\n\t\t\t\tef.SyntheticSuffix = []byte(arg.Vals[0])\n\n\t\t\tcase \"no-point-keys\":\n\t\t\t\tef.HasPointKey = false\n\n\t\t\tcase \"has-range-keys\":\n\t\t\t\tef.HasRangeKey = true\n\n\t\t\tdefault:\n\t\t\t\tusageErr(fmt.Sprintf(\"unknown argument %v\", arg.Key))\n\t\t\t}\n\t\t}\n\t\tif ef.StartKey == nil {\n\t\t\tusageErr(\"no bounds specified\")\n\t\t}\n\n\t\texternal = append(external, ef)\n\t}\n\n\tif _, err := d.IngestExternalFiles(context.Background(), external); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc runLSMCmd(td *datadriven.TestData, d *DB) string {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\tif td.HasArg(\"verbose\") {\n\t\treturn d.mu.versions.currentVersion().DebugString()\n\t}\n\treturn d.mu.versions.currentVersion().String()\n}\n\nfunc parseDBOptionsArgs(opts *Options, args []datadriven.CmdArg) error {\n\tfor _, cmdArg := range args {\n\t\tswitch cmdArg.Key {\n\t\tcase \"auto-compactions\":\n\t\t\tswitch cmdArg.Vals[0] {\n\t\t\tcase \"off\":\n\t\t\t\topts.DisableAutomaticCompactions = true\n\t\t\tcase \"on\":\n\t\t\t\topts.DisableAutomaticCompactions = false\n\t\t\tdefault:\n\t\t\t\treturn errors.Errorf(\"Unrecognized %q arg value: %q\", cmdArg.Key, cmdArg.Vals[0])\n\t\t\t}\n\t\tcase \"block-size\":\n\t\t\tv, err := strconv.Atoi(cmdArg.Vals[0])\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tfor i := range opts.Levels {\n\t\t\t\topts.Levels[i].BlockSize = v\n\t\t\t}\n\t\tcase \"bloom-bits-per-key\":\n\t\t\tv, err := strconv.Atoi(cmdArg.Vals[0])\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tfp := bloom.FilterPolicy(v)\n\t\t\topts.Filters = map[string]FilterPolicy{fp.Name(): fp}\n\t\t\tfor i := range opts.Levels {\n\t\t\t\topts.Levels[i].FilterPolicy = fp\n\t\t\t}\n\t\tcase \"cache-size\":\n\t\t\tif opts.Cache != nil {\n\t\t\t\topts.Cache.Unref()\n\t\t\t\topts.Cache = nil\n\t\t\t}\n\t\t\tsize, err := strconv.ParseInt(cmdArg.Vals[0], 10, 64)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\topts.Cache = NewCache(size)\n\t\tcase \"disable-multi-level\":\n\t\t\topts.Experimental.MultiLevelCompactionHeuristic = NoMultiLevel{}\n\t\tcase \"enable-table-stats\":\n\t\t\tenable, err := strconv.ParseBool(cmdArg.Vals[0])\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Errorf(\"%s: could not parse %q as bool: %s\", cmdArg.Key, cmdArg.Vals[0], err)\n\t\t\t}\n\t\t\topts.DisableTableStats = !enable\n\t\tcase \"format-major-version\":\n\t\t\tv, err := strconv.Atoi(cmdArg.Vals[0])\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\topts.FormatMajorVersion = FormatMajorVersion(v)\n\t\tcase \"index-block-size\":\n\t\t\tv, err := strconv.Atoi(cmdArg.Vals[0])\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tfor i := range opts.Levels {\n\t\t\t\topts.Levels[i].IndexBlockSize = v\n\t\t\t}\n\t\tcase \"inject-errors\":\n\t\t\tinjs := make([]errorfs.Injector, len(cmdArg.Vals))\n\t\t\tfor i := 0; i < len(cmdArg.Vals); i++ {\n\t\t\t\tinj, err := errorfs.ParseDSL(cmdArg.Vals[i])\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tinjs[i] = inj\n\t\t\t}\n\t\t\topts.FS = errorfs.Wrap(opts.FS, errorfs.Any(injs...))\n\t\tcase \"lbase-max-bytes\":\n\t\t\tlbaseMaxBytes, err := strconv.ParseInt(cmdArg.Vals[0], 10, 64)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\topts.LBaseMaxBytes = lbaseMaxBytes\n\t\tcase \"memtable-size\":\n\t\t\tmemTableSize, err := strconv.ParseUint(cmdArg.Vals[0], 10, 64)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\topts.MemTableSize = memTableSize\n\t\tcase \"merger\":\n\t\t\tswitch cmdArg.Vals[0] {\n\t\t\tcase \"appender\":\n\t\t\t\topts.Merger = base.DefaultMerger\n\t\t\tdefault:\n\t\t\t\treturn errors.Newf(\"unrecognized Merger %q\\n\", cmdArg.Vals[0])\n\t\t\t}\n\t\tcase \"readonly\":\n\t\t\topts.ReadOnly = true\n\t\tcase \"target-file-sizes\":\n\t\t\tif len(opts.Levels) < len(cmdArg.Vals) {\n\t\t\t\topts.Levels = slices.Grow(opts.Levels, len(cmdArg.Vals)-len(opts.Levels))[0:len(cmdArg.Vals)]\n\t\t\t}\n\t\t\tfor i := range cmdArg.Vals {\n\t\t\t\tsize, err := strconv.ParseInt(cmdArg.Vals[i], 10, 64)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\topts.Levels[i].TargetFileSize = size\n\t\t\t}\n\t\tcase \"wal-failover\":\n\t\t\tif v := cmdArg.Vals[0]; v == \"off\" || v == \"disabled\" {\n\t\t\t\topts.WALFailover = nil\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\topts.WALFailover = &WALFailoverOptions{\n\t\t\t\tSecondary: wal.Dir{FS: opts.FS, Dirname: cmdArg.Vals[0]},\n\t\t\t}\n\t\t\topts.WALFailover.EnsureDefaults()\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc streamFilterBetweenGrep(start, end string) stream.Filter {\n\tstartRegexp, err := regexp.Compile(start)\n\tif err != nil {\n\t\treturn stream.FilterFunc(func(stream.Arg) error { return err })\n\t}\n\tendRegexp, err := regexp.Compile(end)\n\tif err != nil {\n\t\treturn stream.FilterFunc(func(stream.Arg) error { return err })\n\t}\n\tvar passedStart bool\n\treturn stream.FilterFunc(func(arg stream.Arg) error {\n\t\tfor s := range arg.In {\n\t\t\tif passedStart {\n\t\t\t\tif endRegexp.MatchString(s) {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\targ.Out <- s\n\t\t\t\tcontinue\n\t\t\t} else {\n\t\t\t\tpassedStart = startRegexp.MatchString(s)\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n}\n"
        },
        {
          "name": "db.go",
          "type": "blob",
          "size": 105.9970703125,
          "content": "// Copyright 2012 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\n// Package pebble provides an ordered key/value store.\npackage pebble // import \"github.com/cockroachdb/pebble\"\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\t\"unsafe\"\n\n\t\"github.com/cockroachdb/crlib/crtime\"\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/arenaskl\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/cache\"\n\t\"github.com/cockroachdb/pebble/internal/invalidating\"\n\t\"github.com/cockroachdb/pebble/internal/invariants\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan/keyspanimpl\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n\t\"github.com/cockroachdb/pebble/internal/manual\"\n\t\"github.com/cockroachdb/pebble/objstorage\"\n\t\"github.com/cockroachdb/pebble/objstorage/remote\"\n\t\"github.com/cockroachdb/pebble/rangekey\"\n\t\"github.com/cockroachdb/pebble/record\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/cockroachdb/pebble/vfs/atomicfs\"\n\t\"github.com/cockroachdb/pebble/wal\"\n\t\"github.com/cockroachdb/tokenbucket\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n)\n\nconst (\n\t// minFileCacheSize is the minimum size of the file cache, for a single db.\n\tminFileCacheSize = 64\n\n\t// numNonFileCacheFiles is an approximation for the number of files\n\t// that we don't account for in the file cache, for a given db.\n\tnumNonFileCacheFiles = 10\n)\n\nvar (\n\t// ErrNotFound is returned when a get operation does not find the requested\n\t// key.\n\tErrNotFound = base.ErrNotFound\n\t// ErrClosed is panicked when an operation is performed on a closed snapshot or\n\t// DB. Use errors.Is(err, ErrClosed) to check for this error.\n\tErrClosed = errors.New(\"pebble: closed\")\n\t// ErrReadOnly is returned when a write operation is performed on a read-only\n\t// database.\n\tErrReadOnly = errors.New(\"pebble: read-only\")\n\t// errNoSplit indicates that the user is trying to perform a range key\n\t// operation but the configured Comparer does not provide a Split\n\t// implementation.\n\terrNoSplit = errors.New(\"pebble: Comparer.Split required for range key operations\")\n)\n\n// Reader is a readable key/value store.\n//\n// It is safe to call Get and NewIter from concurrent goroutines.\ntype Reader interface {\n\t// Get gets the value for the given key. It returns ErrNotFound if the DB\n\t// does not contain the key.\n\t//\n\t// The caller should not modify the contents of the returned slice, but it is\n\t// safe to modify the contents of the argument after Get returns. The\n\t// returned slice will remain valid until the returned Closer is closed. On\n\t// success, the caller MUST call closer.Close() or a memory leak will occur.\n\tGet(key []byte) (value []byte, closer io.Closer, err error)\n\n\t// NewIter returns an iterator that is unpositioned (Iterator.Valid() will\n\t// return false). The iterator can be positioned via a call to SeekGE,\n\t// SeekLT, First or Last.\n\tNewIter(o *IterOptions) (*Iterator, error)\n\n\t// NewIterWithContext is like NewIter, and additionally accepts a context\n\t// for tracing.\n\tNewIterWithContext(ctx context.Context, o *IterOptions) (*Iterator, error)\n\n\t// Close closes the Reader. It may or may not close any underlying io.Reader\n\t// or io.Writer, depending on how the DB was created.\n\t//\n\t// It is not safe to close a DB until all outstanding iterators are closed.\n\t// It is valid to call Close multiple times. Other methods should not be\n\t// called after the DB has been closed.\n\tClose() error\n}\n\n// Writer is a writable key/value store.\n//\n// Goroutine safety is dependent on the specific implementation.\ntype Writer interface {\n\t// Apply the operations contained in the batch to the DB.\n\t//\n\t// It is safe to modify the contents of the arguments after Apply returns.\n\tApply(batch *Batch, o *WriteOptions) error\n\n\t// Delete deletes the value for the given key. Deletes are blind all will\n\t// succeed even if the given key does not exist.\n\t//\n\t// It is safe to modify the contents of the arguments after Delete returns.\n\tDelete(key []byte, o *WriteOptions) error\n\n\t// DeleteSized behaves identically to Delete, but takes an additional\n\t// argument indicating the size of the value being deleted. DeleteSized\n\t// should be preferred when the caller has the expectation that there exists\n\t// a single internal KV pair for the key (eg, the key has not been\n\t// overwritten recently), and the caller knows the size of its value.\n\t//\n\t// DeleteSized will record the value size within the tombstone and use it to\n\t// inform compaction-picking heuristics which strive to reduce space\n\t// amplification in the LSM. This \"calling your shot\" mechanic allows the\n\t// storage engine to more accurately estimate and reduce space\n\t// amplification.\n\t//\n\t// It is safe to modify the contents of the arguments after DeleteSized\n\t// returns.\n\tDeleteSized(key []byte, valueSize uint32, _ *WriteOptions) error\n\n\t// SingleDelete is similar to Delete in that it deletes the value for the given key. Like Delete,\n\t// it is a blind operation that will succeed even if the given key does not exist.\n\t//\n\t// WARNING: Undefined (non-deterministic) behavior will result if a key is overwritten and\n\t// then deleted using SingleDelete. The record may appear deleted immediately, but be\n\t// resurrected at a later time after compactions have been performed. Or the record may\n\t// be deleted permanently. A Delete operation lays down a \"tombstone\" which shadows all\n\t// previous versions of a key. The SingleDelete operation is akin to \"anti-matter\" and will\n\t// only delete the most recently written version for a key. These different semantics allow\n\t// the DB to avoid propagating a SingleDelete operation during a compaction as soon as the\n\t// corresponding Set operation is encountered. These semantics require extreme care to handle\n\t// properly. Only use if you have a workload where the performance gain is critical and you\n\t// can guarantee that a record is written once and then deleted once.\n\t//\n\t// SingleDelete is internally transformed into a Delete if the most recent record for a key is either\n\t// a Merge or Delete record.\n\t//\n\t// It is safe to modify the contents of the arguments after SingleDelete returns.\n\tSingleDelete(key []byte, o *WriteOptions) error\n\n\t// DeleteRange deletes all of the point keys (and values) in the range\n\t// [start,end) (inclusive on start, exclusive on end). DeleteRange does NOT\n\t// delete overlapping range keys (eg, keys set via RangeKeySet).\n\t//\n\t// It is safe to modify the contents of the arguments after DeleteRange\n\t// returns.\n\tDeleteRange(start, end []byte, o *WriteOptions) error\n\n\t// LogData adds the specified to the batch. The data will be written to the\n\t// WAL, but not added to memtables or sstables. Log data is never indexed,\n\t// which makes it useful for testing WAL performance.\n\t//\n\t// It is safe to modify the contents of the argument after LogData returns.\n\tLogData(data []byte, opts *WriteOptions) error\n\n\t// Merge merges the value for the given key. The details of the merge are\n\t// dependent upon the configured merge operation.\n\t//\n\t// It is safe to modify the contents of the arguments after Merge returns.\n\tMerge(key, value []byte, o *WriteOptions) error\n\n\t// Set sets the value for the given key. It overwrites any previous value\n\t// for that key; a DB is not a multi-map.\n\t//\n\t// It is safe to modify the contents of the arguments after Set returns.\n\tSet(key, value []byte, o *WriteOptions) error\n\n\t// RangeKeySet sets a range key mapping the key range [start, end) at the MVCC\n\t// timestamp suffix to value. The suffix is optional. If any portion of the key\n\t// range [start, end) is already set by a range key with the same suffix value,\n\t// RangeKeySet overrides it.\n\t//\n\t// It is safe to modify the contents of the arguments after RangeKeySet returns.\n\tRangeKeySet(start, end, suffix, value []byte, opts *WriteOptions) error\n\n\t// RangeKeyUnset removes a range key mapping the key range [start, end) at the\n\t// MVCC timestamp suffix. The suffix may be omitted to remove an unsuffixed\n\t// range key. RangeKeyUnset only removes portions of range keys that fall within\n\t// the [start, end) key span, and only range keys with suffixes that exactly\n\t// match the unset suffix.\n\t//\n\t// It is safe to modify the contents of the arguments after RangeKeyUnset\n\t// returns.\n\tRangeKeyUnset(start, end, suffix []byte, opts *WriteOptions) error\n\n\t// RangeKeyDelete deletes all of the range keys in the range [start,end)\n\t// (inclusive on start, exclusive on end). It does not delete point keys (for\n\t// that use DeleteRange). RangeKeyDelete removes all range keys within the\n\t// bounds, including those with or without suffixes.\n\t//\n\t// It is safe to modify the contents of the arguments after RangeKeyDelete\n\t// returns.\n\tRangeKeyDelete(start, end []byte, opts *WriteOptions) error\n}\n\n// CPUWorkHandle represents a handle used by the CPUWorkPermissionGranter API.\ntype CPUWorkHandle interface {\n\t// Permitted indicates whether Pebble can use additional CPU resources.\n\tPermitted() bool\n}\n\n// CPUWorkPermissionGranter is used to request permission to opportunistically\n// use additional CPUs to speed up internal background work.\ntype CPUWorkPermissionGranter interface {\n\t// GetPermission returns a handle regardless of whether permission is granted\n\t// or not. In the latter case, the handle is only useful for recording\n\t// the CPU time actually spent on this calling goroutine.\n\tGetPermission(time.Duration) CPUWorkHandle\n\t// CPUWorkDone must be called regardless of whether CPUWorkHandle.Permitted\n\t// returns true or false.\n\tCPUWorkDone(CPUWorkHandle)\n}\n\n// Use a default implementation for the CPU work granter to avoid excessive nil\n// checks in the code.\ntype defaultCPUWorkHandle struct{}\n\nfunc (d defaultCPUWorkHandle) Permitted() bool {\n\treturn false\n}\n\ntype defaultCPUWorkGranter struct{}\n\nfunc (d defaultCPUWorkGranter) GetPermission(_ time.Duration) CPUWorkHandle {\n\treturn defaultCPUWorkHandle{}\n}\n\nfunc (d defaultCPUWorkGranter) CPUWorkDone(_ CPUWorkHandle) {}\n\n// DB provides a concurrent, persistent ordered key/value store.\n//\n// A DB's basic operations (Get, Set, Delete) should be self-explanatory. Get\n// and Delete will return ErrNotFound if the requested key is not in the store.\n// Callers are free to ignore this error.\n//\n// A DB also allows for iterating over the key/value pairs in key order. If d\n// is a DB, the code below prints all key/value pairs whose keys are 'greater\n// than or equal to' k:\n//\n//\titer := d.NewIter(readOptions)\n//\tfor iter.SeekGE(k); iter.Valid(); iter.Next() {\n//\t\tfmt.Printf(\"key=%q value=%q\\n\", iter.Key(), iter.Value())\n//\t}\n//\treturn iter.Close()\n//\n// The Options struct holds the optional parameters for the DB, including a\n// Comparer to define a 'less than' relationship over keys. It is always valid\n// to pass a nil *Options, which means to use the default parameter values. Any\n// zero field of a non-nil *Options also means to use the default value for\n// that parameter. Thus, the code below uses a custom Comparer, but the default\n// values for every other parameter:\n//\n//\tdb := pebble.Open(&Options{\n//\t\tComparer: myComparer,\n//\t})\ntype DB struct {\n\t// The count and size of referenced memtables. This includes memtables\n\t// present in DB.mu.mem.queue, as well as memtables that have been flushed\n\t// but are still referenced by an inuse readState, as well as up to one\n\t// memTable waiting to be reused and stored in d.memTableRecycle.\n\tmemTableCount    atomic.Int64\n\tmemTableReserved atomic.Int64 // number of bytes reserved in the cache for memtables\n\t// memTableRecycle holds a pointer to an obsolete memtable. The next\n\t// memtable allocation will reuse this memtable if it has not already been\n\t// recycled.\n\tmemTableRecycle atomic.Pointer[memTable]\n\n\t// The logical size of the current WAL.\n\tlogSize atomic.Uint64\n\t// The number of input bytes to the log. This is the raw size of the\n\t// batches written to the WAL, without the overhead of the record\n\t// envelopes.\n\tlogBytesIn atomic.Uint64\n\n\t// The number of bytes available on disk.\n\tdiskAvailBytes atomic.Uint64\n\n\tcacheID        cache.ID\n\tdirname        string\n\topts           *Options\n\tcmp            Compare\n\tequal          Equal\n\tmerge          Merge\n\tsplit          Split\n\tabbreviatedKey AbbreviatedKey\n\t// The threshold for determining when a batch is \"large\" and will skip being\n\t// inserted into a memtable.\n\tlargeBatchThreshold uint64\n\t// The current OPTIONS file number.\n\toptionsFileNum base.DiskFileNum\n\t// The on-disk size of the current OPTIONS file.\n\toptionsFileSize uint64\n\n\t// objProvider is used to access and manage SSTs.\n\tobjProvider objstorage.Provider\n\n\tfileLock *Lock\n\tdataDir  vfs.File\n\n\tfileCache            *fileCacheContainer\n\tnewIters             tableNewIters\n\ttableNewRangeKeyIter keyspanimpl.TableNewSpanIter\n\n\tcommit *commitPipeline\n\n\t// readState provides access to the state needed for reading without needing\n\t// to acquire DB.mu.\n\treadState struct {\n\t\tsync.RWMutex\n\t\tval *readState\n\t}\n\n\tclosed   *atomic.Value\n\tclosedCh chan struct{}\n\n\tcleanupManager *cleanupManager\n\n\t// During an iterator close, we may asynchronously schedule read compactions.\n\t// We want to wait for those goroutines to finish, before closing the DB.\n\t// compactionShedulers.Wait() should not be called while the DB.mu is held.\n\tcompactionSchedulers sync.WaitGroup\n\n\t// The main mutex protecting internal DB state. This mutex encompasses many\n\t// fields because those fields need to be accessed and updated atomically. In\n\t// particular, the current version, log.*, mem.*, and snapshot list need to\n\t// be accessed and updated atomically during compaction.\n\t//\n\t// Care is taken to avoid holding DB.mu during IO operations. Accomplishing\n\t// this sometimes requires releasing DB.mu in a method that was called with\n\t// it held. See versionSet.logAndApply() and DB.makeRoomForWrite() for\n\t// examples. This is a common pattern, so be careful about expectations that\n\t// DB.mu will be held continuously across a set of calls.\n\tmu struct {\n\t\tsync.Mutex\n\n\t\tformatVers struct {\n\t\t\t// vers is the database's current format major version.\n\t\t\t// Backwards-incompatible features are gated behind new\n\t\t\t// format major versions and not enabled until a database's\n\t\t\t// version is ratcheted upwards.\n\t\t\t//\n\t\t\t// Although this is under the `mu` prefix, readers may read vers\n\t\t\t// atomically without holding d.mu. Writers must only write to this\n\t\t\t// value through finalizeFormatVersUpgrade which requires d.mu is\n\t\t\t// held.\n\t\t\tvers atomic.Uint64\n\t\t\t// marker is the atomic marker for the format major version.\n\t\t\t// When a database's version is ratcheted upwards, the\n\t\t\t// marker is moved in order to atomically record the new\n\t\t\t// version.\n\t\t\tmarker *atomicfs.Marker\n\t\t\t// ratcheting when set to true indicates that the database is\n\t\t\t// currently in the process of ratcheting the format major version\n\t\t\t// to vers + 1. As a part of ratcheting the format major version,\n\t\t\t// migrations may drop and re-acquire the mutex.\n\t\t\tratcheting bool\n\t\t}\n\n\t\t// The ID of the next job. Job IDs are passed to event listener\n\t\t// notifications and act as a mechanism for tying together the events and\n\t\t// log messages for a single job such as a flush, compaction, or file\n\t\t// ingestion. Job IDs are not serialized to disk or used for correctness.\n\t\tnextJobID JobID\n\n\t\t// The collection of immutable versions and state about the log and visible\n\t\t// sequence numbers. Use the pointer here to ensure the atomic fields in\n\t\t// version set are aligned properly.\n\t\tversions *versionSet\n\n\t\tlog struct {\n\t\t\t// manager is not protected by mu, but calls to Create must be\n\t\t\t// serialized, and happen after the previous writer is closed.\n\t\t\tmanager wal.Manager\n\t\t\t// The Writer is protected by commitPipeline.mu. This allows log writes\n\t\t\t// to be performed without holding DB.mu, but requires both\n\t\t\t// commitPipeline.mu and DB.mu to be held when rotating the WAL/memtable\n\t\t\t// (i.e. makeRoomForWrite). Can be nil.\n\t\t\twriter  wal.Writer\n\t\t\tmetrics struct {\n\t\t\t\t// fsyncLatency has its own internal synchronization, and is not\n\t\t\t\t// protected by mu.\n\t\t\t\tfsyncLatency prometheus.Histogram\n\t\t\t\t// Updated whenever a wal.Writer is closed.\n\t\t\t\trecord.LogWriterMetrics\n\t\t\t}\n\t\t}\n\n\t\tmem struct {\n\t\t\t// The current mutable memTable. Readers of the pointer may hold\n\t\t\t// either DB.mu or commitPipeline.mu.\n\t\t\t//\n\t\t\t// Its internal fields are protected by commitPipeline.mu. This\n\t\t\t// allows batch commits to be performed without DB.mu as long as no\n\t\t\t// memtable rotation is required.\n\t\t\t//\n\t\t\t// Both commitPipeline.mu and DB.mu must be held when rotating the\n\t\t\t// memtable.\n\t\t\tmutable *memTable\n\t\t\t// Queue of flushables (the mutable memtable is at end). Elements are\n\t\t\t// added to the end of the slice and removed from the beginning. Once an\n\t\t\t// index is set it is never modified making a fixed slice immutable and\n\t\t\t// safe for concurrent reads.\n\t\t\tqueue flushableList\n\t\t\t// nextSize is the size of the next memtable. The memtable size starts at\n\t\t\t// min(256KB,Options.MemTableSize) and doubles each time a new memtable\n\t\t\t// is allocated up to Options.MemTableSize. This reduces the memory\n\t\t\t// footprint of memtables when lots of DB instances are used concurrently\n\t\t\t// in test environments.\n\t\t\tnextSize uint64\n\t\t}\n\n\t\tcompact struct {\n\t\t\t// Condition variable used to signal when a flush or compaction has\n\t\t\t// completed. Used by the write-stall mechanism to wait for the stall\n\t\t\t// condition to clear. See DB.makeRoomForWrite().\n\t\t\tcond sync.Cond\n\t\t\t// True when a flush is in progress.\n\t\t\tflushing bool\n\t\t\t// The number of ongoing non-download compactions.\n\t\t\tcompactingCount int\n\t\t\t// The number of download compactions.\n\t\t\tdownloadingCount int\n\t\t\t// The list of deletion hints, suggesting ranges for delete-only\n\t\t\t// compactions.\n\t\t\tdeletionHints []deleteCompactionHint\n\t\t\t// The list of manual compactions. The next manual compaction to perform\n\t\t\t// is at the start of the list. New entries are added to the end.\n\t\t\tmanual []*manualCompaction\n\t\t\t// downloads is the list of pending download tasks. The next download to\n\t\t\t// perform is at the start of the list. New entries are added to the end.\n\t\t\tdownloads []*downloadSpanTask\n\t\t\t// inProgress is the set of in-progress flushes and compactions.\n\t\t\t// It's used in the calculation of some metrics and to initialize L0\n\t\t\t// sublevels' state. Some of the compactions contained within this\n\t\t\t// map may have already committed an edit to the version but are\n\t\t\t// lingering performing cleanup, like deleting obsolete files.\n\t\t\tinProgress map[*compaction]struct{}\n\n\t\t\t// rescheduleReadCompaction indicates to an iterator that a read compaction\n\t\t\t// should be scheduled.\n\t\t\trescheduleReadCompaction bool\n\n\t\t\t// readCompactions is a readCompactionQueue which keeps track of the\n\t\t\t// compactions which we might have to perform.\n\t\t\treadCompactions readCompactionQueue\n\n\t\t\t// The cumulative duration of all completed compactions since Open.\n\t\t\t// Does not include flushes.\n\t\t\tduration time.Duration\n\t\t\t// Flush throughput metric.\n\t\t\tflushWriteThroughput ThroughputMetric\n\t\t\t// The idle start time for the flush \"loop\", i.e., when the flushing\n\t\t\t// bool above transitions to false.\n\t\t\tnoOngoingFlushStartTime crtime.Mono\n\t\t}\n\n\t\t// Non-zero when file cleaning is disabled. The disabled count acts as a\n\t\t// reference count to prohibit file cleaning. See\n\t\t// DB.{disable,Enable}FileDeletions().\n\t\tdisableFileDeletions int\n\n\t\tsnapshots struct {\n\t\t\t// The list of active snapshots.\n\t\t\tsnapshotList\n\n\t\t\t// The cumulative count and size of snapshot-pinned keys written to\n\t\t\t// sstables.\n\t\t\tcumulativePinnedCount uint64\n\t\t\tcumulativePinnedSize  uint64\n\t\t}\n\n\t\ttableStats struct {\n\t\t\t// Condition variable used to signal the completion of a\n\t\t\t// job to collect table stats.\n\t\t\tcond sync.Cond\n\t\t\t// True when a stat collection operation is in progress.\n\t\t\tloading bool\n\t\t\t// True if stat collection has loaded statistics for all tables\n\t\t\t// other than those listed explicitly in pending. This flag starts\n\t\t\t// as false when a database is opened and flips to true once stat\n\t\t\t// collection has caught up.\n\t\t\tloadedInitial bool\n\t\t\t// A slice of files for which stats have not been computed.\n\t\t\t// Compactions, ingests, flushes append files to be processed. An\n\t\t\t// active stat collection goroutine clears the list and processes\n\t\t\t// them.\n\t\t\tpending []manifest.NewFileEntry\n\t\t}\n\n\t\ttableValidation struct {\n\t\t\t// cond is a condition variable used to signal the completion of a\n\t\t\t// job to validate one or more sstables.\n\t\t\tcond sync.Cond\n\t\t\t// pending is a slice of metadata for sstables waiting to be\n\t\t\t// validated. Only physical sstables should be added to the pending\n\t\t\t// queue.\n\t\t\tpending []newFileEntry\n\t\t\t// validating is set to true when validation is running.\n\t\t\tvalidating bool\n\t\t}\n\n\t\t// annotators contains various instances of manifest.Annotator which\n\t\t// should be protected from concurrent access.\n\t\tannotators struct {\n\t\t\ttotalSize    *manifest.Annotator[uint64]\n\t\t\tremoteSize   *manifest.Annotator[uint64]\n\t\t\texternalSize *manifest.Annotator[uint64]\n\t\t}\n\t}\n\n\t// Normally equal to time.Now() but may be overridden in tests.\n\ttimeNow func() time.Time\n\t// the time at database Open; may be used to compute metrics like effective\n\t// compaction concurrency\n\topenedAt time.Time\n}\n\nvar _ Reader = (*DB)(nil)\nvar _ Writer = (*DB)(nil)\n\n// TestOnlyWaitForCleaning MUST only be used in tests.\nfunc (d *DB) TestOnlyWaitForCleaning() {\n\td.cleanupManager.Wait()\n}\n\n// Get gets the value for the given key. It returns ErrNotFound if the DB does\n// not contain the key.\n//\n// The caller should not modify the contents of the returned slice, but it is\n// safe to modify the contents of the argument after Get returns. The returned\n// slice will remain valid until the returned Closer is closed. On success, the\n// caller MUST call closer.Close() or a memory leak will occur.\nfunc (d *DB) Get(key []byte) ([]byte, io.Closer, error) {\n\treturn d.getInternal(key, nil /* batch */, nil /* snapshot */)\n}\n\ntype getIterAlloc struct {\n\tdbi    Iterator\n\tkeyBuf []byte\n\tget    getIter\n}\n\nvar getIterAllocPool = sync.Pool{\n\tNew: func() interface{} {\n\t\treturn &getIterAlloc{}\n\t},\n}\n\nfunc (d *DB) getInternal(key []byte, b *Batch, s *Snapshot) ([]byte, io.Closer, error) {\n\tif err := d.closed.Load(); err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Grab and reference the current readState. This prevents the underlying\n\t// files in the associated version from being deleted if there is a current\n\t// compaction. The readState is unref'd by Iterator.Close().\n\treadState := d.loadReadState()\n\n\t// Determine the seqnum to read at after grabbing the read state (current and\n\t// memtables) above.\n\tvar seqNum base.SeqNum\n\tif s != nil {\n\t\tseqNum = s.seqNum\n\t} else {\n\t\tseqNum = d.mu.versions.visibleSeqNum.Load()\n\t}\n\n\tbuf := getIterAllocPool.Get().(*getIterAlloc)\n\n\tget := &buf.get\n\t*get = getIter{\n\t\tcomparer: d.opts.Comparer,\n\t\tnewIters: d.newIters,\n\t\tsnapshot: seqNum,\n\t\titerOpts: IterOptions{\n\t\t\t// TODO(sumeer): replace with a parameter provided by the caller.\n\t\t\tCategory:                      categoryGet,\n\t\t\tlogger:                        d.opts.Logger,\n\t\t\tsnapshotForHideObsoletePoints: seqNum,\n\t\t},\n\t\tkey: key,\n\t\t// Compute the key prefix for bloom filtering.\n\t\tprefix:  key[:d.opts.Comparer.Split(key)],\n\t\tbatch:   b,\n\t\tmem:     readState.memtables,\n\t\tl0:      readState.current.L0SublevelFiles,\n\t\tversion: readState.current,\n\t}\n\n\t// Strip off memtables which cannot possibly contain the seqNum being read\n\t// at.\n\tfor len(get.mem) > 0 {\n\t\tn := len(get.mem)\n\t\tif logSeqNum := get.mem[n-1].logSeqNum; logSeqNum < seqNum {\n\t\t\tbreak\n\t\t}\n\t\tget.mem = get.mem[:n-1]\n\t}\n\n\ti := &buf.dbi\n\tpointIter := get\n\t*i = Iterator{\n\t\tctx:          context.Background(),\n\t\tgetIterAlloc: buf,\n\t\titer:         pointIter,\n\t\tpointIter:    pointIter,\n\t\tmerge:        d.merge,\n\t\tcomparer:     *d.opts.Comparer,\n\t\treadState:    readState,\n\t\tkeyBuf:       buf.keyBuf,\n\t}\n\n\tif !i.First() {\n\t\terr := i.Close()\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t\treturn nil, nil, ErrNotFound\n\t}\n\treturn i.Value(), i, nil\n}\n\n// Set sets the value for the given key. It overwrites any previous value\n// for that key; a DB is not a multi-map.\n//\n// It is safe to modify the contents of the arguments after Set returns.\nfunc (d *DB) Set(key, value []byte, opts *WriteOptions) error {\n\tb := newBatch(d)\n\t_ = b.Set(key, value, opts)\n\tif err := d.Apply(b, opts); err != nil {\n\t\treturn err\n\t}\n\t// Only release the batch on success.\n\treturn b.Close()\n}\n\n// Delete deletes the value for the given key. Deletes are blind all will\n// succeed even if the given key does not exist.\n//\n// It is safe to modify the contents of the arguments after Delete returns.\nfunc (d *DB) Delete(key []byte, opts *WriteOptions) error {\n\tb := newBatch(d)\n\t_ = b.Delete(key, opts)\n\tif err := d.Apply(b, opts); err != nil {\n\t\treturn err\n\t}\n\t// Only release the batch on success.\n\treturn b.Close()\n}\n\n// DeleteSized behaves identically to Delete, but takes an additional\n// argument indicating the size of the value being deleted. DeleteSized\n// should be preferred when the caller has the expectation that there exists\n// a single internal KV pair for the key (eg, the key has not been\n// overwritten recently), and the caller knows the size of its value.\n//\n// DeleteSized will record the value size within the tombstone and use it to\n// inform compaction-picking heuristics which strive to reduce space\n// amplification in the LSM. This \"calling your shot\" mechanic allows the\n// storage engine to more accurately estimate and reduce space amplification.\n//\n// It is safe to modify the contents of the arguments after DeleteSized\n// returns.\nfunc (d *DB) DeleteSized(key []byte, valueSize uint32, opts *WriteOptions) error {\n\tb := newBatch(d)\n\t_ = b.DeleteSized(key, valueSize, opts)\n\tif err := d.Apply(b, opts); err != nil {\n\t\treturn err\n\t}\n\t// Only release the batch on success.\n\treturn b.Close()\n}\n\n// SingleDelete adds an action to the batch that single deletes the entry for key.\n// See Writer.SingleDelete for more details on the semantics of SingleDelete.\n//\n// It is safe to modify the contents of the arguments after SingleDelete returns.\nfunc (d *DB) SingleDelete(key []byte, opts *WriteOptions) error {\n\tb := newBatch(d)\n\t_ = b.SingleDelete(key, opts)\n\tif err := d.Apply(b, opts); err != nil {\n\t\treturn err\n\t}\n\t// Only release the batch on success.\n\treturn b.Close()\n}\n\n// DeleteRange deletes all of the keys (and values) in the range [start,end)\n// (inclusive on start, exclusive on end).\n//\n// It is safe to modify the contents of the arguments after DeleteRange\n// returns.\nfunc (d *DB) DeleteRange(start, end []byte, opts *WriteOptions) error {\n\tb := newBatch(d)\n\t_ = b.DeleteRange(start, end, opts)\n\tif err := d.Apply(b, opts); err != nil {\n\t\treturn err\n\t}\n\t// Only release the batch on success.\n\treturn b.Close()\n}\n\n// Merge adds an action to the DB that merges the value at key with the new\n// value. The details of the merge are dependent upon the configured merge\n// operator.\n//\n// It is safe to modify the contents of the arguments after Merge returns.\nfunc (d *DB) Merge(key, value []byte, opts *WriteOptions) error {\n\tb := newBatch(d)\n\t_ = b.Merge(key, value, opts)\n\tif err := d.Apply(b, opts); err != nil {\n\t\treturn err\n\t}\n\t// Only release the batch on success.\n\treturn b.Close()\n}\n\n// LogData adds the specified to the batch. The data will be written to the\n// WAL, but not added to memtables or sstables. Log data is never indexed,\n// which makes it useful for testing WAL performance.\n//\n// It is safe to modify the contents of the argument after LogData returns.\nfunc (d *DB) LogData(data []byte, opts *WriteOptions) error {\n\tb := newBatch(d)\n\t_ = b.LogData(data, opts)\n\tif err := d.Apply(b, opts); err != nil {\n\t\treturn err\n\t}\n\t// Only release the batch on success.\n\treturn b.Close()\n}\n\n// RangeKeySet sets a range key mapping the key range [start, end) at the MVCC\n// timestamp suffix to value. The suffix is optional. If any portion of the key\n// range [start, end) is already set by a range key with the same suffix value,\n// RangeKeySet overrides it.\n//\n// It is safe to modify the contents of the arguments after RangeKeySet returns.\nfunc (d *DB) RangeKeySet(start, end, suffix, value []byte, opts *WriteOptions) error {\n\tb := newBatch(d)\n\t_ = b.RangeKeySet(start, end, suffix, value, opts)\n\tif err := d.Apply(b, opts); err != nil {\n\t\treturn err\n\t}\n\t// Only release the batch on success.\n\treturn b.Close()\n}\n\n// RangeKeyUnset removes a range key mapping the key range [start, end) at the\n// MVCC timestamp suffix. The suffix may be omitted to remove an unsuffixed\n// range key. RangeKeyUnset only removes portions of range keys that fall within\n// the [start, end) key span, and only range keys with suffixes that exactly\n// match the unset suffix.\n//\n// It is safe to modify the contents of the arguments after RangeKeyUnset\n// returns.\nfunc (d *DB) RangeKeyUnset(start, end, suffix []byte, opts *WriteOptions) error {\n\tb := newBatch(d)\n\t_ = b.RangeKeyUnset(start, end, suffix, opts)\n\tif err := d.Apply(b, opts); err != nil {\n\t\treturn err\n\t}\n\t// Only release the batch on success.\n\treturn b.Close()\n}\n\n// RangeKeyDelete deletes all of the range keys in the range [start,end)\n// (inclusive on start, exclusive on end). It does not delete point keys (for\n// that use DeleteRange). RangeKeyDelete removes all range keys within the\n// bounds, including those with or without suffixes.\n//\n// It is safe to modify the contents of the arguments after RangeKeyDelete\n// returns.\nfunc (d *DB) RangeKeyDelete(start, end []byte, opts *WriteOptions) error {\n\tb := newBatch(d)\n\t_ = b.RangeKeyDelete(start, end, opts)\n\tif err := d.Apply(b, opts); err != nil {\n\t\treturn err\n\t}\n\t// Only release the batch on success.\n\treturn b.Close()\n}\n\n// Apply the operations contained in the batch to the DB. If the batch is large\n// the contents of the batch may be retained by the database. If that occurs\n// the batch contents will be cleared preventing the caller from attempting to\n// reuse them.\n//\n// It is safe to modify the contents of the arguments after Apply returns.\n//\n// Apply returns ErrInvalidBatch if the provided batch is invalid in any way.\nfunc (d *DB) Apply(batch *Batch, opts *WriteOptions) error {\n\treturn d.applyInternal(batch, opts, false)\n}\n\n// ApplyNoSyncWait must only be used when opts.Sync is true and the caller\n// does not want to wait for the WAL fsync to happen. The method will return\n// once the mutation is applied to the memtable and is visible (note that a\n// mutation is visible before the WAL sync even in the wait case, so we have\n// not weakened the durability semantics). The caller must call Batch.SyncWait\n// to wait for the WAL fsync. The caller must not Close the batch without\n// first calling Batch.SyncWait.\n//\n// RECOMMENDATION: Prefer using Apply unless you really understand why you\n// need ApplyNoSyncWait.\n// EXPERIMENTAL: API/feature subject to change. Do not yet use outside\n// CockroachDB.\nfunc (d *DB) ApplyNoSyncWait(batch *Batch, opts *WriteOptions) error {\n\tif !opts.Sync {\n\t\treturn errors.Errorf(\"cannot request asynchonous apply when WriteOptions.Sync is false\")\n\t}\n\treturn d.applyInternal(batch, opts, true)\n}\n\n// REQUIRES: noSyncWait => opts.Sync\nfunc (d *DB) applyInternal(batch *Batch, opts *WriteOptions, noSyncWait bool) error {\n\tif err := d.closed.Load(); err != nil {\n\t\tpanic(err)\n\t}\n\tif batch.committing {\n\t\tpanic(\"pebble: batch already committing\")\n\t}\n\tif batch.applied.Load() {\n\t\tpanic(\"pebble: batch already applied\")\n\t}\n\tif d.opts.ReadOnly {\n\t\treturn ErrReadOnly\n\t}\n\tif batch.db != nil && batch.db != d {\n\t\tpanic(fmt.Sprintf(\"pebble: batch db mismatch: %p != %p\", batch.db, d))\n\t}\n\n\tsync := opts.GetSync()\n\tif sync && d.opts.DisableWAL {\n\t\treturn errors.New(\"pebble: WAL disabled\")\n\t}\n\n\tif fmv := d.FormatMajorVersion(); fmv < batch.minimumFormatMajorVersion {\n\t\tpanic(fmt.Sprintf(\n\t\t\t\"pebble: batch requires at least format major version %d (current: %d)\",\n\t\t\tbatch.minimumFormatMajorVersion, fmv,\n\t\t))\n\t}\n\n\tif batch.countRangeKeys > 0 {\n\t\tif d.split == nil {\n\t\t\treturn errNoSplit\n\t\t}\n\t}\n\tbatch.committing = true\n\n\tif batch.db == nil {\n\t\tif err := batch.refreshMemTableSize(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif batch.memTableSize >= d.largeBatchThreshold {\n\t\tvar err error\n\t\tbatch.flushable, err = newFlushableBatch(batch, d.opts.Comparer)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif err := d.commit.Commit(batch, sync, noSyncWait); err != nil {\n\t\t// There isn't much we can do on an error here. The commit pipeline will be\n\t\t// horked at this point.\n\t\td.opts.Logger.Fatalf(\"pebble: fatal commit error: %v\", err)\n\t}\n\t// If this is a large batch, we need to clear the batch contents as the\n\t// flushable batch may still be present in the flushables queue.\n\t//\n\t// TODO(peter): Currently large batches are written to the WAL. We could\n\t// skip the WAL write and instead wait for the large batch to be flushed to\n\t// an sstable. For a 100 MB batch, this might actually be faster. For a 1\n\t// GB batch this is almost certainly faster.\n\tif batch.flushable != nil {\n\t\tbatch.data = nil\n\t}\n\treturn nil\n}\n\nfunc (d *DB) commitApply(b *Batch, mem *memTable) error {\n\tif b.flushable != nil {\n\t\t// This is a large batch which was already added to the immutable queue.\n\t\treturn nil\n\t}\n\terr := mem.apply(b, b.SeqNum())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// If the batch contains range tombstones and the database is configured\n\t// to flush range deletions, schedule a delayed flush so that disk space\n\t// may be reclaimed without additional writes or an explicit flush.\n\tif b.countRangeDels > 0 && d.opts.FlushDelayDeleteRange > 0 {\n\t\td.mu.Lock()\n\t\td.maybeScheduleDelayedFlush(mem, d.opts.FlushDelayDeleteRange)\n\t\td.mu.Unlock()\n\t}\n\n\t// If the batch contains range keys and the database is configured to flush\n\t// range keys, schedule a delayed flush so that the range keys are cleared\n\t// from the memtable.\n\tif b.countRangeKeys > 0 && d.opts.FlushDelayRangeKey > 0 {\n\t\td.mu.Lock()\n\t\td.maybeScheduleDelayedFlush(mem, d.opts.FlushDelayRangeKey)\n\t\td.mu.Unlock()\n\t}\n\n\tif mem.writerUnref() {\n\t\td.mu.Lock()\n\t\td.maybeScheduleFlush()\n\t\td.mu.Unlock()\n\t}\n\treturn nil\n}\n\nfunc (d *DB) commitWrite(b *Batch, syncWG *sync.WaitGroup, syncErr *error) (*memTable, error) {\n\tvar size int64\n\trepr := b.Repr()\n\n\tif b.flushable != nil {\n\t\t// We have a large batch. Such batches are special in that they don't get\n\t\t// added to the memtable, and are instead inserted into the queue of\n\t\t// memtables. The call to makeRoomForWrite with this batch will force the\n\t\t// current memtable to be flushed. We want the large batch to be part of\n\t\t// the same log, so we add it to the WAL here, rather than after the call\n\t\t// to makeRoomForWrite().\n\t\t//\n\t\t// Set the sequence number since it was not set to the correct value earlier\n\t\t// (see comment in newFlushableBatch()).\n\t\tb.flushable.setSeqNum(b.SeqNum())\n\t\tif !d.opts.DisableWAL {\n\t\t\tvar err error\n\t\t\tsize, err = d.mu.log.writer.WriteRecord(repr, wal.SyncOptions{Done: syncWG, Err: syncErr}, b)\n\t\t\tif err != nil {\n\t\t\t\tpanic(err)\n\t\t\t}\n\t\t}\n\t}\n\n\tvar err error\n\t// Grab a reference to the memtable. We don't hold DB.mu, but we do hold\n\t// d.commit.mu. It's okay for readers of d.mu.mem.mutable to only hold one of\n\t// d.commit.mu or d.mu, because memtable rotations require holding both.\n\tmem := d.mu.mem.mutable\n\t// Batches which contain keys of kind InternalKeyKindIngestSST will\n\t// never be applied to the memtable, so we don't need to make room for\n\t// write.\n\tif !b.ingestedSSTBatch {\n\t\t// Flushable batches will require a rotation of the memtable regardless,\n\t\t// so only attempt an optimistic reservation of space in the current\n\t\t// memtable if this batch is not a large flushable batch.\n\t\tif b.flushable == nil {\n\t\t\terr = d.mu.mem.mutable.prepare(b)\n\t\t}\n\t\tif b.flushable != nil || err == arenaskl.ErrArenaFull {\n\t\t\t// Slow path.\n\t\t\t// We need to acquire DB.mu and rotate the memtable.\n\t\t\tfunc() {\n\t\t\t\td.mu.Lock()\n\t\t\t\tdefer d.mu.Unlock()\n\t\t\t\terr = d.makeRoomForWrite(b)\n\t\t\t\tmem = d.mu.mem.mutable\n\t\t\t}()\n\t\t}\n\t}\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif d.opts.DisableWAL {\n\t\treturn mem, nil\n\t}\n\td.logBytesIn.Add(uint64(len(repr)))\n\n\tif b.flushable == nil {\n\t\tsize, err = d.mu.log.writer.WriteRecord(repr, wal.SyncOptions{Done: syncWG, Err: syncErr}, b)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t}\n\n\td.logSize.Store(uint64(size))\n\treturn mem, err\n}\n\ntype iterAlloc struct {\n\tdbi                 Iterator\n\tkeyBuf              []byte\n\tboundsBuf           [2][]byte\n\tprefixOrFullSeekKey []byte\n\tmerging             mergingIter\n\tmlevels             [3 + numLevels]mergingIterLevel\n\tlevels              [3 + numLevels]levelIter\n\tlevelsPositioned    [3 + numLevels]bool\n}\n\nvar iterAllocPool = sync.Pool{\n\tNew: func() interface{} {\n\t\treturn &iterAlloc{}\n\t},\n}\n\n// snapshotIterOpts denotes snapshot-related iterator options when calling\n// newIter. These are the possible cases for a snapshotIterOpts:\n//   - No snapshot: All fields are zero values.\n//   - Classic snapshot: Only `seqNum` is set. The latest readState will be used\n//     and the specified seqNum will be used as the snapshot seqNum.\n//   - EventuallyFileOnlySnapshot (EFOS) behaving as a classic snapshot. Only\n//     the `seqNum` is set. The latest readState will be used\n//     and the specified seqNum will be used as the snapshot seqNum.\n//   - EFOS in file-only state: Only `seqNum` and `vers` are set. All the\n//     relevant SSTs are referenced by the *version.\n//   - EFOS that has been excised but is in alwaysCreateIters mode (tests only).\n//     Only `seqNum` and `readState` are set.\ntype snapshotIterOpts struct {\n\tseqNum    base.SeqNum\n\tvers      *version\n\treadState *readState\n}\n\ntype batchIterOpts struct {\n\tbatchOnly bool\n}\ntype newIterOpts struct {\n\tsnapshot snapshotIterOpts\n\tbatch    batchIterOpts\n}\n\n// newIter constructs a new iterator, merging in batch iterators as an extra\n// level.\nfunc (d *DB) newIter(\n\tctx context.Context, batch *Batch, newIterOpts newIterOpts, o *IterOptions,\n) *Iterator {\n\tif newIterOpts.batch.batchOnly {\n\t\tif batch == nil {\n\t\t\tpanic(\"batchOnly is true, but batch is nil\")\n\t\t}\n\t\tif newIterOpts.snapshot.vers != nil {\n\t\t\tpanic(\"batchOnly is true, but snapshotIterOpts is initialized\")\n\t\t}\n\t}\n\tif err := d.closed.Load(); err != nil {\n\t\tpanic(err)\n\t}\n\tseqNum := newIterOpts.snapshot.seqNum\n\tif o != nil && o.RangeKeyMasking.Suffix != nil && o.KeyTypes != IterKeyTypePointsAndRanges {\n\t\tpanic(\"pebble: range key masking requires IterKeyTypePointsAndRanges\")\n\t}\n\tif (batch != nil || seqNum != 0) && (o != nil && o.OnlyReadGuaranteedDurable) {\n\t\t// We could add support for OnlyReadGuaranteedDurable on snapshots if\n\t\t// there was a need: this would require checking that the sequence number\n\t\t// of the snapshot has been flushed, by comparing with\n\t\t// DB.mem.queue[0].logSeqNum.\n\t\tpanic(\"OnlyReadGuaranteedDurable is not supported for batches or snapshots\")\n\t}\n\tvar readState *readState\n\tvar newIters tableNewIters\n\tvar newIterRangeKey keyspanimpl.TableNewSpanIter\n\tif !newIterOpts.batch.batchOnly {\n\t\t// Grab and reference the current readState. This prevents the underlying\n\t\t// files in the associated version from being deleted if there is a current\n\t\t// compaction. The readState is unref'd by Iterator.Close().\n\t\tif newIterOpts.snapshot.vers == nil {\n\t\t\tif newIterOpts.snapshot.readState != nil {\n\t\t\t\treadState = newIterOpts.snapshot.readState\n\t\t\t\treadState.ref()\n\t\t\t} else {\n\t\t\t\t// NB: loadReadState() calls readState.ref().\n\t\t\t\treadState = d.loadReadState()\n\t\t\t}\n\t\t} else {\n\t\t\t// vers != nil\n\t\t\tnewIterOpts.snapshot.vers.Ref()\n\t\t}\n\n\t\t// Determine the seqnum to read at after grabbing the read state (current and\n\t\t// memtables) above.\n\t\tif seqNum == 0 {\n\t\t\tseqNum = d.mu.versions.visibleSeqNum.Load()\n\t\t}\n\t\tnewIters = d.newIters\n\t\tnewIterRangeKey = d.tableNewRangeKeyIter\n\t}\n\n\t// Bundle various structures under a single umbrella in order to allocate\n\t// them together.\n\tbuf := iterAllocPool.Get().(*iterAlloc)\n\tdbi := &buf.dbi\n\t*dbi = Iterator{\n\t\tctx:                 ctx,\n\t\talloc:               buf,\n\t\tmerge:               d.merge,\n\t\tcomparer:            *d.opts.Comparer,\n\t\treadState:           readState,\n\t\tversion:             newIterOpts.snapshot.vers,\n\t\tkeyBuf:              buf.keyBuf,\n\t\tprefixOrFullSeekKey: buf.prefixOrFullSeekKey,\n\t\tboundsBuf:           buf.boundsBuf,\n\t\tbatch:               batch,\n\t\tfc:                  d.fileCache,\n\t\tnewIters:            newIters,\n\t\tnewIterRangeKey:     newIterRangeKey,\n\t\tseqNum:              seqNum,\n\t\tbatchOnlyIter:       newIterOpts.batch.batchOnly,\n\t}\n\tif o != nil {\n\t\tdbi.opts = *o\n\t\tdbi.processBounds(o.LowerBound, o.UpperBound)\n\t}\n\tdbi.opts.logger = d.opts.Logger\n\tif d.opts.private.disableLazyCombinedIteration {\n\t\tdbi.opts.disableLazyCombinedIteration = true\n\t}\n\tif batch != nil {\n\t\tdbi.batchSeqNum = dbi.batch.nextSeqNum()\n\t}\n\treturn finishInitializingIter(ctx, buf)\n}\n\n// finishInitializingIter is a helper for doing the non-trivial initialization\n// of an Iterator. It's invoked to perform the initial initialization of an\n// Iterator during NewIter or Clone, and to perform reinitialization due to a\n// change in IterOptions by a call to Iterator.SetOptions.\nfunc finishInitializingIter(ctx context.Context, buf *iterAlloc) *Iterator {\n\t// Short-hand.\n\tdbi := &buf.dbi\n\tvar memtables flushableList\n\tif dbi.readState != nil {\n\t\tmemtables = dbi.readState.memtables\n\t}\n\tif dbi.opts.OnlyReadGuaranteedDurable {\n\t\tmemtables = nil\n\t} else {\n\t\t// We only need to read from memtables which contain sequence numbers older\n\t\t// than seqNum. Trim off newer memtables.\n\t\tfor i := len(memtables) - 1; i >= 0; i-- {\n\t\t\tif logSeqNum := memtables[i].logSeqNum; logSeqNum < dbi.seqNum {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tmemtables = memtables[:i]\n\t\t}\n\t}\n\n\tif dbi.opts.pointKeys() {\n\t\t// Construct the point iterator, initializing dbi.pointIter to point to\n\t\t// dbi.merging. If this is called during a SetOptions call and this\n\t\t// Iterator has already initialized dbi.merging, constructPointIter is a\n\t\t// noop and an initialized pointIter already exists in dbi.pointIter.\n\t\tdbi.constructPointIter(ctx, memtables, buf)\n\t\tdbi.iter = dbi.pointIter\n\t} else {\n\t\tdbi.iter = emptyIter\n\t}\n\n\tif dbi.opts.rangeKeys() {\n\t\tdbi.rangeKeyMasking.init(dbi, &dbi.comparer)\n\n\t\t// When iterating over both point and range keys, don't create the\n\t\t// range-key iterator stack immediately if we can avoid it. This\n\t\t// optimization takes advantage of the expected sparseness of range\n\t\t// keys, and configures the point-key iterator to dynamically switch to\n\t\t// combined iteration when it observes a file containing range keys.\n\t\t//\n\t\t// Lazy combined iteration is not possible if a batch or a memtable\n\t\t// contains any range keys.\n\t\tuseLazyCombinedIteration := dbi.rangeKey == nil &&\n\t\t\tdbi.opts.KeyTypes == IterKeyTypePointsAndRanges &&\n\t\t\t(dbi.batch == nil || dbi.batch.countRangeKeys == 0) &&\n\t\t\t!dbi.opts.disableLazyCombinedIteration\n\t\tif useLazyCombinedIteration {\n\t\t\t// The user requested combined iteration, and there's no indexed\n\t\t\t// batch currently containing range keys that would prevent lazy\n\t\t\t// combined iteration. Check the memtables to see if they contain\n\t\t\t// any range keys.\n\t\t\tfor i := range memtables {\n\t\t\t\tif memtables[i].containsRangeKeys() {\n\t\t\t\t\tuseLazyCombinedIteration = false\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif useLazyCombinedIteration {\n\t\t\tdbi.lazyCombinedIter = lazyCombinedIter{\n\t\t\t\tparent:    dbi,\n\t\t\t\tpointIter: dbi.pointIter,\n\t\t\t\tcombinedIterState: combinedIterState{\n\t\t\t\t\tinitialized: false,\n\t\t\t\t},\n\t\t\t}\n\t\t\tdbi.iter = &dbi.lazyCombinedIter\n\t\t\tdbi.iter = invalidating.MaybeWrapIfInvariants(dbi.iter)\n\t\t} else {\n\t\t\tdbi.lazyCombinedIter.combinedIterState = combinedIterState{\n\t\t\t\tinitialized: true,\n\t\t\t}\n\t\t\tif dbi.rangeKey == nil {\n\t\t\t\tdbi.rangeKey = iterRangeKeyStateAllocPool.Get().(*iteratorRangeKeyState)\n\t\t\t\tdbi.rangeKey.init(dbi.comparer.Compare, dbi.comparer.Split, &dbi.opts)\n\t\t\t\tdbi.constructRangeKeyIter()\n\t\t\t} else {\n\t\t\t\tdbi.rangeKey.iterConfig.SetBounds(dbi.opts.LowerBound, dbi.opts.UpperBound)\n\t\t\t}\n\n\t\t\t// Wrap the point iterator (currently dbi.iter) with an interleaving\n\t\t\t// iterator that interleaves range keys pulled from\n\t\t\t// dbi.rangeKey.rangeKeyIter.\n\t\t\t//\n\t\t\t// NB: The interleaving iterator is always reinitialized, even if\n\t\t\t// dbi already had an initialized range key iterator, in case the point\n\t\t\t// iterator changed or the range key masking suffix changed.\n\t\t\tdbi.rangeKey.iiter.Init(&dbi.comparer, dbi.iter, dbi.rangeKey.rangeKeyIter,\n\t\t\t\tkeyspan.InterleavingIterOpts{\n\t\t\t\t\tMask:       &dbi.rangeKeyMasking,\n\t\t\t\t\tLowerBound: dbi.opts.LowerBound,\n\t\t\t\t\tUpperBound: dbi.opts.UpperBound,\n\t\t\t\t})\n\t\t\tdbi.iter = &dbi.rangeKey.iiter\n\t\t}\n\t} else {\n\t\t// !dbi.opts.rangeKeys()\n\t\t//\n\t\t// Reset the combined iterator state. The initialized=true ensures the\n\t\t// iterator doesn't unnecessarily try to switch to combined iteration.\n\t\tdbi.lazyCombinedIter.combinedIterState = combinedIterState{initialized: true}\n\t}\n\treturn dbi\n}\n\n// ScanInternal scans all internal keys within the specified bounds, truncating\n// any rangedels and rangekeys to those bounds if they span past them. For use\n// when an external user needs to be aware of all internal keys that make up a\n// key range.\n//\n// Keys deleted by range deletions must not be returned or exposed by this\n// method, while the range deletion deleting that key must be exposed using\n// visitRangeDel. Keys that would be masked by range key masking (if an\n// appropriate prefix were set) should be exposed, alongside the range key\n// that would have masked it. This method also collapses all point keys into\n// one InternalKey; so only one internal key at most per user key is returned\n// to visitPointKey.\n//\n// If visitSharedFile is not nil, ScanInternal iterates in skip-shared iteration\n// mode. In this iteration mode, sstables in levels L5 and L6 are skipped, and\n// their metadatas truncated to [lower, upper) and passed into visitSharedFile.\n// ErrInvalidSkipSharedIteration is returned if visitSharedFile is not nil and an\n// sstable in L5 or L6 is found that is not in shared storage according to\n// provider.IsShared, or an sstable in those levels contains a newer key than the\n// snapshot sequence number (only applicable for snapshot.ScanInternal). Examples\n// of when this could happen could be if Pebble started writing sstables before a\n// creator ID was set (as creator IDs are necessary to enable shared storage)\n// resulting in some lower level SSTs being on non-shared storage. Skip-shared\n// iteration is invalid in those cases.\nfunc (d *DB) ScanInternal(\n\tctx context.Context,\n\tcategory sstable.Category,\n\tlower, upper []byte,\n\tvisitPointKey func(key *InternalKey, value LazyValue, iterInfo IteratorLevel) error,\n\tvisitRangeDel func(start, end []byte, seqNum SeqNum) error,\n\tvisitRangeKey func(start, end []byte, keys []rangekey.Key) error,\n\tvisitSharedFile func(sst *SharedSSTMeta) error,\n\tvisitExternalFile func(sst *ExternalFile) error,\n) error {\n\tscanInternalOpts := &scanInternalOptions{\n\t\tcategory:          category,\n\t\tvisitPointKey:     visitPointKey,\n\t\tvisitRangeDel:     visitRangeDel,\n\t\tvisitRangeKey:     visitRangeKey,\n\t\tvisitSharedFile:   visitSharedFile,\n\t\tvisitExternalFile: visitExternalFile,\n\t\tIterOptions: IterOptions{\n\t\t\tKeyTypes:   IterKeyTypePointsAndRanges,\n\t\t\tLowerBound: lower,\n\t\t\tUpperBound: upper,\n\t\t},\n\t}\n\titer, err := d.newInternalIter(ctx, snapshotIterOpts{} /* snapshot */, scanInternalOpts)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer iter.close()\n\treturn scanInternalImpl(ctx, lower, upper, iter, scanInternalOpts)\n}\n\n// newInternalIter constructs and returns a new scanInternalIterator on this db.\n// If o.skipSharedLevels is true, levels below sharedLevelsStart are *not* added\n// to the internal iterator.\n//\n// TODO(bilal): This method has a lot of similarities with db.newIter as well as\n// finishInitializingIter. Both pairs of methods should be refactored to reduce\n// this duplication.\nfunc (d *DB) newInternalIter(\n\tctx context.Context, sOpts snapshotIterOpts, o *scanInternalOptions,\n) (*scanInternalIterator, error) {\n\tif err := d.closed.Load(); err != nil {\n\t\tpanic(err)\n\t}\n\t// Grab and reference the current readState. This prevents the underlying\n\t// files in the associated version from being deleted if there is a current\n\t// compaction. The readState is unref'd by Iterator.Close().\n\tvar readState *readState\n\tif sOpts.vers == nil {\n\t\tif sOpts.readState != nil {\n\t\t\treadState = sOpts.readState\n\t\t\treadState.ref()\n\t\t} else {\n\t\t\treadState = d.loadReadState()\n\t\t}\n\t}\n\tif sOpts.vers != nil {\n\t\tsOpts.vers.Ref()\n\t}\n\n\t// Determine the seqnum to read at after grabbing the read state (current and\n\t// memtables) above.\n\tseqNum := sOpts.seqNum\n\tif seqNum == 0 {\n\t\tseqNum = d.mu.versions.visibleSeqNum.Load()\n\t}\n\n\t// Bundle various structures under a single umbrella in order to allocate\n\t// them together.\n\tbuf := iterAllocPool.Get().(*iterAlloc)\n\tdbi := &scanInternalIterator{\n\t\tctx:             ctx,\n\t\tdb:              d,\n\t\tcomparer:        d.opts.Comparer,\n\t\tmerge:           d.opts.Merger.Merge,\n\t\treadState:       readState,\n\t\tversion:         sOpts.vers,\n\t\talloc:           buf,\n\t\tnewIters:        d.newIters,\n\t\tnewIterRangeKey: d.tableNewRangeKeyIter,\n\t\tseqNum:          seqNum,\n\t\tmergingIter:     &buf.merging,\n\t}\n\tdbi.opts = *o\n\tdbi.opts.logger = d.opts.Logger\n\tif d.opts.private.disableLazyCombinedIteration {\n\t\tdbi.opts.disableLazyCombinedIteration = true\n\t}\n\treturn finishInitializingInternalIter(buf, dbi)\n}\n\nfunc finishInitializingInternalIter(\n\tbuf *iterAlloc, i *scanInternalIterator,\n) (*scanInternalIterator, error) {\n\t// Short-hand.\n\tvar memtables flushableList\n\tif i.readState != nil {\n\t\tmemtables = i.readState.memtables\n\t}\n\t// We only need to read from memtables which contain sequence numbers older\n\t// than seqNum. Trim off newer memtables.\n\tfor j := len(memtables) - 1; j >= 0; j-- {\n\t\tif logSeqNum := memtables[j].logSeqNum; logSeqNum < i.seqNum {\n\t\t\tbreak\n\t\t}\n\t\tmemtables = memtables[:j]\n\t}\n\ti.initializeBoundBufs(i.opts.LowerBound, i.opts.UpperBound)\n\n\tif err := i.constructPointIter(i.opts.category, memtables, buf); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// For internal iterators, we skip the lazy combined iteration optimization\n\t// entirely, and create the range key iterator stack directly.\n\ti.rangeKey = iterRangeKeyStateAllocPool.Get().(*iteratorRangeKeyState)\n\ti.rangeKey.init(i.comparer.Compare, i.comparer.Split, &i.opts.IterOptions)\n\tif err := i.constructRangeKeyIter(); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Wrap the point iterator (currently i.iter) with an interleaving\n\t// iterator that interleaves range keys pulled from\n\t// i.rangeKey.rangeKeyIter.\n\ti.rangeKey.iiter.Init(i.comparer, i.iter, i.rangeKey.rangeKeyIter,\n\t\tkeyspan.InterleavingIterOpts{\n\t\t\tLowerBound: i.opts.LowerBound,\n\t\t\tUpperBound: i.opts.UpperBound,\n\t\t})\n\ti.iter = &i.rangeKey.iiter\n\n\treturn i, nil\n}\n\nfunc (i *Iterator) constructPointIter(\n\tctx context.Context, memtables flushableList, buf *iterAlloc,\n) {\n\tif i.pointIter != nil {\n\t\t// Already have one.\n\t\treturn\n\t}\n\tinternalOpts := internalIterOpts{\n\t\tstats: &i.stats.InternalStats,\n\t}\n\t// If the file cache has a sstable stats collector, ask it for an\n\t// accumulator for this iterator's configured category and QoS. All SSTable\n\t// iterators created by this Iterator will accumulate their stats to it as\n\t// they Close during iteration.\n\tif collector := i.fc.dbOpts.sstStatsCollector; collector != nil {\n\t\tinternalOpts.iterStatsAccumulator = collector.Accumulator(\n\t\t\tuint64(uintptr(unsafe.Pointer(i))),\n\t\t\ti.opts.Category,\n\t\t)\n\t}\n\tif i.opts.RangeKeyMasking.Filter != nil {\n\t\tinternalOpts.boundLimitedFilter = &i.rangeKeyMasking\n\t}\n\n\t// Merging levels and levels from iterAlloc.\n\tmlevels := buf.mlevels[:0]\n\tlevels := buf.levels[:0]\n\n\t// We compute the number of levels needed ahead of time and reallocate a slice if\n\t// the array from the iterAlloc isn't large enough. Doing this allocation once\n\t// should improve the performance.\n\tnumMergingLevels := 0\n\tnumLevelIters := 0\n\tif i.batch != nil {\n\t\tnumMergingLevels++\n\t}\n\n\tvar current *version\n\tif !i.batchOnlyIter {\n\t\tnumMergingLevels += len(memtables)\n\n\t\tcurrent = i.version\n\t\tif current == nil {\n\t\t\tcurrent = i.readState.current\n\t\t}\n\t\tnumMergingLevels += len(current.L0SublevelFiles)\n\t\tnumLevelIters += len(current.L0SublevelFiles)\n\t\tfor level := 1; level < len(current.Levels); level++ {\n\t\t\tif current.Levels[level].Empty() {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tnumMergingLevels++\n\t\t\tnumLevelIters++\n\t\t}\n\t}\n\n\tif numMergingLevels > cap(mlevels) {\n\t\tmlevels = make([]mergingIterLevel, 0, numMergingLevels)\n\t}\n\tif numLevelIters > cap(levels) {\n\t\tlevels = make([]levelIter, 0, numLevelIters)\n\t}\n\n\t// Top-level is the batch, if any.\n\tif i.batch != nil {\n\t\tif i.batch.index == nil {\n\t\t\t// This isn't an indexed batch. We shouldn't have gotten this far.\n\t\t\tpanic(errors.AssertionFailedf(\"creating an iterator over an unindexed batch\"))\n\t\t} else {\n\t\t\ti.batch.initInternalIter(&i.opts, &i.batchPointIter)\n\t\t\ti.batch.initRangeDelIter(&i.opts, &i.batchRangeDelIter, i.batchSeqNum)\n\t\t\t// Only include the batch's rangedel iterator if it's non-empty.\n\t\t\t// This requires some subtle logic in the case a rangedel is later\n\t\t\t// written to the batch and the view of the batch is refreshed\n\t\t\t// during a call to SetOptions—in this case, we need to reconstruct\n\t\t\t// the point iterator to add the batch rangedel iterator.\n\t\t\tvar rangeDelIter keyspan.FragmentIterator\n\t\t\tif i.batchRangeDelIter.Count() > 0 {\n\t\t\t\trangeDelIter = &i.batchRangeDelIter\n\t\t\t}\n\t\t\tmlevels = append(mlevels, mergingIterLevel{\n\t\t\t\titer:         &i.batchPointIter,\n\t\t\t\trangeDelIter: rangeDelIter,\n\t\t\t})\n\t\t}\n\t}\n\n\tif !i.batchOnlyIter {\n\t\t// Next are the memtables.\n\t\tfor j := len(memtables) - 1; j >= 0; j-- {\n\t\t\tmem := memtables[j]\n\t\t\tmlevels = append(mlevels, mergingIterLevel{\n\t\t\t\titer:         mem.newIter(&i.opts),\n\t\t\t\trangeDelIter: mem.newRangeDelIter(&i.opts),\n\t\t\t})\n\t\t}\n\n\t\t// Next are the file levels: L0 sub-levels followed by lower levels.\n\t\tmlevelsIndex := len(mlevels)\n\t\tlevelsIndex := len(levels)\n\t\tmlevels = mlevels[:numMergingLevels]\n\t\tlevels = levels[:numLevelIters]\n\t\ti.opts.snapshotForHideObsoletePoints = buf.dbi.seqNum\n\t\taddLevelIterForFiles := func(files manifest.LevelIterator, level manifest.Layer) {\n\t\t\tli := &levels[levelsIndex]\n\n\t\t\tli.init(ctx, i.opts, &i.comparer, i.newIters, files, level, internalOpts)\n\t\t\tli.initRangeDel(&mlevels[mlevelsIndex])\n\t\t\tli.initCombinedIterState(&i.lazyCombinedIter.combinedIterState)\n\t\t\tmlevels[mlevelsIndex].levelIter = li\n\t\t\tmlevels[mlevelsIndex].iter = invalidating.MaybeWrapIfInvariants(li)\n\n\t\t\tlevelsIndex++\n\t\t\tmlevelsIndex++\n\t\t}\n\n\t\t// Add level iterators for the L0 sublevels, iterating from newest to\n\t\t// oldest.\n\t\tfor i := len(current.L0SublevelFiles) - 1; i >= 0; i-- {\n\t\t\taddLevelIterForFiles(current.L0SublevelFiles[i].Iter(), manifest.L0Sublevel(i))\n\t\t}\n\n\t\t// Add level iterators for the non-empty non-L0 levels.\n\t\tfor level := 1; level < len(current.Levels); level++ {\n\t\t\tif current.Levels[level].Empty() {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\taddLevelIterForFiles(current.Levels[level].Iter(), manifest.Level(level))\n\t\t}\n\t}\n\tbuf.merging.init(&i.opts, &i.stats.InternalStats, i.comparer.Compare, i.comparer.Split, mlevels...)\n\tif len(mlevels) <= cap(buf.levelsPositioned) {\n\t\tbuf.merging.levelsPositioned = buf.levelsPositioned[:len(mlevels)]\n\t}\n\tbuf.merging.snapshot = i.seqNum\n\tbuf.merging.batchSnapshot = i.batchSeqNum\n\tbuf.merging.combinedIterState = &i.lazyCombinedIter.combinedIterState\n\ti.pointIter = invalidating.MaybeWrapIfInvariants(&buf.merging).(topLevelIterator)\n\ti.merging = &buf.merging\n}\n\n// NewBatch returns a new empty write-only batch. Any reads on the batch will\n// return an error. If the batch is committed it will be applied to the DB.\nfunc (d *DB) NewBatch(opts ...BatchOption) *Batch {\n\treturn newBatch(d, opts...)\n}\n\n// NewBatchWithSize is mostly identical to NewBatch, but it will allocate the\n// the specified memory space for the internal slice in advance.\nfunc (d *DB) NewBatchWithSize(size int, opts ...BatchOption) *Batch {\n\treturn newBatchWithSize(d, size, opts...)\n}\n\n// NewIndexedBatch returns a new empty read-write batch. Any reads on the batch\n// will read from both the batch and the DB. If the batch is committed it will\n// be applied to the DB. An indexed batch is slower that a non-indexed batch\n// for insert operations. If you do not need to perform reads on the batch, use\n// NewBatch instead.\nfunc (d *DB) NewIndexedBatch() *Batch {\n\treturn newIndexedBatch(d, d.opts.Comparer)\n}\n\n// NewIndexedBatchWithSize is mostly identical to NewIndexedBatch, but it will\n// allocate the specified memory space for the internal slice in advance.\nfunc (d *DB) NewIndexedBatchWithSize(size int) *Batch {\n\treturn newIndexedBatchWithSize(d, d.opts.Comparer, size)\n}\n\n// NewIter returns an iterator that is unpositioned (Iterator.Valid() will\n// return false). The iterator can be positioned via a call to SeekGE, SeekLT,\n// First or Last. The iterator provides a point-in-time view of the current DB\n// state. This view is maintained by preventing file deletions and preventing\n// memtables referenced by the iterator from being deleted. Using an iterator\n// to maintain a long-lived point-in-time view of the DB state can lead to an\n// apparent memory and disk usage leak. Use snapshots (see NewSnapshot) for\n// point-in-time snapshots which avoids these problems.\nfunc (d *DB) NewIter(o *IterOptions) (*Iterator, error) {\n\treturn d.NewIterWithContext(context.Background(), o)\n}\n\n// NewIterWithContext is like NewIter, and additionally accepts a context for\n// tracing.\nfunc (d *DB) NewIterWithContext(ctx context.Context, o *IterOptions) (*Iterator, error) {\n\treturn d.newIter(ctx, nil /* batch */, newIterOpts{}, o), nil\n}\n\n// NewSnapshot returns a point-in-time view of the current DB state. Iterators\n// created with this handle will all observe a stable snapshot of the current\n// DB state. The caller must call Snapshot.Close() when the snapshot is no\n// longer needed. Snapshots are not persisted across DB restarts (close ->\n// open). Unlike the implicit snapshot maintained by an iterator, a snapshot\n// will not prevent memtables from being released or sstables from being\n// deleted. Instead, a snapshot prevents deletion of sequence numbers\n// referenced by the snapshot.\nfunc (d *DB) NewSnapshot() *Snapshot {\n\tif err := d.closed.Load(); err != nil {\n\t\tpanic(err)\n\t}\n\n\td.mu.Lock()\n\ts := &Snapshot{\n\t\tdb:     d,\n\t\tseqNum: d.mu.versions.visibleSeqNum.Load(),\n\t}\n\td.mu.snapshots.pushBack(s)\n\td.mu.Unlock()\n\treturn s\n}\n\n// NewEventuallyFileOnlySnapshot returns a point-in-time view of the current DB\n// state, similar to NewSnapshot, but with consistency constrained to the\n// provided set of key ranges. See the comment at EventuallyFileOnlySnapshot for\n// its semantics.\nfunc (d *DB) NewEventuallyFileOnlySnapshot(keyRanges []KeyRange) *EventuallyFileOnlySnapshot {\n\tif err := d.closed.Load(); err != nil {\n\t\tpanic(err)\n\t}\n\tfor i := range keyRanges {\n\t\tif i > 0 && d.cmp(keyRanges[i-1].End, keyRanges[i].Start) > 0 {\n\t\t\tpanic(\"pebble: key ranges for eventually-file-only-snapshot not in order\")\n\t\t}\n\t}\n\treturn d.makeEventuallyFileOnlySnapshot(keyRanges)\n}\n\n// Close closes the DB.\n//\n// It is not safe to close a DB until all outstanding iterators are closed\n// or to call Close concurrently with any other DB method. It is not valid\n// to call any of a DB's methods after the DB has been closed.\nfunc (d *DB) Close() error {\n\t// Lock the commit pipeline for the duration of Close. This prevents a race\n\t// with makeRoomForWrite. Rotating the WAL in makeRoomForWrite requires\n\t// dropping d.mu several times for I/O. If Close only holds d.mu, an\n\t// in-progress WAL rotation may re-acquire d.mu only once the database is\n\t// closed.\n\t//\n\t// Additionally, locking the commit pipeline makes it more likely that\n\t// (illegal) concurrent writes will observe d.closed.Load() != nil, creating\n\t// more understable panics if the database is improperly used concurrently\n\t// during Close.\n\td.commit.mu.Lock()\n\tdefer d.commit.mu.Unlock()\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\tif err := d.closed.Load(); err != nil {\n\t\tpanic(err)\n\t}\n\n\t// Clear the finalizer that is used to check that an unreferenced DB has been\n\t// closed. We're closing the DB here, so the check performed by that\n\t// finalizer isn't necessary.\n\t//\n\t// Note: this is a no-op if invariants are disabled or race is enabled.\n\tinvariants.SetFinalizer(d.closed, nil)\n\n\td.closed.Store(errors.WithStack(ErrClosed))\n\tclose(d.closedCh)\n\n\tdefer d.opts.Cache.Unref()\n\n\tfor d.mu.compact.compactingCount > 0 || d.mu.compact.downloadingCount > 0 || d.mu.compact.flushing {\n\t\td.mu.compact.cond.Wait()\n\t}\n\tfor d.mu.tableStats.loading {\n\t\td.mu.tableStats.cond.Wait()\n\t}\n\tfor d.mu.tableValidation.validating {\n\t\td.mu.tableValidation.cond.Wait()\n\t}\n\n\tvar err error\n\tif n := len(d.mu.compact.inProgress); n > 0 {\n\t\terr = errors.Errorf(\"pebble: %d unexpected in-progress compactions\", errors.Safe(n))\n\t}\n\terr = firstError(err, d.mu.formatVers.marker.Close())\n\terr = firstError(err, d.fileCache.close())\n\tif !d.opts.ReadOnly {\n\t\tif d.mu.log.writer != nil {\n\t\t\t_, err2 := d.mu.log.writer.Close()\n\t\t\terr = firstError(err, err2)\n\t\t}\n\t} else if d.mu.log.writer != nil {\n\t\tpanic(\"pebble: log-writer should be nil in read-only mode\")\n\t}\n\terr = firstError(err, d.mu.log.manager.Close())\n\terr = firstError(err, d.fileLock.Close())\n\n\t// Note that versionSet.close() only closes the MANIFEST. The versions list\n\t// is still valid for the checks below.\n\terr = firstError(err, d.mu.versions.close())\n\n\terr = firstError(err, d.dataDir.Close())\n\n\td.readState.val.unrefLocked()\n\n\tcurrent := d.mu.versions.currentVersion()\n\tfor v := d.mu.versions.versions.Front(); true; v = v.Next() {\n\t\trefs := v.Refs()\n\t\tif v == current {\n\t\t\tif refs != 1 {\n\t\t\t\terr = firstError(err, errors.Errorf(\"leaked iterators: current\\n%s\", v))\n\t\t\t}\n\t\t\tbreak\n\t\t}\n\t\tif refs != 0 {\n\t\t\terr = firstError(err, errors.Errorf(\"leaked iterators:\\n%s\", v))\n\t\t}\n\t}\n\n\tfor _, mem := range d.mu.mem.queue {\n\t\t// Usually, we'd want to delete the files returned by readerUnref. But\n\t\t// in this case, even if we're unreferencing the flushables, the\n\t\t// flushables aren't obsolete. They will be reconstructed during WAL\n\t\t// replay.\n\t\tmem.readerUnrefLocked(false)\n\t}\n\t// If there's an unused, recycled memtable, we need to release its memory.\n\tif obsoleteMemTable := d.memTableRecycle.Swap(nil); obsoleteMemTable != nil {\n\t\td.freeMemTable(obsoleteMemTable)\n\t}\n\tif reserved := d.memTableReserved.Load(); reserved != 0 {\n\t\terr = firstError(err, errors.Errorf(\"leaked memtable reservation: %d\", errors.Safe(reserved)))\n\t}\n\n\t// Since we called d.readState.val.unrefLocked() above, we are expected to\n\t// manually schedule deletion of obsolete files.\n\tif len(d.mu.versions.obsoleteTables) > 0 {\n\t\td.deleteObsoleteFiles(d.newJobIDLocked())\n\t}\n\n\td.mu.Unlock()\n\td.compactionSchedulers.Wait()\n\n\t// Wait for all cleaning jobs to finish.\n\td.cleanupManager.Close()\n\n\t// Sanity check metrics.\n\tif invariants.Enabled {\n\t\tm := d.Metrics()\n\t\tif m.Compact.NumInProgress > 0 || m.Compact.InProgressBytes > 0 {\n\t\t\td.mu.Lock()\n\t\t\tpanic(fmt.Sprintf(\"invalid metrics on close:\\n%s\", m))\n\t\t}\n\t}\n\n\td.mu.Lock()\n\n\t// As a sanity check, ensure that there are no zombie tables. A non-zero count\n\t// hints at a reference count leak.\n\tif ztbls := len(d.mu.versions.zombieTables); ztbls > 0 {\n\t\terr = firstError(err, errors.Errorf(\"non-zero zombie file count: %d\", ztbls))\n\t}\n\n\terr = firstError(err, d.objProvider.Close())\n\n\t// If the options include a closer to 'close' the filesystem, close it.\n\tif d.opts.private.fsCloser != nil {\n\t\td.opts.private.fsCloser.Close()\n\t}\n\n\t// Return an error if the user failed to close all open snapshots.\n\tif v := d.mu.snapshots.count(); v > 0 {\n\t\terr = firstError(err, errors.Errorf(\"leaked snapshots: %d open snapshots on DB %p\", v, d))\n\t}\n\n\treturn err\n}\n\n// Compact the specified range of keys in the database.\nfunc (d *DB) Compact(start, end []byte, parallelize bool) error {\n\tif err := d.closed.Load(); err != nil {\n\t\tpanic(err)\n\t}\n\tif d.opts.ReadOnly {\n\t\treturn ErrReadOnly\n\t}\n\tif d.cmp(start, end) >= 0 {\n\t\treturn errors.Errorf(\"Compact start %s is not less than end %s\",\n\t\t\td.opts.Comparer.FormatKey(start), d.opts.Comparer.FormatKey(end))\n\t}\n\n\td.mu.Lock()\n\tmaxLevelWithFiles := 1\n\tcur := d.mu.versions.currentVersion()\n\tfor level := 0; level < numLevels; level++ {\n\t\toverlaps := cur.Overlaps(level, base.UserKeyBoundsInclusive(start, end))\n\t\tif !overlaps.Empty() {\n\t\t\tmaxLevelWithFiles = level + 1\n\t\t}\n\t}\n\n\t// Determine if any memtable overlaps with the compaction range. We wait for\n\t// any such overlap to flush (initiating a flush if necessary).\n\tmem, err := func() (*flushableEntry, error) {\n\t\t// Check to see if any files overlap with any of the memtables. The queue\n\t\t// is ordered from oldest to newest with the mutable memtable being the\n\t\t// last element in the slice. We want to wait for the newest table that\n\t\t// overlaps.\n\t\tfor i := len(d.mu.mem.queue) - 1; i >= 0; i-- {\n\t\t\tmem := d.mu.mem.queue[i]\n\t\t\tvar anyOverlaps bool\n\t\t\tmem.computePossibleOverlaps(func(b bounded) shouldContinue {\n\t\t\t\tanyOverlaps = true\n\t\t\t\treturn stopIteration\n\t\t\t}, KeyRange{Start: start, End: end})\n\t\t\tif !anyOverlaps {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tvar err error\n\t\t\tif mem.flushable == d.mu.mem.mutable {\n\t\t\t\t// We have to hold both commitPipeline.mu and DB.mu when calling\n\t\t\t\t// makeRoomForWrite(). Lock order requirements elsewhere force us to\n\t\t\t\t// unlock DB.mu in order to grab commitPipeline.mu first.\n\t\t\t\td.mu.Unlock()\n\t\t\t\td.commit.mu.Lock()\n\t\t\t\td.mu.Lock()\n\t\t\t\tdefer d.commit.mu.Unlock()\n\t\t\t\tif mem.flushable == d.mu.mem.mutable {\n\t\t\t\t\t// Only flush if the active memtable is unchanged.\n\t\t\t\t\terr = d.makeRoomForWrite(nil)\n\t\t\t\t}\n\t\t\t}\n\t\t\tmem.flushForced = true\n\t\t\td.maybeScheduleFlush()\n\t\t\treturn mem, err\n\t\t}\n\t\treturn nil, nil\n\t}()\n\n\td.mu.Unlock()\n\n\tif err != nil {\n\t\treturn err\n\t}\n\tif mem != nil {\n\t\t<-mem.flushed\n\t}\n\n\tfor level := 0; level < maxLevelWithFiles; {\n\t\tfor {\n\t\t\tif err := d.manualCompact(\n\t\t\t\tstart, end, level, parallelize); err != nil {\n\t\t\t\tif errors.Is(err, ErrCancelledCompaction) {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tbreak\n\t\t}\n\t\tlevel++\n\t\tif level == numLevels-1 {\n\t\t\t// A manual compaction of the bottommost level occurred.\n\t\t\t// There is no next level to try and compact.\n\t\t\tbreak\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (d *DB) manualCompact(start, end []byte, level int, parallelize bool) error {\n\td.mu.Lock()\n\tcurr := d.mu.versions.currentVersion()\n\tfiles := curr.Overlaps(level, base.UserKeyBoundsInclusive(start, end))\n\tif files.Empty() {\n\t\td.mu.Unlock()\n\t\treturn nil\n\t}\n\n\tvar compactions []*manualCompaction\n\tif parallelize {\n\t\tcompactions = append(compactions, d.splitManualCompaction(start, end, level)...)\n\t} else {\n\t\tcompactions = append(compactions, &manualCompaction{\n\t\t\tlevel: level,\n\t\t\tdone:  make(chan error, 1),\n\t\t\tstart: start,\n\t\t\tend:   end,\n\t\t})\n\t}\n\td.mu.compact.manual = append(d.mu.compact.manual, compactions...)\n\td.maybeScheduleCompaction()\n\td.mu.Unlock()\n\n\t// Each of the channels is guaranteed to be eventually sent to once. After a\n\t// compaction is possibly picked in d.maybeScheduleCompaction(), either the\n\t// compaction is dropped, executed after being scheduled, or retried later.\n\t// Assuming eventual progress when a compaction is retried, all outcomes send\n\t// a value to the done channel. Since the channels are buffered, it is not\n\t// necessary to read from each channel, and so we can exit early in the event\n\t// of an error.\n\tfor _, compaction := range compactions {\n\t\tif err := <-compaction.done; err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// splitManualCompaction splits a manual compaction over [start,end] on level\n// such that the resulting compactions have no key overlap.\nfunc (d *DB) splitManualCompaction(\n\tstart, end []byte, level int,\n) (splitCompactions []*manualCompaction) {\n\tcurr := d.mu.versions.currentVersion()\n\tendLevel := level + 1\n\tbaseLevel := d.mu.versions.picker.getBaseLevel()\n\tif level == 0 {\n\t\tendLevel = baseLevel\n\t}\n\tkeyRanges := curr.CalculateInuseKeyRanges(level, endLevel, start, end)\n\tfor _, keyRange := range keyRanges {\n\t\tsplitCompactions = append(splitCompactions, &manualCompaction{\n\t\t\tlevel: level,\n\t\t\tdone:  make(chan error, 1),\n\t\t\tstart: keyRange.Start,\n\t\t\tend:   keyRange.End.Key,\n\t\t\tsplit: true,\n\t\t})\n\t}\n\treturn splitCompactions\n}\n\n// Flush the memtable to stable storage.\nfunc (d *DB) Flush() error {\n\tflushDone, err := d.AsyncFlush()\n\tif err != nil {\n\t\treturn err\n\t}\n\t<-flushDone\n\treturn nil\n}\n\n// AsyncFlush asynchronously flushes the memtable to stable storage.\n//\n// If no error is returned, the caller can receive from the returned channel in\n// order to wait for the flush to complete.\nfunc (d *DB) AsyncFlush() (<-chan struct{}, error) {\n\tif err := d.closed.Load(); err != nil {\n\t\tpanic(err)\n\t}\n\tif d.opts.ReadOnly {\n\t\treturn nil, ErrReadOnly\n\t}\n\n\td.commit.mu.Lock()\n\tdefer d.commit.mu.Unlock()\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\tflushed := d.mu.mem.queue[len(d.mu.mem.queue)-1].flushed\n\terr := d.makeRoomForWrite(nil)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn flushed, nil\n}\n\n// Metrics returns metrics about the database.\nfunc (d *DB) Metrics() *Metrics {\n\tmetrics := &Metrics{}\n\twalStats := d.mu.log.manager.Stats()\n\n\td.mu.Lock()\n\tvers := d.mu.versions.currentVersion()\n\t*metrics = d.mu.versions.metrics\n\tmetrics.Compact.EstimatedDebt = d.mu.versions.picker.estimatedCompactionDebt(0)\n\tmetrics.Compact.InProgressBytes = d.mu.versions.atomicInProgressBytes.Load()\n\t// TODO(radu): split this to separate the download compactions.\n\tmetrics.Compact.NumInProgress = int64(d.mu.compact.compactingCount + d.mu.compact.downloadingCount)\n\tmetrics.Compact.MarkedFiles = vers.Stats.MarkedForCompaction\n\tmetrics.Compact.Duration = d.mu.compact.duration\n\tfor c := range d.mu.compact.inProgress {\n\t\tif c.kind != compactionKindFlush && c.kind != compactionKindIngestedFlushable {\n\t\t\tmetrics.Compact.Duration += d.timeNow().Sub(c.beganAt)\n\t\t}\n\t}\n\n\tfor _, m := range d.mu.mem.queue {\n\t\tmetrics.MemTable.Size += m.totalBytes()\n\t}\n\tmetrics.Snapshots.Count = d.mu.snapshots.count()\n\tif metrics.Snapshots.Count > 0 {\n\t\tmetrics.Snapshots.EarliestSeqNum = d.mu.snapshots.earliest()\n\t}\n\tmetrics.Snapshots.PinnedKeys = d.mu.snapshots.cumulativePinnedCount\n\tmetrics.Snapshots.PinnedSize = d.mu.snapshots.cumulativePinnedSize\n\tmetrics.MemTable.Count = int64(len(d.mu.mem.queue))\n\tmetrics.MemTable.ZombieCount = d.memTableCount.Load() - metrics.MemTable.Count\n\tmetrics.MemTable.ZombieSize = uint64(d.memTableReserved.Load()) - metrics.MemTable.Size\n\tmetrics.WAL.ObsoleteFiles = int64(walStats.ObsoleteFileCount)\n\tmetrics.WAL.ObsoletePhysicalSize = walStats.ObsoleteFileSize\n\tmetrics.WAL.Files = int64(walStats.LiveFileCount)\n\t// The current WAL's size (d.logSize) is the logical size, which may be less\n\t// than the WAL's physical size if it was recycled. walStats.LiveFileSize\n\t// includes the physical size of all live WALs, but for the current WAL it\n\t// reflects the physical size when it was opened. So it is possible that\n\t// d.atomic.logSize has exceeded that physical size. We allow for this\n\t// anomaly.\n\tmetrics.WAL.PhysicalSize = walStats.LiveFileSize\n\tmetrics.WAL.BytesIn = d.logBytesIn.Load()\n\tmetrics.WAL.Size = d.logSize.Load()\n\tfor i, n := 0, len(d.mu.mem.queue)-1; i < n; i++ {\n\t\tmetrics.WAL.Size += d.mu.mem.queue[i].logSize\n\t}\n\tmetrics.WAL.BytesWritten = metrics.Levels[0].BytesIn + metrics.WAL.Size\n\tmetrics.WAL.Failover = walStats.Failover\n\n\tif p := d.mu.versions.picker; p != nil {\n\t\tcompactions := d.getInProgressCompactionInfoLocked(nil)\n\t\tfor level, score := range p.getScores(compactions) {\n\t\t\tmetrics.Levels[level].Score = score\n\t\t}\n\t}\n\tmetrics.Table.ZombieCount = int64(len(d.mu.versions.zombieTables))\n\tfor _, info := range d.mu.versions.zombieTables {\n\t\tmetrics.Table.ZombieSize += info.FileSize\n\t\tif info.isLocal {\n\t\t\tmetrics.Table.Local.ZombieSize += info.FileSize\n\t\t}\n\t}\n\tmetrics.private.optionsFileSize = d.optionsFileSize\n\n\t// TODO(jackson): Consider making these metrics optional.\n\tmetrics.Keys.RangeKeySetsCount = *rangeKeySetsAnnotator.MultiLevelAnnotation(vers.RangeKeyLevels[:])\n\tmetrics.Keys.TombstoneCount = *tombstonesAnnotator.MultiLevelAnnotation(vers.Levels[:])\n\n\td.mu.versions.logLock()\n\tmetrics.private.manifestFileSize = uint64(d.mu.versions.manifest.Size())\n\tbackingCount, backingTotalSize := d.mu.versions.virtualBackings.Stats()\n\tmetrics.Table.BackingTableCount = uint64(backingCount)\n\tmetrics.Table.BackingTableSize = backingTotalSize\n\td.mu.versions.logUnlock()\n\n\tmetrics.LogWriter.FsyncLatency = d.mu.log.metrics.fsyncLatency\n\tif err := metrics.LogWriter.Merge(&d.mu.log.metrics.LogWriterMetrics); err != nil {\n\t\td.opts.Logger.Errorf(\"metrics error: %s\", err)\n\t}\n\tmetrics.Flush.WriteThroughput = d.mu.compact.flushWriteThroughput\n\tif d.mu.compact.flushing {\n\t\tmetrics.Flush.NumInProgress = 1\n\t}\n\tfor i := 0; i < numLevels; i++ {\n\t\tmetrics.Levels[i].Additional.ValueBlocksSize = *valueBlockSizeAnnotator.LevelAnnotation(vers.Levels[i])\n\t\tcompressionTypes := compressionTypeAnnotator.LevelAnnotation(vers.Levels[i])\n\t\tmetrics.Table.CompressedCountUnknown += int64(compressionTypes.unknown)\n\t\tmetrics.Table.CompressedCountSnappy += int64(compressionTypes.snappy)\n\t\tmetrics.Table.CompressedCountZstd += int64(compressionTypes.zstd)\n\t\tmetrics.Table.CompressedCountNone += int64(compressionTypes.none)\n\t}\n\n\td.mu.Unlock()\n\n\tmetrics.BlockCache = d.opts.Cache.Metrics()\n\tmetrics.FileCache, metrics.Filter = d.fileCache.metrics()\n\tmetrics.TableIters = int64(d.fileCache.iterCount())\n\tmetrics.CategoryStats = d.fileCache.dbOpts.sstStatsCollector.GetStats()\n\n\tmetrics.SecondaryCacheMetrics = d.objProvider.Metrics()\n\n\tmetrics.Uptime = d.timeNow().Sub(d.openedAt)\n\n\tmetrics.manualMemory = manual.GetMetrics()\n\n\treturn metrics\n}\n\n// sstablesOptions hold the optional parameters to retrieve TableInfo for all sstables.\ntype sstablesOptions struct {\n\t// set to true will return the sstable properties in TableInfo\n\twithProperties bool\n\n\t// if set, return sstables that overlap the key range (end-exclusive)\n\tstart []byte\n\tend   []byte\n\n\twithApproximateSpanBytes bool\n}\n\n// SSTablesOption set optional parameter used by `DB.SSTables`.\ntype SSTablesOption func(*sstablesOptions)\n\n// WithProperties enable return sstable properties in each TableInfo.\n//\n// NOTE: if most of the sstable properties need to be read from disk,\n// this options may make method `SSTables` quite slow.\nfunc WithProperties() SSTablesOption {\n\treturn func(opt *sstablesOptions) {\n\t\topt.withProperties = true\n\t}\n}\n\n// WithKeyRangeFilter ensures returned sstables overlap start and end (end-exclusive)\n// if start and end are both nil these properties have no effect.\nfunc WithKeyRangeFilter(start, end []byte) SSTablesOption {\n\treturn func(opt *sstablesOptions) {\n\t\topt.end = end\n\t\topt.start = start\n\t}\n}\n\n// WithApproximateSpanBytes enables capturing the approximate number of bytes that\n// overlap the provided key span for each sstable.\n// NOTE: This option requires WithKeyRangeFilter.\nfunc WithApproximateSpanBytes() SSTablesOption {\n\treturn func(opt *sstablesOptions) {\n\t\topt.withApproximateSpanBytes = true\n\t}\n}\n\n// BackingType denotes the type of storage backing a given sstable.\ntype BackingType int\n\nconst (\n\t// BackingTypeLocal denotes an sstable stored on local disk according to the\n\t// objprovider. This file is completely owned by us.\n\tBackingTypeLocal BackingType = iota\n\t// BackingTypeShared denotes an sstable stored on shared storage, created\n\t// by this Pebble instance and possibly shared by other Pebble instances.\n\t// These types of files have lifecycle managed by Pebble.\n\tBackingTypeShared\n\t// BackingTypeSharedForeign denotes an sstable stored on shared storage,\n\t// created by a Pebble instance other than this one. These types of files have\n\t// lifecycle managed by Pebble.\n\tBackingTypeSharedForeign\n\t// BackingTypeExternal denotes an sstable stored on external storage,\n\t// not owned by any Pebble instance and with no refcounting/cleanup methods\n\t// or lifecycle management. An example of an external file is a file restored\n\t// from a backup.\n\tBackingTypeExternal\n\tbackingTypeCount\n)\n\nvar backingTypeToString = [backingTypeCount]string{\n\tBackingTypeLocal:         \"local\",\n\tBackingTypeShared:        \"shared\",\n\tBackingTypeSharedForeign: \"shared-foreign\",\n\tBackingTypeExternal:      \"external\",\n}\n\n// String implements fmt.Stringer.\nfunc (b BackingType) String() string {\n\treturn backingTypeToString[b]\n}\n\n// SSTableInfo export manifest.TableInfo with sstable.Properties alongside\n// other file backing info.\ntype SSTableInfo struct {\n\tmanifest.TableInfo\n\t// Virtual indicates whether the sstable is virtual.\n\tVirtual bool\n\t// BackingSSTNum is the disk file number associated with the backing sstable.\n\t// If Virtual is false, BackingSSTNum == PhysicalTableDiskFileNum(FileNum).\n\tBackingSSTNum base.DiskFileNum\n\t// BackingType is the type of storage backing this sstable.\n\tBackingType BackingType\n\t// Locator is the remote.Locator backing this sstable, if the backing type is\n\t// not BackingTypeLocal.\n\tLocator remote.Locator\n\t// ApproximateSpanBytes describes the approximate number of bytes within the\n\t// sstable that fall within a particular span. It's populated only when the\n\t// ApproximateSpanBytes option is passed into DB.SSTables.\n\tApproximateSpanBytes uint64 `json:\"ApproximateSpanBytes,omitempty\"`\n\n\t// Properties is the sstable properties of this table. If Virtual is true,\n\t// then the Properties are associated with the backing sst.\n\tProperties *sstable.Properties\n}\n\n// SSTables retrieves the current sstables. The returned slice is indexed by\n// level and each level is indexed by the position of the sstable within the\n// level. Note that this information may be out of date due to concurrent\n// flushes and compactions.\nfunc (d *DB) SSTables(opts ...SSTablesOption) ([][]SSTableInfo, error) {\n\topt := &sstablesOptions{}\n\tfor _, fn := range opts {\n\t\tfn(opt)\n\t}\n\n\tif opt.withApproximateSpanBytes && (opt.start == nil || opt.end == nil) {\n\t\treturn nil, errors.Errorf(\"cannot use WithApproximateSpanBytes without WithKeyRangeFilter option\")\n\t}\n\n\t// Grab and reference the current readState.\n\treadState := d.loadReadState()\n\tdefer readState.unref()\n\n\t// TODO(peter): This is somewhat expensive, especially on a large\n\t// database. It might be worthwhile to unify TableInfo and FileMetadata and\n\t// then we could simply return current.Files. Note that RocksDB is doing\n\t// something similar to the current code, so perhaps it isn't too bad.\n\tsrcLevels := readState.current.Levels\n\tvar totalTables int\n\tfor i := range srcLevels {\n\t\ttotalTables += srcLevels[i].Len()\n\t}\n\n\tdestTables := make([]SSTableInfo, totalTables)\n\tdestLevels := make([][]SSTableInfo, len(srcLevels))\n\tfor i := range destLevels {\n\t\titer := srcLevels[i].Iter()\n\t\tj := 0\n\t\tfor m := iter.First(); m != nil; m = iter.Next() {\n\t\t\tif opt.start != nil && opt.end != nil {\n\t\t\t\tb := base.UserKeyBoundsEndExclusive(opt.start, opt.end)\n\t\t\t\tif !m.Overlaps(d.opts.Comparer.Compare, &b) {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\t\tdestTables[j] = SSTableInfo{TableInfo: m.TableInfo()}\n\t\t\tif opt.withProperties {\n\t\t\t\tp, err := d.fileCache.getTableProperties(\n\t\t\t\t\tm,\n\t\t\t\t)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\tdestTables[j].Properties = p\n\t\t\t}\n\t\t\tdestTables[j].Virtual = m.Virtual\n\t\t\tdestTables[j].BackingSSTNum = m.FileBacking.DiskFileNum\n\t\t\tobjMeta, err := d.objProvider.Lookup(fileTypeTable, m.FileBacking.DiskFileNum)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tif objMeta.IsRemote() {\n\t\t\t\tif objMeta.IsShared() {\n\t\t\t\t\tif d.objProvider.IsSharedForeign(objMeta) {\n\t\t\t\t\t\tdestTables[j].BackingType = BackingTypeSharedForeign\n\t\t\t\t\t} else {\n\t\t\t\t\t\tdestTables[j].BackingType = BackingTypeShared\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tdestTables[j].BackingType = BackingTypeExternal\n\t\t\t\t}\n\t\t\t\tdestTables[j].Locator = objMeta.Remote.Locator\n\t\t\t} else {\n\t\t\t\tdestTables[j].BackingType = BackingTypeLocal\n\t\t\t}\n\n\t\t\tif opt.withApproximateSpanBytes {\n\t\t\t\tif m.ContainedWithinSpan(d.opts.Comparer.Compare, opt.start, opt.end) {\n\t\t\t\t\tdestTables[j].ApproximateSpanBytes = m.Size\n\t\t\t\t} else {\n\t\t\t\t\tsize, err := d.fileCache.estimateSize(m, opt.start, opt.end)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn nil, err\n\t\t\t\t\t}\n\t\t\t\t\tdestTables[j].ApproximateSpanBytes = size\n\t\t\t\t}\n\t\t\t}\n\t\t\tj++\n\t\t}\n\t\tdestLevels[i] = destTables[:j]\n\t\tdestTables = destTables[j:]\n\t}\n\n\treturn destLevels, nil\n}\n\n// makeFileSizeAnnotator returns an annotator that computes the total size of\n// files that meet some criteria defined by filter.\nfunc (d *DB) makeFileSizeAnnotator(filter func(f *fileMetadata) bool) *manifest.Annotator[uint64] {\n\treturn &manifest.Annotator[uint64]{\n\t\tAggregator: manifest.SumAggregator{\n\t\t\tAccumulateFunc: func(f *fileMetadata) (uint64, bool) {\n\t\t\t\tif filter(f) {\n\t\t\t\t\treturn f.Size, true\n\t\t\t\t}\n\t\t\t\treturn 0, true\n\t\t\t},\n\t\t\tAccumulatePartialOverlapFunc: func(f *fileMetadata, bounds base.UserKeyBounds) uint64 {\n\t\t\t\tif filter(f) {\n\t\t\t\t\tsize, err := d.fileCache.estimateSize(f, bounds.Start, bounds.End.Key)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn 0\n\t\t\t\t\t}\n\t\t\t\t\treturn size\n\t\t\t\t}\n\t\t\t\treturn 0\n\t\t\t},\n\t\t},\n\t}\n}\n\n// EstimateDiskUsage returns the estimated filesystem space used in bytes for\n// storing the range `[start, end]`. The estimation is computed as follows:\n//\n//   - For sstables fully contained in the range the whole file size is included.\n//   - For sstables partially contained in the range the overlapping data block sizes\n//     are included. Even if a data block partially overlaps, or we cannot determine\n//     overlap due to abbreviated index keys, the full data block size is included in\n//     the estimation. Note that unlike fully contained sstables, none of the\n//     meta-block space is counted for partially overlapped files.\n//   - For virtual sstables, we use the overlap between start, end and the virtual\n//     sstable bounds to determine disk usage.\n//   - There may also exist WAL entries for unflushed keys in this range. This\n//     estimation currently excludes space used for the range in the WAL.\nfunc (d *DB) EstimateDiskUsage(start, end []byte) (uint64, error) {\n\tbytes, _, _, err := d.EstimateDiskUsageByBackingType(start, end)\n\treturn bytes, err\n}\n\n// EstimateDiskUsageByBackingType is like EstimateDiskUsage but additionally\n// returns the subsets of that size in remote ane external files.\nfunc (d *DB) EstimateDiskUsageByBackingType(\n\tstart, end []byte,\n) (totalSize, remoteSize, externalSize uint64, _ error) {\n\tif err := d.closed.Load(); err != nil {\n\t\tpanic(err)\n\t}\n\n\tbounds := base.UserKeyBoundsInclusive(start, end)\n\tif !bounds.Valid(d.cmp) {\n\t\treturn 0, 0, 0, errors.New(\"invalid key-range specified (start > end)\")\n\t}\n\n\t// Grab and reference the current readState. This prevents the underlying\n\t// files in the associated version from being deleted if there is a concurrent\n\t// compaction.\n\treadState := d.loadReadState()\n\tdefer readState.unref()\n\n\ttotalSize = *d.mu.annotators.totalSize.VersionRangeAnnotation(readState.current, bounds)\n\tremoteSize = *d.mu.annotators.remoteSize.VersionRangeAnnotation(readState.current, bounds)\n\texternalSize = *d.mu.annotators.externalSize.VersionRangeAnnotation(readState.current, bounds)\n\treturn\n}\n\nfunc (d *DB) walPreallocateSize() int {\n\t// Set the WAL preallocate size to 110% of the memtable size. Note that there\n\t// is a bit of apples and oranges in units here as the memtabls size\n\t// corresponds to the memory usage of the memtable while the WAL size is the\n\t// size of the batches (plus overhead) stored in the WAL.\n\t//\n\t// TODO(peter): 110% of the memtable size is quite hefty for a block\n\t// size. This logic is taken from GetWalPreallocateBlockSize in\n\t// RocksDB. Could a smaller preallocation block size be used?\n\tsize := d.opts.MemTableSize\n\tsize = (size / 10) + size\n\treturn int(size)\n}\n\nfunc (d *DB) newMemTable(\n\tlogNum base.DiskFileNum, logSeqNum base.SeqNum, minSize uint64,\n) (*memTable, *flushableEntry) {\n\ttargetSize := minSize + uint64(memTableEmptySize)\n\t// The targetSize should be less than MemTableSize, because any batch >=\n\t// MemTableSize/2 should be treated as a large flushable batch.\n\tif targetSize > d.opts.MemTableSize {\n\t\tpanic(errors.AssertionFailedf(\"attempting to allocate memtable larger than MemTableSize\"))\n\t}\n\t// Double until the next memtable size is at least large enough to fit\n\t// minSize.\n\tfor d.mu.mem.nextSize < targetSize {\n\t\td.mu.mem.nextSize = min(2*d.mu.mem.nextSize, d.opts.MemTableSize)\n\t}\n\tsize := d.mu.mem.nextSize\n\t// The next memtable should be double the size, up to Options.MemTableSize.\n\tif d.mu.mem.nextSize < d.opts.MemTableSize {\n\t\td.mu.mem.nextSize = min(2*d.mu.mem.nextSize, d.opts.MemTableSize)\n\t}\n\n\tmemtblOpts := memTableOptions{\n\t\tOptions:   d.opts,\n\t\tlogSeqNum: logSeqNum,\n\t}\n\n\t// Before attempting to allocate a new memtable, check if there's one\n\t// available for recycling in memTableRecycle. Large contiguous allocations\n\t// can be costly as fragmentation makes it more difficult to find a large\n\t// contiguous free space. We've observed 64MB allocations taking 10ms+.\n\t//\n\t// To reduce these costly allocations, up to 1 obsolete memtable is stashed\n\t// in `d.memTableRecycle` to allow a future memtable rotation to reuse\n\t// existing memory.\n\tvar mem *memTable\n\tmem = d.memTableRecycle.Swap(nil)\n\tif mem != nil && uint64(mem.arenaBuf.Len()) != size {\n\t\td.freeMemTable(mem)\n\t\tmem = nil\n\t}\n\tif mem != nil {\n\t\t// Carry through the existing buffer and memory reservation.\n\t\tmemtblOpts.arenaBuf = mem.arenaBuf\n\t\tmemtblOpts.releaseAccountingReservation = mem.releaseAccountingReservation\n\t} else {\n\t\tmem = new(memTable)\n\t\tmemtblOpts.arenaBuf = manual.New(manual.MemTable, uintptr(size))\n\t\tmemtblOpts.releaseAccountingReservation = d.opts.Cache.Reserve(int(size))\n\t\td.memTableCount.Add(1)\n\t\td.memTableReserved.Add(int64(size))\n\n\t\t// Note: this is a no-op if invariants are disabled or race is enabled.\n\t\tinvariants.SetFinalizer(mem, checkMemTable)\n\t}\n\tmem.init(memtblOpts)\n\n\tentry := d.newFlushableEntry(mem, logNum, logSeqNum)\n\tentry.releaseMemAccounting = func() {\n\t\t// If the user leaks iterators, we may be releasing the memtable after\n\t\t// the DB is already closed. In this case, we want to just release the\n\t\t// memory because DB.Close won't come along to free it for us.\n\t\tif err := d.closed.Load(); err != nil {\n\t\t\td.freeMemTable(mem)\n\t\t\treturn\n\t\t}\n\n\t\t// The next memtable allocation might be able to reuse this memtable.\n\t\t// Stash it on d.memTableRecycle.\n\t\tif unusedMem := d.memTableRecycle.Swap(mem); unusedMem != nil {\n\t\t\t// There was already a memtable waiting to be recycled. We're now\n\t\t\t// responsible for freeing it.\n\t\t\td.freeMemTable(unusedMem)\n\t\t}\n\t}\n\treturn mem, entry\n}\n\nfunc (d *DB) freeMemTable(m *memTable) {\n\td.memTableCount.Add(-1)\n\td.memTableReserved.Add(-int64(m.arenaBuf.Len()))\n\tm.free()\n}\n\nfunc (d *DB) newFlushableEntry(\n\tf flushable, logNum base.DiskFileNum, logSeqNum base.SeqNum,\n) *flushableEntry {\n\tfe := &flushableEntry{\n\t\tflushable:      f,\n\t\tflushed:        make(chan struct{}),\n\t\tlogNum:         logNum,\n\t\tlogSeqNum:      logSeqNum,\n\t\tdeleteFn:       d.mu.versions.addObsolete,\n\t\tdeleteFnLocked: d.mu.versions.addObsoleteLocked,\n\t}\n\tfe.readerRefs.Store(1)\n\treturn fe\n}\n\n// maybeInduceWriteStall is called before performing a memtable rotation in\n// makeRoomForWrite. In some conditions, we prefer to stall the user's write\n// workload rather than continuing to accept writes that may result in resource\n// exhaustion or prohibitively slow reads.\n//\n// There are a couple reasons we might wait to rotate the memtable and\n// instead induce a write stall:\n//  1. If too many memtables have queued, we wait for a flush to finish before\n//     creating another memtable.\n//  2. If L0 read amplification has grown too high, we wait for compactions\n//     to reduce the read amplification before accepting more writes that will\n//     increase write pressure.\n//\n// maybeInduceWriteStall checks these stall conditions, and if present, waits\n// for them to abate.\nfunc (d *DB) maybeInduceWriteStall(b *Batch) {\n\tstalled := false\n\t// This function will call EventListener.WriteStallBegin at most once.  If\n\t// it does call it, it will call EventListener.WriteStallEnd once before\n\t// returning.\n\tfor {\n\t\tvar size uint64\n\t\tfor i := range d.mu.mem.queue {\n\t\t\tsize += d.mu.mem.queue[i].totalBytes()\n\t\t}\n\t\t// If ElevateWriteStallThresholdForFailover is true, we give an\n\t\t// unlimited memory budget for memtables. This is simpler than trying to\n\t\t// configure an explicit value, given that memory resources can vary.\n\t\t// When using WAL failover in CockroachDB, an OOM risk is worth\n\t\t// tolerating for workloads that have a strict latency SLO. Also, an\n\t\t// unlimited budget here does not mean that the disk stall in the\n\t\t// primary will go unnoticed until the OOM -- CockroachDB is monitoring\n\t\t// disk stalls, and we expect it to fail the node after ~60s if the\n\t\t// primary is stalled.\n\t\tif size >= uint64(d.opts.MemTableStopWritesThreshold)*d.opts.MemTableSize &&\n\t\t\t!d.mu.log.manager.ElevateWriteStallThresholdForFailover() {\n\t\t\t// We have filled up the current memtable, but already queued memtables\n\t\t\t// are still flushing, so we wait.\n\t\t\tif !stalled {\n\t\t\t\tstalled = true\n\t\t\t\td.opts.EventListener.WriteStallBegin(WriteStallBeginInfo{\n\t\t\t\t\tReason: \"memtable count limit reached\",\n\t\t\t\t})\n\t\t\t}\n\t\t\tbeforeWait := crtime.NowMono()\n\t\t\td.mu.compact.cond.Wait()\n\t\t\tif b != nil {\n\t\t\t\tb.commitStats.MemTableWriteStallDuration += beforeWait.Elapsed()\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tl0ReadAmp := d.mu.versions.currentVersion().L0Sublevels.ReadAmplification()\n\t\tif l0ReadAmp >= d.opts.L0StopWritesThreshold {\n\t\t\t// There are too many level-0 files, so we wait.\n\t\t\tif !stalled {\n\t\t\t\tstalled = true\n\t\t\t\td.opts.EventListener.WriteStallBegin(WriteStallBeginInfo{\n\t\t\t\t\tReason: \"L0 file count limit exceeded\",\n\t\t\t\t})\n\t\t\t}\n\t\t\tbeforeWait := crtime.NowMono()\n\t\t\td.mu.compact.cond.Wait()\n\t\t\tif b != nil {\n\t\t\t\tb.commitStats.L0ReadAmpWriteStallDuration += beforeWait.Elapsed()\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\t// Not stalled.\n\t\tif stalled {\n\t\t\td.opts.EventListener.WriteStallEnd()\n\t\t}\n\t\treturn\n\t}\n}\n\n// makeRoomForWrite rotates the current mutable memtable, ensuring that the\n// resulting mutable memtable has room to hold the contents of the provided\n// Batch. The current memtable is rotated (marked as immutable) and a new\n// mutable memtable is allocated. It reserves space in the new memtable and adds\n// a reference to the memtable. The caller must later ensure that the memtable\n// is unreferenced. This memtable rotation also causes a log rotation.\n//\n// If the current memtable is not full but the caller wishes to trigger a\n// rotation regardless, the caller may pass a nil Batch, and no space in the\n// resulting mutable memtable will be reserved.\n//\n// Both DB.mu and commitPipeline.mu must be held by the caller. Note that DB.mu\n// may be released and reacquired.\nfunc (d *DB) makeRoomForWrite(b *Batch) error {\n\tif b != nil && b.ingestedSSTBatch {\n\t\tpanic(\"pebble: invalid function call\")\n\t}\n\td.maybeInduceWriteStall(b)\n\n\tvar newLogNum base.DiskFileNum\n\tvar prevLogSize uint64\n\tif !d.opts.DisableWAL {\n\t\tbeforeRotate := crtime.NowMono()\n\t\tnewLogNum, prevLogSize = d.rotateWAL()\n\t\tif b != nil {\n\t\t\tb.commitStats.WALRotationDuration += beforeRotate.Elapsed()\n\t\t}\n\t}\n\timmMem := d.mu.mem.mutable\n\timm := d.mu.mem.queue[len(d.mu.mem.queue)-1]\n\timm.logSize = prevLogSize\n\n\tvar logSeqNum base.SeqNum\n\tvar minSize uint64\n\tif b != nil {\n\t\tlogSeqNum = b.SeqNum()\n\t\tif b.flushable != nil {\n\t\t\tlogSeqNum += base.SeqNum(b.Count())\n\t\t\t// The batch is too large to fit in the memtable so add it directly to\n\t\t\t// the immutable queue. The flushable batch is associated with the same\n\t\t\t// log as the immutable memtable, but logically occurs after it in\n\t\t\t// seqnum space. We ensure while flushing that the flushable batch\n\t\t\t// is flushed along with the previous memtable in the flushable\n\t\t\t// queue. See the top level comment in DB.flush1 to learn how this\n\t\t\t// is ensured.\n\t\t\t//\n\t\t\t// See DB.commitWrite for the special handling of log writes for large\n\t\t\t// batches. In particular, the large batch has already written to\n\t\t\t// imm.logNum.\n\t\t\tentry := d.newFlushableEntry(b.flushable, imm.logNum, b.SeqNum())\n\t\t\t// The large batch is by definition large. Reserve space from the cache\n\t\t\t// for it until it is flushed.\n\t\t\tentry.releaseMemAccounting = d.opts.Cache.Reserve(int(b.flushable.totalBytes()))\n\t\t\td.mu.mem.queue = append(d.mu.mem.queue, entry)\n\t\t} else {\n\t\t\tminSize = b.memTableSize\n\t\t}\n\t} else {\n\t\t// b == nil\n\t\t//\n\t\t// This is a manual forced flush.\n\t\tlogSeqNum = base.SeqNum(d.mu.versions.logSeqNum.Load())\n\t\timm.flushForced = true\n\t\t// If we are manually flushing and we used less than half of the bytes in\n\t\t// the memtable, don't increase the size for the next memtable. This\n\t\t// reduces memtable memory pressure when an application is frequently\n\t\t// manually flushing.\n\t\tif uint64(immMem.availBytes()) > immMem.totalBytes()/2 {\n\t\t\td.mu.mem.nextSize = immMem.totalBytes()\n\t\t}\n\t}\n\td.rotateMemtable(newLogNum, logSeqNum, immMem, minSize)\n\tif b != nil && b.flushable == nil {\n\t\terr := d.mu.mem.mutable.prepare(b)\n\t\t// Reserving enough space for the batch after rotation must never fail.\n\t\t// We pass in a minSize that's equal to b.memtableSize to ensure that\n\t\t// memtable rotation allocates a memtable sufficiently large. We also\n\t\t// held d.commit.mu for the entirety of this function, ensuring that no\n\t\t// other committers may have reserved memory in the new memtable yet.\n\t\tif err == arenaskl.ErrArenaFull {\n\t\t\tpanic(errors.AssertionFailedf(\"memtable still full after rotation\"))\n\t\t}\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// Both DB.mu and commitPipeline.mu must be held by the caller.\nfunc (d *DB) rotateMemtable(\n\tnewLogNum base.DiskFileNum, logSeqNum base.SeqNum, prev *memTable, minSize uint64,\n) {\n\t// Create a new memtable, scheduling the previous one for flushing. We do\n\t// this even if the previous memtable was empty because the DB.Flush\n\t// mechanism is dependent on being able to wait for the empty memtable to\n\t// flush. We can't just mark the empty memtable as flushed here because we\n\t// also have to wait for all previous immutable tables to\n\t// flush. Additionally, the memtable is tied to particular WAL file and we\n\t// want to go through the flush path in order to recycle that WAL file.\n\t//\n\t// NB: newLogNum corresponds to the WAL that contains mutations that are\n\t// present in the new memtable. When immutable memtables are flushed to\n\t// disk, a VersionEdit will be created telling the manifest the minimum\n\t// unflushed log number (which will be the next one in d.mu.mem.mutable\n\t// that was not flushed).\n\t//\n\t// NB: prev should be the current mutable memtable.\n\tvar entry *flushableEntry\n\td.mu.mem.mutable, entry = d.newMemTable(newLogNum, logSeqNum, minSize)\n\td.mu.mem.queue = append(d.mu.mem.queue, entry)\n\t// d.logSize tracks the log size of the WAL file corresponding to the most\n\t// recent flushable. The log size of the previous mutable memtable no longer\n\t// applies to the current mutable memtable.\n\t//\n\t// It's tempting to perform this update in rotateWAL, but that would not be\n\t// atomic with the enqueue of the new flushable. A call to DB.Metrics()\n\t// could acquire DB.mu after the WAL has been rotated but before the new\n\t// memtable has been appended; this would result in omitting the log size of\n\t// the most recent flushable.\n\td.logSize.Store(0)\n\td.updateReadStateLocked(nil)\n\tif prev.writerUnref() {\n\t\td.maybeScheduleFlush()\n\t}\n}\n\n// rotateWAL creates a new write-ahead log, possibly recycling a previous WAL's\n// files. It returns the file number assigned to the new WAL, and the size of\n// the previous WAL file.\n//\n// Both DB.mu and commitPipeline.mu must be held by the caller. Note that DB.mu\n// may be released and reacquired.\nfunc (d *DB) rotateWAL() (newLogNum base.DiskFileNum, prevLogSize uint64) {\n\tif d.opts.DisableWAL {\n\t\tpanic(\"pebble: invalid function call\")\n\t}\n\tjobID := d.newJobIDLocked()\n\tnewLogNum = d.mu.versions.getNextDiskFileNum()\n\n\td.mu.Unlock()\n\t// Close the previous log first. This writes an EOF trailer\n\t// signifying the end of the file and syncs it to disk. We must\n\t// close the previous log before linking the new log file,\n\t// otherwise a crash could leave both logs with unclean tails, and\n\t// Open will treat the previous log as corrupt.\n\toffset, err := d.mu.log.writer.Close()\n\tif err != nil {\n\t\t// What to do here? Stumbling on doesn't seem worthwhile. If we failed to\n\t\t// close the previous log it is possible we lost a write.\n\t\tpanic(err)\n\t}\n\tprevLogSize = uint64(offset)\n\tmetrics := d.mu.log.writer.Metrics()\n\n\td.mu.Lock()\n\tif err := d.mu.log.metrics.LogWriterMetrics.Merge(&metrics); err != nil {\n\t\td.opts.Logger.Errorf(\"metrics error: %s\", err)\n\t}\n\n\td.mu.Unlock()\n\twriter, err := d.mu.log.manager.Create(wal.NumWAL(newLogNum), int(jobID))\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\td.mu.Lock()\n\td.mu.log.writer = writer\n\treturn newLogNum, prevLogSize\n}\n\nfunc (d *DB) getEarliestUnflushedSeqNumLocked() base.SeqNum {\n\tseqNum := base.SeqNumMax\n\tfor i := range d.mu.mem.queue {\n\t\tlogSeqNum := d.mu.mem.queue[i].logSeqNum\n\t\tif seqNum > logSeqNum {\n\t\t\tseqNum = logSeqNum\n\t\t}\n\t}\n\treturn seqNum\n}\n\nfunc (d *DB) getInProgressCompactionInfoLocked(finishing *compaction) (rv []compactionInfo) {\n\tfor c := range d.mu.compact.inProgress {\n\t\tif len(c.flushing) == 0 && (finishing == nil || c != finishing) {\n\t\t\tinfo := compactionInfo{\n\t\t\t\tversionEditApplied: c.versionEditApplied,\n\t\t\t\tinputs:             c.inputs,\n\t\t\t\tsmallest:           c.smallest,\n\t\t\t\tlargest:            c.largest,\n\t\t\t\toutputLevel:        -1,\n\t\t\t}\n\t\t\tif c.outputLevel != nil {\n\t\t\t\tinfo.outputLevel = c.outputLevel.level\n\t\t\t}\n\t\t\trv = append(rv, info)\n\t\t}\n\t}\n\treturn\n}\n\nfunc inProgressL0Compactions(inProgress []compactionInfo) []manifest.L0Compaction {\n\tvar compactions []manifest.L0Compaction\n\tfor _, info := range inProgress {\n\t\t// Skip in-progress compactions that have already committed; the L0\n\t\t// sublevels initialization code requires the set of in-progress\n\t\t// compactions to be consistent with the current version. Compactions\n\t\t// with versionEditApplied=true are already applied to the current\n\t\t// version and but are performing cleanup without the database mutex.\n\t\tif info.versionEditApplied {\n\t\t\tcontinue\n\t\t}\n\t\tl0 := false\n\t\tfor _, cl := range info.inputs {\n\t\t\tl0 = l0 || cl.level == 0\n\t\t}\n\t\tif !l0 {\n\t\t\tcontinue\n\t\t}\n\t\tcompactions = append(compactions, manifest.L0Compaction{\n\t\t\tSmallest:  info.smallest,\n\t\t\tLargest:   info.largest,\n\t\t\tIsIntraL0: info.outputLevel == 0,\n\t\t})\n\t}\n\treturn compactions\n}\n\n// firstError returns the first non-nil error of err0 and err1, or nil if both\n// are nil.\nfunc firstError(err0, err1 error) error {\n\tif err0 != nil {\n\t\treturn err0\n\t}\n\treturn err1\n}\n\n// SetCreatorID sets the CreatorID which is needed in order to use shared objects.\n// Remote object usage is disabled until this method is called the first time.\n// Once set, the Creator ID is persisted and cannot change.\n//\n// Does nothing if SharedStorage was not set in the options when the DB was\n// opened or if the DB is in read-only mode.\nfunc (d *DB) SetCreatorID(creatorID uint64) error {\n\tif d.opts.Experimental.RemoteStorage == nil || d.opts.ReadOnly {\n\t\treturn nil\n\t}\n\treturn d.objProvider.SetCreatorID(objstorage.CreatorID(creatorID))\n}\n\n// KeyStatistics keeps track of the number of keys that have been pinned by a\n// snapshot as well as counts of the different key kinds in the lsm.\n//\n// One way of using the accumulated stats, when we only have sets and dels,\n// and say the counts are represented as del_count, set_count,\n// del_latest_count, set_latest_count, snapshot_pinned_count.\n//\n//   - del_latest_count + set_latest_count is the set of unique user keys\n//     (unique).\n//\n//   - set_latest_count is the set of live unique user keys (live_unique).\n//\n//   - Garbage is del_count + set_count - live_unique.\n//\n//   - If everything were in the LSM, del_count+set_count-snapshot_pinned_count\n//     would also be the set of unique user keys (note that\n//     snapshot_pinned_count is counting something different -- see comment below).\n//     But snapshot_pinned_count only counts keys in the LSM so the excess here\n//     must be keys in memtables.\ntype KeyStatistics struct {\n\t// TODO(sumeer): the SnapshotPinned* are incorrect in that these older\n\t// versions can be in a different level. Either fix the accounting or\n\t// rename these fields.\n\n\t// SnapshotPinnedKeys represents obsolete keys that cannot be elided during\n\t// a compaction, because they are required by an open snapshot.\n\tSnapshotPinnedKeys int\n\t// SnapshotPinnedKeysBytes is the total number of bytes of all snapshot\n\t// pinned keys.\n\tSnapshotPinnedKeysBytes uint64\n\t// KindsCount is the count for each kind of key. It includes point keys,\n\t// range deletes and range keys.\n\tKindsCount [InternalKeyKindMax + 1]int\n\t// LatestKindsCount is the count for each kind of key when it is the latest\n\t// kind for a user key. It is only populated for point keys.\n\tLatestKindsCount [InternalKeyKindMax + 1]int\n}\n\n// LSMKeyStatistics is used by DB.ScanStatistics.\ntype LSMKeyStatistics struct {\n\tAccumulated KeyStatistics\n\t// Levels contains statistics only for point keys. Range deletions and range keys will\n\t// appear in Accumulated but not Levels.\n\tLevels [numLevels]KeyStatistics\n\t// BytesRead represents the logical, pre-compression size of keys and values read\n\tBytesRead uint64\n}\n\n// ScanStatisticsOptions is used by DB.ScanStatistics.\ntype ScanStatisticsOptions struct {\n\t// LimitBytesPerSecond indicates the number of bytes that are able to be read\n\t// per second using ScanInternal.\n\t// A value of 0 indicates that there is no limit set.\n\tLimitBytesPerSecond int64\n}\n\n// ScanStatistics returns the count of different key kinds within the lsm for a\n// key span [lower, upper) as well as the number of snapshot keys.\nfunc (d *DB) ScanStatistics(\n\tctx context.Context, lower, upper []byte, opts ScanStatisticsOptions,\n) (LSMKeyStatistics, error) {\n\tstats := LSMKeyStatistics{}\n\tvar prevKey InternalKey\n\tvar rateLimitFunc func(key *InternalKey, val LazyValue) error\n\ttb := tokenbucket.TokenBucket{}\n\n\tif opts.LimitBytesPerSecond != 0 {\n\t\t// Each \"token\" roughly corresponds to a byte that was read.\n\t\ttb.Init(tokenbucket.TokensPerSecond(opts.LimitBytesPerSecond), tokenbucket.Tokens(1024))\n\t\trateLimitFunc = func(key *InternalKey, val LazyValue) error {\n\t\t\treturn tb.WaitCtx(ctx, tokenbucket.Tokens(key.Size()+val.Len()))\n\t\t}\n\t}\n\n\tscanInternalOpts := &scanInternalOptions{\n\t\tvisitPointKey: func(key *InternalKey, value LazyValue, iterInfo IteratorLevel) error {\n\t\t\t// If the previous key is equal to the current point key, the current key was\n\t\t\t// pinned by a snapshot.\n\t\t\tsize := uint64(key.Size())\n\t\t\tkind := key.Kind()\n\t\t\tsameKey := d.equal(prevKey.UserKey, key.UserKey)\n\t\t\tif iterInfo.Kind == IteratorLevelLSM && sameKey {\n\t\t\t\tstats.Levels[iterInfo.Level].SnapshotPinnedKeys++\n\t\t\t\tstats.Levels[iterInfo.Level].SnapshotPinnedKeysBytes += size\n\t\t\t\tstats.Accumulated.SnapshotPinnedKeys++\n\t\t\t\tstats.Accumulated.SnapshotPinnedKeysBytes += size\n\t\t\t}\n\t\t\tif iterInfo.Kind == IteratorLevelLSM {\n\t\t\t\tstats.Levels[iterInfo.Level].KindsCount[kind]++\n\t\t\t}\n\t\t\tif !sameKey {\n\t\t\t\tif iterInfo.Kind == IteratorLevelLSM {\n\t\t\t\t\tstats.Levels[iterInfo.Level].LatestKindsCount[kind]++\n\t\t\t\t}\n\t\t\t\tstats.Accumulated.LatestKindsCount[kind]++\n\t\t\t}\n\n\t\t\tstats.Accumulated.KindsCount[kind]++\n\t\t\tprevKey.CopyFrom(*key)\n\t\t\tstats.BytesRead += uint64(key.Size() + value.Len())\n\t\t\treturn nil\n\t\t},\n\t\tvisitRangeDel: func(start, end []byte, seqNum base.SeqNum) error {\n\t\t\tstats.Accumulated.KindsCount[InternalKeyKindRangeDelete]++\n\t\t\tstats.BytesRead += uint64(len(start) + len(end))\n\t\t\treturn nil\n\t\t},\n\t\tvisitRangeKey: func(start, end []byte, keys []rangekey.Key) error {\n\t\t\tstats.BytesRead += uint64(len(start) + len(end))\n\t\t\tfor _, key := range keys {\n\t\t\t\tstats.Accumulated.KindsCount[key.Kind()]++\n\t\t\t\tstats.BytesRead += uint64(len(key.Value) + len(key.Suffix))\n\t\t\t}\n\t\t\treturn nil\n\t\t},\n\t\tincludeObsoleteKeys: true,\n\t\tIterOptions: IterOptions{\n\t\t\tKeyTypes:   IterKeyTypePointsAndRanges,\n\t\t\tLowerBound: lower,\n\t\t\tUpperBound: upper,\n\t\t},\n\t\trateLimitFunc: rateLimitFunc,\n\t}\n\titer, err := d.newInternalIter(ctx, snapshotIterOpts{}, scanInternalOpts)\n\tif err != nil {\n\t\treturn LSMKeyStatistics{}, err\n\t}\n\tdefer iter.close()\n\n\terr = scanInternalImpl(ctx, lower, upper, iter, scanInternalOpts)\n\n\tif err != nil {\n\t\treturn LSMKeyStatistics{}, err\n\t}\n\n\treturn stats, nil\n}\n\n// ObjProvider returns the objstorage.Provider for this database. Meant to be\n// used for internal purposes only.\nfunc (d *DB) ObjProvider() objstorage.Provider {\n\treturn d.objProvider\n}\n\nfunc (d *DB) checkVirtualBounds(m *fileMetadata) {\n\tif !invariants.Enabled {\n\t\treturn\n\t}\n\n\tobjMeta, err := d.objProvider.Lookup(fileTypeTable, m.FileBacking.DiskFileNum)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tif objMeta.IsExternal() {\n\t\t// Nothing to do; bounds are expected to be loose.\n\t\treturn\n\t}\n\n\titers, err := d.newIters(context.TODO(), m, nil, internalIterOpts{}, iterPointKeys|iterRangeDeletions|iterRangeKeys)\n\tif err != nil {\n\t\tpanic(errors.Wrap(err, \"pebble: error creating iterators\"))\n\t}\n\tdefer iters.CloseAll()\n\n\tif m.HasPointKeys {\n\t\tpointIter := iters.Point()\n\t\trangeDelIter := iters.RangeDeletion()\n\n\t\t// Check that the lower bound is tight.\n\t\tpointKV := pointIter.First()\n\t\trangeDel, err := rangeDelIter.First()\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\tif (rangeDel == nil || d.cmp(rangeDel.SmallestKey().UserKey, m.SmallestPointKey.UserKey) != 0) &&\n\t\t\t(pointKV == nil || d.cmp(pointKV.K.UserKey, m.SmallestPointKey.UserKey) != 0) {\n\t\t\tpanic(errors.Newf(\"pebble: virtual sstable %s lower point key bound is not tight\", m.FileNum))\n\t\t}\n\n\t\t// Check that the upper bound is tight.\n\t\tpointKV = pointIter.Last()\n\t\trangeDel, err = rangeDelIter.Last()\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\tif (rangeDel == nil || d.cmp(rangeDel.LargestKey().UserKey, m.LargestPointKey.UserKey) != 0) &&\n\t\t\t(pointKV == nil || d.cmp(pointKV.K.UserKey, m.LargestPointKey.UserKey) != 0) {\n\t\t\tpanic(errors.Newf(\"pebble: virtual sstable %s upper point key bound is not tight\", m.FileNum))\n\t\t}\n\n\t\t// Check that iterator keys are within bounds.\n\t\tfor kv := pointIter.First(); kv != nil; kv = pointIter.Next() {\n\t\t\tif d.cmp(kv.K.UserKey, m.SmallestPointKey.UserKey) < 0 || d.cmp(kv.K.UserKey, m.LargestPointKey.UserKey) > 0 {\n\t\t\t\tpanic(errors.Newf(\"pebble: virtual sstable %s point key %s is not within bounds\", m.FileNum, kv.K.UserKey))\n\t\t\t}\n\t\t}\n\t\ts, err := rangeDelIter.First()\n\t\tfor ; s != nil; s, err = rangeDelIter.Next() {\n\t\t\tif d.cmp(s.SmallestKey().UserKey, m.SmallestPointKey.UserKey) < 0 {\n\t\t\t\tpanic(errors.Newf(\"pebble: virtual sstable %s point key %s is not within bounds\", m.FileNum, s.SmallestKey().UserKey))\n\t\t\t}\n\t\t\tif d.cmp(s.LargestKey().UserKey, m.LargestPointKey.UserKey) > 0 {\n\t\t\t\tpanic(errors.Newf(\"pebble: virtual sstable %s point key %s is not within bounds\", m.FileNum, s.LargestKey().UserKey))\n\t\t\t}\n\t\t}\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t}\n\n\tif !m.HasRangeKeys {\n\t\treturn\n\t}\n\trangeKeyIter := iters.RangeKey()\n\n\t// Check that the lower bound is tight.\n\tif s, err := rangeKeyIter.First(); err != nil {\n\t\tpanic(err)\n\t} else if d.cmp(s.SmallestKey().UserKey, m.SmallestRangeKey.UserKey) != 0 {\n\t\tpanic(errors.Newf(\"pebble: virtual sstable %s lower range key bound is not tight\", m.FileNum))\n\t}\n\n\t// Check that upper bound is tight.\n\tif s, err := rangeKeyIter.Last(); err != nil {\n\t\tpanic(err)\n\t} else if d.cmp(s.LargestKey().UserKey, m.LargestRangeKey.UserKey) != 0 {\n\t\tpanic(errors.Newf(\"pebble: virtual sstable %s upper range key bound is not tight\", m.FileNum))\n\t}\n\n\ts, err := rangeKeyIter.First()\n\tfor ; s != nil; s, err = rangeKeyIter.Next() {\n\t\tif d.cmp(s.SmallestKey().UserKey, m.SmallestRangeKey.UserKey) < 0 {\n\t\t\tpanic(errors.Newf(\"pebble: virtual sstable %s point key %s is not within bounds\", m.FileNum, s.SmallestKey().UserKey))\n\t\t}\n\t\tif d.cmp(s.LargestKey().UserKey, m.LargestRangeKey.UserKey) > 0 {\n\t\t\tpanic(errors.Newf(\"pebble: virtual sstable %s point key %s is not within bounds\", m.FileNum, s.LargestKey().UserKey))\n\t\t}\n\t}\n\tif err != nil {\n\t\tpanic(err)\n\t}\n}\n\n// DebugString returns a debugging string describing the LSM.\nfunc (d *DB) DebugString() string {\n\treturn d.DebugCurrentVersion().DebugString()\n}\n\n// DebugCurrentVersion returns the current LSM tree metadata. Should only be\n// used for testing/debugging.\nfunc (d *DB) DebugCurrentVersion() *manifest.Version {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\treturn d.mu.versions.currentVersion()\n}\n"
        },
        {
          "name": "db_internals.go",
          "type": "blob",
          "size": 0.7431640625,
          "content": "// Copyright 2012 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\n// JobID identifies a job (like a compaction). Job IDs are passed to event\n// listener notifications and act as a mechanism for tying together the events\n// and log messages for a single job such as a flush, compaction, or file\n// ingestion. Job IDs are not serialized to disk or used for correctness.\ntype JobID int\n\n// newJobIDLocked returns a new JobID; DB.mu must be held.\nfunc (d *DB) newJobIDLocked() JobID {\n\tres := d.mu.nextJobID\n\td.mu.nextJobID++\n\treturn res\n}\n\nfunc (d *DB) newJobID() JobID {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\treturn d.newJobIDLocked()\n}\n"
        },
        {
          "name": "db_test.go",
          "type": "blob",
          "size": 66.6513671875,
          "content": "// Copyright 2012 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"math/rand/v2\"\n\t\"path/filepath\"\n\t\"regexp\"\n\t\"runtime\"\n\t\"slices\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/crlib/fifo\"\n\t\"github.com/cockroachdb/datadriven\"\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/cache\"\n\t\"github.com/cockroachdb/pebble/internal/testkeys\"\n\t\"github.com/cockroachdb/pebble/internal/testutils\"\n\t\"github.com/cockroachdb/pebble/objstorage/objstorageprovider\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/cockroachdb/pebble/vfs/errorfs\"\n\t\"github.com/cockroachdb/pebble/wal\"\n\t\"github.com/stretchr/testify/require\"\n)\n\n// try repeatedly calls f, sleeping between calls with exponential back-off,\n// until f returns a nil error or the total sleep time is greater than or equal\n// to maxTotalSleep. It always calls f at least once.\nfunc try(initialSleep, maxTotalSleep time.Duration, f func() error) error {\n\ttotalSleep := time.Duration(0)\n\tfor d := initialSleep; ; d *= 2 {\n\t\ttime.Sleep(d)\n\t\ttotalSleep += d\n\t\tif err := f(); err == nil || totalSleep >= maxTotalSleep {\n\t\t\treturn err\n\t\t}\n\t}\n}\n\nfunc TestTry(t *testing.T) {\n\tc := make(chan struct{})\n\tgo func() {\n\t\ttime.Sleep(1 * time.Millisecond)\n\t\tclose(c)\n\t}()\n\n\tattemptsMu := sync.Mutex{}\n\tattempts := 0\n\n\terr := try(100*time.Microsecond, 20*time.Second, func() error {\n\t\tattemptsMu.Lock()\n\t\tattempts++\n\t\tattemptsMu.Unlock()\n\n\t\tselect {\n\t\tdefault:\n\t\t\treturn errors.New(\"timed out\")\n\t\tcase <-c:\n\t\t\treturn nil\n\t\t}\n\t})\n\trequire.NoError(t, err)\n\n\tattemptsMu.Lock()\n\ta := attempts\n\tattemptsMu.Unlock()\n\n\tif a == 0 {\n\t\tt.Fatalf(\"attempts: got 0, want > 0\")\n\t}\n}\n\nfunc TestBasicReads(t *testing.T) {\n\ttestCases := []struct {\n\t\tdirname string\n\t\twantMap map[string]string\n\t}{\n\t\t{\n\t\t\t\"db-stage-1\",\n\t\t\tmap[string]string{\n\t\t\t\t\"aaa\":  \"\",\n\t\t\t\t\"bar\":  \"\",\n\t\t\t\t\"baz\":  \"\",\n\t\t\t\t\"foo\":  \"\",\n\t\t\t\t\"quux\": \"\",\n\t\t\t\t\"zzz\":  \"\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t\"db-stage-2\",\n\t\t\tmap[string]string{\n\t\t\t\t\"aaa\":  \"\",\n\t\t\t\t\"bar\":  \"\",\n\t\t\t\t\"baz\":  \"three\",\n\t\t\t\t\"foo\":  \"four\",\n\t\t\t\t\"quux\": \"\",\n\t\t\t\t\"zzz\":  \"\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t\"db-stage-3\",\n\t\t\tmap[string]string{\n\t\t\t\t\"aaa\":  \"\",\n\t\t\t\t\"bar\":  \"\",\n\t\t\t\t\"baz\":  \"three\",\n\t\t\t\t\"foo\":  \"four\",\n\t\t\t\t\"quux\": \"\",\n\t\t\t\t\"zzz\":  \"\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t\"db-stage-4\",\n\t\t\tmap[string]string{\n\t\t\t\t\"aaa\":  \"\",\n\t\t\t\t\"bar\":  \"\",\n\t\t\t\t\"baz\":  \"\",\n\t\t\t\t\"foo\":  \"five\",\n\t\t\t\t\"quux\": \"six\",\n\t\t\t\t\"zzz\":  \"\",\n\t\t\t},\n\t\t},\n\t}\n\tfor _, tc := range testCases {\n\t\tt.Run(tc.dirname, func(t *testing.T) {\n\t\t\tfs := vfs.NewMem()\n\t\t\t_, err := vfs.Clone(vfs.Default, fs, filepath.Join(\"testdata\", tc.dirname), tc.dirname)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"%s: cloneFileSystem failed: %v\", tc.dirname, err)\n\t\t\t}\n\t\t\td, err := Open(tc.dirname, testingRandomized(t, &Options{\n\t\t\t\tFS: fs,\n\t\t\t}))\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"%s: Open failed: %v\", tc.dirname, err)\n\t\t\t}\n\t\t\tfor key, want := range tc.wantMap {\n\t\t\t\tgot, closer, err := d.Get([]byte(key))\n\t\t\t\tif err != nil && err != ErrNotFound {\n\t\t\t\t\tt.Fatalf(\"%s: Get(%q) failed: %v\", tc.dirname, key, err)\n\t\t\t\t}\n\t\t\t\tif string(got) != string(want) {\n\t\t\t\t\tt.Fatalf(\"%s: Get(%q): got %q, want %q\", tc.dirname, key, got, want)\n\t\t\t\t}\n\t\t\t\tif closer != nil {\n\t\t\t\t\tcloser.Close()\n\t\t\t\t}\n\t\t\t}\n\t\t\terr = d.Close()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"%s: Close failed: %v\", tc.dirname, err)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestBasicWrites(t *testing.T) {\n\td, err := Open(\"\", testingRandomized(t, &Options{\n\t\tFS: vfs.NewMem(),\n\t}))\n\trequire.NoError(t, err)\n\n\tnames := []string{\n\t\t\"Alatar\",\n\t\t\"Gandalf\",\n\t\t\"Pallando\",\n\t\t\"Radagast\",\n\t\t\"Saruman\",\n\t\t\"Joe\",\n\t}\n\twantMap := map[string]string{}\n\n\tinBatch, batch, pending := false, &Batch{}, [][]string(nil)\n\tset0 := func(k, v string) error {\n\t\treturn d.Set([]byte(k), []byte(v), nil)\n\t}\n\tdel0 := func(k string) error {\n\t\treturn d.Delete([]byte(k), nil)\n\t}\n\tset1 := func(k, v string) error {\n\t\tbatch.Set([]byte(k), []byte(v), nil)\n\t\treturn nil\n\t}\n\tdel1 := func(k string) error {\n\t\tbatch.Delete([]byte(k), nil)\n\t\treturn nil\n\t}\n\tset, del := set0, del0\n\n\ttestCases := []string{\n\t\t\"set Gandalf Grey\",\n\t\t\"set Saruman White\",\n\t\t\"set Radagast Brown\",\n\t\t\"delete Saruman\",\n\t\t\"set Gandalf White\",\n\t\t\"batch\",\n\t\t\"  set Alatar AliceBlue\",\n\t\t\"apply\",\n\t\t\"delete Pallando\",\n\t\t\"set Alatar AntiqueWhite\",\n\t\t\"set Pallando PapayaWhip\",\n\t\t\"batch\",\n\t\t\"apply\",\n\t\t\"set Pallando PaleVioletRed\",\n\t\t\"batch\",\n\t\t\"  delete Alatar\",\n\t\t\"  set Gandalf GhostWhite\",\n\t\t\"  set Saruman Seashell\",\n\t\t\"  delete Saruman\",\n\t\t\"  set Saruman SeaGreen\",\n\t\t\"  set Radagast RosyBrown\",\n\t\t\"  delete Pallando\",\n\t\t\"apply\",\n\t\t\"delete Radagast\",\n\t\t\"delete Radagast\",\n\t\t\"delete Radagast\",\n\t\t\"set Gandalf Goldenrod\",\n\t\t\"set Pallando PeachPuff\",\n\t\t\"batch\",\n\t\t\"  delete Joe\",\n\t\t\"  delete Saruman\",\n\t\t\"  delete Radagast\",\n\t\t\"  delete Pallando\",\n\t\t\"  delete Gandalf\",\n\t\t\"  delete Alatar\",\n\t\t\"apply\",\n\t\t\"set Joe Plumber\",\n\t}\n\tfor i, tc := range testCases {\n\t\ts := strings.Split(strings.TrimSpace(tc), \" \")\n\t\tswitch s[0] {\n\t\tcase \"set\":\n\t\t\tif err := set(s[1], s[2]); err != nil {\n\t\t\t\tt.Fatalf(\"#%d %s: %v\", i, tc, err)\n\t\t\t}\n\t\t\tif inBatch {\n\t\t\t\tpending = append(pending, s)\n\t\t\t} else {\n\t\t\t\twantMap[s[1]] = s[2]\n\t\t\t}\n\t\tcase \"delete\":\n\t\t\tif err := del(s[1]); err != nil {\n\t\t\t\tt.Fatalf(\"#%d %s: %v\", i, tc, err)\n\t\t\t}\n\t\t\tif inBatch {\n\t\t\t\tpending = append(pending, s)\n\t\t\t} else {\n\t\t\t\tdelete(wantMap, s[1])\n\t\t\t}\n\t\tcase \"batch\":\n\t\t\tinBatch, batch, set, del = true, &Batch{}, set1, del1\n\t\tcase \"apply\":\n\t\t\tif err := d.Apply(batch, nil); err != nil {\n\t\t\t\tt.Fatalf(\"#%d %s: %v\", i, tc, err)\n\t\t\t}\n\t\t\tfor _, p := range pending {\n\t\t\t\tswitch p[0] {\n\t\t\t\tcase \"set\":\n\t\t\t\t\twantMap[p[1]] = p[2]\n\t\t\t\tcase \"delete\":\n\t\t\t\t\tdelete(wantMap, p[1])\n\t\t\t\t}\n\t\t\t}\n\t\t\tinBatch, pending, set, del = false, nil, set0, del0\n\t\tdefault:\n\t\t\tt.Fatalf(\"#%d %s: bad test case: %q\", i, tc, s)\n\t\t}\n\n\t\tfail := false\n\t\tfor _, name := range names {\n\t\t\tg, closer, err := d.Get([]byte(name))\n\t\t\tif err != nil && err != ErrNotFound {\n\t\t\t\tt.Errorf(\"#%d %s: Get(%q): %v\", i, tc, name, err)\n\t\t\t\tfail = true\n\t\t\t}\n\t\t\tgot, gOK := string(g), err == nil\n\t\t\twant, wOK := wantMap[name]\n\t\t\tif got != want || gOK != wOK {\n\t\t\t\tt.Errorf(\"#%d %s: Get(%q): got %q, %t, want %q, %t\",\n\t\t\t\t\ti, tc, name, got, gOK, want, wOK)\n\t\t\t\tfail = true\n\t\t\t}\n\t\t\tif closer != nil {\n\t\t\t\tcloser.Close()\n\t\t\t}\n\t\t}\n\t\tif fail {\n\t\t\treturn\n\t\t}\n\t}\n\n\trequire.NoError(t, d.Close())\n}\n\nfunc TestRandomWrites(t *testing.T) {\n\td, err := Open(\"\", testingRandomized(t, &Options{\n\t\tFS:           vfs.NewMem(),\n\t\tMemTableSize: 8 * 1024,\n\t}))\n\trequire.NoError(t, err)\n\n\tkeys := [64][]byte{}\n\twants := [64]int{}\n\tfor k := range keys {\n\t\tkeys[k] = []byte(strconv.Itoa(k))\n\t\twants[k] = -1\n\t}\n\txxx := bytes.Repeat([]byte(\"x\"), 512)\n\n\trng := rand.New(rand.NewPCG(0, 123))\n\tconst N = 1000\n\tfor i := 0; i < N; i++ {\n\t\tk := rng.IntN(len(keys))\n\t\tif rng.IntN(20) != 0 {\n\t\t\twants[k] = rng.IntN(len(xxx) + 1)\n\t\t\tif err := d.Set(keys[k], xxx[:wants[k]], nil); err != nil {\n\t\t\t\tt.Fatalf(\"i=%d: Set: %v\", i, err)\n\t\t\t}\n\t\t} else {\n\t\t\twants[k] = -1\n\t\t\tif err := d.Delete(keys[k], nil); err != nil {\n\t\t\t\tt.Fatalf(\"i=%d: Delete: %v\", i, err)\n\t\t\t}\n\t\t}\n\n\t\tif i != N-1 || rng.IntN(50) != 0 {\n\t\t\tcontinue\n\t\t}\n\t\tfor k := range keys {\n\t\t\tgot := -1\n\t\t\tif v, closer, err := d.Get(keys[k]); err != nil {\n\t\t\t\tif err != ErrNotFound {\n\t\t\t\t\tt.Fatalf(\"Get: %v\", err)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tgot = len(v)\n\t\t\t\tcloser.Close()\n\t\t\t}\n\t\t\tif got != wants[k] {\n\t\t\t\tt.Errorf(\"i=%d, k=%d: got %d, want %d\", i, k, got, wants[k])\n\t\t\t}\n\t\t}\n\t}\n\n\trequire.NoError(t, d.Close())\n}\n\nfunc TestLargeBatch(t *testing.T) {\n\td, err := Open(\"\", testingRandomized(t, &Options{\n\t\tFS:                          vfs.NewMem(),\n\t\tMemTableSize:                1400,\n\t\tMemTableStopWritesThreshold: 100,\n\t}))\n\trequire.NoError(t, err)\n\n\tverifyLSM := func(expected string) func() error {\n\t\treturn func() error {\n\t\t\td.mu.Lock()\n\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\td.mu.Unlock()\n\t\t\tif expected != s {\n\t\t\t\tif testing.Verbose() {\n\t\t\t\t\tfmt.Println(strings.TrimSpace(s))\n\t\t\t\t}\n\t\t\t\treturn errors.Errorf(\"expected %s, but found %s\", expected, s)\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\t}\n\n\tgetLatestLog := func() wal.LogicalLog {\n\t\td.mu.Lock()\n\t\tdefer d.mu.Unlock()\n\t\tlogs := d.mu.log.manager.List()\n\t\treturn logs[len(logs)-1]\n\t}\n\tmemTableCreationSeqNum := func() base.SeqNum {\n\t\td.mu.Lock()\n\t\tdefer d.mu.Unlock()\n\t\treturn d.mu.mem.mutable.logSeqNum\n\t}\n\n\tstartLog := getLatestLog()\n\tstartLogStartSize, err := startLog.PhysicalSize()\n\trequire.NoError(t, err)\n\tstartSeqNum := d.mu.versions.logSeqNum.Load()\n\n\t// Write a key with a value larger than the memtable size.\n\trequire.NoError(t, d.Set([]byte(\"a\"), bytes.Repeat([]byte(\"a\"), 512), nil))\n\n\t// Verify that the large batch was written to the WAL that existed before it\n\t// was committed. We verify that WAL rotation occurred, where the large batch\n\t// was written to, and that the new WAL is empty.\n\tendLog := getLatestLog()\n\tif startLog.Num == endLog.Num {\n\t\tt.Fatal(\"expected WAL rotation\")\n\t}\n\tstartLogEndSize, err := startLog.PhysicalSize()\n\trequire.NoError(t, err)\n\tif startLogEndSize == startLogStartSize {\n\t\tt.Fatalf(\"expected large batch to be written to %s.log, but file size unchanged at %d\",\n\t\t\tstartLog.Num, startLogEndSize)\n\t}\n\tendLogSize, err := endLog.PhysicalSize()\n\trequire.NoError(t, err)\n\tif endLogSize != 0 {\n\t\tt.Fatalf(\"expected %s to be empty, but found %d\", endLog, endLogSize)\n\t}\n\tif creationSeqNum := memTableCreationSeqNum(); creationSeqNum <= startSeqNum {\n\t\tt.Fatalf(\"expected memTable.logSeqNum=%d > largeBatch.seqNum=%d\", creationSeqNum, startSeqNum)\n\t}\n\n\t// Verify this results in one L0 table being created.\n\trequire.NoError(t, try(100*time.Microsecond, 20*time.Second,\n\t\tverifyLSM(\"L0.0:\\n  000005:[a#10,SET-a#10,SET]\\n\")))\n\n\trequire.NoError(t, d.Set([]byte(\"b\"), bytes.Repeat([]byte(\"b\"), 512), nil))\n\n\t// Verify this results in a second L0 table being created.\n\trequire.NoError(t, try(100*time.Microsecond, 20*time.Second,\n\t\tverifyLSM(\"L0.0:\\n  000005:[a#10,SET-a#10,SET]\\n  000007:[b#11,SET-b#11,SET]\\n\")))\n\n\t// Allocate a bunch of batches to exhaust the batchPool. None of these\n\t// batches should have a non-zero count.\n\tfor i := 0; i < 10; i++ {\n\t\tb := d.NewBatch()\n\t\trequire.EqualValues(t, 0, b.Count())\n\t}\n\n\trequire.NoError(t, d.Close())\n}\n\nfunc TestGetNoCache(t *testing.T) {\n\tcache := NewCache(0)\n\tdefer cache.Unref()\n\n\td, err := Open(\"\", testingRandomized(t, &Options{\n\t\tCache: cache,\n\t\tFS:    vfs.NewMem(),\n\t}))\n\trequire.NoError(t, err)\n\n\trequire.NoError(t, d.Set([]byte(\"a\"), []byte(\"aa\"), nil))\n\trequire.NoError(t, d.Flush())\n\tverifyGet(t, d, []byte(\"a\"), []byte(\"aa\"))\n\n\trequire.NoError(t, d.Close())\n}\n\nfunc TestGetMerge(t *testing.T) {\n\td, err := Open(\"\", testingRandomized(t, &Options{\n\t\tFS: vfs.NewMem(),\n\t}))\n\trequire.NoError(t, err)\n\n\tkey := []byte(\"a\")\n\tverify := func(expected string) {\n\t\tval, closer, err := d.Get(key)\n\t\trequire.NoError(t, err)\n\n\t\tif expected != string(val) {\n\t\t\tt.Fatalf(\"expected %s, but got %s\", expected, val)\n\t\t}\n\t\tcloser.Close()\n\t}\n\n\tconst val = \"1\"\n\tfor i := 1; i <= 3; i++ {\n\t\trequire.NoError(t, d.Merge(key, []byte(val), nil))\n\n\t\texpected := strings.Repeat(val, i)\n\t\tverify(expected)\n\n\t\trequire.NoError(t, d.Flush())\n\t\tverify(expected)\n\t}\n\n\trequire.NoError(t, d.Close())\n}\n\nfunc TestMergeOrderSameAfterFlush(t *testing.T) {\n\t// Ensure compaction iterator (used by flush) and user iterator process merge\n\t// operands in the same order\n\td, err := Open(\"\", testingRandomized(t, &Options{\n\t\tFS: vfs.NewMem(),\n\t}))\n\trequire.NoError(t, err)\n\n\tkey := []byte(\"a\")\n\tverify := func(expected string) {\n\t\titer, _ := d.NewIter(nil)\n\t\tif !iter.SeekGE([]byte(\"a\")) {\n\t\t\tt.Fatal(\"expected one value, but got empty iterator\")\n\t\t}\n\t\tif expected != string(iter.Value()) {\n\t\t\tt.Fatalf(\"expected %s, but got %s\", expected, string(iter.Value()))\n\t\t}\n\t\tif !iter.SeekLT([]byte(\"b\")) {\n\t\t\tt.Fatal(\"expected one value, but got empty iterator\")\n\t\t}\n\t\tif expected != string(iter.Value()) {\n\t\t\tt.Fatalf(\"expected %s, but got %s\", expected, string(iter.Value()))\n\t\t}\n\t\trequire.NoError(t, iter.Close())\n\t}\n\n\trequire.NoError(t, d.Merge(key, []byte(\"0\"), nil))\n\trequire.NoError(t, d.Merge(key, []byte(\"1\"), nil))\n\n\tverify(\"01\")\n\trequire.NoError(t, d.Flush())\n\tverify(\"01\")\n\n\trequire.NoError(t, d.Close())\n}\n\ntype closableMerger struct {\n\tlastBuf []byte\n\tclosed  bool\n}\n\nfunc (m *closableMerger) MergeNewer(value []byte) error {\n\tm.lastBuf = append(m.lastBuf[:0], value...)\n\treturn nil\n}\n\nfunc (m *closableMerger) MergeOlder(value []byte) error {\n\tm.lastBuf = append(m.lastBuf[:0], value...)\n\treturn nil\n}\n\nfunc (m *closableMerger) Finish(includesBase bool) ([]byte, io.Closer, error) {\n\treturn m.lastBuf, m, nil\n}\n\nfunc (m *closableMerger) Close() error {\n\tm.closed = true\n\treturn nil\n}\n\nfunc TestMergerClosing(t *testing.T) {\n\tm := &closableMerger{}\n\n\td, err := Open(\"\", testingRandomized(t, &Options{\n\t\tFS: vfs.NewMem(),\n\t\tMerger: &Merger{\n\t\t\tMerge: func(key, value []byte) (base.ValueMerger, error) {\n\t\t\t\treturn m, m.MergeNewer(value)\n\t\t\t},\n\t\t},\n\t}))\n\trequire.NoError(t, err)\n\n\tdefer func() {\n\t\trequire.NoError(t, d.Close())\n\t}()\n\n\terr = d.Merge([]byte(\"a\"), []byte(\"b\"), nil)\n\trequire.NoError(t, err)\n\trequire.False(t, m.closed)\n\n\tval, closer, err := d.Get([]byte(\"a\"))\n\trequire.NoError(t, err)\n\trequire.Equal(t, []byte(\"b\"), val)\n\trequire.NotNil(t, closer)\n\trequire.False(t, m.closed)\n\t_ = closer.Close()\n\trequire.True(t, m.closed)\n}\n\nfunc TestLogData(t *testing.T) {\n\td, err := Open(\"\", testingRandomized(t, &Options{\n\t\tFS: vfs.NewMem(),\n\t}))\n\trequire.NoError(t, err)\n\n\tdefer func() {\n\t\trequire.NoError(t, d.Close())\n\t}()\n\n\trequire.NoError(t, d.LogData([]byte(\"foo\"), Sync))\n\trequire.NoError(t, d.LogData([]byte(\"bar\"), Sync))\n\t// TODO(itsbilal): Confirm that we wrote some bytes to the WAL.\n\t// For now, LogData proceeding ahead without a panic is good enough.\n}\n\nfunc TestSingleDeleteGet(t *testing.T) {\n\td, err := Open(\"\", testingRandomized(t, &Options{\n\t\tFS: vfs.NewMem(),\n\t}))\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, d.Close())\n\t}()\n\n\tkey := []byte(\"key\")\n\tval := []byte(\"val\")\n\n\trequire.NoError(t, d.Set(key, val, nil))\n\tverifyGet(t, d, key, val)\n\n\tkey2 := []byte(\"key2\")\n\tval2 := []byte(\"val2\")\n\n\trequire.NoError(t, d.Set(key2, val2, nil))\n\tverifyGet(t, d, key2, val2)\n\n\trequire.NoError(t, d.SingleDelete(key2, nil))\n\tverifyGetNotFound(t, d, key2)\n}\n\nfunc TestSingleDeleteFlush(t *testing.T) {\n\td, err := Open(\"\", testingRandomized(t, &Options{\n\t\tFS: vfs.NewMem(),\n\t}))\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, d.Close())\n\t}()\n\n\tkey := []byte(\"key\")\n\tvalFirst := []byte(\"first\")\n\tvalSecond := []byte(\"second\")\n\tkey2 := []byte(\"key2\")\n\tval2 := []byte(\"val2\")\n\n\trequire.NoError(t, d.Set(key, valFirst, nil))\n\trequire.NoError(t, d.Set(key2, val2, nil))\n\trequire.NoError(t, d.Flush())\n\n\trequire.NoError(t, d.SingleDelete(key, nil))\n\trequire.NoError(t, d.Set(key, valSecond, nil))\n\trequire.NoError(t, d.Delete(key2, nil))\n\trequire.NoError(t, d.Set(key2, val2, nil))\n\trequire.NoError(t, d.Flush())\n\n\trequire.NoError(t, d.SingleDelete(key, nil))\n\trequire.NoError(t, d.Delete(key2, nil))\n\trequire.NoError(t, d.Flush())\n\n\tverifyGetNotFound(t, d, key)\n\tverifyGetNotFound(t, d, key2)\n}\n\nfunc TestUnremovableSingleDelete(t *testing.T) {\n\td, err := Open(\"\", testingRandomized(t, &Options{\n\t\tFS:                    vfs.NewMem(),\n\t\tL0CompactionThreshold: 8,\n\t}))\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, d.Close())\n\t}()\n\n\tkey := []byte(\"key\")\n\tvalFirst := []byte(\"valFirst\")\n\tvalSecond := []byte(\"valSecond\")\n\n\trequire.NoError(t, d.Set(key, valFirst, nil))\n\tss := d.NewSnapshot()\n\tdefer ss.Close()\n\trequire.NoError(t, d.SingleDelete(key, nil))\n\trequire.NoError(t, d.Set(key, valSecond, nil))\n\trequire.NoError(t, d.Flush())\n\n\tverifyGet(t, ss, key, valFirst)\n\tverifyGet(t, d, key, valSecond)\n\n\trequire.NoError(t, d.SingleDelete(key, nil))\n\n\tverifyGet(t, ss, key, valFirst)\n\tverifyGetNotFound(t, d, key)\n\n\trequire.NoError(t, d.Flush())\n\n\tverifyGet(t, ss, key, valFirst)\n\tverifyGetNotFound(t, d, key)\n}\n\nfunc TestIterLeak(t *testing.T) {\n\tfor _, leak := range []bool{true, false} {\n\t\tt.Run(fmt.Sprintf(\"leak=%t\", leak), func(t *testing.T) {\n\t\t\tfor _, flush := range []bool{true, false} {\n\t\t\t\tt.Run(fmt.Sprintf(\"flush=%t\", flush), func(t *testing.T) {\n\t\t\t\t\td, err := Open(\"\", testingRandomized(t, &Options{\n\t\t\t\t\t\tFS: vfs.NewMem(),\n\t\t\t\t\t}))\n\t\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\t\trequire.NoError(t, d.Set([]byte(\"a\"), []byte(\"a\"), nil))\n\t\t\t\t\tif flush {\n\t\t\t\t\t\trequire.NoError(t, d.Flush())\n\t\t\t\t\t}\n\t\t\t\t\titer, _ := d.NewIter(nil)\n\t\t\t\t\titer.First()\n\t\t\t\t\tif !leak {\n\t\t\t\t\t\trequire.NoError(t, iter.Close())\n\t\t\t\t\t\trequire.NoError(t, d.Close())\n\t\t\t\t\t} else {\n\t\t\t\t\t\tdefer iter.Close()\n\t\t\t\t\t\tif err := d.Close(); err == nil {\n\t\t\t\t\t\t\tt.Fatalf(\"expected failure, but found success\")\n\t\t\t\t\t\t} else if !strings.HasPrefix(err.Error(), \"leaked iterators:\") {\n\t\t\t\t\t\t\tt.Fatalf(\"expected leaked iterators, but found %+v\", err)\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tt.Log(err.Error())\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t})\n\t\t\t}\n\t\t})\n\t}\n}\n\n// Make sure that we detect an iter leak when only one DB closes\n// while the second db still holds a reference to the FileCache.\nfunc TestIterLeakSharedCache(t *testing.T) {\n\tfor _, leak := range []bool{true, false} {\n\t\tt.Run(fmt.Sprintf(\"leak=%t\", leak), func(t *testing.T) {\n\t\t\tfor _, flush := range []bool{true, false} {\n\t\t\t\tt.Run(fmt.Sprintf(\"flush=%t\", flush), func(t *testing.T) {\n\t\t\t\t\td1, err := Open(\"\", &Options{\n\t\t\t\t\t\tFS: vfs.NewMem(),\n\t\t\t\t\t})\n\t\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\t\td2, err := Open(\"\", &Options{\n\t\t\t\t\t\tFS: vfs.NewMem(),\n\t\t\t\t\t})\n\t\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\t\trequire.NoError(t, d1.Set([]byte(\"a\"), []byte(\"a\"), nil))\n\t\t\t\t\tif flush {\n\t\t\t\t\t\trequire.NoError(t, d1.Flush())\n\t\t\t\t\t}\n\n\t\t\t\t\trequire.NoError(t, d2.Set([]byte(\"a\"), []byte(\"a\"), nil))\n\t\t\t\t\tif flush {\n\t\t\t\t\t\trequire.NoError(t, d2.Flush())\n\t\t\t\t\t}\n\n\t\t\t\t\t// Check if leak detection works with only one db closing.\n\t\t\t\t\t{\n\t\t\t\t\t\titer1, _ := d1.NewIter(nil)\n\t\t\t\t\t\titer1.First()\n\t\t\t\t\t\tif !leak {\n\t\t\t\t\t\t\trequire.NoError(t, iter1.Close())\n\t\t\t\t\t\t\trequire.NoError(t, d1.Close())\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tdefer iter1.Close()\n\t\t\t\t\t\t\tif err := d1.Close(); err == nil {\n\t\t\t\t\t\t\t\tt.Fatalf(\"expected failure, but found success\")\n\t\t\t\t\t\t\t} else if !strings.HasPrefix(err.Error(), \"leaked iterators:\") {\n\t\t\t\t\t\t\t\tt.Fatalf(\"expected leaked iterators, but found %+v\", err)\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\tt.Log(err.Error())\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t{\n\t\t\t\t\t\titer2, _ := d2.NewIter(nil)\n\t\t\t\t\t\titer2.First()\n\t\t\t\t\t\tif !leak {\n\t\t\t\t\t\t\trequire.NoError(t, iter2.Close())\n\t\t\t\t\t\t\trequire.NoError(t, d2.Close())\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tdefer iter2.Close()\n\t\t\t\t\t\t\tif err := d2.Close(); err == nil {\n\t\t\t\t\t\t\t\tt.Fatalf(\"expected failure, but found success\")\n\t\t\t\t\t\t\t} else if !strings.HasPrefix(err.Error(), \"leaked iterators:\") {\n\t\t\t\t\t\t\t\tt.Fatalf(\"expected leaked iterators, but found %+v\", err)\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\tt.Log(err.Error())\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t})\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestMemTableReservation(t *testing.T) {\n\topts := &Options{\n\t\tCache: NewCache(128 << 10 /* 128 KB */),\n\t\t// We're going to be looking at and asserting the global memtable reservation\n\t\t// amount below so we don't want to race with any triggered stats collections.\n\t\tDisableTableStats: true,\n\t\tMemTableSize:      initialMemTableSize,\n\t\tFS:                vfs.NewMem(),\n\t}\n\tdefer opts.Cache.Unref()\n\topts.testingRandomized(t)\n\topts.EnsureDefaults()\n\n\t// Add a block to the cache. Note that the memtable size is larger than the\n\t// cache size, so opening the DB should cause this block to be evicted.\n\ttmpID := opts.Cache.NewID()\n\thelloWorld := []byte(\"hello world\")\n\tvalue := cache.Alloc(len(helloWorld))\n\tcopy(value.RawBuffer(), helloWorld)\n\topts.Cache.Set(tmpID, base.DiskFileNum(0), 0, value)\n\tvalue.Release()\n\n\td, err := Open(\"\", opts)\n\trequire.NoError(t, err)\n\n\tcheckReserved := func(expected int64) {\n\t\tt.Helper()\n\t\tif reserved := d.memTableReserved.Load(); expected != reserved {\n\t\t\tt.Fatalf(\"expected %d reserved, but found %d\", expected, reserved)\n\t\t}\n\t}\n\n\tcheckReserved(int64(opts.MemTableSize))\n\tif refs := d.mu.mem.queue[len(d.mu.mem.queue)-1].readerRefs.Load(); refs != 2 {\n\t\tt.Fatalf(\"expected 2 refs, but found %d\", refs)\n\t}\n\t// Verify the memtable reservation has caused our test block to be evicted.\n\tif cv := opts.Cache.Get(tmpID, base.DiskFileNum(0), 0); cv != nil {\n\t\tt.Fatalf(\"expected failure, but found success: %#v\", cv)\n\t}\n\n\t// Flush the memtable. The memtable reservation should double because old\n\t// memtable will be recycled, saved for the next memtable allocation.\n\trequire.NoError(t, d.Flush())\n\tcheckReserved(int64(2 * opts.MemTableSize))\n\t// Flush again. The memtable reservation should be unchanged because at most\n\t// 1 memtable may be preserved for recycling.\n\n\t// Flush in the presence of an active iterator. The iterator will hold a\n\t// reference to a readState which will in turn hold a reader reference to the\n\t// memtable.\n\titer, _ := d.NewIter(nil)\n\trequire.NoError(t, d.Flush())\n\t// The flush moved the recycled memtable into position as an active mutable\n\t// memtable. There are now two allocated memtables: 1 mutable and 1 pinned\n\t// by the iterator's read state.\n\tcheckReserved(2 * int64(opts.MemTableSize))\n\n\t// Flushing again should increase the reservation total to 3x: 1 active\n\t// mutable, 1 for recycling, 1 pinned by iterator's read state.\n\trequire.NoError(t, d.Flush())\n\tcheckReserved(3 * int64(opts.MemTableSize))\n\n\t// Closing the iterator will release the iterator's read state, and the old\n\t// memtable will be moved into position as the next memtable to recycle.\n\t// There was already a memtable ready to be recycled, so that memtable will\n\t// be freed and the overall reservation total is reduced to 2x.\n\trequire.NoError(t, iter.Close())\n\tcheckReserved(2 * int64(opts.MemTableSize))\n\n\trequire.NoError(t, d.Close())\n}\n\nfunc TestMemTableReservationLeak(t *testing.T) {\n\td, err := Open(\"\", &Options{FS: vfs.NewMem()})\n\trequire.NoError(t, err)\n\n\td.mu.Lock()\n\tlast := d.mu.mem.queue[len(d.mu.mem.queue)-1]\n\tlast.readerRef()\n\tdefer func() {\n\t\tlast.readerUnref(true)\n\t}()\n\td.mu.Unlock()\n\tif err := d.Close(); err == nil {\n\t\tt.Fatalf(\"expected failure, but found success\")\n\t} else if !strings.HasPrefix(err.Error(), \"leaked memtable reservation:\") {\n\t\tt.Fatalf(\"expected leaked memtable reservation, but found %+v\", err)\n\t} else {\n\t\tt.Log(err.Error())\n\t}\n}\n\nfunc TestCacheEvict(t *testing.T) {\n\tcache := NewCache(10 << 20)\n\tdefer cache.Unref()\n\n\td, err := Open(\"\", &Options{\n\t\tCache: cache,\n\t\tFS:    vfs.NewMem(),\n\t})\n\trequire.NoError(t, err)\n\n\tfor i := 0; i < 1000; i++ {\n\t\tkey := []byte(fmt.Sprintf(\"%04d\", i))\n\t\trequire.NoError(t, d.Set(key, key, nil))\n\t}\n\n\trequire.NoError(t, d.Flush())\n\titer, _ := d.NewIter(nil)\n\tfor iter.First(); iter.Valid(); iter.Next() {\n\t}\n\trequire.NoError(t, iter.Close())\n\n\tif size := cache.Size(); size == 0 {\n\t\tt.Fatalf(\"expected non-zero cache size\")\n\t}\n\n\tfor i := 0; i < 1000; i++ {\n\t\tkey := []byte(fmt.Sprintf(\"%04d\", i))\n\t\trequire.NoError(t, d.Delete(key, nil))\n\t}\n\n\trequire.NoError(t, d.Compact([]byte(\"0\"), []byte(\"1\"), false))\n\n\trequire.NoError(t, d.Close())\n\n\tif size := cache.Size(); size != 0 {\n\t\tt.Fatalf(\"expected empty cache, but found %d\", size)\n\t}\n}\n\nfunc TestFlushEmpty(t *testing.T) {\n\td, err := Open(\"\", testingRandomized(t, &Options{\n\t\tFS: vfs.NewMem(),\n\t}))\n\trequire.NoError(t, err)\n\n\t// Flushing an empty memtable should not fail.\n\trequire.NoError(t, d.Flush())\n\trequire.NoError(t, d.Close())\n}\n\nfunc TestRollManifest(t *testing.T) {\n\ttoPreserve := rand.Int32N(5) + 1\n\topts := &Options{\n\t\tMaxManifestFileSize:   1,\n\t\tL0CompactionThreshold: 10,\n\t\tL0StopWritesThreshold: 1000,\n\t\tFS:                    vfs.NewMem(),\n\t\tNumPrevManifest:       int(toPreserve),\n\t}\n\topts.DisableAutomaticCompactions = true\n\topts.testingRandomized(t)\n\td, err := Open(\"\", opts)\n\trequire.NoError(t, err)\n\n\tmanifestFileNumber := func() base.DiskFileNum {\n\t\td.mu.Lock()\n\t\tdefer d.mu.Unlock()\n\t\treturn d.mu.versions.manifestFileNum\n\t}\n\tsizeRolloverState := func() (int64, int64) {\n\t\td.mu.Lock()\n\t\tdefer d.mu.Unlock()\n\t\treturn d.mu.versions.rotationHelper.DebugInfo()\n\t}\n\n\tcurrent := func() string {\n\t\tdesc, err := Peek(d.dirname, d.opts.FS)\n\t\trequire.NoError(t, err)\n\t\treturn desc.ManifestFilename\n\t}\n\n\tlastManifestNum := manifestFileNumber()\n\tmanifestNums := []base.DiskFileNum{lastManifestNum}\n\tfor i := 0; i < 5; i++ {\n\t\t// MaxManifestFileSize is 1, but the rollover logic also counts edits\n\t\t// since the last snapshot to decide on rollover, so do as many flushes as\n\t\t// it demands.\n\t\tlastSnapshotCount, editsSinceSnapshotCount := sizeRolloverState()\n\t\tvar expectedLastSnapshotCount, expectedEditsSinceSnapshotCount int64\n\t\tswitch i {\n\t\tcase 0:\n\t\t\t// DB is empty.\n\t\t\texpectedLastSnapshotCount, expectedEditsSinceSnapshotCount = 0, 0\n\t\tcase 1:\n\t\t\t// First edit that caused rollover is not in the snapshot.\n\t\t\texpectedLastSnapshotCount, expectedEditsSinceSnapshotCount = 0, 1\n\t\tcase 2:\n\t\t\t// One flush is in the snapshot. One flush in the edit.\n\t\t\texpectedLastSnapshotCount, expectedEditsSinceSnapshotCount = 1, 1\n\t\tcase 3:\n\t\t\t// Two flushes in the snapshot. One flush in the edit. Will need to do\n\t\t\t// two more flushes, the first of which will be in the next snapshot.\n\t\t\texpectedLastSnapshotCount, expectedEditsSinceSnapshotCount = 2, 1\n\t\tcase 4:\n\t\t\t// Four flushes in the snapshot. One flush in the edit. Will need to do\n\t\t\t// four more flushes, three of which will be in the snapshot.\n\t\t\texpectedLastSnapshotCount, expectedEditsSinceSnapshotCount = 4, 1\n\t\t}\n\t\trequire.Equal(t, expectedLastSnapshotCount, lastSnapshotCount)\n\t\trequire.Equal(t, expectedEditsSinceSnapshotCount, editsSinceSnapshotCount)\n\t\t// Number of flushes to do to trigger the rollover.\n\t\tsteps := int(lastSnapshotCount - editsSinceSnapshotCount + 1)\n\t\t// Steps can be <= 0, but we need to do at least one edit to trigger the\n\t\t// rollover logic.\n\t\tif steps <= 0 {\n\t\t\tsteps = 1\n\t\t}\n\t\tfor j := 0; j < steps; j++ {\n\t\t\trequire.NoError(t, d.Set([]byte(\"a\"), nil, nil))\n\t\t\trequire.NoError(t, d.Flush())\n\t\t}\n\t\td.TestOnlyWaitForCleaning()\n\t\tnum := manifestFileNumber()\n\t\tif lastManifestNum == num {\n\t\t\tt.Fatalf(\"manifest failed to roll %d: %d == %d\", i, lastManifestNum, num)\n\t\t}\n\n\t\tmanifestNums = append(manifestNums, num)\n\t\tlastManifestNum = num\n\n\t\texpectedCurrent := fmt.Sprintf(\"MANIFEST-%s\", lastManifestNum)\n\t\tif v := current(); expectedCurrent != v {\n\t\t\tt.Fatalf(\"expected %s, but found %s\", expectedCurrent, v)\n\t\t}\n\t}\n\tlastSnapshotCount, editsSinceSnapshotCount := sizeRolloverState()\n\trequire.EqualValues(t, 8, lastSnapshotCount)\n\trequire.EqualValues(t, 1, editsSinceSnapshotCount)\n\n\tfiles, err := d.opts.FS.List(\"\")\n\trequire.NoError(t, err)\n\n\tvar manifests []string\n\tfor _, filename := range files {\n\t\tfileType, _, ok := base.ParseFilename(d.opts.FS, filename)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tif fileType == fileTypeManifest {\n\t\t\tmanifests = append(manifests, filename)\n\t\t}\n\t}\n\tslices.Sort(manifests)\n\n\tvar expected []string\n\tfor i := len(manifestNums) - int(toPreserve) - 1; i < len(manifestNums); i++ {\n\t\texpected = append(\n\t\t\texpected,\n\t\t\tfmt.Sprintf(\"MANIFEST-%s\", manifestNums[i]),\n\t\t)\n\t}\n\trequire.EqualValues(t, expected, manifests)\n\n\t// Test the logic that uses the future snapshot size to rollover.\n\t// Reminder: we have a snapshot with 8 files and the manifest has 1 edit\n\t// (flush) with 1 file.\n\t// Add 8 more files with a different key.\n\tlastManifestNum = manifestFileNumber()\n\tfor j := 0; j < 8; j++ {\n\t\trequire.NoError(t, d.Set([]byte(\"c\"), nil, nil))\n\t\trequire.NoError(t, d.Flush())\n\t}\n\tlastSnapshotCount, editsSinceSnapshotCount = sizeRolloverState()\n\t// Need 16 more files in edits to trigger a rollover.\n\trequire.EqualValues(t, 16, lastSnapshotCount)\n\trequire.EqualValues(t, 1, editsSinceSnapshotCount)\n\trequire.NotEqual(t, manifestFileNumber(), lastManifestNum)\n\tlastManifestNum = manifestFileNumber()\n\t// Do a compaction that moves 8 of the files from L0 to 1 file in L6. This\n\t// adds 9 files in edits. We still need 6 more files in edits based on the\n\t// last snapshot. But the current version has only 9 L0 files and 1 L6 file,\n\t// for a total of 10 files. So 1 flush should push us over that threshold.\n\td.Compact([]byte(\"c\"), []byte(\"d\"), false)\n\tlastSnapshotCount, editsSinceSnapshotCount = sizeRolloverState()\n\trequire.EqualValues(t, 16, lastSnapshotCount)\n\trequire.EqualValues(t, 10, editsSinceSnapshotCount)\n\trequire.Equal(t, manifestFileNumber(), lastManifestNum)\n\trequire.NoError(t, d.Set([]byte(\"c\"), nil, nil))\n\trequire.NoError(t, d.Flush())\n\tlastSnapshotCount, editsSinceSnapshotCount = sizeRolloverState()\n\trequire.EqualValues(t, 10, lastSnapshotCount)\n\trequire.EqualValues(t, 1, editsSinceSnapshotCount)\n\trequire.NotEqual(t, manifestFileNumber(), lastManifestNum)\n\n\trequire.NoError(t, d.Close())\n}\n\nfunc TestDBClosed(t *testing.T) {\n\td, err := Open(\"\", &Options{\n\t\tFS: vfs.NewMem(),\n\t})\n\trequire.NoError(t, err)\n\trequire.NoError(t, d.Close())\n\n\tcatch := func(f func()) (err error) {\n\t\tdefer func() {\n\t\t\tif r := recover(); r != nil {\n\t\t\t\terr = r.(error)\n\t\t\t}\n\t\t}()\n\t\tf()\n\t\treturn nil\n\t}\n\n\trequire.True(t, errors.Is(catch(func() { _ = d.Close() }), ErrClosed))\n\n\trequire.True(t, errors.Is(catch(func() { _ = d.Compact(nil, nil, false) }), ErrClosed))\n\trequire.True(t, errors.Is(catch(func() { _ = d.Flush() }), ErrClosed))\n\trequire.True(t, errors.Is(catch(func() { _, _ = d.AsyncFlush() }), ErrClosed))\n\n\trequire.True(t, errors.Is(catch(func() { _, _, _ = d.Get(nil) }), ErrClosed))\n\trequire.True(t, errors.Is(catch(func() { _ = d.Delete(nil, nil) }), ErrClosed))\n\trequire.True(t, errors.Is(catch(func() { _ = d.DeleteRange(nil, nil, nil) }), ErrClosed))\n\trequire.True(t, errors.Is(catch(func() { _ = d.Ingest(context.Background(), nil) }), ErrClosed))\n\trequire.True(t, errors.Is(catch(func() { _ = d.LogData(nil, nil) }), ErrClosed))\n\trequire.True(t, errors.Is(catch(func() { _ = d.Merge(nil, nil, nil) }), ErrClosed))\n\trequire.True(t, errors.Is(catch(func() { _ = d.RatchetFormatMajorVersion(internalFormatNewest) }), ErrClosed))\n\trequire.True(t, errors.Is(catch(func() { _ = d.Set(nil, nil, nil) }), ErrClosed))\n\n\trequire.True(t, errors.Is(catch(func() { _ = d.NewSnapshot() }), ErrClosed))\n\n\tb := d.NewIndexedBatch()\n\trequire.True(t, errors.Is(catch(func() { _ = b.Commit(nil) }), ErrClosed))\n\trequire.True(t, errors.Is(catch(func() { _ = d.Apply(b, nil) }), ErrClosed))\n\trequire.True(t, errors.Is(catch(func() { _, _ = b.NewIter(nil) }), ErrClosed))\n}\n\nfunc TestDBConcurrentCommitCompactFlush(t *testing.T) {\n\td, err := Open(\"\", testingRandomized(t, &Options{\n\t\tFS: vfs.NewMem(),\n\t}))\n\trequire.NoError(t, err)\n\n\t// Concurrently commit, compact, and flush in order to stress the locking around\n\t// those operations.\n\tconst n = 1000\n\tvar wg sync.WaitGroup\n\twg.Add(n)\n\tfor i := 0; i < n; i++ {\n\t\tgo func(i int) {\n\t\t\tdefer wg.Done()\n\t\t\t_ = d.Set([]byte(fmt.Sprint(i)), nil, nil)\n\t\t\tvar err error\n\t\t\tswitch i % 3 {\n\t\t\tcase 0:\n\t\t\t\terr = d.Compact(nil, []byte(\"\\xff\"), false)\n\t\t\tcase 1:\n\t\t\t\terr = d.Flush()\n\t\t\tcase 2:\n\t\t\t\t_, err = d.AsyncFlush()\n\t\t\t}\n\t\t\trequire.NoError(t, err)\n\t\t}(i)\n\t}\n\twg.Wait()\n\n\trequire.NoError(t, d.Close())\n}\n\nfunc TestDBConcurrentCompactClose(t *testing.T) {\n\t// Test closing while a compaction is ongoing. This ensures compaction code\n\t// detects the close and finishes cleanly.\n\tmem := vfs.NewMem()\n\tfor i := 0; i < 100; i++ {\n\t\topts := &Options{\n\t\t\tFS: mem,\n\t\t\tMaxConcurrentCompactions: func() int {\n\t\t\t\treturn 2\n\t\t\t},\n\t\t}\n\t\td, err := Open(\"\", testingRandomized(t, opts))\n\t\trequire.NoError(t, err)\n\n\t\t// Ingest a series of files containing a single key each. As the outer\n\t\t// loop progresses, these ingestions will build up compaction debt\n\t\t// causing compactions to be running concurrently with the close below.\n\t\tfor j := 0; j < 10; j++ {\n\t\t\tpath := fmt.Sprintf(\"ext%d\", j)\n\t\t\tf, err := mem.Create(path, vfs.WriteCategoryUnspecified)\n\t\t\trequire.NoError(t, err)\n\t\t\tw := sstable.NewWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{\n\t\t\t\tTableFormat: d.TableFormat(),\n\t\t\t})\n\t\t\trequire.NoError(t, w.Set([]byte(fmt.Sprint(j)), nil))\n\t\t\trequire.NoError(t, w.Close())\n\t\t\trequire.NoError(t, d.Ingest(context.Background(), []string{path}))\n\t\t}\n\n\t\trequire.NoError(t, d.Close())\n\t}\n}\n\nfunc TestDBApplyBatchNilDB(t *testing.T) {\n\td, err := Open(\"\", &Options{FS: vfs.NewMem()})\n\trequire.NoError(t, err)\n\n\tb1 := &Batch{}\n\tb1.Set([]byte(\"test\"), nil, nil)\n\n\tb2 := &Batch{}\n\tb2.Apply(b1, nil)\n\tif b2.memTableSize != 0 {\n\t\tt.Fatalf(\"expected memTableSize to not be set\")\n\t}\n\trequire.NoError(t, d.Apply(b2, nil))\n\tif b1.memTableSize != b2.memTableSize {\n\t\tt.Fatalf(\"expected memTableSize %d, but found %d\", b1.memTableSize, b2.memTableSize)\n\t}\n\n\trequire.NoError(t, d.Close())\n}\n\nfunc TestDBApplyBatchMismatch(t *testing.T) {\n\tsrcDB, err := Open(\"\", &Options{FS: vfs.NewMem()})\n\trequire.NoError(t, err)\n\n\tapplyDB, err := Open(\"\", &Options{FS: vfs.NewMem()})\n\trequire.NoError(t, err)\n\n\terr = func() (err error) {\n\t\tdefer func() {\n\t\t\tif v := recover(); v != nil {\n\t\t\t\terr = errors.Errorf(\"%v\", v)\n\t\t\t}\n\t\t}()\n\n\t\tb := srcDB.NewBatch()\n\t\tb.Set([]byte(\"test\"), nil, nil)\n\t\treturn applyDB.Apply(b, nil)\n\t}()\n\tif err == nil || !strings.Contains(err.Error(), \"pebble: batch db mismatch:\") {\n\t\tt.Fatalf(\"expected error, but found %v\", err)\n\t}\n\n\trequire.NoError(t, srcDB.Close())\n\trequire.NoError(t, applyDB.Close())\n}\n\nfunc TestCloseCleanerRace(t *testing.T) {\n\tmem := vfs.NewMem()\n\tfor i := 0; i < 20; i++ {\n\t\tdb, err := Open(\"\", testingRandomized(t, &Options{FS: mem}))\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, db.Set([]byte(\"a\"), []byte(\"something\"), Sync))\n\t\trequire.NoError(t, db.Flush())\n\t\t// Ref the sstables so cannot be deleted.\n\t\tit, _ := db.NewIter(nil)\n\t\trequire.NotNil(t, it)\n\t\trequire.NoError(t, db.DeleteRange([]byte(\"a\"), []byte(\"b\"), Sync))\n\t\trequire.NoError(t, db.Compact([]byte(\"a\"), []byte(\"b\"), false))\n\t\t// Only the iterator is keeping the sstables alive.\n\t\tfiles, err := mem.List(\"/\")\n\t\trequire.NoError(t, err)\n\t\tvar found bool\n\t\tfor _, f := range files {\n\t\t\tif strings.HasSuffix(f, \".sst\") {\n\t\t\t\tfound = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\trequire.True(t, found)\n\t\t// Close the iterator and the db in succession so file cleaning races with DB.Close() --\n\t\t// latter should wait for file cleaning to finish.\n\t\trequire.NoError(t, it.Close())\n\t\trequire.NoError(t, db.Close())\n\t\tfiles, err = mem.List(\"/\")\n\t\trequire.NoError(t, err)\n\t\tfor _, f := range files {\n\t\t\tif strings.HasSuffix(f, \".sst\") {\n\t\t\t\tt.Fatalf(\"found sst: %s\", f)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestSSTablesWithApproximateSpanBytes(t *testing.T) {\n\td, err := Open(\"\", &Options{\n\t\tFS: vfs.NewMem(),\n\t})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\tif d != nil {\n\t\t\trequire.NoError(t, d.Close())\n\t\t}\n\t}()\n\n\t// Create two sstables.\n\t// sstable is contained within keyspan (fileNum = 5).\n\trequire.NoError(t, d.Set([]byte(\"c\"), nil, nil))\n\trequire.NoError(t, d.Set([]byte(\"d\"), nil, nil))\n\trequire.NoError(t, d.Flush())\n\n\t// sstable partially overlaps keyspan (fileNum = 7).\n\trequire.NoError(t, d.Set([]byte(\"d\"), nil, nil))\n\trequire.NoError(t, d.Set([]byte(\"g\"), nil, nil))\n\trequire.NoError(t, d.Flush())\n\n\t// cannot use WithApproximateSpanBytes without WithKeyRangeFilter.\n\t_, err = d.SSTables(WithProperties(), WithApproximateSpanBytes())\n\trequire.Error(t, err)\n\n\ttableInfos, err := d.SSTables(WithProperties(), WithKeyRangeFilter([]byte(\"a\"), []byte(\"e\")), WithApproximateSpanBytes())\n\trequire.NoError(t, err)\n\n\tfor _, levelTables := range tableInfos {\n\t\tfor _, table := range levelTables {\n\t\t\tif table.FileNum == 5 {\n\t\t\t\trequire.Equal(t, table.ApproximateSpanBytes, table.Size)\n\t\t\t}\n\t\t\tif table.FileNum == 7 {\n\t\t\t\trequire.Less(t, table.ApproximateSpanBytes, table.Size)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestFilterSSTablesWithOption(t *testing.T) {\n\td, err := Open(\"\", &Options{\n\t\tFS: vfs.NewMem(),\n\t})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\tif d != nil {\n\t\t\trequire.NoError(t, d.Close())\n\t\t}\n\t}()\n\n\t// Create two sstables.\n\trequire.NoError(t, d.Set([]byte(\"/Table/5\"), nil, nil))\n\trequire.NoError(t, d.Flush())\n\trequire.NoError(t, d.Set([]byte(\"/Table/10\"), nil, nil))\n\trequire.NoError(t, d.Flush())\n\n\ttableInfos, err := d.SSTables(WithKeyRangeFilter([]byte(\"/Table/5\"), []byte(\"/Table/6\")))\n\trequire.NoError(t, err)\n\n\ttotalTables := 0\n\tfor _, levelTables := range tableInfos {\n\t\ttotalTables += len(levelTables)\n\t}\n\n\t// with filter second sstable should not be returned\n\trequire.EqualValues(t, 1, totalTables)\n\n\ttableInfos, err = d.SSTables()\n\trequire.NoError(t, err)\n\n\ttotalTables = 0\n\tfor _, levelTables := range tableInfos {\n\t\ttotalTables += len(levelTables)\n\t}\n\n\t// without filter\n\trequire.EqualValues(t, 2, totalTables)\n}\n\nfunc TestSSTables(t *testing.T) {\n\td, err := Open(\"\", &Options{\n\t\tFS: vfs.NewMem(),\n\t})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\tif d != nil {\n\t\t\trequire.NoError(t, d.Close())\n\t\t}\n\t}()\n\n\t// Create two sstables.\n\trequire.NoError(t, d.Set([]byte(\"hello\"), nil, nil))\n\trequire.NoError(t, d.Flush())\n\trequire.NoError(t, d.Set([]byte(\"world\"), nil, nil))\n\trequire.NoError(t, d.Flush())\n\n\t// by default returned table infos should not contain Properties\n\ttableInfos, err := d.SSTables()\n\trequire.NoError(t, err)\n\tfor _, levelTables := range tableInfos {\n\t\tfor _, info := range levelTables {\n\t\t\trequire.Nil(t, info.Properties)\n\t\t}\n\t}\n\n\t// with opt `WithProperties()` the `Properties` in table info should not be nil\n\ttableInfos, err = d.SSTables(WithProperties())\n\trequire.NoError(t, err)\n\tfor _, levelTables := range tableInfos {\n\t\tfor _, info := range levelTables {\n\t\t\trequire.NotNil(t, info.Properties)\n\t\t}\n\t}\n}\n\ntype testTracer struct {\n\tenabledOnlyForNonBackgroundContext bool\n\tbuf                                strings.Builder\n}\n\nfunc (t *testTracer) Infof(format string, args ...interface{})  {}\nfunc (t *testTracer) Errorf(format string, args ...interface{}) {}\nfunc (t *testTracer) Fatalf(format string, args ...interface{}) {}\n\nfunc (t *testTracer) Eventf(ctx context.Context, format string, args ...interface{}) {\n\tif t.enabledOnlyForNonBackgroundContext && ctx == context.Background() {\n\t\treturn\n\t}\n\tstr := fmt.Sprintf(format, args...)\n\t// Redact known strings that depend on source code line numbers.\n\tstr = regexp.MustCompile(`\\(fileNum=[^)]+\\)$`).ReplaceAllString(str, \"(<redacted>)\")\n\tt.buf.WriteString(str)\n\tt.buf.WriteString(\"\\n\")\n}\n\nfunc (t *testTracer) IsTracingEnabled(ctx context.Context) bool {\n\tif t.enabledOnlyForNonBackgroundContext && ctx == context.Background() {\n\t\treturn false\n\t}\n\treturn true\n}\n\nfunc TestTracing(t *testing.T) {\n\tdefer sstable.DeterministicReadBlockDurationForTesting()()\n\n\tvar tracer testTracer\n\tbuf := &tracer.buf\n\tvar db *DB\n\tdefer func() {\n\t\tif db != nil {\n\t\t\tdb.Close()\n\t\t}\n\t}()\n\tcache := NewCache(0)\n\tdefer cache.Unref()\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\tdatadriven.RunTest(t, \"testdata/tracing\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tbuf.Reset()\n\t\ttracer.enabledOnlyForNonBackgroundContext = td.HasArg(\"disable-background-tracing\")\n\t\tctx := ctx\n\t\tswitch td.Cmd {\n\t\tcase \"build\":\n\t\t\tif db != nil {\n\t\t\t\tdb.Close()\n\t\t\t}\n\t\t\tdb = testutils.CheckErr(Open(\"\", &Options{\n\t\t\t\tFS:              vfs.NewMem(),\n\t\t\t\tComparer:        testkeys.Comparer,\n\t\t\t\tCache:           cache,\n\t\t\t\tLoggerAndTracer: &tracer,\n\t\t\t}))\n\n\t\t\tb := db.NewBatch()\n\t\t\trequire.NoError(t, runBatchDefineCmd(td, b))\n\t\t\trequire.NoError(t, b.Commit(nil))\n\t\t\trequire.NoError(t, db.Flush())\n\t\t\treturn \"\"\n\n\t\tcase \"get\":\n\t\t\tfor _, key := range strings.Split(td.Input, \"\\n\") {\n\t\t\t\tv, closer, err := db.Get([]byte(key))\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\tfmt.Fprintf(buf, \"%s:%s\\n\", key, v)\n\t\t\t\tcloser.Close()\n\t\t\t}\n\n\t\tcase \"iter\":\n\t\t\titer, _ := db.NewIterWithContext(ctx, &IterOptions{\n\t\t\t\tKeyTypes: IterKeyTypePointsAndRanges,\n\t\t\t})\n\t\t\tbuf.WriteString(runIterCmd(td, iter, true))\n\n\t\tcase \"snapshot-iter\":\n\t\t\tsnap := db.NewSnapshot()\n\t\t\tdefer snap.Close()\n\t\t\titer, _ := snap.NewIterWithContext(ctx, &IterOptions{\n\t\t\t\tKeyTypes: IterKeyTypePointsAndRanges,\n\t\t\t})\n\t\t\tbuf.WriteString(runIterCmd(td, iter, true))\n\n\t\tcase \"indexed-batch-iter\":\n\t\t\tb := db.NewIndexedBatch()\n\t\t\tdefer b.Close()\n\t\t\titer, _ := b.NewIterWithContext(ctx, &IterOptions{\n\t\t\t\tKeyTypes: IterKeyTypePointsAndRanges,\n\t\t\t})\n\t\t\tbuf.WriteString(runIterCmd(td, iter, true))\n\n\t\tdefault:\n\t\t\ttd.Fatalf(t, \"unknown command: %s\", td.Cmd)\n\t\t}\n\t\treturn buf.String()\n\t})\n}\n\nfunc TestMemtableIngestInversion(t *testing.T) {\n\tmemFS := vfs.NewMem()\n\topts := &Options{\n\t\tFS:                          memFS,\n\t\tMemTableSize:                256 << 10, // 4KB\n\t\tMemTableStopWritesThreshold: 1000,\n\t\tL0StopWritesThreshold:       1000,\n\t\tL0CompactionThreshold:       2,\n\t\tMaxConcurrentCompactions: func() int {\n\t\t\treturn 1000\n\t\t},\n\t}\n\n\tconst channelTimeout = 5 * time.Second\n\n\t// We induce delay in compactions by passing in an EventListener that stalls on\n\t// the first TableCreated event for a compaction job we want to block.\n\t// FlushBegin and CompactionBegin has info on compaction start/output levels\n\t// which is what we need to identify what compactions to block. However\n\t// FlushBegin and CompactionBegin are called while holding db.mu, so we cannot\n\t// block those events forever. Instead, we grab the job ID from those events\n\t// and store it. Then during TableCreated, we check if we're creating an output\n\t// for a job we have identified earlier as one to block, and then hold on a\n\t// semaphore there until there's a signal from the test code to resume with the\n\t// compaction.\n\t//\n\t// If nextBlockedCompaction is non-zero, we must block the next compaction\n\t// out of the nextBlockedCompaction - 3 start level. 1 means block the next\n\t// intra-L0 compaction and 2 means block the next flush (as flushes have\n\t// a -1 start level).\n\tvar nextBlockedCompaction, blockedJobID int\n\tvar blockedCompactionsMu sync.Mutex // protects the above two variables.\n\tnextSem := make(chan chan struct{}, 1)\n\tvar el EventListener\n\tel.EnsureDefaults(testLogger{t: t})\n\tel.FlushBegin = func(info FlushInfo) {\n\t\tblockedCompactionsMu.Lock()\n\t\tdefer blockedCompactionsMu.Unlock()\n\t\tif nextBlockedCompaction == 2 {\n\t\t\tnextBlockedCompaction = 0\n\t\t\tblockedJobID = info.JobID\n\t\t}\n\t}\n\tel.CompactionBegin = func(info CompactionInfo) {\n\t\t// 0 = block nothing, 1 = block intra-L0 compaction, 2 = block flush,\n\t\t// 3 = block L0 -> LBase compaction, 4 = block compaction out of L1, and so on.\n\t\tblockedCompactionsMu.Lock()\n\t\tdefer blockedCompactionsMu.Unlock()\n\t\tblockValue := info.Input[0].Level + 3\n\t\tif info.Input[0].Level == 0 && info.Output.Level == 0 {\n\t\t\t// Intra L0 compaction, denoted by casValue of 1.\n\t\t\tblockValue = 1\n\t\t}\n\t\tif nextBlockedCompaction == blockValue {\n\t\t\tnextBlockedCompaction = 0\n\t\t\tblockedJobID = info.JobID\n\t\t}\n\t}\n\tel.TableCreated = func(info TableCreateInfo) {\n\t\tblockedCompactionsMu.Lock()\n\t\tif info.JobID != blockedJobID {\n\t\t\tblockedCompactionsMu.Unlock()\n\t\t\treturn\n\t\t}\n\t\tblockedJobID = 0\n\t\tblockedCompactionsMu.Unlock()\n\t\tsem := make(chan struct{})\n\t\tnextSem <- sem\n\t\t<-sem\n\t}\n\ttel := TeeEventListener(MakeLoggingEventListener(testLogger{t: t}), el)\n\topts.EventListener = &tel\n\topts.Experimental.L0CompactionConcurrency = 1\n\td, err := Open(\"\", opts)\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\tif d != nil {\n\t\t\trequire.NoError(t, d.Close())\n\t\t}\n\t}()\n\n\tprintLSM := func() {\n\t\td.mu.Lock()\n\t\ts := d.mu.versions.currentVersion().String()\n\t\td.mu.Unlock()\n\t\tt.Logf(\"%s\", s)\n\t}\n\n\t// Create some sstables. These should go into L6. These are irrelevant for\n\t// the rest of the test.\n\trequire.NoError(t, d.Set([]byte(\"b\"), []byte(\"foo\"), nil))\n\trequire.NoError(t, d.Flush())\n\trequire.NoError(t, d.Set([]byte(\"d\"), []byte(\"bar\"), nil))\n\trequire.NoError(t, d.Flush())\n\trequire.NoError(t, d.Compact([]byte(\"a\"), []byte(\"z\"), true))\n\n\tvar baseCompactionSem, flushSem, intraL0Sem chan struct{}\n\t// Block an L0 -> LBase compaction. This is necessary to induce intra-L0\n\t// compactions later on.\n\tblockedCompactionsMu.Lock()\n\tnextBlockedCompaction = 3\n\tblockedCompactionsMu.Unlock()\n\ttimeoutSem := time.After(channelTimeout)\n\tt.Log(\"blocking an L0 -> LBase compaction\")\n\t// Write an sstable to L0 until we're blocked on an L0 -> LBase compaction.\n\tbreakLoop := false\n\tfor !breakLoop {\n\t\tselect {\n\t\tcase sem := <-nextSem:\n\t\t\tbaseCompactionSem = sem\n\t\t\tbreakLoop = true\n\t\tcase <-timeoutSem:\n\t\t\tt.Fatal(\"did not get blocked on an LBase compaction\")\n\t\tdefault:\n\t\t\trequire.NoError(t, d.Set([]byte(\"b\"), []byte(\"foo\"), nil))\n\t\t\trequire.NoError(t, d.Set([]byte(\"g\"), []byte(\"bar\"), nil))\n\t\t\trequire.NoError(t, d.Flush())\n\t\t\ttime.Sleep(100 * time.Millisecond)\n\t\t}\n\t}\n\tprintLSM()\n\n\t// Do 4 ingests, one with the key cc, one with bb and cc, and two with just bb.\n\t// The purpose of the sstable containing cc is to inflate the L0 sublevel\n\t// count of the interval at cc, as that's where we want the intra-L0 compaction\n\t// to be seeded. However we also need a file left of that interval to have\n\t// the same (or higher) sublevel to trigger the bug in\n\t// cockroachdb/cockroach#101896. That's why we ingest a file after it to\n\t// \"bridge\" the bb/cc intervals, and then ingest a file at bb. These go\n\t// into sublevels like this:\n\t//\n\t//    bb\n\t//    bb\n\t//    bb-----cc\n\t//           cc\n\t//\n\t// Eventually, we'll drop an ingested file containing a range del starting at\n\t// cc around here:\n\t//\n\t//    bb\n\t//    bb     cc---...\n\t//    bb-----cc\n\t//           cc\n\t{\n\t\tpath := \"ingest1.sst\"\n\t\tf, err := memFS.Create(path, vfs.WriteCategoryUnspecified)\n\t\trequire.NoError(t, err)\n\t\tw := sstable.NewWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{\n\t\t\tTableFormat: d.TableFormat(),\n\t\t})\n\t\trequire.NoError(t, w.Set([]byte(\"cc\"), []byte(\"foo\")))\n\t\trequire.NoError(t, w.Close())\n\t\trequire.NoError(t, d.Ingest(context.Background(), []string{path}))\n\t}\n\t{\n\t\tpath := \"ingest2.sst\"\n\t\tf, err := memFS.Create(path, vfs.WriteCategoryUnspecified)\n\t\trequire.NoError(t, err)\n\t\tw := sstable.NewWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{\n\t\t\tTableFormat: d.TableFormat(),\n\t\t})\n\t\trequire.NoError(t, w.Set([]byte(\"bb\"), []byte(\"foo2\")))\n\t\trequire.NoError(t, w.Set([]byte(\"cc\"), []byte(\"foo2\")))\n\t\trequire.NoError(t, w.Close())\n\t\trequire.NoError(t, d.Ingest(context.Background(), []string{path}))\n\t}\n\t{\n\t\tpath := \"ingest3.sst\"\n\t\tf, err := memFS.Create(path, vfs.WriteCategoryUnspecified)\n\t\trequire.NoError(t, err)\n\t\tw := sstable.NewWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{\n\t\t\tTableFormat: d.TableFormat(),\n\t\t})\n\t\trequire.NoError(t, w.Set([]byte(\"bb\"), []byte(\"foo3\")))\n\t\trequire.NoError(t, w.Close())\n\t\trequire.NoError(t, d.Ingest(context.Background(), []string{path}))\n\t}\n\t{\n\t\tpath := \"ingest4.sst\"\n\t\tf, err := memFS.Create(path, vfs.WriteCategoryUnspecified)\n\t\trequire.NoError(t, err)\n\t\tw := sstable.NewWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{\n\t\t\tTableFormat: d.TableFormat(),\n\t\t})\n\t\trequire.NoError(t, w.Set([]byte(\"bb\"), []byte(\"foo4\")))\n\t\trequire.NoError(t, w.Close())\n\t\trequire.NoError(t, d.Ingest(context.Background(), []string{path}))\n\t}\n\n\t// We now have a base compaction blocked. Block a memtable flush to cause\n\t// memtables to queue up.\n\t//\n\t// Memtable (stuck):\n\t//\n\t//   b-----------------g\n\t//\n\t// Relevant L0 ssstables\n\t//\n\t//    bb\n\t//    bb\n\t//    bb-----cc\n\t//           cc\n\tblockedCompactionsMu.Lock()\n\tnextBlockedCompaction = 2\n\tblockedCompactionsMu.Unlock()\n\tt.Log(\"blocking a flush\")\n\trequire.NoError(t, d.Set([]byte(\"b\"), []byte(\"foo2\"), nil))\n\trequire.NoError(t, d.Set([]byte(\"g\"), []byte(\"bar2\"), nil))\n\t_, _ = d.AsyncFlush()\n\tselect {\n\tcase sem := <-nextSem:\n\t\tflushSem = sem\n\tcase <-time.After(channelTimeout):\n\t\tt.Fatal(\"did not get blocked on a flush\")\n\t}\n\t// Add one memtable to flush queue, and finish it off.\n\t//\n\t// Memtables (stuck):\n\t//\n\t//   b-----------------g (waiting to flush)\n\t//   b-----------------g (flushing, blocked)\n\t//\n\t// Relevant L0 ssstables\n\t//\n\t//    bb\n\t//    bb\n\t//    bb-----cc\n\t//           cc\n\trequire.NoError(t, d.Set([]byte(\"b\"), []byte(\"foo3\"), nil))\n\trequire.NoError(t, d.Set([]byte(\"g\"), []byte(\"bar3\"), nil))\n\t// note: this flush will wait for the earlier, blocked flush, but it closes\n\t// off the memtable which is what we want.\n\t_, _ = d.AsyncFlush()\n\n\t// Open a new mutable memtable. This gets us an earlier earlierUnflushedSeqNum\n\t// than the ingest below it.\n\trequire.NoError(t, d.Set([]byte(\"c\"), []byte(\"somethingbigishappening\"), nil))\n\t// Block an intra-L0 compaction, as one might happen around this time.\n\tblockedCompactionsMu.Lock()\n\tnextBlockedCompaction = 1\n\tblockedCompactionsMu.Unlock()\n\tt.Log(\"blocking an intra-L0 compaction\")\n\t// Ingest a file containing a cc-e rangedel.\n\t//\n\t// Memtables:\n\t//\n\t//         c             (mutable)\n\t//   b-----------------g (waiting to flush)\n\t//   b-----------------g (flushing, blocked)\n\t//\n\t// Relevant L0 ssstables\n\t//\n\t//    bb\n\t//    bb     cc-----e (just ingested)\n\t//    bb-----cc\n\t//           cc\n\t{\n\t\tpath := \"ingest5.sst\"\n\t\tf, err := memFS.Create(path, vfs.WriteCategoryUnspecified)\n\t\trequire.NoError(t, err)\n\t\tw := sstable.NewWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{\n\t\t\tTableFormat: d.TableFormat(),\n\t\t})\n\t\trequire.NoError(t, w.DeleteRange([]byte(\"cc\"), []byte(\"e\")))\n\t\trequire.NoError(t, w.Close())\n\t\trequire.NoError(t, d.Ingest(context.Background(), []string{path}))\n\t}\n\tt.Log(\"main ingest complete\")\n\tprintLSM()\n\tt.Logf(\"%s\", d.Metrics().String())\n\n\trequire.NoError(t, d.Set([]byte(\"d\"), []byte(\"ThisShouldNotBeDeleted\"), nil))\n\n\t// Do another ingest with a seqnum newer than d. The purpose of this is to\n\t// increase the LargestSeqNum of the intra-L0 compaction output *beyond*\n\t// the flush that contains d=ThisShouldNotBeDeleted, therefore causing\n\t// that point key to be deleted (in the buggy code).\n\t//\n\t// Memtables:\n\t//\n\t//         c-----d       (mutable)\n\t//   b-----------------g (waiting to flush)\n\t//   b-----------------g (flushing, blocked)\n\t//\n\t// Relevant L0 ssstables\n\t//\n\t//    bb     cc\n\t//    bb     cc-----e (just ingested)\n\t//    bb-----cc\n\t//           cc\n\t{\n\t\tpath := \"ingest6.sst\"\n\t\tf, err := memFS.Create(path, vfs.WriteCategoryUnspecified)\n\t\trequire.NoError(t, err)\n\t\tw := sstable.NewWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{\n\t\t\tTableFormat: d.TableFormat(),\n\t\t})\n\t\trequire.NoError(t, w.Set([]byte(\"cc\"), []byte(\"doesntmatter\")))\n\t\trequire.NoError(t, w.Close())\n\t\trequire.NoError(t, d.Ingest(context.Background(), []string{path}))\n\t}\n\n\t// Unblock earlier flushes. We will first finish flushing the blocked\n\t// memtable, and end up in this state:\n\t//\n\t// Memtables:\n\t//\n\t//         c-----d       (mutable)\n\t//   b-----------------g (waiting to flush)\n\t//\n\t// Relevant L0 ssstables\n\t//\n\t//  b-------------------g (irrelevant, just flushed)\n\t//    bb     cc (has LargestSeqNum > earliestUnflushedSeqNum)\n\t//    bb     cc-----e (has a rangedel)\n\t//    bb-----cc\n\t//           cc\n\t//\n\t// Note that while b----g is relatively old (and so has a low LargestSeqNum),\n\t// it bridges a bunch of intervals. Had we regenerated sublevels from scratch,\n\t// it'd have gone below the cc-e sstable. But due to #101896, we just slapped\n\t// it on top. Now, as long as our seed interval is the one at cc and our seed\n\t// file is the just-flushed L0 sstable, we will go down and include anything\n\t// in that interval even if it has a LargestSeqNum > earliestUnflushedSeqNum.\n\t//\n\t// All asterisked L0 sstables should now get picked in an intra-L0 compaction\n\t// right after the flush finishes, that we then block:\n\t//\n\t//  b-------------------g*\n\t//    bb*    cc*\n\t//    bb*    cc-----e*\n\t//    bb-----cc*\n\t//           cc*\n\tt.Log(\"unblocking flush\")\n\tflushSem <- struct{}{}\n\tprintLSM()\n\n\tselect {\n\tcase sem := <-nextSem:\n\t\tintraL0Sem = sem\n\tcase <-time.After(channelTimeout):\n\t\tt.Fatal(\"did not get blocked on an intra L0 compaction\")\n\t}\n\n\t// Ensure all memtables are flushed. This will mean d=ThisShouldNotBeDeleted\n\t// will land in L0 and since that was the last key written to a memtable,\n\t// and the ingestion at cc came after it, the output of the intra-L0\n\t// compaction will elevate the cc-e rangedel above it and delete it\n\t// (if #101896 is not fixed).\n\tch, _ := d.AsyncFlush()\n\t<-ch\n\n\t// Unblock earlier intra-L0 compaction.\n\tt.Log(\"unblocking intraL0\")\n\tintraL0Sem <- struct{}{}\n\tprintLSM()\n\n\t// Try reading d a couple times.\n\tfor i := 0; i < 2; i++ {\n\t\tval, closer, err := d.Get([]byte(\"d\"))\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, []byte(\"ThisShouldNotBeDeleted\"), val)\n\t\tif closer != nil {\n\t\t\tcloser.Close()\n\t\t}\n\t\ttime.Sleep(100 * time.Millisecond)\n\t}\n\n\t// Unblock everything.\n\tbaseCompactionSem <- struct{}{}\n}\n\nfunc BenchmarkDelete(b *testing.B) {\n\trng := rand.New(rand.NewPCG(0, uint64(time.Now().UnixNano())))\n\tconst keyCount = 10000\n\tvar keys [keyCount][]byte\n\tfor i := 0; i < keyCount; i++ {\n\t\tkeys[i] = []byte(strconv.Itoa(rng.Int()))\n\t}\n\tval := bytes.Repeat([]byte(\"x\"), 10)\n\n\tbenchmark := func(b *testing.B, useSingleDelete bool) {\n\t\td, err := Open(\n\t\t\t\"\",\n\t\t\t&Options{\n\t\t\t\tFS: vfs.NewMem(),\n\t\t\t})\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t\tdefer func() {\n\t\t\tif err := d.Close(); err != nil {\n\t\t\t\tb.Fatal(err)\n\t\t\t}\n\t\t}()\n\n\t\tb.StartTimer()\n\t\tfor _, key := range keys {\n\t\t\t_ = d.Set(key, val, nil)\n\t\t\tif useSingleDelete {\n\t\t\t\t_ = d.SingleDelete(key, nil)\n\t\t\t} else {\n\t\t\t\t_ = d.Delete(key, nil)\n\t\t\t}\n\t\t}\n\t\t// Manually flush as it is flushing/compaction where SingleDelete\n\t\t// performance shows up. With SingleDelete, we can elide all of the\n\t\t// SingleDelete and Set records.\n\t\tif err := d.Flush(); err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t\tb.StopTimer()\n\t}\n\n\tb.Run(\"delete\", func(b *testing.B) {\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\tbenchmark(b, false)\n\t\t}\n\t})\n\n\tb.Run(\"single-delete\", func(b *testing.B) {\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\tbenchmark(b, true)\n\t\t}\n\t})\n}\n\nfunc BenchmarkNewIterReadAmp(b *testing.B) {\n\tfor _, readAmp := range []int{10, 100, 1000} {\n\t\tb.Run(strconv.Itoa(readAmp), func(b *testing.B) {\n\t\t\topts := &Options{\n\t\t\t\tFS:                    vfs.NewMem(),\n\t\t\t\tL0StopWritesThreshold: 1000,\n\t\t\t}\n\t\t\topts.DisableAutomaticCompactions = true\n\n\t\t\td, err := Open(\"\", opts)\n\t\t\trequire.NoError(b, err)\n\n\t\t\tfor i := 0; i < readAmp; i++ {\n\t\t\t\trequire.NoError(b, d.Set([]byte(\"a\"), []byte(\"b\"), NoSync))\n\t\t\t\trequire.NoError(b, d.Flush())\n\t\t\t}\n\n\t\t\trequire.Equal(b, d.Metrics().ReadAmp(), readAmp)\n\n\t\t\tb.StopTimer()\n\t\t\tb.ResetTimer()\n\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\tb.StartTimer()\n\t\t\t\titer, _ := d.NewIter(nil)\n\t\t\t\tb.StopTimer()\n\t\t\t\trequire.NoError(b, iter.Close())\n\t\t\t}\n\n\t\t\trequire.NoError(b, d.Close())\n\t\t})\n\t}\n}\n\nfunc verifyGet(t *testing.T, r Reader, key, expected []byte) {\n\tval, closer, err := r.Get(key)\n\trequire.NoError(t, err)\n\tif !bytes.Equal(expected, val) {\n\t\tt.Fatalf(\"expected %s, but got %s\", expected, val)\n\t}\n\tcloser.Close()\n}\n\nfunc verifyGetNotFound(t *testing.T, r Reader, key []byte) {\n\tval, _, err := r.Get(key)\n\tif err != base.ErrNotFound {\n\t\tt.Fatalf(\"expected nil, but got %s\", val)\n\t}\n}\n\nfunc BenchmarkRotateMemtables(b *testing.B) {\n\to := &Options{FS: vfs.NewMem(), MemTableSize: 64 << 20 /* 64 MB */}\n\td, err := Open(\"\", o)\n\trequire.NoError(b, err)\n\n\t// We want to jump to full-sized memtables.\n\td.mu.Lock()\n\td.mu.mem.nextSize = o.MemTableSize\n\td.mu.Unlock()\n\trequire.NoError(b, d.Flush())\n\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\tif err := d.Flush(); err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n}\n\n// TODO(sumeer): rewrite test when LogRecycler is hidden from this package.\nfunc TestRecycleLogs(t *testing.T) {\n\tmem := vfs.NewMem()\n\td, err := Open(\"\", &Options{\n\t\tFS: mem,\n\t})\n\trequire.NoError(t, err)\n\n\tlogNum := func() base.DiskFileNum {\n\t\td.mu.Lock()\n\t\tdefer d.mu.Unlock()\n\t\twalNums := d.mu.log.manager.List()\n\t\treturn base.DiskFileNum(walNums[len(walNums)-1].Num)\n\t}\n\tlogCount := func() int {\n\t\td.mu.Lock()\n\t\tdefer d.mu.Unlock()\n\t\twalNums := d.mu.log.manager.List()\n\t\treturn len(walNums)\n\t}\n\n\trecycler := d.mu.log.manager.RecyclerForTesting()\n\t// Flush the memtable a few times, forcing rotation of the WAL. We should see\n\t// the recycled logs change as expected.\n\trequire.EqualValues(t, []base.DiskFileNum(nil), recycler.LogNumsForTesting())\n\tcurLog := logNum()\n\n\trequire.NoError(t, d.Flush())\n\n\trequire.EqualValues(t, []base.DiskFileNum{curLog}, recycler.LogNumsForTesting())\n\tcurLog = logNum()\n\n\trequire.NoError(t, d.Flush())\n\n\trequire.EqualValues(t, []base.DiskFileNum{curLog}, recycler.LogNumsForTesting())\n\n\trequire.NoError(t, d.Close())\n\n\td, err = Open(\"\", &Options{\n\t\tFS:     mem,\n\t\tLogger: testLogger{t},\n\t})\n\trequire.NoError(t, err)\n\trecycler = d.mu.log.manager.RecyclerForTesting()\n\tmetrics := d.Metrics()\n\tif n := logCount(); n != int(metrics.WAL.Files) {\n\t\tt.Fatalf(\"expected %d WAL files, but found %d\", n, metrics.WAL.Files)\n\t}\n\tif n, sz := recycler.Stats(); n != int(metrics.WAL.ObsoleteFiles) {\n\t\tt.Fatalf(\"expected %d obsolete WAL files, but found %d\", n, metrics.WAL.ObsoleteFiles)\n\t} else if sz != metrics.WAL.ObsoletePhysicalSize {\n\t\tt.Fatalf(\"expected %d obsolete physical WAL size, but found %d\", sz, metrics.WAL.ObsoletePhysicalSize)\n\t}\n\tif recycled := recycler.LogNumsForTesting(); len(recycled) != 0 {\n\t\tt.Fatalf(\"expected no recycled WAL files after recovery, but found %d\", recycled)\n\t}\n\trequire.NoError(t, d.Close())\n}\n\ntype sstAndLogFileBlockingFS struct {\n\tvfs.FS\n\tunblocker sync.WaitGroup\n}\n\nvar _ vfs.FS = &sstAndLogFileBlockingFS{}\n\nfunc (fs *sstAndLogFileBlockingFS) Create(\n\tname string, category vfs.DiskWriteCategory,\n) (vfs.File, error) {\n\tif strings.HasSuffix(name, \".log\") || strings.HasSuffix(name, \".sst\") {\n\t\tfs.unblocker.Wait()\n\t}\n\treturn fs.FS.Create(name, category)\n}\n\nfunc (fs *sstAndLogFileBlockingFS) unblock() {\n\tfs.unblocker.Done()\n}\n\nfunc newBlockingFS(fs vfs.FS) *sstAndLogFileBlockingFS {\n\tlfbfs := &sstAndLogFileBlockingFS{FS: fs}\n\tlfbfs.unblocker.Add(1)\n\treturn lfbfs\n}\n\nfunc TestWALFailoverAvoidsWriteStall(t *testing.T) {\n\tmem := vfs.NewMem()\n\t// All sst and log creation is blocked.\n\tprimaryFS := newBlockingFS(mem)\n\t// Secondary for WAL failover can do log creation.\n\tsecondary := wal.Dir{FS: mem, Dirname: \"secondary\"}\n\twalFailover := &WALFailoverOptions{Secondary: secondary, FailoverOptions: wal.FailoverOptions{\n\t\tUnhealthySamplingInterval:          100 * time.Millisecond,\n\t\tUnhealthyOperationLatencyThreshold: func() (time.Duration, bool) { return time.Second, true },\n\t}}\n\to := &Options{\n\t\tFS:                          primaryFS,\n\t\tMemTableSize:                4 << 20,\n\t\tMemTableStopWritesThreshold: 2,\n\t\tWALFailover:                 walFailover,\n\t}\n\td, err := Open(\"\", o)\n\trequire.NoError(t, err)\n\tdefer d.Close()\n\tvalue := make([]byte, 1<<20)\n\tfor i := range value {\n\t\tvalue[i] = byte(rand.Uint32())\n\t}\n\t// After ~8 writes, the default write stall threshold is exceeded, but the\n\t// writes will not block indefinitely since failover has or will happen, and\n\t// wal.Manager.ElevateWriteStallThresholdForFailover() will return true.\n\tfor i := 0; i < 200; i++ {\n\t\trequire.NoError(t, d.Set([]byte(fmt.Sprintf(\"%d\", i)), value, nil))\n\t}\n\t// Validate that the default write stall threshold was exceeded.\n\trequire.Greater(\n\t\tt, d.Metrics().MemTable.Size, o.MemTableSize*uint64(o.MemTableStopWritesThreshold))\n\t// Unblock the writes to allow the DB to close.\n\tprimaryFS.unblock()\n}\n\n// TestDeterminism is a datadriven test intended to validate determinism of\n// operations in the face of concurrency or randomizing of operations. The test\n// data defines a sequence of commands run sequentially. Then the test may\n// re-run the sequence introducing latencies, reorderings, parallelism, etc,\n// ensuring that all re-runs produce the same output.\nfunc TestDeterminism(t *testing.T) {\n\tvar d *DB\n\tvar fs vfs.FS = vfs.NewMem()\n\tdefer func() {\n\t\tif d != nil {\n\t\t\trequire.NoError(t, d.Close())\n\t\t}\n\t}()\n\n\ttype step struct {\n\t\tfn     func(td *datadriven.TestData) string\n\t\ttd     datadriven.TestData\n\t\toutput string\n\t}\n\tvar sequence []step\n\taddStep := func(td *datadriven.TestData, fn func(td *datadriven.TestData) string) string {\n\t\ts := strings.TrimSpace(fn(td))\n\t\tsequence = append(sequence, step{\n\t\t\tfn:     fn,\n\t\t\ttd:     *td,\n\t\t\toutput: s,\n\t\t})\n\t\tif len(s) > 0 {\n\t\t\ts = s + \"\\n\"\n\t\t}\n\t\treturn s + fmt.Sprintf(\"%d:%s\", len(sequence)-1, td.Cmd)\n\t}\n\n\tdatadriven.RunTest(t, \"testdata/determinism\",\n\t\tfunc(t *testing.T, td *datadriven.TestData) string {\n\t\t\tswitch td.Cmd {\n\t\t\tcase \"reset\":\n\t\t\t\tfs = vfs.NewMem()\n\t\t\t\tsequence = nil\n\t\t\t\treturn \"\"\n\t\t\tcase \"define\":\n\t\t\t\treturn addStep(td, func(td *datadriven.TestData) string {\n\t\t\t\t\topts := &Options{\n\t\t\t\t\t\tFS:                          fs,\n\t\t\t\t\t\tDebugCheck:                  DebugCheckLevels,\n\t\t\t\t\t\tLogger:                      testLogger{t},\n\t\t\t\t\t\tFormatMajorVersion:          FormatNewest,\n\t\t\t\t\t\tDisableAutomaticCompactions: true,\n\t\t\t\t\t}\n\t\t\t\t\topts.Experimental.IngestSplit = func() bool { return rand.IntN(2) == 1 }\n\t\t\t\t\tvar err error\n\t\t\t\t\tif d, err = runDBDefineCmdReuseFS(td, opts); err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t\treturn d.mu.versions.currentVersion().String()\n\t\t\t\t})\n\t\t\tcase \"batch\":\n\t\t\t\treturn addStep(td, func(td *datadriven.TestData) string {\n\t\t\t\t\tb := d.NewBatch()\n\t\t\t\t\trequire.NoError(t, runBatchDefineCmd(td, b))\n\t\t\t\t\trequire.NoError(t, b.Commit(nil))\n\t\t\t\t\treturn \"\"\n\t\t\t\t})\n\t\t\tcase \"build\":\n\t\t\t\treturn addStep(td, func(td *datadriven.TestData) string {\n\t\t\t\t\trequire.NoError(t, runBuildCmd(td, d, fs))\n\t\t\t\t\treturn \"\"\n\t\t\t\t})\n\t\t\tcase \"flush\":\n\t\t\t\treturn addStep(td, func(td *datadriven.TestData) string {\n\t\t\t\t\t_, err := d.AsyncFlush()\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t\treturn \"\"\n\t\t\t\t})\n\t\t\tcase \"ingest-and-excise\":\n\t\t\t\treturn addStep(td, func(td *datadriven.TestData) string {\n\t\t\t\t\tif err := runIngestAndExciseCmd(td, d, fs); err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t\treturn \"\"\n\t\t\t\t})\n\t\t\tcase \"maybe-compact\":\n\t\t\t\treturn addStep(td, func(td *datadriven.TestData) string {\n\t\t\t\t\td.mu.Lock()\n\t\t\t\t\tdefer d.mu.Unlock()\n\t\t\t\t\td.opts.DisableAutomaticCompactions = false\n\t\t\t\t\td.maybeScheduleCompaction()\n\t\t\t\t\td.opts.DisableAutomaticCompactions = true\n\t\t\t\t\treturn \"\"\n\t\t\t\t})\n\t\t\tcase \"memtable-info\":\n\t\t\t\treturn addStep(td, func(td *datadriven.TestData) string {\n\t\t\t\t\td.commit.mu.Lock()\n\t\t\t\t\tdefer d.commit.mu.Unlock()\n\t\t\t\t\td.mu.Lock()\n\t\t\t\t\tdefer d.mu.Unlock()\n\t\t\t\t\tvar buf bytes.Buffer\n\t\t\t\t\tfmt.Fprintf(&buf, \"flushable queue: %d entries\\n\", len(d.mu.mem.queue))\n\t\t\t\t\tfmt.Fprintf(&buf, \"mutable:\\n\")\n\t\t\t\t\tfmt.Fprintf(&buf, \"  alloced:  %d\\n\", d.mu.mem.mutable.totalBytes())\n\t\t\t\t\tif td.HasArg(\"reserved\") {\n\t\t\t\t\t\tfmt.Fprintf(&buf, \"  reserved: %d\\n\", d.mu.mem.mutable.reserved)\n\t\t\t\t\t}\n\t\t\t\t\tif td.HasArg(\"in-use\") {\n\t\t\t\t\t\tfmt.Fprintf(&buf, \"  in-use:   %d\\n\", d.mu.mem.mutable.inuseBytes())\n\t\t\t\t\t}\n\t\t\t\t\treturn buf.String()\n\t\t\t\t})\n\t\t\tcase \"run\":\n\t\t\t\tvar mkfs func() vfs.FS = func() vfs.FS { return vfs.NewMem() }\n\t\t\t\tvar beforeStep func() = func() {}\n\t\t\t\tfor _, cmdArg := range td.CmdArgs {\n\t\t\t\t\tswitch cmdArg.Key {\n\t\t\t\t\tcase \"io-latency\", \"step-latency\":\n\t\t\t\t\t\tp, err := strconv.ParseFloat(cmdArg.Vals[0], 64)\n\t\t\t\t\t\trequire.NoError(t, err)\n\t\t\t\t\t\tmean, err := time.ParseDuration(cmdArg.Vals[1])\n\t\t\t\t\t\trequire.NoError(t, err)\n\t\t\t\t\t\tif cmdArg.Key == \"io-latency\" {\n\t\t\t\t\t\t\tprevMkfs := mkfs\n\t\t\t\t\t\t\tmkfs = func() vfs.FS {\n\t\t\t\t\t\t\t\treturn errorfs.Wrap(prevMkfs(), errorfs.RandomLatency(\n\t\t\t\t\t\t\t\t\terrorfs.Randomly(p, 0), mean, 0 /* seed */, 0 /* no limit */))\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t} else if cmdArg.Key == \"step-latency\" {\n\t\t\t\t\t\t\tbeforeStep = func() {\n\t\t\t\t\t\t\t\tif rand.Float64() < p {\n\t\t\t\t\t\t\t\t\ttime.Sleep(time.Duration(min(rand.ExpFloat64(), 20.0) * float64(mean)))\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tordering := parseOrdering(td.Input)\n\n\t\t\t\tvar sb strings.Builder\n\t\t\t\trerunSequence := func() string {\n\t\t\t\t\tsb.Reset()\n\t\t\t\t\tfs = mkfs()\n\t\t\t\t\toutput := make([]string, len(sequence))\n\t\t\t\t\tordering.visit(func(i int) {\n\t\t\t\t\t\tbeforeStep()\n\t\t\t\t\t\toutput[i] = strings.TrimSpace(sequence[i].fn(&sequence[i].td))\n\t\t\t\t\t})\n\t\t\t\t\tfor i := range output {\n\t\t\t\t\t\tif output[i] != sequence[i].output {\n\t\t\t\t\t\t\tfmt.Fprintf(&sb, \"# step %d: %s\\n\", i, sequence[i].td.Cmd)\n\t\t\t\t\t\t\tfmt.Fprintf(&sb, \"expected:\\n%s\\ngot:\\n%s\", sequence[i].output, output[i])\n\t\t\t\t\t\t\tfmt.Fprintln(&sb)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\treturn sb.String()\n\t\t\t\t}\n\t\t\t\tretries := 10\n\t\t\t\ttd.MaybeScanArgs(t, \"count\", &retries)\n\t\t\t\tfor i := 0; i < retries; i++ {\n\t\t\t\t\tif diff := rerunSequence(); len(diff) > 0 {\n\t\t\t\t\t\treturn diff\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn \"ok\"\n\t\t\tdefault:\n\t\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t\t}\n\t\t})\n}\n\ntype orderingNode interface {\n\tvisit(func(int))\n}\n\ntype sequential []orderingNode\n\nfunc (s sequential) visit(fn func(int)) {\n\tfor _, n := range s {\n\t\tn.visit(fn)\n\t}\n}\n\ntype reorder []orderingNode\n\nfunc (r reorder) visit(fn func(int)) {\n\tfor _, i := range rand.Perm(len(r)) {\n\t\tr[i].visit(fn)\n\t}\n}\n\ntype parallel []orderingNode\n\nfunc (p parallel) visit(fn func(int)) {\n\tvar wg sync.WaitGroup\n\twg.Add(len(p))\n\tfor i := range p {\n\t\tgo func(i int) {\n\t\t\tdefer wg.Done()\n\t\t\tp[i].visit(fn)\n\t\t}(i)\n\t}\n\twg.Wait()\n}\n\ntype leaf int\n\nfunc (l leaf) visit(fn func(int)) { fn(int(l)) }\n\nfunc parseOrdering(s string) orderingNode {\n\tn, _ := parseOrderingTokens(strings.Fields(s))\n\treturn n\n}\n\nfunc parseOrderingTokens(tokens []string) (orderingNode, int) {\n\tif len(tokens) == 0 {\n\t\treturn nil, 0\n\t}\n\tswitch tokens[0] {\n\tcase \")\":\n\t\tpanic(\"unexpected )\")\n\tcase \"sequential(\", \"reorder(\", \"parallel(\":\n\t\tvar nodes []orderingNode\n\t\ti := 1\n\t\tfor i < len(tokens) {\n\t\t\tif tokens[i] == \")\" {\n\t\t\t\ti++\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tn, m := parseOrderingTokens(tokens[i:])\n\t\t\tnodes = append(nodes, n)\n\t\t\ti += m\n\t\t}\n\t\tswitch tokens[0] {\n\t\tcase \"sequential(\":\n\t\t\treturn sequential(nodes), i\n\t\tcase \"reorder(\":\n\t\t\treturn reorder(nodes), i\n\t\tcase \"parallel(\":\n\t\t\treturn parallel(nodes), i\n\t\tdefault:\n\t\t\tpanic(\"unreachable\")\n\t\t}\n\tdefault:\n\t\tn := strings.IndexByte(tokens[0], ':')\n\t\tif n == -1 {\n\t\t\tn = len(tokens[0])\n\t\t}\n\t\tv, err := strconv.Atoi(tokens[0][:n])\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\treturn leaf(v), 1\n\t}\n}\n\ntype readTrackFS struct {\n\tvfs.FS\n\n\tcurrReadCount atomic.Int32\n\tmaxReadCount  atomic.Int32\n}\n\ntype readTrackFile struct {\n\tvfs.File\n\tfs *readTrackFS\n}\n\nfunc (fs *readTrackFS) Open(name string, opts ...vfs.OpenOption) (vfs.File, error) {\n\tfile, err := fs.FS.Open(name, opts...)\n\tif err != nil || !strings.HasSuffix(name, \".sst\") {\n\t\treturn file, err\n\t}\n\treturn &readTrackFile{\n\t\tFile: file,\n\t\tfs:   fs,\n\t}, nil\n}\n\nfunc (f *readTrackFile) ReadAt(p []byte, off int64) (n int, err error) {\n\tval := f.fs.currReadCount.Add(1)\n\tdefer f.fs.currReadCount.Add(-1)\n\tfor maxVal := f.fs.maxReadCount.Load(); val > maxVal; maxVal = f.fs.maxReadCount.Load() {\n\t\tif f.fs.maxReadCount.CompareAndSwap(maxVal, val) {\n\t\t\tbreak\n\t\t}\n\t}\n\treturn f.File.ReadAt(p, off)\n}\n\nfunc TestLoadBlockSema(t *testing.T) {\n\tfs := &readTrackFS{FS: vfs.NewMem()}\n\tsema := fifo.NewSemaphore(100)\n\tdb, err := Open(\"\", testingRandomized(t, &Options{\n\t\tCache:         cache.New(1),\n\t\tFS:            fs,\n\t\tLoadBlockSema: sema,\n\t}))\n\trequire.NoError(t, err)\n\n\tkey := func(i, j int) []byte {\n\t\treturn []byte(fmt.Sprintf(\"%02d/%02d\", i, j))\n\t}\n\n\t// Create 20 regions and compact them separately, so we end up with 20\n\t// disjoint tables.\n\tconst numRegions = 20\n\tconst numKeys = 20\n\tfor i := 0; i < numRegions; i++ {\n\t\tfor j := 0; j < numKeys; j++ {\n\t\t\trequire.NoError(t, db.Set(key(i, j), []byte(\"value\"), nil))\n\t\t}\n\t\trequire.NoError(t, db.Compact(key(i, 0), key(i, numKeys-1), false))\n\t}\n\n\t// Read all regions to warm up the file cache.\n\tfor i := 0; i < numRegions; i++ {\n\t\tval, closer, err := db.Get(key(i, 1))\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, []byte(\"value\"), val)\n\t\tif closer != nil {\n\t\t\tcloser.Close()\n\t\t}\n\t}\n\n\tfor _, n := range []int64{1, 2, 4} {\n\t\tt.Run(fmt.Sprintf(\"%d\", n), func(t *testing.T) {\n\t\t\tsema.UpdateCapacity(n)\n\t\t\tfs.maxReadCount.Store(0)\n\t\t\tvar wg sync.WaitGroup\n\t\t\t// Spin up workers that perform random reads.\n\t\t\tconst numWorkers = 20\n\t\t\tfor i := 0; i < numWorkers; i++ {\n\t\t\t\twg.Add(1)\n\t\t\t\tgo func() {\n\t\t\t\t\tdefer wg.Done()\n\t\t\t\t\tconst numQueries = 100\n\t\t\t\t\tfor i := 0; i < numQueries; i++ {\n\t\t\t\t\t\tval, closer, err := db.Get(key(rand.IntN(numRegions), rand.IntN(numKeys)))\n\t\t\t\t\t\trequire.NoError(t, err)\n\t\t\t\t\t\trequire.Equal(t, []byte(\"value\"), val)\n\t\t\t\t\t\tif closer != nil {\n\t\t\t\t\t\t\tcloser.Close()\n\t\t\t\t\t\t}\n\t\t\t\t\t\truntime.Gosched()\n\t\t\t\t\t}\n\t\t\t\t}()\n\t\t\t}\n\t\t\twg.Wait()\n\t\t\t// Verify the maximum read count did not exceed the limit.\n\t\t\tmaxReadCount := fs.maxReadCount.Load()\n\t\t\trequire.Greater(t, maxReadCount, int32(0))\n\t\t\trequire.LessOrEqual(t, maxReadCount, int32(n))\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "download.go",
          "type": "blob",
          "size": 19.08984375,
          "content": "// Copyright 2024 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"cmp\"\n\t\"context\"\n\t\"fmt\"\n\t\"slices\"\n\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n\t\"github.com/cockroachdb/pebble/objstorage\"\n)\n\n// DownloadSpan is a key range passed to the Download method.\ntype DownloadSpan struct {\n\tStartKey []byte\n\t// EndKey is exclusive.\n\tEndKey []byte\n\t// ViaBackingFileDownload, if true, indicates the span should be downloaded by\n\t// downloading any remote backing files byte-for-byte and replacing them with\n\t// the downloaded local files, while otherwise leaving the virtual SSTables\n\t// as-is. If false, a \"normal\" rewriting compaction of the span, that iterates\n\t// the keys and produces a new SSTable, is used instead. Downloading raw files\n\t// can be faster when the whole file is being downloaded, as it avoids some\n\t// cpu-intensive steps involved in iteration and new file construction such as\n\t// compression, however it can also be wasteful when only a small portion of a\n\t// larger backing file is being used by a virtual file. Additionally, if the\n\t// virtual file has expensive read-time transformations, such as prefix\n\t// replacement, rewriting once can persist the result of these for future use\n\t// while copying only the backing file will obligate future reads to continue\n\t// to compute such transforms.\n\tViaBackingFileDownload bool\n}\n\n// Download ensures that the LSM does not use any external sstables for the\n// given key ranges. It does so by performing appropriate compactions so that\n// all external data becomes available locally.\n//\n// Note that calling this method does not imply that all other compactions stop;\n// it simply informs Pebble of a list of spans for which external data should be\n// downloaded with high priority.\n//\n// The method returns once no external sstables overlap the given spans, the\n// context is canceled, the db is closed, or an error is hit.\n//\n// Note that despite the best effort of this method, if external ingestions\n// happen in parallel, a new external file can always appear right as we're\n// returning.\n//\n// TODO(radu): consider passing a priority/impact knob to express how important\n// the download is (versus live traffic performance, LSM health).\nfunc (d *DB) Download(ctx context.Context, spans []DownloadSpan) error {\n\tctx, cancel := context.WithCancel(ctx)\n\tdefer cancel()\n\tif err := d.closed.Load(); err != nil {\n\t\tpanic(err)\n\t}\n\tif d.opts.ReadOnly {\n\t\treturn ErrReadOnly\n\t}\n\tinfo := DownloadInfo{\n\t\tJobID: int(d.newJobID()),\n\t\tSpans: spans,\n\t}\n\tstartTime := d.timeNow()\n\td.opts.EventListener.DownloadBegin(info)\n\n\tfor info.RestartCount = 0; ; info.RestartCount++ {\n\t\ttasks := d.createDownloadTasks(spans)\n\t\tinfo.Duration = d.timeNow().Sub(startTime)\n\t\tif len(tasks) == 0 {\n\t\t\t// We are done.\n\t\t\tinfo.Done = true\n\t\t\td.opts.EventListener.DownloadEnd(info)\n\t\t\treturn nil\n\t\t}\n\t\tif info.RestartCount > 0 {\n\t\t\td.opts.EventListener.DownloadBegin(info)\n\t\t}\n\n\t\t// Install the tasks.\n\t\td.mu.Lock()\n\t\td.mu.compact.downloads = append(d.mu.compact.downloads, tasks...)\n\t\td.maybeScheduleCompaction()\n\t\td.mu.Unlock()\n\n\t\terr := d.waitForDownloadTasks(ctx, tasks)\n\t\tfor _, t := range tasks {\n\t\t\tinfo.DownloadCompactionsLaunched += t.numLaunchedDownloads\n\t\t}\n\n\t\tif err != nil {\n\t\t\tinfo.Err = err\n\t\t\tinfo.Duration = d.timeNow().Sub(startTime)\n\t\t\td.opts.EventListener.DownloadEnd(info)\n\t\t\treturn err\n\t\t}\n\t}\n}\n\n// createDownloadTasks creates downloadSpanTasks for the download spans that\n// overlap external files in the given version.\nfunc (d *DB) createDownloadTasks(spans []DownloadSpan) []*downloadSpanTask {\n\td.mu.Lock()\n\tvers := d.mu.versions.currentVersion()\n\td.mu.Unlock()\n\n\ttasks := make([]*downloadSpanTask, 0, len(spans))\n\tfor i := range spans {\n\t\ttask, ok := d.newDownloadSpanTask(vers, spans[i])\n\t\t// If !ok, there are no external files in this span.\n\t\tif ok {\n\t\t\ttasks = append(tasks, task)\n\t\t}\n\t}\n\treturn tasks\n}\n\n// waitForDownloadTasks waits until all download tasks complete.\nfunc (d *DB) waitForDownloadTasks(ctx context.Context, tasks []*downloadSpanTask) error {\n\tfor i := range tasks {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\td.removeDownloadTasks(tasks)\n\t\t\treturn ctx.Err()\n\n\t\tcase err := <-tasks[i].taskCompletedChan:\n\t\t\tif err != nil {\n\t\t\t\td.removeDownloadTasks(tasks)\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\n// removeDownloadTasks removes all tasks in the given slice from\n// d.mu.compact.downloads.\nfunc (d *DB) removeDownloadTasks(tasks []*downloadSpanTask) {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\td.mu.compact.downloads = slices.DeleteFunc(d.mu.compact.downloads, func(t *downloadSpanTask) bool {\n\t\treturn slices.Contains(tasks, t)\n\t})\n}\n\n// downloadSpanTask tracks the task of downloading external files that overlap\n// with a DownloadSpan.\n//\n// A downloadSpanTask is spawned only if at least one overlapping external file\n// is found in the current version.\n//\n// When a downloadSpanTask completes (i.e. taskCompletedChan is signaled)\n// without an error, it is guaranteed that all external files that were\n// overlapping the download span at the beginning of the task are downloaded.\n//\n// == Implementation ==\n//\n// A download span task moves through the LSM within the given bounds in\n// top-down level order (L0, L1, etc.), and in Smallest.UserKey order within\n// each level (and breaking ties in L0 according to LargestSeqNum). We introduce\n// the concept of a \"download cursor\" to keep track of where we are in this\n// process, in a way that is independent of any one version. A cursor stores the\n// level, a user key which is a lower bound for Smallest.UserKey within that\n// level, and a sequence number which is a lower bound for the LargestSeqNum for\n// files on that level starting at exactly that key.\n//\n// While a download task is running, tables with external backings can disappear\n// due to excises or compactions; tables can move *down* (to a lower LSM level);\n// or tables can have their bounds shrink due to excises (and these will appear\n// as new tables, even though they have the same backing). The top-down,\n// left-to-right-start-key ordering ensures that we don't miss any table\n// (instead, we may examine it multiple times).\n//\n// We use a cursor that advances as our download task makes progress. Each time\n// we encounter a file that needs downloading, we create a \"bookmark\". A\n// bookmark conceptually represents a key range within a level and it\n// corresponds to the bounds of the file that we discovered. It is represented\n// as a cursor position (corresponding to the start) and an end boundary key. We\n// need to remember the bookmark because the download compaction can fail (e.g.\n// it can get canceled by an excise) and the file might get excised so we need\n// to look again at all files within the original key range.\n//\n// It is also possible that we encounter files that are already part of a\n// compaction. These can be move compaction, or can get canceled, so we can't\n// just ignore these files; we create bookmarks for such files as well.\n//\n// We maintain no more than maxConcurrentDownloads bookmarks - the idea being\n// that files that are part of compactions are getting downloaded anyway and we\n// can effectively count them toward the limit. When we cannot create any more\n// bookmarks, we stop advancing the task cursor. Note that it is not this code's\n// job to enforce the maximum concurrency, this is simply a reasonable limit - we\n// don't want to accumulate arbitrarily many bookmarks, since we check each one\n// whenever tryLaunchDownloadCompaction is called (after every compaction\n// completing).\n//\n// This implementation achieves O(maxConcurrentDownloads * N) level iterator\n// operations across the entire task, where N is the (average) number of files\n// within the bounds.\ntype downloadSpanTask struct {\n\tdownloadSpan DownloadSpan\n\n\t// The download task pertains to sstables which *start* (as per\n\t// Smallest.UserKey) within these bounds.\n\tbounds base.UserKeyBounds\n\n\t// taskCompletedChan is signaled when we have finished download compactions\n\t// for all external files encountered within the bounds, or when one of these\n\t// compactions reports an error (other than ErrCancelledCompaction).\n\ttaskCompletedChan chan error\n\n\tnumLaunchedDownloads int\n\n\t// Keeps track of the current position; all files up to these position were\n\t// examined and were either downloaded or we have bookmarks for them.\n\tcursor downloadCursor\n\n\t// Bookmarks remember areas which correspond to downloads that we started or\n\t// files that were undergoing other compactions and which we need to check\n\t// again before completing the task.\n\tbookmarks []downloadBookmark\n\n\t// Testing hooks.\n\ttesting struct {\n\t\tlaunchDownloadCompaction func(f *fileMetadata) (chan error, bool)\n\t}\n}\n\n// downloadBookmark represents an area that was swept by the task cursor which\n// corresponds to a file that was part of a running compaction or download.\ntype downloadBookmark struct {\n\tstart    downloadCursor\n\tendBound base.UserKeyBoundary\n\t// downloadDoneCh is set if this bookmark corresponds to a download we\n\t// started; in this case the channel will report the status of that\n\t// compaction.\n\tdownloadDoneCh chan error\n}\n\nfunc (d *DB) newDownloadSpanTask(vers *version, sp DownloadSpan) (_ *downloadSpanTask, ok bool) {\n\tbounds := base.UserKeyBoundsEndExclusive(sp.StartKey, sp.EndKey)\n\t// We are interested in all external sstables that *overlap* with\n\t// [sp.StartKey, sp.EndKey). Expand the bounds to the left so that we\n\t// include the start keys of any external sstables that overlap with\n\t// sp.StartKey.\n\tvers.IterAllLevelsAndSublevels(func(iter manifest.LevelIterator, level manifest.Layer) {\n\t\tif f := iter.SeekGE(d.cmp, sp.StartKey); f != nil &&\n\t\t\tobjstorage.IsExternalTable(d.objProvider, f.FileBacking.DiskFileNum) &&\n\t\t\td.cmp(f.Smallest.UserKey, bounds.Start) < 0 {\n\t\t\tbounds.Start = f.Smallest.UserKey\n\t\t}\n\t})\n\tstartCursor := downloadCursor{\n\t\tlevel:  0,\n\t\tkey:    bounds.Start,\n\t\tseqNum: 0,\n\t}\n\tf, level := startCursor.NextExternalFile(d.cmp, d.objProvider, bounds, vers)\n\tif f == nil {\n\t\t// No external files in the given span.\n\t\treturn nil, false\n\t}\n\n\treturn &downloadSpanTask{\n\t\tdownloadSpan:      sp,\n\t\tbounds:            bounds,\n\t\ttaskCompletedChan: make(chan error, 1),\n\t\tcursor:            makeCursorAtFile(f, level),\n\t}, true\n}\n\n// downloadCursor represents a position in the download process, which does not\n// depend on a specific version.\n//\n// The Download process scans for external files level-by-level (starting with\n// L0), and left-to-right (in terms of Smallest.UserKey) within each level. In\n// L0, we break ties by the LargestSeqNum.\n//\n// A cursor can be thought of as a boundary between two files in a version\n// (ordered by level, then by Smallest.UserKey, then by LargestSeqNum). A file\n// is either \"before\" or \"after\" the cursor.\ntype downloadCursor struct {\n\t// LSM level (0 to NumLevels). When level=NumLevels, the cursor is at the end.\n\tlevel int\n\t// Inclusive lower bound for Smallest.UserKey for tables on level.\n\tkey []byte\n\t// Inclusive lower bound for sequence number for tables on level with\n\t// Smallest.UserKey equaling key. Used to break ties within L0, and also used\n\t// to position a cursor immediately after a given file.\n\tseqNum base.SeqNum\n}\n\nvar endCursor = downloadCursor{level: manifest.NumLevels}\n\n// AtEnd returns true if the cursor is after all relevant files.\nfunc (c downloadCursor) AtEnd() bool {\n\treturn c.level >= manifest.NumLevels\n}\n\nfunc (c downloadCursor) String() string {\n\treturn fmt.Sprintf(\"level=%d key=%q seqNum=%d\", c.level, c.key, c.seqNum)\n}\n\n// makeCursorAtFile returns a downloadCursor that is immediately before the\n// given file. Calling nextExternalFile on the resulting cursor (using the same\n// version) should return f.\nfunc makeCursorAtFile(f *fileMetadata, level int) downloadCursor {\n\treturn downloadCursor{\n\t\tlevel:  level,\n\t\tkey:    f.Smallest.UserKey,\n\t\tseqNum: f.LargestSeqNum,\n\t}\n}\n\n// makeCursorAfterFile returns a downloadCursor that is immediately\n// after the given file.\nfunc makeCursorAfterFile(f *fileMetadata, level int) downloadCursor {\n\treturn downloadCursor{\n\t\tlevel:  level,\n\t\tkey:    f.Smallest.UserKey,\n\t\tseqNum: f.LargestSeqNum + 1,\n\t}\n}\n\nfunc (c downloadCursor) FileIsAfterCursor(cmp base.Compare, f *fileMetadata, level int) bool {\n\treturn c.Compare(cmp, makeCursorAfterFile(f, level)) < 0\n}\n\nfunc (c downloadCursor) Compare(keyCmp base.Compare, other downloadCursor) int {\n\tif c := cmp.Compare(c.level, other.level); c != 0 {\n\t\treturn c\n\t}\n\tif c := keyCmp(c.key, other.key); c != 0 {\n\t\treturn c\n\t}\n\treturn cmp.Compare(c.seqNum, other.seqNum)\n}\n\n// NextExternalFile returns the first file after the cursor, returning the file\n// and the level. If no such file exists, returns nil fileMetadata.\nfunc (c downloadCursor) NextExternalFile(\n\tcmp base.Compare, objProvider objstorage.Provider, bounds base.UserKeyBounds, v *version,\n) (_ *fileMetadata, level int) {\n\tfor !c.AtEnd() {\n\t\tif f := c.NextExternalFileOnLevel(cmp, objProvider, bounds.End, v); f != nil {\n\t\t\treturn f, c.level\n\t\t}\n\t\t// Go to the next level.\n\t\tc.key = bounds.Start\n\t\tc.seqNum = 0\n\t\tc.level++\n\t}\n\treturn nil, manifest.NumLevels\n}\n\n// NextExternalFileOnLevel returns the first external file on c.level which is\n// after c and with Smallest.UserKey within the end bound.\nfunc (c downloadCursor) NextExternalFileOnLevel(\n\tcmp base.Compare, objProvider objstorage.Provider, endBound base.UserKeyBoundary, v *version,\n) *fileMetadata {\n\tif c.level > 0 {\n\t\tit := v.Levels[c.level].Iter()\n\t\treturn firstExternalFileInLevelIter(cmp, objProvider, c, it, endBound)\n\t}\n\t// For L0, we look at all sublevel iterators and take the first file.\n\tvar first *fileMetadata\n\tvar firstCursor downloadCursor\n\tfor _, sublevel := range v.L0SublevelFiles {\n\t\tf := firstExternalFileInLevelIter(cmp, objProvider, c, sublevel.Iter(), endBound)\n\t\tif f != nil {\n\t\t\tc := makeCursorAtFile(f, c.level)\n\t\t\tif first == nil || c.Compare(cmp, firstCursor) < 0 {\n\t\t\t\tfirst = f\n\t\t\t\tfirstCursor = c\n\t\t\t}\n\t\t\t// Trim the end bound as an optimization.\n\t\t\tendBound = base.UserKeyInclusive(f.Smallest.UserKey)\n\t\t}\n\t}\n\treturn first\n}\n\n// firstExternalFileInLevelIter finds the first external file after the cursor\n// but which starts before the endBound. It is assumed that the iterator\n// corresponds to cursor.level.\nfunc firstExternalFileInLevelIter(\n\tcmp base.Compare,\n\tobjProvider objstorage.Provider,\n\tcursor downloadCursor,\n\tit manifest.LevelIterator,\n\tendBound base.UserKeyBoundary,\n) *fileMetadata {\n\tf := it.SeekGE(cmp, cursor.key)\n\t// Skip the file if it starts before cursor.key or is at that same key with lower\n\t// sequence number.\n\tfor f != nil && !cursor.FileIsAfterCursor(cmp, f, cursor.level) {\n\t\tf = it.Next()\n\t}\n\tfor ; f != nil && endBound.IsUpperBoundFor(cmp, f.Smallest.UserKey); f = it.Next() {\n\t\tif f.Virtual && objstorage.IsExternalTable(objProvider, f.FileBacking.DiskFileNum) {\n\t\t\treturn f\n\t\t}\n\t}\n\treturn nil\n}\n\n// tryLaunchDownloadForFile attempt to launch a download compaction for the\n// given file. Returns true on success, or false if the file is already\n// involved in a compaction.\nfunc (d *DB) tryLaunchDownloadForFile(\n\tvers *version, env compactionEnv, download *downloadSpanTask, level int, f *fileMetadata,\n) (doneCh chan error, ok bool) {\n\tif f.IsCompacting() {\n\t\treturn nil, false\n\t}\n\tif download.testing.launchDownloadCompaction != nil {\n\t\treturn download.testing.launchDownloadCompaction(f)\n\t}\n\tkind := compactionKindRewrite\n\tif download.downloadSpan.ViaBackingFileDownload {\n\t\tkind = compactionKindCopy\n\t}\n\tpc := pickDownloadCompaction(vers, d.opts, env, d.mu.versions.picker.getBaseLevel(), kind, level, f)\n\tif pc == nil {\n\t\t// We are not able to run this download compaction at this time.\n\t\treturn nil, false\n\t}\n\n\tdownload.numLaunchedDownloads++\n\tdoneCh = make(chan error, 1)\n\tc := newCompaction(pc, d.opts, d.timeNow(), d.objProvider, nil /* slot */)\n\tc.isDownload = true\n\td.mu.compact.downloadingCount++\n\td.addInProgressCompaction(c)\n\tgo d.compact(c, doneCh)\n\treturn doneCh, true\n}\n\ntype launchDownloadResult int8\n\nconst (\n\tlaunchedCompaction launchDownloadResult = iota\n\tdidNotLaunchCompaction\n\tdownloadTaskCompleted\n)\n\nfunc (d *DB) tryLaunchDownloadCompaction(\n\tdownload *downloadSpanTask, vers *manifest.Version, env compactionEnv, maxConcurrentDownloads int,\n) launchDownloadResult {\n\t// First, check the bookmarks.\n\tfor i := 0; i < len(download.bookmarks); i++ {\n\t\tb := &download.bookmarks[i]\n\t\tif b.downloadDoneCh != nil {\n\t\t\t// First check if the compaction we launched completed.\n\t\t\tselect {\n\t\t\tcase compactionErr := <-b.downloadDoneCh:\n\t\t\t\tif compactionErr != nil && !errors.Is(compactionErr, ErrCancelledCompaction) {\n\t\t\t\t\tdownload.taskCompletedChan <- compactionErr\n\t\t\t\t\treturn downloadTaskCompleted\n\t\t\t\t}\n\t\t\t\tb.downloadDoneCh = nil\n\n\t\t\t\t// Even if the compaction finished without an error, we still want to\n\t\t\t\t// check the rest of the bookmark range for external files.\n\t\t\t\t//\n\t\t\t\t// For example, say that we encounter a file [\"a\", \"f\"] and start a\n\t\t\t\t// download (creating a bookmark). Then that file gets excised into new\n\t\t\t\t// files [\"a\", \"b\"] and [\"e\", \"f\"] and the excise causes the download\n\t\t\t\t// compaction to be cancelled. We will start another download compaction\n\t\t\t\t// for [\"a\", \"c\"]; once that is complete, we still need to look at the\n\t\t\t\t// rest of the bookmark range (i.e. up to \"f\") to discover the\n\t\t\t\t// [\"e\", \"f\"] file.\n\n\t\t\tdefault:\n\t\t\t\t// The compaction is still running, go to the next bookmark.\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\t// If downloadDoneCh was nil, we are waiting on a compaction that we did not\n\t\t// start. We are effectively polling the status by checking the external\n\t\t// files within the bookmark. This is ok because this method is called (for\n\t\t// this download task) at most once every time a compaction completes.\n\n\t\tf := b.start.NextExternalFileOnLevel(d.cmp, d.objProvider, b.endBound, vers)\n\t\tif f == nil {\n\t\t\t// No more external files for this bookmark, remove it.\n\t\t\tdownload.bookmarks = slices.Delete(download.bookmarks, i, i+1)\n\t\t\ti--\n\t\t\tcontinue\n\t\t}\n\n\t\t// Move up the bookmark position to point at this file.\n\t\tb.start = makeCursorAtFile(f, b.start.level)\n\t\tdoneCh, ok := d.tryLaunchDownloadForFile(vers, env, download, b.start.level, f)\n\t\tif ok {\n\t\t\tb.downloadDoneCh = doneCh\n\t\t\treturn launchedCompaction\n\t\t}\n\t\t// We could not launch a download, which means the file is part of another\n\t\t// compaction. We leave the bookmark in place and will poll the status in\n\t\t// the code above.\n\t}\n\n\t// Try to advance the cursor and launch more downloads.\n\tfor len(download.bookmarks) < maxConcurrentDownloads {\n\t\tf, level := download.cursor.NextExternalFile(d.cmp, d.objProvider, download.bounds, vers)\n\t\tif f == nil {\n\t\t\tdownload.cursor = endCursor\n\t\t\tif len(download.bookmarks) == 0 {\n\t\t\t\tdownload.taskCompletedChan <- nil\n\t\t\t\treturn downloadTaskCompleted\n\t\t\t}\n\t\t\treturn didNotLaunchCompaction\n\t\t}\n\t\tdownload.cursor = makeCursorAfterFile(f, level)\n\n\t\tdownload.bookmarks = append(download.bookmarks, downloadBookmark{\n\t\t\tstart:    makeCursorAtFile(f, level),\n\t\t\tendBound: base.UserKeyInclusive(f.Largest.UserKey),\n\t\t})\n\t\tdoneCh, ok := d.tryLaunchDownloadForFile(vers, env, download, level, f)\n\t\tif ok {\n\t\t\t// We launched a download for this file.\n\t\t\tdownload.bookmarks[len(download.bookmarks)-1].downloadDoneCh = doneCh\n\t\t\treturn launchedCompaction\n\t\t}\n\t}\n\n\treturn didNotLaunchCompaction\n}\n"
        },
        {
          "name": "download_test.go",
          "type": "blob",
          "size": 6.869140625,
          "content": "// Copyright 2024 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/cockroachdb/datadriven\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n\t\"github.com/cockroachdb/pebble/objstorage\"\n\t\"github.com/cockroachdb/pebble/objstorage/objstorageprovider\"\n\t\"github.com/cockroachdb/pebble/objstorage/remote\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestDownloadCursor(t *testing.T) {\n\tcmp := bytes.Compare\n\tobjProvider := initDownloadTestProvider(t)\n\n\tvar vers *manifest.Version\n\tvar cursor downloadCursor\n\tdatadriven.RunTest(t, \"testdata/download_cursor\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"define\":\n\t\t\tvar err error\n\t\t\tconst flushSplitBytes = 10 * 1024 * 1024\n\t\t\tvers, err = manifest.ParseVersionDebug(base.DefaultComparer, flushSplitBytes, td.Input)\n\t\t\tif err != nil {\n\t\t\t\ttd.Fatalf(t, \"%v\", err)\n\t\t\t}\n\t\t\treturn vers.DebugString()\n\n\t\tcase \"cursor\":\n\t\t\tvar lower, upper string\n\t\t\ttd.ScanArgs(t, \"lower\", &lower)\n\t\t\ttd.ScanArgs(t, \"upper\", &upper)\n\t\t\tbounds := base.UserKeyBoundsEndExclusive([]byte(lower), []byte(upper))\n\n\t\t\tvar buf strings.Builder\n\t\t\tfor _, line := range strings.Split(td.Input, \"\\n\") {\n\t\t\t\tfields := strings.Fields(line)\n\t\t\t\tfmt.Fprintf(&buf, \"%s:\\n\", fields[0])\n\t\t\t\tswitch cmd := fields[0]; cmd {\n\t\t\t\tcase \"start\":\n\t\t\t\t\tcursor = downloadCursor{\n\t\t\t\t\t\tlevel:  0,\n\t\t\t\t\t\tkey:    bounds.Start,\n\t\t\t\t\t\tseqNum: 0,\n\t\t\t\t\t}\n\t\t\t\t\tfmt.Fprintf(&buf, \"  %s\\n\", cursor)\n\n\t\t\t\tcase \"next-file\":\n\t\t\t\t\tf, level := cursor.NextExternalFile(cmp, objProvider, bounds, vers)\n\t\t\t\t\tif f != nil {\n\t\t\t\t\t\t// Verify that fCursor still points to this file.\n\t\t\t\t\t\tf2, level2 := makeCursorAtFile(f, level).NextExternalFile(cmp, objProvider, bounds, vers)\n\t\t\t\t\t\tif f != f2 {\n\t\t\t\t\t\t\ttd.Fatalf(t, \"nextExternalFile returned different file\")\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif level != level2 {\n\t\t\t\t\t\t\ttd.Fatalf(t, \"nextExternalFile returned different level\")\n\t\t\t\t\t\t}\n\t\t\t\t\t\tcursor = makeCursorAfterFile(f, level)\n\t\t\t\t\t}\n\t\t\t\t\tfmt.Fprintf(&buf, \"  file: %v  level: %d\\n\", f, level)\n\n\t\t\t\tcase \"iterate\":\n\t\t\t\t\tfor {\n\t\t\t\t\t\tf, level := cursor.NextExternalFile(cmp, objProvider, bounds, vers)\n\t\t\t\t\t\tif f == nil {\n\t\t\t\t\t\t\tfmt.Fprintf(&buf, \"  no more files\\n\")\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\t\t\t\t\t\tfmt.Fprintf(&buf, \"  file: %v  level: %d\\n\", f, level)\n\t\t\t\t\t\tcursor = makeCursorAfterFile(f, level)\n\t\t\t\t\t}\n\n\t\t\t\tdefault:\n\t\t\t\t\ttd.Fatalf(t, \"unknown cursor command %q\", cmd)\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn buf.String()\n\n\t\tdefault:\n\t\t\ttd.Fatalf(t, \"unknown command: %s\", td.Cmd)\n\t\t\treturn \"\"\n\t\t}\n\t})\n}\n\nfunc TestDownloadTask(t *testing.T) {\n\tcmp := bytes.Compare\n\tobjProvider := initDownloadTestProvider(t)\n\td := DB{\n\t\tcmp:         cmp,\n\t\tobjProvider: objProvider,\n\t}\n\n\tvar vers *manifest.Version\n\tvar task *downloadSpanTask\n\tprintTask := func(b *strings.Builder) {\n\t\tfor i := range task.bookmarks {\n\t\t\tfmt.Fprintf(b, \"bookmark %d: %s  end-bound=%q\\n\", i, task.bookmarks[i].start, task.bookmarks[i].endBound.Key)\n\t\t}\n\t\tfmt.Fprintf(b, \"cursor: %s\\n\", task.cursor.String())\n\t}\n\tdatadriven.RunTest(t, \"testdata/download_task\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"define\":\n\t\t\tvar err error\n\t\t\tconst flushSplitBytes = 10 * 1024 * 1024\n\t\t\tvers, err = manifest.ParseVersionDebug(base.DefaultComparer, flushSplitBytes, td.Input)\n\t\t\tif err != nil {\n\t\t\t\ttd.Fatalf(t, \"%v\", err)\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"set-compacting\":\n\t\t\t// Parse a list of tables that are compacting and set compacting status on\n\t\t\t// all tables in the current version.\n\t\t\tcompacting := make(map[base.FileNum]struct{})\n\t\t\tfor _, f := range strings.Fields(td.Input) {\n\t\t\t\tn, err := strconv.Atoi(f)\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\tcompacting[base.FileNum(n)] = struct{}{}\n\t\t\t}\n\t\t\tfor _, lm := range vers.Levels {\n\t\t\t\titer := lm.Iter()\n\t\t\t\tfor f := iter.First(); f != nil; f = iter.Next() {\n\t\t\t\t\tif _, ok := compacting[f.FileNum]; ok {\n\t\t\t\t\t\tf.CompactionState = manifest.CompactionStateCompacting\n\t\t\t\t\t\tdelete(compacting, f.FileNum)\n\t\t\t\t\t} else {\n\t\t\t\t\t\tf.CompactionState = manifest.CompactionStateNotCompacting\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tfor n := range compacting {\n\t\t\t\ttd.Fatalf(t, \"unknowm table %s\", n)\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"new-task\":\n\t\t\tvar start, end string\n\t\t\ttd.ScanArgs(t, \"start\", &start)\n\t\t\ttd.ScanArgs(t, \"end\", &end)\n\t\t\tvar ok bool\n\t\t\ttask, ok = d.newDownloadSpanTask(vers, DownloadSpan{\n\t\t\t\tStartKey: []byte(start),\n\t\t\t\tEndKey:   []byte(end),\n\t\t\t})\n\t\t\tif !ok {\n\t\t\t\treturn \"nothing to do\"\n\t\t\t}\n\t\t\tvar buf strings.Builder\n\t\t\tprintTask(&buf)\n\t\t\treturn buf.String()\n\n\t\tcase \"try-launch\":\n\t\t\tif task == nil {\n\t\t\t\treturn \"no task\"\n\t\t\t}\n\t\t\tvar buf strings.Builder\n\t\t\tmaxConcurrentDownloads := 1\n\t\t\ttd.MaybeScanArgs(t, \"max-concurrent-downloads\", &maxConcurrentDownloads)\n\t\t\ttask.testing.launchDownloadCompaction = func(f *fileMetadata) (chan error, bool) {\n\t\t\t\tch := make(chan error, 1)\n\t\t\t\tif td.HasArg(\"fail\") {\n\t\t\t\t\tfmt.Fprintf(&buf, \"launching download for %s and cancelling it\\n\", f.FileNum)\n\t\t\t\t\tch <- ErrCancelledCompaction\n\t\t\t\t} else {\n\t\t\t\t\tfmt.Fprintf(&buf, \"downloading %s\\n\", f.FileNum)\n\t\t\t\t\tf.Virtual = false\n\t\t\t\t\tf.FileBacking.DiskFileNum = base.DiskFileNum(f.FileNum)\n\t\t\t\t\tch <- nil\n\t\t\t\t}\n\t\t\t\treturn ch, true\n\t\t\t}\n\t\t\tres := d.tryLaunchDownloadCompaction(task, vers, compactionEnv{}, maxConcurrentDownloads)\n\t\t\tprintTask(&buf)\n\t\t\tif res == downloadTaskCompleted {\n\t\t\t\tfmt.Fprintf(&buf, \"task completed\")\n\t\t\t\ttask = nil\n\t\t\t}\n\t\t\treturn buf.String()\n\n\t\tdefault:\n\t\t\ttd.Fatalf(t, \"unknown command: %s\", td.Cmd)\n\t\t\treturn \"\"\n\t\t}\n\t})\n}\n\n// initDownloadTestProvider returns an objstorage provider that is initialized\n// with local backings 1 through 99 and external backings 100 through 199.\nfunc initDownloadTestProvider(t *testing.T) objstorage.Provider {\n\tproviderSettings := objstorageprovider.Settings{\n\t\tLogger: base.DefaultLogger,\n\t\tFS:     vfs.NewMem(),\n\t}\n\tproviderSettings.Remote.StorageFactory = remote.MakeSimpleFactory(map[remote.Locator]remote.Storage{\n\t\t\"\": remote.NewInMem(),\n\t})\n\n\tobjProvider, err := objstorageprovider.Open(providerSettings)\n\trequire.NoError(t, err)\n\n\t// Create some dummy backings. 1 through 99 are local, 100 through 199 are external.\n\n\tfor i := base.DiskFileNum(1); i < 100; i++ {\n\t\tw, _, err := objProvider.Create(context.Background(), base.FileTypeTable, i, objstorage.CreateOptions{})\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, w.Finish())\n\t}\n\tvar remoteObjs []objstorage.RemoteObjectToAttach\n\tfor i := base.DiskFileNum(100); i < 200; i++ {\n\t\tbacking, err := objProvider.CreateExternalObjectBacking(\"\", fmt.Sprintf(\"external-%d\", i))\n\t\trequire.NoError(t, err)\n\t\tremoteObjs = append(remoteObjs, objstorage.RemoteObjectToAttach{\n\t\t\tFileNum:  i,\n\t\t\tFileType: base.FileTypeTable,\n\t\t\tBacking:  backing,\n\t\t})\n\t}\n\t_, err = objProvider.AttachRemoteObjects(remoteObjs)\n\trequire.NoError(t, err)\n\treturn objProvider\n}\n"
        },
        {
          "name": "error_iter.go",
          "type": "blob",
          "size": 2.7080078125,
          "content": "// Copyright 2018 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"context\"\n\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan\"\n\t\"github.com/cockroachdb/pebble/internal/treeprinter\"\n)\n\ntype errorIter struct {\n\terr error\n}\n\n// errorIter implements the base.InternalIterator interface.\nvar _ internalIterator = (*errorIter)(nil)\n\nfunc (c *errorIter) SeekGE(key []byte, flags base.SeekGEFlags) *base.InternalKV {\n\treturn nil\n}\n\nfunc (c *errorIter) SeekPrefixGE(prefix, key []byte, flags base.SeekGEFlags) *base.InternalKV {\n\treturn c.SeekPrefixGEStrict(prefix, key, flags)\n}\n\nfunc (c *errorIter) SeekPrefixGEStrict(\n\tprefix, key []byte, flags base.SeekGEFlags,\n) *base.InternalKV {\n\treturn nil\n}\n\nfunc (c *errorIter) SeekLT(key []byte, flags base.SeekLTFlags) *base.InternalKV {\n\treturn nil\n}\n\nfunc (c *errorIter) First() *base.InternalKV {\n\treturn nil\n}\n\nfunc (c *errorIter) Last() *base.InternalKV {\n\treturn nil\n}\n\nfunc (c *errorIter) Next() *base.InternalKV {\n\treturn nil\n}\n\nfunc (c *errorIter) Prev() *base.InternalKV {\n\treturn nil\n}\n\nfunc (c *errorIter) NextPrefix([]byte) *base.InternalKV {\n\treturn nil\n}\n\nfunc (c *errorIter) Error() error {\n\treturn c.err\n}\n\nfunc (c *errorIter) Close() error {\n\treturn c.err\n}\n\nfunc (c *errorIter) String() string {\n\treturn \"error\"\n}\n\nfunc (c *errorIter) SetBounds(lower, upper []byte) {}\n\nfunc (c *errorIter) SetContext(_ context.Context) {}\n\nfunc (c *errorIter) DebugTree(tp treeprinter.Node) {\n\ttp.Childf(\"%T(%p)\", c, c)\n}\n\ntype errorKeyspanIter struct {\n\terr error\n}\n\n// errorKeyspanIter implements the keyspan.FragmentIterator interface.\nvar _ keyspan.FragmentIterator = (*errorKeyspanIter)(nil)\n\nfunc (i *errorKeyspanIter) SeekGE(key []byte) (*keyspan.Span, error) { return nil, i.err }\nfunc (i *errorKeyspanIter) SeekLT(key []byte) (*keyspan.Span, error) { return nil, i.err }\nfunc (i *errorKeyspanIter) First() (*keyspan.Span, error)            { return nil, i.err }\nfunc (i *errorKeyspanIter) Last() (*keyspan.Span, error)             { return nil, i.err }\nfunc (i *errorKeyspanIter) Next() (*keyspan.Span, error)             { return nil, i.err }\nfunc (i *errorKeyspanIter) Prev() (*keyspan.Span, error)             { return nil, i.err }\nfunc (i *errorKeyspanIter) SetContext(ctx context.Context)           {}\nfunc (i *errorKeyspanIter) Close()                                   {}\nfunc (*errorKeyspanIter) String() string                             { return \"error\" }\nfunc (*errorKeyspanIter) WrapChildren(wrap keyspan.WrapFn)           {}\nfunc (i *errorKeyspanIter) DebugTree(tp treeprinter.Node)            { tp.Childf(\"%T(%p)\", i, i) }\n"
        },
        {
          "name": "error_test.go",
          "type": "blob",
          "size": 15.5166015625,
          "content": "// Copyright 2019 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand/v2\"\n\t\"runtime\"\n\t\"strings\"\n\t\"sync/atomic\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/testkeys\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/cockroachdb/pebble/vfs/errorfs\"\n\t\"github.com/stretchr/testify/require\"\n)\n\ntype panicLogger struct{}\n\nfunc (l panicLogger) Infof(format string, args ...interface{})  {}\nfunc (l panicLogger) Errorf(format string, args ...interface{}) {}\n\nfunc (l panicLogger) Fatalf(format string, args ...interface{}) {\n\tpanic(errors.Errorf(\"fatal: \"+format, args...))\n}\n\n// corruptFS injects a corruption in the `index`th byte read.\ntype corruptFS struct {\n\tvfs.FS\n\t// index is the index of the byte which we will corrupt.\n\tindex     atomic.Int32\n\tbytesRead atomic.Int32\n}\n\nfunc (fs *corruptFS) maybeCorrupt(n int32, p []byte) {\n\tnewBytesRead := fs.bytesRead.Add(n)\n\tpIdx := newBytesRead - 1 - fs.index.Load()\n\tif pIdx >= 0 && pIdx < n {\n\t\tp[pIdx]++\n\t}\n}\n\nfunc (fs *corruptFS) maybeCorruptAt(n int32, p []byte, offset int64) {\n\tpIdx := fs.index.Load() - int32(offset)\n\tif pIdx >= 0 && pIdx < n {\n\t\tp[pIdx]++\n\t}\n}\n\nfunc (fs *corruptFS) Open(name string, opts ...vfs.OpenOption) (vfs.File, error) {\n\tf, err := fs.FS.Open(name)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tcf := corruptFile{f, fs}\n\tfor _, opt := range opts {\n\t\topt.Apply(cf)\n\t}\n\treturn cf, nil\n}\n\ntype corruptFile struct {\n\tvfs.File\n\tfs *corruptFS\n}\n\nfunc (f corruptFile) Read(p []byte) (int, error) {\n\tn, err := f.File.Read(p)\n\tf.fs.maybeCorrupt(int32(n), p)\n\treturn n, err\n}\n\nfunc (f corruptFile) ReadAt(p []byte, off int64) (int, error) {\n\tn, err := f.File.ReadAt(p, off)\n\tf.fs.maybeCorruptAt(int32(n), p, off)\n\treturn n, err\n}\n\nfunc expectLSM(expected string, d *DB, t *testing.T) {\n\tt.Helper()\n\texpected = strings.TrimSpace(expected)\n\td.mu.Lock()\n\tactual := d.mu.versions.currentVersion().String()\n\td.mu.Unlock()\n\tactual = strings.TrimSpace(actual)\n\tif expected != actual {\n\t\tt.Fatalf(\"expected\\n%s\\nbut found\\n%s\", expected, actual)\n\t}\n}\n\n// TestErrors repeatedly runs a short sequence of operations, injecting FS\n// errors at different points, until success is achieved.\nfunc TestErrors(t *testing.T) {\n\trun := func(fs *errorfs.FS) (err error) {\n\t\tdefer func() {\n\t\t\tif r := recover(); r != nil {\n\t\t\t\tif e, ok := r.(error); ok {\n\t\t\t\t\terr = e\n\t\t\t\t} else {\n\t\t\t\t\tt.Fatal(r)\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\n\t\td, err := Open(\"\", &Options{\n\t\t\tFS:     fs,\n\t\t\tLogger: panicLogger{},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tkey := []byte(\"a\")\n\t\tvalue := []byte(\"b\")\n\t\tif err := d.Set(key, value, nil); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err := d.Flush(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err := d.Compact(nil, []byte(\"\\xff\"), false); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\titer, _ := d.NewIter(nil)\n\t\tfor valid := iter.First(); valid; valid = iter.Next() {\n\t\t}\n\t\tif err := iter.Close(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn d.Close()\n\t}\n\n\terrorCounts := make(map[string]int)\n\tfor i := int32(0); ; i++ {\n\t\tfs := errorfs.Wrap(vfs.NewMem(), errorfs.ErrInjected.If(errorfs.OnIndex(i)))\n\t\terr := run(fs)\n\t\tif err == nil {\n\t\t\tt.Logf(\"success %d\\n\", i)\n\t\t\tbreak\n\t\t}\n\t\terrMsg := err.Error()\n\t\tif !strings.Contains(errMsg, \"injected error\") {\n\t\t\tt.Fatalf(\"unexpected errors: %v\", err)\n\t\t}\n\t\terrorCounts[errMsg]++\n\t}\n\n\texpectedErrors := []string{\n\t\t\"fatal: MANIFEST flush failed: injected error\",\n\t\t\"fatal: MANIFEST sync failed: injected error\",\n\t\t\"fatal: MANIFEST set current failed: injected error\",\n\t}\n\tfor _, expected := range expectedErrors {\n\t\tif errorCounts[expected] == 0 {\n\t\t\tt.Errorf(\"expected error %q did not occur\", expected)\n\t\t}\n\t}\n}\n\n// TestRequireReadError injects FS errors into read operations at successively later\n// points until all operations can complete. It requires an operation fails any time\n// an error was injected. This differs from the TestErrors case above as that one\n// cannot require operations fail since it involves flush/compaction, which retry\n// internally and succeed following an injected error.\nfunc TestRequireReadError(t *testing.T) {\n\trun := func(formatVersion FormatMajorVersion, index int32) (err error) {\n\t\t// Perform setup with error injection disabled as it involves writes/background ops.\n\t\tii := errorfs.OnIndex(-1)\n\t\tfs := errorfs.Wrap(vfs.NewMem(), errorfs.ErrInjected.If(ii))\n\t\topts := &Options{\n\t\t\tDisableTableStats:  true,\n\t\t\tFS:                 fs,\n\t\t\tLogger:             panicLogger{},\n\t\t\tFormatMajorVersion: formatVersion,\n\t\t}\n\t\td, err := Open(\"\", opts)\n\t\trequire.NoError(t, err)\n\n\t\tdefer func() {\n\t\t\tif d != nil {\n\t\t\t\trequire.NoError(t, d.Close())\n\t\t\t}\n\t\t}()\n\n\t\tkey1 := []byte(\"a1\")\n\t\tkey2 := []byte(\"a2\")\n\t\tvalue := []byte(\"b\")\n\t\trequire.NoError(t, d.Set(key1, value, nil))\n\t\trequire.NoError(t, d.Set(key2, value, nil))\n\t\trequire.NoError(t, d.Flush())\n\t\trequire.NoError(t, d.Compact(key1, key2, false))\n\t\trequire.NoError(t, d.DeleteRange(key1, key2, nil))\n\t\trequire.NoError(t, d.Set(key1, value, nil))\n\t\trequire.NoError(t, d.Flush())\n\t\texpectLSM(`\nL0.0:\n  000007:[a1#13,SET-a2#inf,RANGEDEL]\nL6:\n  000005:[a1#10,SET-a2#11,SET]\n`, d, t)\n\n\t\t// Now perform foreground ops with error injection enabled.\n\t\tii.Store(index)\n\t\titer, _ := d.NewIter(nil)\n\t\tif err := iter.Error(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tnumFound := 0\n\t\texpectedKeys := [][]byte{key1, key2}\n\t\tfor valid := iter.First(); valid; valid = iter.Next() {\n\t\t\tif !bytes.Equal(iter.Key(), expectedKeys[numFound]) {\n\t\t\t\tt.Fatalf(\"expected key %v; found %v\", expectedKeys[numFound], iter.Key())\n\t\t\t}\n\t\t\tif !bytes.Equal(iter.Value(), value) {\n\t\t\t\tt.Fatalf(\"expected value %v; found %v\", value, iter.Value())\n\t\t\t}\n\t\t\tnumFound++\n\t\t}\n\t\tif err := iter.Close(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err := d.Close(); err != nil {\n\t\t\td = nil\n\t\t\treturn err\n\t\t}\n\t\td = nil\n\t\t// Reaching here implies all read operations succeeded. This\n\t\t// should only happen when we reached a large enough index at\n\t\t// which `errorfs.FS` did not return any error.\n\t\tif i := ii.Load(); i < 0 {\n\t\t\tt.Errorf(\"FS error injected %d ops ago went unreported\", -i)\n\t\t}\n\t\tif numFound != 2 {\n\t\t\tt.Fatalf(\"expected 2 values; found %d\", numFound)\n\t\t}\n\t\treturn nil\n\t}\n\n\tversions := []FormatMajorVersion{FormatMinSupported, internalFormatNewest}\n\tfor _, version := range versions {\n\t\tt.Run(fmt.Sprintf(\"version-%s\", version), func(t *testing.T) {\n\t\t\tfor i := int32(0); ; i++ {\n\t\t\t\terr := run(version, i)\n\t\t\t\tif err == nil {\n\t\t\t\t\tt.Logf(\"no failures reported at index %d\", i)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\n// TestCorruptReadError verifies that reads to a corrupted file detect the\n// corruption and return an error. In this case the filesystem reads return\n// successful status but the data they return is corrupt.\nfunc TestCorruptReadError(t *testing.T) {\n\trun := func(formatVersion FormatMajorVersion, index int32) (err error) {\n\t\t// Perform setup with corruption injection disabled as it involves writes/background ops.\n\t\tfs := &corruptFS{\n\t\t\tFS: vfs.NewMem(),\n\t\t}\n\t\tfs.index.Store(-1)\n\t\topts := &Options{\n\t\t\tDisableTableStats:  true,\n\t\t\tFS:                 fs,\n\t\t\tLogger:             panicLogger{},\n\t\t\tFormatMajorVersion: formatVersion,\n\t\t}\n\t\td, err := Open(\"\", opts)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"%v\", err)\n\t\t}\n\t\tdefer func() {\n\t\t\tif d != nil {\n\t\t\t\trequire.NoError(t, d.Close())\n\t\t\t}\n\t\t}()\n\n\t\tkey1 := []byte(\"a1\")\n\t\tkey2 := []byte(\"a2\")\n\t\tvalue := []byte(\"b\")\n\t\trequire.NoError(t, d.Set(key1, value, nil))\n\t\trequire.NoError(t, d.Set(key2, value, nil))\n\t\trequire.NoError(t, d.Flush())\n\t\trequire.NoError(t, d.Compact(key1, key2, false))\n\t\trequire.NoError(t, d.DeleteRange(key1, key2, nil))\n\t\trequire.NoError(t, d.Set(key1, value, nil))\n\t\trequire.NoError(t, d.Flush())\n\t\texpectLSM(`\nL0.0:\n  000007:[a1#13,SET-a2#inf,RANGEDEL]\nL6:\n  000005:[a1#10,SET-a2#11,SET]\n`, d, t)\n\n\t\t// Now perform foreground ops with corruption injection enabled.\n\t\tfs.index.Store(index)\n\t\titer, _ := d.NewIter(nil)\n\t\tif err := iter.Error(); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tnumFound := 0\n\t\texpectedKeys := [][]byte{key1, key2}\n\t\tfor valid := iter.First(); valid; valid = iter.Next() {\n\t\t\tif !bytes.Equal(iter.Key(), expectedKeys[numFound]) {\n\t\t\t\tt.Fatalf(\"expected key %v; found %v\", expectedKeys[numFound], iter.Key())\n\t\t\t}\n\t\t\tif !bytes.Equal(iter.Value(), value) {\n\t\t\t\tt.Fatalf(\"expected value %v; found %v\", value, iter.Value())\n\t\t\t}\n\t\t\tnumFound++\n\t\t}\n\t\tif err := iter.Close(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err := d.Close(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\td = nil\n\t\t// Reaching here implies all read operations succeeded. This\n\t\t// should only happen when we reached a large enough index at\n\t\t// which `corruptFS` did not inject any corruption.\n\t\tif bytesRead := fs.bytesRead.Load(); bytesRead > index {\n\t\t\tt.Errorf(\"corruption error injected at index %d went unreported\", index)\n\t\t}\n\t\tif numFound != 2 {\n\t\t\tt.Fatalf(\"expected 2 values; found %d\", numFound)\n\t\t}\n\t\treturn nil\n\t}\n\tversions := []FormatMajorVersion{FormatMinSupported, internalFormatNewest}\n\tfor _, version := range versions {\n\t\tt.Run(fmt.Sprintf(\"version-%s\", version), func(t *testing.T) {\n\t\t\tfor i := int32(0); ; i++ {\n\t\t\t\terr := run(version, i)\n\t\t\t\tif err == nil {\n\t\t\t\t\tt.Logf(\"no failures reported at index %d\", i)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestDBWALRotationCrash(t *testing.T) {\n\tmemfs := vfs.NewCrashableMem()\n\n\tvar crashFS *vfs.MemFS\n\tvar index atomic.Int32\n\tinj := errorfs.InjectorFunc(func(op errorfs.Op) error {\n\t\tif op.Kind.ReadOrWrite() == errorfs.OpIsWrite && index.Add(-1) == -1 {\n\t\t\tcrashFS = memfs.CrashClone(vfs.CrashCloneCfg{UnsyncedDataPercent: 0})\n\t\t}\n\t\treturn nil\n\t})\n\ttriggered := func() bool { return index.Load() < 0 }\n\n\trun := func(fs *errorfs.FS, k int32) (err error) {\n\t\topts := &Options{\n\t\t\tDisableTableStats: true,\n\t\t\tFS:                fs,\n\t\t\tLogger:            panicLogger{},\n\t\t\tMemTableSize:      2048,\n\t\t}\n\t\td, err := Open(\"\", opts)\n\t\tif err != nil || triggered() {\n\t\t\treturn err\n\t\t}\n\n\t\t// Write keys with the FS set up to simulate a crash by ignoring\n\t\t// syncs on the k-th write operation.\n\t\tindex.Store(k)\n\t\tkey := []byte(\"test\")\n\t\tfor i := 0; i < 10; i++ {\n\t\t\tv := []byte(strings.Repeat(\"b\", i))\n\t\t\terr = d.Set(key, v, nil)\n\t\t\tif err != nil || triggered() {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\terr = firstError(err, d.Close())\n\t\treturn err\n\t}\n\n\tfs := errorfs.Wrap(memfs, inj)\n\tfor k := int32(0); ; k++ {\n\t\t// Run, simulating a crash after the k-th write operation after Open.\n\t\tindex.Store(math.MaxInt32)\n\t\terr := run(fs, k)\n\t\tif !triggered() {\n\t\t\t// Stop when we reach a value of k greater than the number of\n\t\t\t// write operations performed during `run`.\n\t\t\tt.Logf(\"No crash at write operation %d\\n\", k)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Filesystem did not 'crash', but error returned: %s\", err)\n\t\t\t}\n\t\t\tbreak\n\t\t}\n\t\tt.Logf(\"Crashed at write operation % 2d, error: %v\\n\", k, err)\n\n\t\t// Reset the filesystem to its state right before the simulated\n\t\t// \"crash\", restore syncs, and run again without crashing.\n\t\tmemfs = crashFS\n\t\tindex.Store(math.MaxInt32)\n\t\trequire.NoError(t, run(fs, k))\n\t}\n}\n\nfunc TestDBCompactionCrash(t *testing.T) {\n\tseed := time.Now().UnixNano()\n\tt.Log(\"seed\", seed)\n\n\t// This test uses the strict MemFS with the error injector to simulate\n\t// crashes. Each subtest runs with a crash induced at the k-th write\n\t// operation. Each subsequent run increases k by +1-5 until a subtest runs\n\t// to completion without performing a k-th write operation.\n\t//\n\t// crashIndex holds the value of k at which the crash is induced and is\n\t// decremented by the errorfs on each write operation.\n\tvar crashIndex atomic.Int32\n\tvar crashFS *vfs.MemFS\n\tcrashRNG := rand.New(rand.NewPCG(0, uint64(seed)))\n\tmkFS := func() vfs.FS {\n\t\tmemfs := vfs.NewCrashableMem()\n\t\tinj := errorfs.InjectorFunc(func(op errorfs.Op) error {\n\t\t\tif op.Kind.ReadOrWrite() == errorfs.OpIsWrite && crashIndex.Add(-1) == -1 {\n\t\t\t\t// Allow an arbitrary subset of non-synced state to survive beyond the\n\t\t\t\t// crash point.\n\t\t\t\tcrashFS = memfs.CrashClone(vfs.CrashCloneCfg{UnsyncedDataPercent: 10, RNG: crashRNG})\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\t\treturn errorfs.Wrap(memfs, inj)\n\t}\n\ttriggered := func() bool { return crashIndex.Load() < 0 }\n\n\t// run opens the store and performs some random writes, simulating a crash\n\t// at the k-th write operation.\n\trun := func(t *testing.T, fs vfs.FS, k int32, seed int64) (i int64, err error) {\n\t\t// On windows, time.Sleep usually takes at least 15ms, which makes this test\n\t\t// really slow. See https://github.com/golang/go/issues/61042,\n\t\t// https://github.com/golang/go/issues/44343.\n\t\tif runtime.GOOS != \"windows\" {\n\t\t\tfs = errorfs.Wrap(fs, errorfs.RandomLatency(nil, 20*time.Microsecond, seed, 0 /* no limit */))\n\t\t}\n\t\trng := rand.New(rand.NewPCG(0, uint64(seed)))\n\t\tmaxConcurrentCompactions := rng.IntN(3) + 2\n\t\topts := &Options{\n\t\t\tDisableTableStats:           true,\n\t\t\tFS:                          fs,\n\t\t\tLogger:                      testLogger{t: t},\n\t\t\tMemTableSize:                128 << 10,\n\t\t\tMaxConcurrentCompactions:    func() int { return maxConcurrentCompactions },\n\t\t\tLBaseMaxBytes:               64 << 10,\n\t\t\tL0CompactionThreshold:       2,\n\t\t\tL0CompactionFileThreshold:   2,\n\t\t\tMemTableStopWritesThreshold: 10,\n\t\t\tL0StopWritesThreshold:       10,\n\t\t}\n\t\tif testing.Verbose() {\n\t\t\tlel := MakeLoggingEventListener(opts.Logger)\n\t\t\topts.EventListener = &lel\n\t\t}\n\t\td, err := Open(\"\", opts)\n\t\tif err != nil || triggered() {\n\t\t\treturn 0, err\n\t\t}\n\n\t\t// Set index to k so that the k-th write operation decrements it to zero\n\t\t// and simulates a crash.\n\t\tcrashIndex.Store(k)\n\n\t\t// Write keys in random order in batches of random sizes.\n\t\tconst maxKeyLength = 2\n\t\tconst valLength = 4 << 10\n\t\ttimestamps := []int{10, 5}\n\t\tks := testkeys.Alpha(maxKeyLength)\n\t\tks = ks.EveryN(10)\n\t\tbuf := make([]byte, ks.MaxLen()+testkeys.MaxSuffixLen)\n\t\tvbuf := make([]byte, valLength)\n\t\tb := d.NewBatch()\n\t\tperm := rng.Perm(int(ks.Count()))\n\tdone:\n\t\tfor _, ts := range timestamps {\n\t\t\tfor _, i := range perm {\n\t\t\t\tn := testkeys.WriteKeyAt(buf, ks, int64(i), int64(ts))\n\t\t\t\tfor j := range vbuf {\n\t\t\t\t\tvbuf[j] = byte(rng.Uint32())\n\t\t\t\t}\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\trequire.NoError(t, b.Set(buf[:n], vbuf, nil))\n\t\t\t\tif rng.IntN(10) == 1 {\n\t\t\t\t\tif err = d.Apply(b, nil); err != nil || triggered() {\n\t\t\t\t\t\tb = nil\n\t\t\t\t\t\tbreak done\n\t\t\t\t\t}\n\t\t\t\t\tb.Reset()\n\t\t\t\t}\n\t\t\t\tif rng.IntN(100) == 1 {\n\t\t\t\t\tif err = d.Flush(); err != nil || triggered() {\n\t\t\t\t\t\tbreak done\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif b != nil && b.Count() > 0 {\n\t\t\terr = firstError(err, d.Apply(b, nil))\n\t\t}\n\t\terr = firstError(err, d.Close())\n\t\treturn i, err\n\t}\n\n\t// Run the test with increasing values of k until a subtest runs to\n\t// completion without performing a k-th write operation.\n\tdone := false\n\trng := rand.New(rand.NewPCG(0, uint64(seed)))\n\tfor k := int32(0); !done; k += rng.Int32N(5) + 1 {\n\t\tt.Run(fmt.Sprintf(\"k=%d\", k), func(t *testing.T) {\n\t\t\t// Run, simulating a crash by ignoring syncs after the k-th write\n\t\t\t// operation after Open.\n\t\t\tcrashIndex.Store(math.MaxInt32)\n\t\t\ti, err := run(t, mkFS(), k, seed)\n\t\t\tif !triggered() {\n\t\t\t\t// Stop when we reach a value of k greater than the number of\n\t\t\t\t// write operations performed during `run`.\n\t\t\t\tt.Logf(\"No crash at write operation %d\\n\", k)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatalf(\"Filesystem did not 'crash', but error returned: %s\", err)\n\t\t\t\t}\n\t\t\t\tdone = true\n\t\t\t\treturn\n\t\t\t}\n\t\t\tt.Logf(\"Simulated crash at write operation % 2d after writing %d keys, error: %v\\n\", k, i, err)\n\n\t\t\t// Reset the filesystem to its state right before the simulated\n\t\t\t// \"crash\", restore syncs and run again without crashing. No errors\n\t\t\t// should be encountered.\n\t\t\tcrashIndex.Store(math.MaxInt32)\n\t\t\t_, err = run(t, crashFS, math.MaxInt32, seed)\n\t\t\trequire.False(t, triggered())\n\t\t\t// TODO(jackson): Add assertions on the database keys.\n\t\t\trequire.NoError(t, err)\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "event.go",
          "type": "blob",
          "size": 29.3232421875,
          "content": "// Copyright 2018 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"fmt\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/humanize\"\n\t\"github.com/cockroachdb/pebble/internal/invariants\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/cockroachdb/redact\"\n)\n\n// TableInfo exports the manifest.TableInfo type.\ntype TableInfo = manifest.TableInfo\n\nfunc tablesTotalSize(tables []TableInfo) uint64 {\n\tvar size uint64\n\tfor i := range tables {\n\t\tsize += tables[i].Size\n\t}\n\treturn size\n}\n\nfunc formatFileNums(tables []TableInfo) string {\n\tvar buf strings.Builder\n\tfor i := range tables {\n\t\tif i > 0 {\n\t\t\tbuf.WriteString(\" \")\n\t\t}\n\t\tbuf.WriteString(tables[i].FileNum.String())\n\t}\n\treturn buf.String()\n}\n\n// LevelInfo contains info pertaining to a particular level.\ntype LevelInfo struct {\n\tLevel  int\n\tTables []TableInfo\n\tScore  float64\n}\n\nfunc (i LevelInfo) String() string {\n\treturn redact.StringWithoutMarkers(i)\n}\n\n// SafeFormat implements redact.SafeFormatter.\nfunc (i LevelInfo) SafeFormat(w redact.SafePrinter, _ rune) {\n\tw.Printf(\"L%d [%s] (%s) Score=%.2f\",\n\t\tredact.Safe(i.Level),\n\t\tredact.Safe(formatFileNums(i.Tables)),\n\t\tredact.Safe(humanize.Bytes.Uint64(tablesTotalSize(i.Tables))),\n\t\tredact.Safe(i.Score))\n}\n\n// CompactionInfo contains the info for a compaction event.\ntype CompactionInfo struct {\n\t// JobID is the ID of the compaction job.\n\tJobID int\n\t// Reason is the reason for the compaction.\n\tReason string\n\t// Input contains the input tables for the compaction organized by level.\n\tInput []LevelInfo\n\t// Output contains the output tables generated by the compaction. The output\n\t// tables are empty for the compaction begin event.\n\tOutput LevelInfo\n\t// Duration is the time spent compacting, including reading and writing\n\t// sstables.\n\tDuration time.Duration\n\t// TotalDuration is the total wall-time duration of the compaction,\n\t// including applying the compaction to the database. TotalDuration is\n\t// always ≥ Duration.\n\tTotalDuration time.Duration\n\tDone          bool\n\tErr           error\n\n\tSingleLevelOverlappingRatio float64\n\tMultiLevelOverlappingRatio  float64\n\n\t// Annotations specifies additional info to appear in a compaction's event log line\n\tAnnotations compactionAnnotations\n}\n\ntype compactionAnnotations []string\n\n// SafeFormat implements redact.SafeFormatter.\nfunc (ca compactionAnnotations) SafeFormat(w redact.SafePrinter, _ rune) {\n\tif len(ca) == 0 {\n\t\treturn\n\t}\n\tfor i := range ca {\n\t\tif i != 0 {\n\t\t\tw.Print(\" \")\n\t\t}\n\t\tw.Printf(\"%s\", redact.SafeString(ca[i]))\n\t}\n}\n\nfunc (i CompactionInfo) String() string {\n\treturn redact.StringWithoutMarkers(i)\n}\n\n// SafeFormat implements redact.SafeFormatter.\nfunc (i CompactionInfo) SafeFormat(w redact.SafePrinter, _ rune) {\n\tif i.Err != nil {\n\t\tw.Printf(\"[JOB %d] compaction(%s) to L%d error: %s\",\n\t\t\tredact.Safe(i.JobID), redact.SafeString(i.Reason), redact.Safe(i.Output.Level), i.Err)\n\t\treturn\n\t}\n\n\tif !i.Done {\n\t\tw.Printf(\"[JOB %d] compacting(%s) \",\n\t\t\tredact.Safe(i.JobID),\n\t\t\tredact.SafeString(i.Reason))\n\t\tif len(i.Annotations) > 0 {\n\t\t\tw.Printf(\"%s \", i.Annotations)\n\t\t}\n\t\tw.Printf(\"%s; \", levelInfos(i.Input))\n\t\tw.Printf(\"OverlappingRatio: Single %.2f, Multi %.2f\", i.SingleLevelOverlappingRatio, i.MultiLevelOverlappingRatio)\n\t\treturn\n\t}\n\toutputSize := tablesTotalSize(i.Output.Tables)\n\tw.Printf(\"[JOB %d] compacted(%s) \", redact.Safe(i.JobID), redact.SafeString(i.Reason))\n\tif len(i.Annotations) > 0 {\n\t\tw.Printf(\"%s \", i.Annotations)\n\t}\n\tw.Print(levelInfos(i.Input))\n\tw.Printf(\" -> L%d [%s] (%s), in %.1fs (%.1fs total), output rate %s/s\",\n\t\tredact.Safe(i.Output.Level),\n\t\tredact.Safe(formatFileNums(i.Output.Tables)),\n\t\tredact.Safe(humanize.Bytes.Uint64(outputSize)),\n\t\tredact.Safe(i.Duration.Seconds()),\n\t\tredact.Safe(i.TotalDuration.Seconds()),\n\t\tredact.Safe(humanize.Bytes.Uint64(uint64(float64(outputSize)/i.Duration.Seconds()))))\n}\n\ntype levelInfos []LevelInfo\n\nfunc (i levelInfos) SafeFormat(w redact.SafePrinter, _ rune) {\n\tfor j, levelInfo := range i {\n\t\tif j > 0 {\n\t\t\tw.Printf(\" + \")\n\t\t}\n\t\tw.Print(levelInfo)\n\t}\n}\n\n// DiskSlowInfo contains the info for a disk slowness event when writing to a\n// file.\ntype DiskSlowInfo = vfs.DiskSlowInfo\n\n// FlushInfo contains the info for a flush event.\ntype FlushInfo struct {\n\t// JobID is the ID of the flush job.\n\tJobID int\n\t// Reason is the reason for the flush.\n\tReason string\n\t// Input contains the count of input memtables that were flushed.\n\tInput int\n\t// InputBytes contains the total in-memory size of the memtable(s) that were\n\t// flushed. This size includes skiplist indexing data structures.\n\tInputBytes uint64\n\t// Output contains the ouptut table generated by the flush. The output info\n\t// is empty for the flush begin event.\n\tOutput []TableInfo\n\t// Duration is the time spent flushing. This duration includes writing and\n\t// syncing all of the flushed keys to sstables.\n\tDuration time.Duration\n\t// TotalDuration is the total wall-time duration of the flush, including\n\t// applying the flush to the database. TotalDuration is always ≥ Duration.\n\tTotalDuration time.Duration\n\t// Ingest is set to true if the flush is handling tables that were added to\n\t// the flushable queue via an ingestion operation.\n\tIngest bool\n\t// IngestLevels are the output levels for each ingested table in the flush.\n\t// This field is only populated when Ingest is true.\n\tIngestLevels []int\n\tDone         bool\n\tErr          error\n}\n\nfunc (i FlushInfo) String() string {\n\treturn redact.StringWithoutMarkers(i)\n}\n\n// SafeFormat implements redact.SafeFormatter.\nfunc (i FlushInfo) SafeFormat(w redact.SafePrinter, _ rune) {\n\tif i.Err != nil {\n\t\tw.Printf(\"[JOB %d] flush error: %s\", redact.Safe(i.JobID), i.Err)\n\t\treturn\n\t}\n\n\tplural := redact.SafeString(\"s\")\n\tif i.Input == 1 {\n\t\tplural = \"\"\n\t}\n\tif !i.Done {\n\t\tw.Printf(\"[JOB %d] \", redact.Safe(i.JobID))\n\t\tif !i.Ingest {\n\t\t\tw.Printf(\"flushing %d memtable\", redact.Safe(i.Input))\n\t\t\tw.SafeString(plural)\n\t\t\tw.Printf(\" (%s) to L0\", redact.Safe(humanize.Bytes.Uint64(i.InputBytes)))\n\t\t} else {\n\t\t\tw.Printf(\"flushing %d ingested table%s\", redact.Safe(i.Input), plural)\n\t\t}\n\t\treturn\n\t}\n\n\toutputSize := tablesTotalSize(i.Output)\n\tif !i.Ingest {\n\t\tif invariants.Enabled && len(i.IngestLevels) > 0 {\n\t\t\tpanic(errors.AssertionFailedf(\"pebble: expected len(IngestedLevels) == 0\"))\n\t\t}\n\t\tw.Printf(\"[JOB %d] flushed %d memtable%s (%s) to L0 [%s] (%s), in %.1fs (%.1fs total), output rate %s/s\",\n\t\t\tredact.Safe(i.JobID), redact.Safe(i.Input), plural,\n\t\t\tredact.Safe(humanize.Bytes.Uint64(i.InputBytes)),\n\t\t\tredact.Safe(formatFileNums(i.Output)),\n\t\t\tredact.Safe(humanize.Bytes.Uint64(outputSize)),\n\t\t\tredact.Safe(i.Duration.Seconds()),\n\t\t\tredact.Safe(i.TotalDuration.Seconds()),\n\t\t\tredact.Safe(humanize.Bytes.Uint64(uint64(float64(outputSize)/i.Duration.Seconds()))))\n\t} else {\n\t\tif invariants.Enabled && len(i.IngestLevels) == 0 {\n\t\t\tpanic(errors.AssertionFailedf(\"pebble: expected len(IngestedLevels) > 0\"))\n\t\t}\n\t\tw.Printf(\"[JOB %d] flushed %d ingested flushable%s\",\n\t\t\tredact.Safe(i.JobID), redact.Safe(len(i.Output)), plural)\n\t\tfor j, level := range i.IngestLevels {\n\t\t\tfile := i.Output[j]\n\t\t\tif j > 0 {\n\t\t\t\tw.Printf(\" +\")\n\t\t\t}\n\t\t\tw.Printf(\" L%d:%s (%s)\", level, file.FileNum, humanize.Bytes.Uint64(file.Size))\n\t\t}\n\t\tw.Printf(\" in %.1fs (%.1fs total), output rate %s/s\",\n\t\t\tredact.Safe(i.Duration.Seconds()),\n\t\t\tredact.Safe(i.TotalDuration.Seconds()),\n\t\t\tredact.Safe(humanize.Bytes.Uint64(uint64(float64(outputSize)/i.Duration.Seconds()))))\n\t}\n}\n\n// DownloadInfo contains the info for a DB.Download() event.\ntype DownloadInfo struct {\n\t// JobID is the ID of the download job.\n\tJobID int\n\n\tSpans []DownloadSpan\n\n\t// Duration is the time since the operation was started.\n\tDuration                    time.Duration\n\tDownloadCompactionsLaunched int\n\n\t// RestartCount indicates that the download operation restarted because it\n\t// noticed that new external files were ingested. A DownloadBegin event with\n\t// RestartCount = 0 is the start of the operation; each time we restart it we\n\t// have another DownloadBegin event with RestartCount > 0.\n\tRestartCount int\n\tDone         bool\n\tErr          error\n}\n\nfunc (i DownloadInfo) String() string {\n\treturn redact.StringWithoutMarkers(i)\n}\n\n// SafeFormat implements redact.SafeFormatter.\nfunc (i DownloadInfo) SafeFormat(w redact.SafePrinter, _ rune) {\n\tswitch {\n\tcase i.Err != nil:\n\t\tw.Printf(\"[JOB %d] download error after %1.fs: %s\", redact.Safe(i.JobID), redact.Safe(i.Duration.Seconds()), i.Err)\n\n\tcase i.Done:\n\t\tw.Printf(\"[JOB %d] download finished in %.1fs (launched %d compactions)\",\n\t\t\tredact.Safe(i.JobID), redact.Safe(i.Duration.Seconds()), redact.Safe(i.DownloadCompactionsLaunched))\n\n\tdefault:\n\t\tif i.RestartCount == 0 {\n\t\t\tw.Printf(\"[JOB %d] starting download for %d spans\", redact.Safe(i.JobID), redact.Safe(len(i.Spans)))\n\t\t} else {\n\t\t\tw.Printf(\"[JOB %d] restarting download (restart #%d, time so far %.1fs, launched %d compactions)\",\n\t\t\t\tredact.Safe(i.JobID), redact.Safe(i.RestartCount), redact.Safe(i.Duration.Seconds()),\n\t\t\t\tredact.Safe(i.DownloadCompactionsLaunched))\n\t\t}\n\t}\n}\n\n// ManifestCreateInfo contains info about a manifest creation event.\ntype ManifestCreateInfo struct {\n\t// JobID is the ID of the job the caused the manifest to be created.\n\tJobID int\n\tPath  string\n\t// The file number of the new Manifest.\n\tFileNum base.DiskFileNum\n\tErr     error\n}\n\nfunc (i ManifestCreateInfo) String() string {\n\treturn redact.StringWithoutMarkers(i)\n}\n\n// SafeFormat implements redact.SafeFormatter.\nfunc (i ManifestCreateInfo) SafeFormat(w redact.SafePrinter, _ rune) {\n\tif i.Err != nil {\n\t\tw.Printf(\"[JOB %d] MANIFEST create error: %s\", redact.Safe(i.JobID), i.Err)\n\t\treturn\n\t}\n\tw.Printf(\"[JOB %d] MANIFEST created %s\", redact.Safe(i.JobID), i.FileNum)\n}\n\n// ManifestDeleteInfo contains the info for a Manifest deletion event.\ntype ManifestDeleteInfo struct {\n\t// JobID is the ID of the job the caused the Manifest to be deleted.\n\tJobID   int\n\tPath    string\n\tFileNum base.DiskFileNum\n\tErr     error\n}\n\nfunc (i ManifestDeleteInfo) String() string {\n\treturn redact.StringWithoutMarkers(i)\n}\n\n// SafeFormat implements redact.SafeFormatter.\nfunc (i ManifestDeleteInfo) SafeFormat(w redact.SafePrinter, _ rune) {\n\tif i.Err != nil {\n\t\tw.Printf(\"[JOB %d] MANIFEST delete error: %s\", redact.Safe(i.JobID), i.Err)\n\t\treturn\n\t}\n\tw.Printf(\"[JOB %d] MANIFEST deleted %s\", redact.Safe(i.JobID), i.FileNum)\n}\n\n// TableCreateInfo contains the info for a table creation event.\ntype TableCreateInfo struct {\n\tJobID int\n\t// Reason is the reason for the table creation: \"compacting\", \"flushing\", or\n\t// \"ingesting\".\n\tReason  string\n\tPath    string\n\tFileNum base.DiskFileNum\n}\n\nfunc (i TableCreateInfo) String() string {\n\treturn redact.StringWithoutMarkers(i)\n}\n\n// SafeFormat implements redact.SafeFormatter.\nfunc (i TableCreateInfo) SafeFormat(w redact.SafePrinter, _ rune) {\n\tw.Printf(\"[JOB %d] %s: sstable created %s\",\n\t\tredact.Safe(i.JobID), redact.Safe(i.Reason), i.FileNum)\n}\n\n// TableDeleteInfo contains the info for a table deletion event.\ntype TableDeleteInfo struct {\n\tJobID   int\n\tPath    string\n\tFileNum base.DiskFileNum\n\tErr     error\n}\n\nfunc (i TableDeleteInfo) String() string {\n\treturn redact.StringWithoutMarkers(i)\n}\n\n// SafeFormat implements redact.SafeFormatter.\nfunc (i TableDeleteInfo) SafeFormat(w redact.SafePrinter, _ rune) {\n\tif i.Err != nil {\n\t\tw.Printf(\"[JOB %d] sstable delete error %s: %s\",\n\t\t\tredact.Safe(i.JobID), i.FileNum, i.Err)\n\t\treturn\n\t}\n\tw.Printf(\"[JOB %d] sstable deleted %s\", redact.Safe(i.JobID), i.FileNum)\n}\n\n// TableIngestInfo contains the info for a table ingestion event.\ntype TableIngestInfo struct {\n\t// JobID is the ID of the job the caused the table to be ingested.\n\tJobID  int\n\tTables []struct {\n\t\tTableInfo\n\t\tLevel int\n\t}\n\t// GlobalSeqNum is the sequence number that was assigned to all entries in\n\t// the ingested table.\n\tGlobalSeqNum base.SeqNum\n\t// flushable indicates whether the ingested sstable was treated as a\n\t// flushable.\n\tflushable bool\n\tErr       error\n}\n\nfunc (i TableIngestInfo) String() string {\n\treturn redact.StringWithoutMarkers(i)\n}\n\n// SafeFormat implements redact.SafeFormatter.\nfunc (i TableIngestInfo) SafeFormat(w redact.SafePrinter, _ rune) {\n\tif i.Err != nil {\n\t\tw.Printf(\"[JOB %d] ingest error: %s\", redact.Safe(i.JobID), i.Err)\n\t\treturn\n\t}\n\n\tif i.flushable {\n\t\tw.Printf(\"[JOB %d] ingested as flushable\", redact.Safe(i.JobID))\n\t} else {\n\t\tw.Printf(\"[JOB %d] ingested\", redact.Safe(i.JobID))\n\t}\n\n\tfor j := range i.Tables {\n\t\tt := &i.Tables[j]\n\t\tif j > 0 {\n\t\t\tw.Printf(\",\")\n\t\t}\n\t\tlevelStr := \"\"\n\t\tif !i.flushable {\n\t\t\tlevelStr = fmt.Sprintf(\"L%d:\", t.Level)\n\t\t}\n\t\tw.Printf(\" %s%s (%s)\", redact.Safe(levelStr), t.FileNum,\n\t\t\tredact.Safe(humanize.Bytes.Uint64(t.Size)))\n\t}\n}\n\n// TableStatsInfo contains the info for a table stats loaded event.\ntype TableStatsInfo struct {\n\t// JobID is the ID of the job that finished loading the initial tables'\n\t// stats.\n\tJobID int\n}\n\nfunc (i TableStatsInfo) String() string {\n\treturn redact.StringWithoutMarkers(i)\n}\n\n// SafeFormat implements redact.SafeFormatter.\nfunc (i TableStatsInfo) SafeFormat(w redact.SafePrinter, _ rune) {\n\tw.Printf(\"[JOB %d] all initial table stats loaded\", redact.Safe(i.JobID))\n}\n\n// TableValidatedInfo contains information on the result of a validation run\n// on an sstable.\ntype TableValidatedInfo struct {\n\tJobID int\n\tMeta  *fileMetadata\n}\n\nfunc (i TableValidatedInfo) String() string {\n\treturn redact.StringWithoutMarkers(i)\n}\n\n// SafeFormat implements redact.SafeFormatter.\nfunc (i TableValidatedInfo) SafeFormat(w redact.SafePrinter, _ rune) {\n\tw.Printf(\"[JOB %d] validated table: %s\", redact.Safe(i.JobID), i.Meta)\n}\n\n// WALCreateInfo contains info about a WAL creation event.\ntype WALCreateInfo struct {\n\t// JobID is the ID of the job the caused the WAL to be created.\n\tJobID int\n\tPath  string\n\t// The file number of the new WAL.\n\tFileNum base.DiskFileNum\n\t// The file number of a previous WAL which was recycled to create this\n\t// one. Zero if recycling did not take place.\n\tRecycledFileNum base.DiskFileNum\n\tErr             error\n}\n\nfunc (i WALCreateInfo) String() string {\n\treturn redact.StringWithoutMarkers(i)\n}\n\n// SafeFormat implements redact.SafeFormatter.\nfunc (i WALCreateInfo) SafeFormat(w redact.SafePrinter, _ rune) {\n\tif i.Err != nil {\n\t\tw.Printf(\"[JOB %d] WAL create error: %s\", redact.Safe(i.JobID), i.Err)\n\t\treturn\n\t}\n\n\tif i.RecycledFileNum == 0 {\n\t\tw.Printf(\"[JOB %d] WAL created %s\", redact.Safe(i.JobID), i.FileNum)\n\t\treturn\n\t}\n\n\tw.Printf(\"[JOB %d] WAL created %s (recycled %s)\",\n\t\tredact.Safe(i.JobID), i.FileNum, i.RecycledFileNum)\n}\n\n// WALDeleteInfo contains the info for a WAL deletion event.\n//\n// TODO(sumeer): extend WALDeleteInfo for the failover case in case the path\n// is insufficient to infer whether primary or secondary.\ntype WALDeleteInfo struct {\n\t// JobID is the ID of the job the caused the WAL to be deleted.\n\tJobID   int\n\tPath    string\n\tFileNum base.DiskFileNum\n\tErr     error\n}\n\nfunc (i WALDeleteInfo) String() string {\n\treturn redact.StringWithoutMarkers(i)\n}\n\n// SafeFormat implements redact.SafeFormatter.\nfunc (i WALDeleteInfo) SafeFormat(w redact.SafePrinter, _ rune) {\n\tif i.Err != nil {\n\t\tw.Printf(\"[JOB %d] WAL delete error: %s\", redact.Safe(i.JobID), i.Err)\n\t\treturn\n\t}\n\tw.Printf(\"[JOB %d] WAL deleted %s\", redact.Safe(i.JobID), i.FileNum)\n}\n\n// WriteStallBeginInfo contains the info for a write stall begin event.\ntype WriteStallBeginInfo struct {\n\tReason string\n}\n\nfunc (i WriteStallBeginInfo) String() string {\n\treturn redact.StringWithoutMarkers(i)\n}\n\n// SafeFormat implements redact.SafeFormatter.\nfunc (i WriteStallBeginInfo) SafeFormat(w redact.SafePrinter, _ rune) {\n\tw.Printf(\"write stall beginning: %s\", redact.Safe(i.Reason))\n}\n\n// PossibleAPIMisuseInfo contains the information for a PossibleAPIMisuse event.\ntype PossibleAPIMisuseInfo struct {\n\tKind APIMisuseKind\n\n\t// UserKey is set for the following kinds:\n\t//  - IneffectualSingleDelete,\n\t//  - NondeterministicSingleDelete.\n\tUserKey []byte\n}\n\nfunc (i PossibleAPIMisuseInfo) String() string {\n\tswitch i.Kind {\n\tcase IneffectualSingleDelete, NondeterministicSingleDelete:\n\t\treturn fmt.Sprintf(\"%s (key=%q)\", i.Kind, i.UserKey)\n\tdefault:\n\t\treturn \"invalid\"\n\t}\n}\n\n// APIMisuseKind identifies the type of API misuse represented by a\n// PossibleAPIMisuse event.\ntype APIMisuseKind int8\n\nconst (\n\t// IneffectualSingleDelete is emitted in compactions/flushes if any\n\t// single delete is being elided without deleting a point set/merge.\n\t//\n\t// This event can sometimes be a false positive because of delete-only\n\t// compactions which can cause a recent RANGEDEL to peek below an older\n\t// SINGLEDEL and delete an arbitrary subset of data below that SINGLEDEL.\n\t//\n\t// Example:\n\t//   RANGEDEL [a, c)#10 in L0\n\t//   SINGLEDEL b#5 in L1\n\t//   SET b#3 in L6\n\t//\n\t// If the L6 file containing the SET is narrow and the L1 file containing\n\t// the SINGLEDEL is wide, a delete-only compaction can remove the file in\n\t// L2 before the SINGLEDEL is compacted down. Then when the SINGLEDEL is\n\t// compacted down, it will not find any SET to delete, resulting in the\n\t// ineffectual callback.\n\tIneffectualSingleDelete APIMisuseKind = iota\n\n\t// NondeterministicSingleDelete is emitted in compactions/flushes if any\n\t// single delete has consumed a Set/Merge, and there is another immediately\n\t// older Set/SetWithDelete/Merge. The user of Pebble has violated the\n\t// invariant under which SingleDelete can be used correctly.\n\t//\n\t// Consider the sequence SingleDelete#3, Set#2, Set#1. There are three\n\t// ways some of these keys can first meet in a compaction.\n\t//\n\t// - All 3 keys in the same compaction: this callback will detect the\n\t//   violation.\n\t//\n\t// - SingleDelete#3, Set#2 meet in a compaction first: Both keys will\n\t//   disappear. The violation will not be detected, and the DB will have\n\t//   Set#1 which is likely incorrect (from the user's perspective).\n\t//\n\t// - Set#2, Set#1 meet in a compaction first: The output will be Set#2,\n\t//   which will later be consumed by SingleDelete#3. The violation will\n\t//   not be detected and the DB will be correct.\n\t//\n\t// This event can sometimes be a false positive because of delete-only\n\t// compactions which can cause a recent RANGEDEL to peek below an older\n\t// SINGLEDEL and delete an arbitrary subset of data below that SINGLEDEL.\n\t//\n\t// Example:\n\t//   RANGEDEL [a, z)#60 in L0\n\t//   SINGLEDEL g#50 in L1\n\t//   SET g#40 in L2\n\t//   RANGEDEL [g,h)#30 in L3\n\t//   SET g#20 in L6\n\t//\n\t// In this example, the two SETs represent the same user write, and the\n\t// RANGEDELs are caused by the CockroachDB range being dropped. That is,\n\t// the user wrote to g once, range was dropped, then added back, which\n\t// caused the SET again, then at some point g was validly deleted using a\n\t// SINGLEDEL, and then the range was dropped again. The older RANGEDEL can\n\t// get fragmented due to compactions it has been part of. Say this L3 file\n\t// containing the RANGEDEL is very narrow, while the L1, L2, L6 files are\n\t// wider than the RANGEDEL in L0. Then the RANGEDEL in L3 can be dropped\n\t// using a delete-only compaction, resulting in an LSM with state:\n\t//\n\t//   RANGEDEL [a, z)#60 in L0\n\t//   SINGLEDEL g#50 in L1\n\t//   SET g#40 in L2\n\t//   SET g#20 in L6\n\t//\n\t// A multi-level compaction involving L1, L2, L6 will cause the invariant\n\t// violation callback. This example doesn't need multi-level compactions:\n\t// say there was a Pebble snapshot at g#21 preventing g#20 from being\n\t// dropped when it meets g#40 in a compaction. That snapshot will not save\n\t// RANGEDEL [g,h)#30, so we can have:\n\t//\n\t//   SINGLEDEL g#50 in L1\n\t//   SET g#40, SET g#20 in L6\n\t//\n\t// And say the snapshot is removed and then the L1 and L6 compaction\n\t// happens, resulting in the invariant violation callback.\n\tNondeterministicSingleDelete\n)\n\nfunc (k APIMisuseKind) String() string {\n\tswitch k {\n\tcase IneffectualSingleDelete:\n\t\treturn \"ineffectual SINGLEDEL\"\n\tcase NondeterministicSingleDelete:\n\t\treturn \"nondeterministicv SINGLEDEL\"\n\tdefault:\n\t\treturn \"unknown\"\n\t}\n}\n\n// EventListener contains a set of functions that will be invoked when various\n// significant DB events occur. Note that the functions should not run for an\n// excessive amount of time as they are invoked synchronously by the DB and may\n// block continued DB work. For a similar reason it is advisable to not perform\n// any synchronous calls back into the DB.\ntype EventListener struct {\n\t// BackgroundError is invoked whenever an error occurs during a background\n\t// operation such as flush or compaction.\n\tBackgroundError func(error)\n\n\t// CompactionBegin is invoked after the inputs to a compaction have been\n\t// determined, but before the compaction has produced any output.\n\tCompactionBegin func(CompactionInfo)\n\n\t// CompactionEnd is invoked after a compaction has completed and the result\n\t// has been installed.\n\tCompactionEnd func(CompactionInfo)\n\n\t// DiskSlow is invoked after a disk write operation on a file created with a\n\t// disk health checking vfs.FS (see vfs.DefaultWithDiskHealthChecks) is\n\t// observed to exceed the specified disk slowness threshold duration. DiskSlow\n\t// is called on a goroutine that is monitoring slowness/stuckness. The callee\n\t// MUST return without doing any IO, or blocking on anything (like a mutex)\n\t// that is waiting on IO. This is imperative in order to reliably monitor for\n\t// slowness, since if this goroutine gets stuck, the monitoring will stop\n\t// working.\n\tDiskSlow func(DiskSlowInfo)\n\n\t// FlushBegin is invoked after the inputs to a flush have been determined,\n\t// but before the flush has produced any output.\n\tFlushBegin func(FlushInfo)\n\n\t// FlushEnd is invoked after a flush has complated and the result has been\n\t// installed.\n\tFlushEnd func(FlushInfo)\n\n\t// DownloadBegin is invoked when a db.Download operation starts or restarts\n\t// (restarts are caused by new external tables being ingested during the\n\t// operation).\n\tDownloadBegin func(DownloadInfo)\n\n\t// DownloadEnd is invoked when a db.Download operation completes.\n\tDownloadEnd func(DownloadInfo)\n\n\t// FormatUpgrade is invoked after the database's FormatMajorVersion\n\t// is upgraded.\n\tFormatUpgrade func(FormatMajorVersion)\n\n\t// ManifestCreated is invoked after a manifest has been created.\n\tManifestCreated func(ManifestCreateInfo)\n\n\t// ManifestDeleted is invoked after a manifest has been deleted.\n\tManifestDeleted func(ManifestDeleteInfo)\n\n\t// TableCreated is invoked when a table has been created.\n\tTableCreated func(TableCreateInfo)\n\n\t// TableDeleted is invoked after a table has been deleted.\n\tTableDeleted func(TableDeleteInfo)\n\n\t// TableIngested is invoked after an externally created table has been\n\t// ingested via a call to DB.Ingest().\n\tTableIngested func(TableIngestInfo)\n\n\t// TableStatsLoaded is invoked at most once, when the table stats\n\t// collector has loaded statistics for all tables that existed at Open.\n\tTableStatsLoaded func(TableStatsInfo)\n\n\t// TableValidated is invoked after validation runs on an sstable.\n\tTableValidated func(TableValidatedInfo)\n\n\t// WALCreated is invoked after a WAL has been created.\n\tWALCreated func(WALCreateInfo)\n\n\t// WALDeleted is invoked after a WAL has been deleted.\n\tWALDeleted func(WALDeleteInfo)\n\n\t// WriteStallBegin is invoked when writes are intentionally delayed.\n\tWriteStallBegin func(WriteStallBeginInfo)\n\n\t// WriteStallEnd is invoked when delayed writes are released.\n\tWriteStallEnd func()\n\n\t// PossibleAPIMisuse is invoked when a possible API misuse is detected.\n\tPossibleAPIMisuse func(PossibleAPIMisuseInfo)\n}\n\n// EnsureDefaults ensures that background error events are logged to the\n// specified logger if a handler for those events hasn't been otherwise\n// specified. Ensure all handlers are non-nil so that we don't have to check\n// for nil-ness before invoking.\nfunc (l *EventListener) EnsureDefaults(logger Logger) {\n\tif l.BackgroundError == nil {\n\t\tif logger != nil {\n\t\t\tl.BackgroundError = func(err error) {\n\t\t\t\tlogger.Errorf(\"background error: %s\", err)\n\t\t\t}\n\t\t} else {\n\t\t\tl.BackgroundError = func(error) {}\n\t\t}\n\t}\n\tif l.CompactionBegin == nil {\n\t\tl.CompactionBegin = func(info CompactionInfo) {}\n\t}\n\tif l.CompactionEnd == nil {\n\t\tl.CompactionEnd = func(info CompactionInfo) {}\n\t}\n\tif l.DiskSlow == nil {\n\t\tl.DiskSlow = func(info DiskSlowInfo) {}\n\t}\n\tif l.FlushBegin == nil {\n\t\tl.FlushBegin = func(info FlushInfo) {}\n\t}\n\tif l.FlushEnd == nil {\n\t\tl.FlushEnd = func(info FlushInfo) {}\n\t}\n\tif l.DownloadBegin == nil {\n\t\tl.DownloadBegin = func(info DownloadInfo) {}\n\t}\n\tif l.DownloadEnd == nil {\n\t\tl.DownloadEnd = func(info DownloadInfo) {}\n\t}\n\tif l.FormatUpgrade == nil {\n\t\tl.FormatUpgrade = func(v FormatMajorVersion) {}\n\t}\n\tif l.ManifestCreated == nil {\n\t\tl.ManifestCreated = func(info ManifestCreateInfo) {}\n\t}\n\tif l.ManifestDeleted == nil {\n\t\tl.ManifestDeleted = func(info ManifestDeleteInfo) {}\n\t}\n\tif l.TableCreated == nil {\n\t\tl.TableCreated = func(info TableCreateInfo) {}\n\t}\n\tif l.TableDeleted == nil {\n\t\tl.TableDeleted = func(info TableDeleteInfo) {}\n\t}\n\tif l.TableIngested == nil {\n\t\tl.TableIngested = func(info TableIngestInfo) {}\n\t}\n\tif l.TableStatsLoaded == nil {\n\t\tl.TableStatsLoaded = func(info TableStatsInfo) {}\n\t}\n\tif l.TableValidated == nil {\n\t\tl.TableValidated = func(validated TableValidatedInfo) {}\n\t}\n\tif l.WALCreated == nil {\n\t\tl.WALCreated = func(info WALCreateInfo) {}\n\t}\n\tif l.WALDeleted == nil {\n\t\tl.WALDeleted = func(info WALDeleteInfo) {}\n\t}\n\tif l.WriteStallBegin == nil {\n\t\tl.WriteStallBegin = func(info WriteStallBeginInfo) {}\n\t}\n\tif l.WriteStallEnd == nil {\n\t\tl.WriteStallEnd = func() {}\n\t}\n\tif l.PossibleAPIMisuse == nil {\n\t\tl.PossibleAPIMisuse = func(info PossibleAPIMisuseInfo) {}\n\t}\n}\n\n// MakeLoggingEventListener creates an EventListener that logs all events to the\n// specified logger.\nfunc MakeLoggingEventListener(logger Logger) EventListener {\n\tif logger == nil {\n\t\tlogger = DefaultLogger\n\t}\n\n\treturn EventListener{\n\t\tBackgroundError: func(err error) {\n\t\t\tlogger.Errorf(\"background error: %s\", err)\n\t\t},\n\t\tCompactionBegin: func(info CompactionInfo) {\n\t\t\tlogger.Infof(\"%s\", info)\n\t\t},\n\t\tCompactionEnd: func(info CompactionInfo) {\n\t\t\tlogger.Infof(\"%s\", info)\n\t\t},\n\t\tDiskSlow: func(info DiskSlowInfo) {\n\t\t\tlogger.Infof(\"%s\", info)\n\t\t},\n\t\tFlushBegin: func(info FlushInfo) {\n\t\t\tlogger.Infof(\"%s\", info)\n\t\t},\n\t\tFlushEnd: func(info FlushInfo) {\n\t\t\tlogger.Infof(\"%s\", info)\n\t\t},\n\t\tDownloadBegin: func(info DownloadInfo) {\n\t\t\tlogger.Infof(\"%s\", info)\n\t\t},\n\t\tDownloadEnd: func(info DownloadInfo) {\n\t\t\tlogger.Infof(\"%s\", info)\n\t\t},\n\t\tFormatUpgrade: func(v FormatMajorVersion) {\n\t\t\tlogger.Infof(\"upgraded to format version: %s\", v)\n\t\t},\n\t\tManifestCreated: func(info ManifestCreateInfo) {\n\t\t\tlogger.Infof(\"%s\", info)\n\t\t},\n\t\tManifestDeleted: func(info ManifestDeleteInfo) {\n\t\t\tlogger.Infof(\"%s\", info)\n\t\t},\n\t\tTableCreated: func(info TableCreateInfo) {\n\t\t\tlogger.Infof(\"%s\", info)\n\t\t},\n\t\tTableDeleted: func(info TableDeleteInfo) {\n\t\t\tlogger.Infof(\"%s\", info)\n\t\t},\n\t\tTableIngested: func(info TableIngestInfo) {\n\t\t\tlogger.Infof(\"%s\", info)\n\t\t},\n\t\tTableStatsLoaded: func(info TableStatsInfo) {\n\t\t\tlogger.Infof(\"%s\", info)\n\t\t},\n\t\tTableValidated: func(info TableValidatedInfo) {\n\t\t\tlogger.Infof(\"%s\", info)\n\t\t},\n\t\tWALCreated: func(info WALCreateInfo) {\n\t\t\tlogger.Infof(\"%s\", info)\n\t\t},\n\t\tWALDeleted: func(info WALDeleteInfo) {\n\t\t\tlogger.Infof(\"%s\", info)\n\t\t},\n\t\tWriteStallBegin: func(info WriteStallBeginInfo) {\n\t\t\tlogger.Infof(\"%s\", info)\n\t\t},\n\t\tWriteStallEnd: func() {\n\t\t\tlogger.Infof(\"write stall ending\")\n\t\t},\n\t\tPossibleAPIMisuse: func(info PossibleAPIMisuseInfo) {\n\t\t\tlogger.Infof(\"API misuse: %s\", info)\n\t\t},\n\t}\n}\n\n// TeeEventListener wraps two EventListeners, forwarding all events to both.\nfunc TeeEventListener(a, b EventListener) EventListener {\n\ta.EnsureDefaults(nil)\n\tb.EnsureDefaults(nil)\n\treturn EventListener{\n\t\tBackgroundError: func(err error) {\n\t\t\ta.BackgroundError(err)\n\t\t\tb.BackgroundError(err)\n\t\t},\n\t\tCompactionBegin: func(info CompactionInfo) {\n\t\t\ta.CompactionBegin(info)\n\t\t\tb.CompactionBegin(info)\n\t\t},\n\t\tCompactionEnd: func(info CompactionInfo) {\n\t\t\ta.CompactionEnd(info)\n\t\t\tb.CompactionEnd(info)\n\t\t},\n\t\tDiskSlow: func(info DiskSlowInfo) {\n\t\t\ta.DiskSlow(info)\n\t\t\tb.DiskSlow(info)\n\t\t},\n\t\tFlushBegin: func(info FlushInfo) {\n\t\t\ta.FlushBegin(info)\n\t\t\tb.FlushBegin(info)\n\t\t},\n\t\tFlushEnd: func(info FlushInfo) {\n\t\t\ta.FlushEnd(info)\n\t\t\tb.FlushEnd(info)\n\t\t},\n\t\tDownloadBegin: func(info DownloadInfo) {\n\t\t\ta.DownloadBegin(info)\n\t\t\tb.DownloadBegin(info)\n\t\t},\n\t\tDownloadEnd: func(info DownloadInfo) {\n\t\t\ta.DownloadEnd(info)\n\t\t\tb.DownloadEnd(info)\n\t\t},\n\t\tFormatUpgrade: func(v FormatMajorVersion) {\n\t\t\ta.FormatUpgrade(v)\n\t\t\tb.FormatUpgrade(v)\n\t\t},\n\t\tManifestCreated: func(info ManifestCreateInfo) {\n\t\t\ta.ManifestCreated(info)\n\t\t\tb.ManifestCreated(info)\n\t\t},\n\t\tManifestDeleted: func(info ManifestDeleteInfo) {\n\t\t\ta.ManifestDeleted(info)\n\t\t\tb.ManifestDeleted(info)\n\t\t},\n\t\tTableCreated: func(info TableCreateInfo) {\n\t\t\ta.TableCreated(info)\n\t\t\tb.TableCreated(info)\n\t\t},\n\t\tTableDeleted: func(info TableDeleteInfo) {\n\t\t\ta.TableDeleted(info)\n\t\t\tb.TableDeleted(info)\n\t\t},\n\t\tTableIngested: func(info TableIngestInfo) {\n\t\t\ta.TableIngested(info)\n\t\t\tb.TableIngested(info)\n\t\t},\n\t\tTableStatsLoaded: func(info TableStatsInfo) {\n\t\t\ta.TableStatsLoaded(info)\n\t\t\tb.TableStatsLoaded(info)\n\t\t},\n\t\tTableValidated: func(info TableValidatedInfo) {\n\t\t\ta.TableValidated(info)\n\t\t\tb.TableValidated(info)\n\t\t},\n\t\tWALCreated: func(info WALCreateInfo) {\n\t\t\ta.WALCreated(info)\n\t\t\tb.WALCreated(info)\n\t\t},\n\t\tWALDeleted: func(info WALDeleteInfo) {\n\t\t\ta.WALDeleted(info)\n\t\t\tb.WALDeleted(info)\n\t\t},\n\t\tWriteStallBegin: func(info WriteStallBeginInfo) {\n\t\t\ta.WriteStallBegin(info)\n\t\t\tb.WriteStallBegin(info)\n\t\t},\n\t\tWriteStallEnd: func() {\n\t\t\ta.WriteStallEnd()\n\t\t\tb.WriteStallEnd()\n\t\t},\n\t\tPossibleAPIMisuse: func(info PossibleAPIMisuseInfo) {\n\t\t\ta.PossibleAPIMisuse(info)\n\t\t\tb.PossibleAPIMisuse(info)\n\t\t},\n\t}\n}\n"
        },
        {
          "name": "event_listener_test.go",
          "type": "blob",
          "size": 10.0595703125,
          "content": "// Copyright 2018 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"runtime\"\n\t\"strings\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/datadriven\"\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/objstorage/objstorageprovider\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/cockroachdb/redact\"\n\t\"github.com/stretchr/testify/require\"\n)\n\n// Verify event listener actions, as well as expected filesystem operations.\nfunc TestEventListener(t *testing.T) {\n\tif runtime.GOARCH == \"386\" {\n\t\tt.Skip(\"skipped on 32-bit due to slightly varied output\")\n\t}\n\tvar d *DB\n\tvar memLog base.InMemLogger\n\tmem := vfs.NewMem()\n\trequire.NoError(t, mem.MkdirAll(\"ext\", 0755))\n\n\tdatadriven.RunTest(t, \"testdata/event_listener\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"open\":\n\t\t\tmemLog.Reset()\n\t\t\tlel := MakeLoggingEventListener(&memLog)\n\t\t\tflushBegin, flushEnd := lel.FlushBegin, lel.FlushEnd\n\t\t\tlel.FlushBegin = func(info FlushInfo) {\n\t\t\t\t// Make deterministic.\n\t\t\t\tinfo.InputBytes = 100\n\t\t\t\tflushBegin(info)\n\t\t\t}\n\t\t\tlel.FlushEnd = func(info FlushInfo) {\n\t\t\t\t// Make deterministic.\n\t\t\t\tinfo.InputBytes = 100\n\t\t\t\tflushEnd(info)\n\t\t\t}\n\t\t\topts := &Options{\n\t\t\t\t// The table stats collector runs asynchronously and its\n\t\t\t\t// timing is less predictable. It increments nextJobID, which\n\t\t\t\t// can make these tests flaky. The TableStatsLoaded event is\n\t\t\t\t// tested separately in TestTableStats.\n\t\t\t\tDisableTableStats:     true,\n\t\t\t\tFS:                    vfs.WithLogging(mem, memLog.Infof),\n\t\t\t\tFormatMajorVersion:    internalFormatNewest,\n\t\t\t\tEventListener:         &lel,\n\t\t\t\tMaxManifestFileSize:   1,\n\t\t\t\tL0CompactionThreshold: 10,\n\t\t\t\tWALDir:                \"wal\",\n\t\t\t}\n\t\t\topts.Experimental.EnableColumnarBlocks = func() bool { return true }\n\t\t\tvar err error\n\t\t\td, err = Open(\"db\", opts)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tt := time.Now()\n\t\t\td.timeNow = func() time.Time {\n\t\t\t\tt = t.Add(time.Second)\n\t\t\t\treturn t\n\t\t\t}\n\t\t\td.opts.private.testingAlwaysWaitForCleanup = true\n\t\t\treturn memLog.String()\n\n\t\tcase \"close\":\n\t\t\tmemLog.Reset()\n\t\t\tif err := d.Close(); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn memLog.String()\n\n\t\tcase \"flush\":\n\t\t\tmemLog.Reset()\n\t\t\tif err := d.Set([]byte(\"a\"), nil, nil); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tif err := d.Flush(); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn memLog.String()\n\n\t\tcase \"compact\":\n\t\t\tmemLog.Reset()\n\t\t\tif err := d.Set([]byte(\"a\"), nil, nil); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tif err := d.Compact([]byte(\"a\"), []byte(\"b\"), false); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn memLog.String()\n\n\t\tcase \"checkpoint\":\n\t\t\tmemLog.Reset()\n\t\t\tif err := d.Checkpoint(\"checkpoint\"); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn memLog.String()\n\n\t\tcase \"disable-file-deletions\":\n\t\t\tmemLog.Reset()\n\t\t\td.mu.Lock()\n\t\t\td.disableFileDeletions()\n\t\t\td.mu.Unlock()\n\t\t\treturn memLog.String()\n\n\t\tcase \"enable-file-deletions\":\n\t\t\tmemLog.Reset()\n\t\t\tfunc() {\n\t\t\t\tdefer func() {\n\t\t\t\t\tif r := recover(); r != nil {\n\t\t\t\t\t\tmemLog.Infof(\"%v\", r)\n\t\t\t\t\t}\n\t\t\t\t}()\n\t\t\t\td.mu.Lock()\n\t\t\t\tdefer d.mu.Unlock()\n\t\t\t\td.enableFileDeletions()\n\t\t\t}()\n\t\t\td.TestOnlyWaitForCleaning()\n\t\t\treturn memLog.String()\n\n\t\tcase \"ingest\":\n\t\t\tmemLog.Reset()\n\t\t\tf, err := mem.Create(\"ext/0\", vfs.WriteCategoryUnspecified)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tw := sstable.NewWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{\n\t\t\t\tTableFormat: d.TableFormat(),\n\t\t\t})\n\t\t\tif err := w.Set([]byte(\"a\"), nil); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tif err := w.Close(); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tif err := d.Ingest(context.Background(), []string{\"ext/0\"}); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn memLog.String()\n\n\t\tcase \"ingest-flushable\":\n\t\t\tmemLog.Reset()\n\n\t\t\t// Prevent flushes during this test to ensure determinism.\n\t\t\td.mu.Lock()\n\t\t\td.mu.compact.flushing = true\n\t\t\td.mu.Unlock()\n\n\t\t\tb := d.NewBatch()\n\t\t\tif err := b.Set([]byte(\"a\"), nil, nil); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tif err := d.Apply(b, nil); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\twriteTable := func(name string, key byte) error {\n\t\t\t\tf, err := mem.Create(name, vfs.WriteCategoryUnspecified)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tw := sstable.NewWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{\n\t\t\t\t\tTableFormat: d.TableFormat(),\n\t\t\t\t})\n\t\t\t\tif err := w.Set([]byte{key}, nil); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tif err := w.Close(); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\ttableA, tableB := \"ext/a\", \"ext/b\"\n\t\t\tif err := writeTable(tableA, 'a'); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tif err := writeTable(tableB, 'b'); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tif err := d.Ingest(context.Background(), []string{tableA, tableB}); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\n\t\t\t// Re-enable flushes, to allow the subsequent flush to proceed.\n\t\t\td.mu.Lock()\n\t\t\td.mu.compact.flushing = false\n\t\t\td.mu.Unlock()\n\t\t\tif err := d.Flush(); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn memLog.String()\n\n\t\tcase \"metrics\":\n\t\t\t// The asynchronous loading of table stats can change metrics, so\n\t\t\t// wait for all the tables' stats to be loaded.\n\t\t\td.mu.Lock()\n\t\t\td.waitTableStats()\n\t\t\td.mu.Unlock()\n\n\t\t\treturn d.Metrics().StringForTests()\n\n\t\tcase \"sstables\":\n\t\t\tvar buf bytes.Buffer\n\t\t\ttableInfos, _ := d.SSTables()\n\t\t\tfor i, level := range tableInfos {\n\t\t\t\tif len(level) == 0 {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tfmt.Fprintf(&buf, \"%d:\\n\", i)\n\t\t\t\tfor _, m := range level {\n\t\t\t\t\tfmt.Fprintf(&buf, \"  %d:[%s-%s]\\n\",\n\t\t\t\t\t\tm.FileNum, m.Smallest.UserKey, m.Largest.UserKey)\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn buf.String()\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc TestWriteStallEvents(t *testing.T) {\n\tconst flushCount = 10\n\tconst writeStallEnd = \"write stall ending\"\n\n\ttestCases := []struct {\n\t\tdelayFlush bool\n\t\texpected   string\n\t}{\n\t\t{true, \"memtable count limit reached\"},\n\t\t{false, \"L0 file count limit exceeded\"},\n\t}\n\n\tfor _, c := range testCases {\n\t\tt.Run(\"\", func(t *testing.T) {\n\t\t\tstallEnded := make(chan struct{}, 1)\n\t\t\tcreateReleased := make(chan struct{}, flushCount)\n\t\t\tvar log base.InMemLogger\n\t\t\tvar delayOnce sync.Once\n\t\t\tlistener := &EventListener{\n\t\t\t\tTableCreated: func(info TableCreateInfo) {\n\t\t\t\t\tif c.delayFlush == (info.Reason == \"flushing\") {\n\t\t\t\t\t\tdelayOnce.Do(func() {\n\t\t\t\t\t\t\t<-createReleased\n\t\t\t\t\t\t})\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\tWriteStallBegin: func(info WriteStallBeginInfo) {\n\t\t\t\t\tlog.Infof(\"%s\", info.String())\n\t\t\t\t\tcreateReleased <- struct{}{}\n\t\t\t\t},\n\t\t\t\tWriteStallEnd: func() {\n\t\t\t\t\tlog.Infof(\"%s\", writeStallEnd)\n\t\t\t\t\tselect {\n\t\t\t\t\tcase stallEnded <- struct{}{}:\n\t\t\t\t\tdefault:\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t}\n\t\t\td, err := Open(\"db\", &Options{\n\t\t\t\tEventListener:               listener,\n\t\t\t\tFS:                          vfs.NewMem(),\n\t\t\t\tMemTableSize:                initialMemTableSize,\n\t\t\t\tMemTableStopWritesThreshold: 2,\n\t\t\t\tL0CompactionThreshold:       2,\n\t\t\t\tL0StopWritesThreshold:       2,\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\t\t\tdefer d.Close()\n\n\t\t\tfor i := 0; i < flushCount; i++ {\n\t\t\t\trequire.NoError(t, d.Set([]byte(\"a\"), nil, NoSync))\n\n\t\t\t\tch, err := d.AsyncFlush()\n\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\t// If we're delaying the flush (because we're testing for memtable\n\t\t\t\t// write stalls), we can't wait for the flush to finish as doing so\n\t\t\t\t// would deadlock. If we're not delaying the flush (because we're\n\t\t\t\t// testing for L0 write stals), we wait for the flush to finish so we\n\t\t\t\t// don't create too many memtables which would trigger a memtable write\n\t\t\t\t// stall.\n\t\t\t\tif !c.delayFlush {\n\t\t\t\t\t<-ch\n\t\t\t\t}\n\t\t\t\tif strings.Contains(log.String(), c.expected) {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\t<-stallEnded\n\n\t\t\tevents := log.String()\n\t\t\trequire.Contains(t, events, c.expected)\n\t\t\trequire.Contains(t, events, writeStallEnd)\n\t\t\tif testing.Verbose() {\n\t\t\t\tt.Logf(\"\\n%s\", events)\n\t\t\t}\n\t\t})\n\t}\n}\n\ntype redactLogger struct {\n\tlogger Logger\n}\n\n// Infof implements the Logger.Infof interface.\nfunc (l redactLogger) Infof(format string, args ...interface{}) {\n\tl.logger.Infof(\"%s\", redact.Sprintf(format, args...).Redact())\n}\n\n// Errorf implements the Logger.Errorf interface.\nfunc (l redactLogger) Errorf(format string, args ...interface{}) {\n\tl.logger.Errorf(\"%s\", redact.Sprintf(format, args...).Redact())\n}\n\n// Fatalf implements the Logger.Fatalf interface.\nfunc (l redactLogger) Fatalf(format string, args ...interface{}) {\n\tl.logger.Fatalf(\"%s\", redact.Sprintf(format, args...).Redact())\n}\n\nfunc TestEventListenerRedact(t *testing.T) {\n\t// The vast majority of event listener fields logged are safe and do not\n\t// need to be redacted. Verify that the rare, unsafe error does appear in\n\t// the log redacted.\n\tvar log base.InMemLogger\n\tl := MakeLoggingEventListener(redactLogger{logger: &log})\n\tl.WALDeleted(WALDeleteInfo{\n\t\tJobID:   5,\n\t\tFileNum: base.DiskFileNum(20),\n\t\tErr:     errors.Errorf(\"unredacted error: %s\", \"unredacted string\"),\n\t})\n\trequire.Equal(t, \"[JOB 5] WAL delete error: unredacted error: ‹×›\\n\", log.String())\n}\n\nfunc TestEventListenerEnsureDefaultsBackgroundError(t *testing.T) {\n\te := EventListener{}\n\te.EnsureDefaults(nil)\n\te.BackgroundError(errors.New(\"an example error\"))\n}\n\nfunc TestEventListenerEnsureDefaultsSetsAllCallbacks(t *testing.T) {\n\te := EventListener{}\n\te.EnsureDefaults(nil)\n\ttestAllCallbacksSetInEventListener(t, e)\n}\n\nfunc TestMakeLoggingEventListenerSetsAllCallbacks(t *testing.T) {\n\te := MakeLoggingEventListener(nil)\n\ttestAllCallbacksSetInEventListener(t, e)\n}\n\nfunc TestTeeEventListenerSetsAllCallbacks(t *testing.T) {\n\te := TeeEventListener(EventListener{}, EventListener{})\n\ttestAllCallbacksSetInEventListener(t, e)\n}\n\nfunc testAllCallbacksSetInEventListener(t *testing.T, e EventListener) {\n\tt.Helper()\n\tv := reflect.ValueOf(e)\n\tfor i := 0; i < v.NumField(); i++ {\n\t\tfType := v.Type().Field(i)\n\t\tfVal := v.Field(i)\n\t\trequire.Equal(t, reflect.Func, fType.Type.Kind(), \"unexpected non-func field: %s\", fType.Name)\n\t\trequire.False(t, fVal.IsNil(), \"unexpected nil field: %s\", fType.Name)\n\t}\n}\n"
        },
        {
          "name": "example_test.go",
          "type": "blob",
          "size": 0.759765625,
          "content": "// Copyright 2020 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble_test\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\n\t\"github.com/cockroachdb/pebble\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n)\n\nfunc Example() {\n\tdb, err := pebble.Open(\"\", &pebble.Options{FS: vfs.NewMem()})\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tkey := []byte(\"hello\")\n\tif err := db.Set(key, []byte(\"world\"), pebble.Sync); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tvalue, closer, err := db.Get(key)\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tfmt.Printf(\"%s %s\\n\", key, value)\n\tif err := closer.Close(); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tif err := db.Close(); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\t// Output:\n\t// hello world\n}\n"
        },
        {
          "name": "external_iterator.go",
          "type": "blob",
          "size": 9.677734375,
          "content": "// Copyright 2022 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"context\"\n\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n)\n\n// NewExternalIter takes an input 2d array of sstable files which may overlap\n// across subarrays but not within a subarray (at least as far as points are\n// concerned; range keys are allowed to overlap arbitrarily even within a\n// subarray), and returns an Iterator over the merged contents of the sstables.\n// Input sstables may contain point keys, range keys, range deletions, etc. The\n// input files slice must be sorted in reverse chronological ordering. A key in a\n// file at a lower index subarray will shadow a key with an identical user key\n// contained within a file at a higher index subarray. Each subarray must be\n// sorted in internal key order, where lower index files contain keys that sort\n// left of files with higher indexes.\n//\n// Input sstables must only contain keys with the zero sequence number.\n//\n// Iterators constructed through NewExternalIter do not support all iterator\n// options, including block-property and table filters. NewExternalIter errors\n// if an incompatible option is set.\nfunc NewExternalIter(\n\to *Options, iterOpts *IterOptions, files [][]sstable.ReadableFile,\n) (it *Iterator, err error) {\n\treturn NewExternalIterWithContext(context.Background(), o, iterOpts, files)\n}\n\n// NewExternalIterWithContext is like NewExternalIter, and additionally\n// accepts a context for tracing.\nfunc NewExternalIterWithContext(\n\tctx context.Context, o *Options, iterOpts *IterOptions, files [][]sstable.ReadableFile,\n) (it *Iterator, err error) {\n\tif iterOpts != nil {\n\t\tif err := validateExternalIterOpts(iterOpts); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tvar readers [][]*sstable.Reader\n\n\tseqNumOffset := 0\n\tfor _, levelFiles := range files {\n\t\tseqNumOffset += len(levelFiles)\n\t}\n\tfor _, levelFiles := range files {\n\t\tseqNumOffset -= len(levelFiles)\n\t\tsubReaders, err := openExternalTables(ctx, o, levelFiles, seqNumOffset, o.MakeReaderOptions())\n\t\treaders = append(readers, subReaders)\n\t\tif err != nil {\n\t\t\t// Close all the opened readers.\n\t\t\tfor i := range readers {\n\t\t\t\tfor j := range readers[i] {\n\t\t\t\t\t_ = readers[i][j].Close()\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tbuf := iterAllocPool.Get().(*iterAlloc)\n\tdbi := &buf.dbi\n\t*dbi = Iterator{\n\t\tctx:                 ctx,\n\t\talloc:               buf,\n\t\tmerge:               o.Merger.Merge,\n\t\tcomparer:            *o.Comparer,\n\t\treadState:           nil,\n\t\tkeyBuf:              buf.keyBuf,\n\t\tprefixOrFullSeekKey: buf.prefixOrFullSeekKey,\n\t\tboundsBuf:           buf.boundsBuf,\n\t\tbatch:               nil,\n\t\t// Add the readers to the Iterator so that Close closes them, and\n\t\t// SetOptions can re-construct iterators from them.\n\t\texternalReaders: readers,\n\t\tnewIters: func(context.Context, *manifest.FileMetadata, *IterOptions,\n\t\t\tinternalIterOpts, iterKinds) (iterSet, error) {\n\t\t\t// NB: External iterators are currently constructed without any\n\t\t\t// `levelIters`. newIters should never be called. When we support\n\t\t\t// organizing multiple non-overlapping files into a single level\n\t\t\t// (see TODO below), we'll need to adjust this tableNewIters\n\t\t\t// implementation to open iterators by looking up f in a map\n\t\t\t// of readers indexed by *fileMetadata.\n\t\t\tpanic(\"unreachable\")\n\t\t},\n\t\tseqNum: base.SeqNumMax,\n\t}\n\tif iterOpts != nil {\n\t\tdbi.opts = *iterOpts\n\t\tdbi.processBounds(iterOpts.LowerBound, iterOpts.UpperBound)\n\t}\n\tif err := finishInitializingExternal(ctx, dbi); err != nil {\n\t\tdbi.Close()\n\t\treturn nil, err\n\t}\n\treturn dbi, nil\n}\n\nfunc validateExternalIterOpts(iterOpts *IterOptions) error {\n\tswitch {\n\tcase iterOpts.PointKeyFilters != nil:\n\t\treturn errors.Errorf(\"pebble: external iterator: PointKeyFilters unsupported\")\n\tcase iterOpts.RangeKeyFilters != nil:\n\t\treturn errors.Errorf(\"pebble: external iterator: RangeKeyFilters unsupported\")\n\tcase iterOpts.OnlyReadGuaranteedDurable:\n\t\treturn errors.Errorf(\"pebble: external iterator: OnlyReadGuaranteedDurable unsupported\")\n\tcase iterOpts.UseL6Filters:\n\t\treturn errors.Errorf(\"pebble: external iterator: UseL6Filters unsupported\")\n\t}\n\treturn nil\n}\n\nfunc createExternalPointIter(ctx context.Context, it *Iterator) (topLevelIterator, error) {\n\t// TODO(jackson): In some instances we could generate fewer levels by using\n\t// L0Sublevels code to organize nonoverlapping files into the same level.\n\t// This would allow us to use levelIters and keep a smaller set of data and\n\t// files in-memory. However, it would also require us to identify the bounds\n\t// of all the files upfront.\n\n\tif !it.opts.pointKeys() {\n\t\treturn emptyIter, nil\n\t} else if it.pointIter != nil {\n\t\treturn it.pointIter, nil\n\t}\n\tmlevels := it.alloc.mlevels[:0]\n\n\t// TODO(jackson): External iterators never provide categorized iterator\n\t// stats today because they exist outside the context of a *DB. If the\n\t// sstables being read are on the physical filesystem, we may still want to\n\t// thread a CategoryStatsCollector through so that we collect their stats.\n\n\tif len(it.externalReaders) > cap(mlevels) {\n\t\tmlevels = make([]mergingIterLevel, 0, len(it.externalReaders))\n\t}\n\t// We set a synthetic sequence number, with lower levels having higer numbers.\n\tseqNum := 0\n\tfor _, readers := range it.externalReaders {\n\t\tseqNum += len(readers)\n\t}\n\tfor _, readers := range it.externalReaders {\n\t\tfor _, r := range readers {\n\t\t\tvar (\n\t\t\t\trangeDelIter keyspan.FragmentIterator\n\t\t\t\tpointIter    internalIterator\n\t\t\t\terr          error\n\t\t\t)\n\t\t\t// We could set hideObsoletePoints=true, since we are reading at\n\t\t\t// InternalKeySeqNumMax, but we don't bother since these sstables should\n\t\t\t// not have obsolete points (so the performance optimization is\n\t\t\t// unnecessary), and we don't want to bother constructing a\n\t\t\t// BlockPropertiesFilterer that includes obsoleteKeyBlockPropertyFilter.\n\t\t\ttransforms := sstable.IterTransforms{SyntheticSeqNum: sstable.SyntheticSeqNum(seqNum)}\n\t\t\tseqNum--\n\t\t\tpointIter, err = r.NewPointIter(\n\t\t\t\tctx, transforms, it.opts.LowerBound, it.opts.UpperBound, nil, /* BlockPropertiesFilterer */\n\t\t\t\tsstable.NeverUseFilterBlock,\n\t\t\t\t&it.stats.InternalStats, nil,\n\t\t\t\tsstable.MakeTrivialReaderProvider(r))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\trangeDelIter, err = r.NewRawRangeDelIter(ctx, sstable.FragmentIterTransforms{\n\t\t\t\tSyntheticSeqNum: sstable.SyntheticSeqNum(seqNum),\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tmlevels = append(mlevels, mergingIterLevel{\n\t\t\t\titer:         pointIter,\n\t\t\t\trangeDelIter: rangeDelIter,\n\t\t\t})\n\t\t}\n\t}\n\n\tit.alloc.merging.init(&it.opts, &it.stats.InternalStats, it.comparer.Compare, it.comparer.Split, mlevels...)\n\tit.alloc.merging.snapshot = base.SeqNumMax\n\tif len(mlevels) <= cap(it.alloc.levelsPositioned) {\n\t\tit.alloc.merging.levelsPositioned = it.alloc.levelsPositioned[:len(mlevels)]\n\t}\n\treturn &it.alloc.merging, nil\n}\n\nfunc finishInitializingExternal(ctx context.Context, it *Iterator) error {\n\tpointIter, err := createExternalPointIter(ctx, it)\n\tif err != nil {\n\t\treturn err\n\t}\n\tit.pointIter = pointIter\n\tit.iter = it.pointIter\n\n\tif it.opts.rangeKeys() {\n\t\tit.rangeKeyMasking.init(it, &it.comparer)\n\t\tvar rangeKeyIters []keyspan.FragmentIterator\n\t\tif it.rangeKey == nil {\n\t\t\t// We could take advantage of the lack of overlaps in range keys within\n\t\t\t// each slice in it.externalReaders, and generate keyspanimpl.LevelIters\n\t\t\t// out of those. However, since range keys are expected to be sparse to\n\t\t\t// begin with, the performance gain might not be significant enough to\n\t\t\t// warrant it.\n\t\t\t//\n\t\t\t// TODO(bilal): Explore adding a simpleRangeKeyLevelIter that does not\n\t\t\t// operate on FileMetadatas (similar to simpleLevelIter), and implements\n\t\t\t// this optimization.\n\t\t\t// We set a synthetic sequence number, with lower levels having higer numbers.\n\t\t\tseqNum := 0\n\t\t\tfor _, readers := range it.externalReaders {\n\t\t\t\tseqNum += len(readers)\n\t\t\t}\n\t\t\tfor _, readers := range it.externalReaders {\n\t\t\t\tfor _, r := range readers {\n\t\t\t\t\ttransforms := sstable.FragmentIterTransforms{SyntheticSeqNum: sstable.SyntheticSeqNum(seqNum)}\n\t\t\t\t\tseqNum--\n\t\t\t\t\tif rki, err := r.NewRawRangeKeyIter(ctx, transforms); err != nil {\n\t\t\t\t\t\treturn err\n\t\t\t\t\t} else if rki != nil {\n\t\t\t\t\t\trangeKeyIters = append(rangeKeyIters, rki)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif len(rangeKeyIters) > 0 {\n\t\t\t\tit.rangeKey = iterRangeKeyStateAllocPool.Get().(*iteratorRangeKeyState)\n\t\t\t\tit.rangeKey.init(it.comparer.Compare, it.comparer.Split, &it.opts)\n\t\t\t\tit.rangeKey.rangeKeyIter = it.rangeKey.iterConfig.Init(\n\t\t\t\t\t&it.comparer,\n\t\t\t\t\tbase.SeqNumMax,\n\t\t\t\t\tit.opts.LowerBound, it.opts.UpperBound,\n\t\t\t\t\t&it.hasPrefix, &it.prefixOrFullSeekKey,\n\t\t\t\t\tfalse /* internalKeys */, &it.rangeKey.internal,\n\t\t\t\t)\n\t\t\t\tfor i := range rangeKeyIters {\n\t\t\t\t\tit.rangeKey.iterConfig.AddLevel(rangeKeyIters[i])\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif it.rangeKey != nil {\n\t\t\tit.rangeKey.iiter.Init(&it.comparer, it.iter, it.rangeKey.rangeKeyIter,\n\t\t\t\tkeyspan.InterleavingIterOpts{\n\t\t\t\t\tMask:       &it.rangeKeyMasking,\n\t\t\t\t\tLowerBound: it.opts.LowerBound,\n\t\t\t\t\tUpperBound: it.opts.UpperBound,\n\t\t\t\t})\n\t\t\tit.iter = &it.rangeKey.iiter\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc openExternalTables(\n\tctx context.Context,\n\to *Options,\n\tfiles []sstable.ReadableFile,\n\tseqNumOffset int,\n\treaderOpts sstable.ReaderOptions,\n) (readers []*sstable.Reader, err error) {\n\treaders = make([]*sstable.Reader, 0, len(files))\n\tfor i := range files {\n\t\treadable, err := sstable.NewSimpleReadable(files[i])\n\t\tif err != nil {\n\t\t\treturn readers, err\n\t\t}\n\t\tr, err := sstable.NewReader(ctx, readable, readerOpts)\n\t\tif err != nil {\n\t\t\treturn readers, err\n\t\t}\n\t\treaders = append(readers, r)\n\t}\n\treturn readers, err\n}\n"
        },
        {
          "name": "external_iterator_test.go",
          "type": "blob",
          "size": 3.7568359375,
          "content": "// Copyright 2022 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/cockroachdb/datadriven\"\n\t\"github.com/cockroachdb/pebble/internal/testkeys\"\n\t\"github.com/cockroachdb/pebble/objstorage/objstorageprovider\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestExternalIterator(t *testing.T) {\n\tmem := vfs.NewMem()\n\to := &Options{\n\t\tFS:                 mem,\n\t\tFormatMajorVersion: internalFormatNewest,\n\t\tComparer:           testkeys.Comparer,\n\t}\n\to.Experimental.EnableColumnarBlocks = func() bool { return true }\n\to.testingRandomized(t)\n\to.EnsureDefaults()\n\td, err := Open(\"\", o)\n\trequire.NoError(t, err)\n\tdefer func() { require.NoError(t, d.Close()) }()\n\n\tdatadriven.RunTest(t, \"testdata/external_iterator\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"reset\":\n\t\t\tmem = vfs.NewMem()\n\t\t\treturn \"\"\n\t\tcase \"build\":\n\t\t\tif err := runBuildCmd(td, d, mem); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\t\tcase \"iter\":\n\t\t\topts := IterOptions{KeyTypes: IterKeyTypePointsAndRanges}\n\t\t\tvar files [][]sstable.ReadableFile\n\t\t\tfor _, arg := range td.CmdArgs {\n\t\t\t\tswitch arg.Key {\n\t\t\t\tcase \"mask-suffix\":\n\t\t\t\t\topts.RangeKeyMasking.Suffix = []byte(arg.Vals[0])\n\t\t\t\tcase \"lower\":\n\t\t\t\t\topts.LowerBound = []byte(arg.Vals[0])\n\t\t\t\tcase \"upper\":\n\t\t\t\t\topts.UpperBound = []byte(arg.Vals[0])\n\t\t\t\tcase \"files\":\n\t\t\t\t\tfor _, v := range arg.Vals {\n\t\t\t\t\t\tf, err := mem.Open(v)\n\t\t\t\t\t\trequire.NoError(t, err)\n\t\t\t\t\t\tfiles = append(files, []sstable.ReadableFile{f})\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tit, err := NewExternalIter(o, &opts, files)\n\t\t\trequire.NoError(t, err)\n\t\t\treturn runIterCmd(td, it, true /* close iter */)\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc BenchmarkExternalIter_NonOverlapping_Scan(b *testing.B) {\n\tks := testkeys.Alpha(6)\n\topts := (&Options{Comparer: testkeys.Comparer}).EnsureDefaults()\n\titerOpts := &IterOptions{\n\t\tKeyTypes: IterKeyTypePointsAndRanges,\n\t}\n\twriteOpts := opts.MakeWriterOptions(6, sstable.TableFormatPebblev2)\n\n\tfor _, keyCount := range []int{100, 10_000, 100_000} {\n\t\tb.Run(fmt.Sprintf(\"keys=%d\", keyCount), func(b *testing.B) {\n\t\t\tfor _, fileCount := range []int{1, 10, 100} {\n\t\t\t\tb.Run(fmt.Sprintf(\"files=%d\", fileCount), func(b *testing.B) {\n\t\t\t\t\tvar fs vfs.FS = vfs.NewMem()\n\t\t\t\t\tfilenames := make([]string, fileCount)\n\t\t\t\t\tvar keys [][]byte\n\t\t\t\t\tfor i := 0; i < fileCount; i++ {\n\t\t\t\t\t\tfilename := fmt.Sprintf(\"%03d.sst\", i)\n\t\t\t\t\t\twf, err := fs.Create(filename, vfs.WriteCategoryUnspecified)\n\t\t\t\t\t\trequire.NoError(b, err)\n\t\t\t\t\t\tw := sstable.NewWriter(objstorageprovider.NewFileWritable(wf), writeOpts)\n\t\t\t\t\t\tfor j := 0; j < keyCount/fileCount; j++ {\n\t\t\t\t\t\t\tkey := testkeys.Key(ks, int64(len(keys)))\n\t\t\t\t\t\t\tkeys = append(keys, key)\n\t\t\t\t\t\t\trequire.NoError(b, w.Set(key, key))\n\t\t\t\t\t\t}\n\t\t\t\t\t\trequire.NoError(b, w.Close())\n\t\t\t\t\t\tfilenames[i] = filename\n\t\t\t\t\t}\n\n\t\t\t\t\tb.ResetTimer()\n\t\t\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t\t\tfunc() {\n\t\t\t\t\t\t\tfiles := make([][]sstable.ReadableFile, fileCount)\n\t\t\t\t\t\t\tfor i := 0; i < fileCount; i++ {\n\t\t\t\t\t\t\t\tf, err := fs.Open(filenames[i])\n\t\t\t\t\t\t\t\trequire.NoError(b, err)\n\t\t\t\t\t\t\t\tfiles[i] = []sstable.ReadableFile{f}\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tit, err := NewExternalIter(opts, iterOpts, files)\n\t\t\t\t\t\t\trequire.NoError(b, err)\n\t\t\t\t\t\t\tdefer it.Close()\n\n\t\t\t\t\t\t\tk := 0\n\t\t\t\t\t\t\tfor valid := it.First(); valid; valid = it.NextPrefix() {\n\t\t\t\t\t\t\t\tif !bytes.Equal(it.Key(), keys[k]) {\n\t\t\t\t\t\t\t\t\tb.Fatalf(\"expected key %q, found %q\", keys[k+1], it.Key())\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\tk++\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif k != len(keys) {\n\t\t\t\t\t\t\t\tb.Fatalf(\"k=%d, expected %d\\n\", k, len(keys))\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}()\n\t\t\t\t\t}\n\t\t\t\t})\n\t\t\t}\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "external_test.go",
          "type": "blob",
          "size": 4.1904296875,
          "content": "// Copyright 2024 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble_test\n\nimport (\n\t\"bytes\"\n\t\"io\"\n\t\"math/rand/v2\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/pebble/metamorphic\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/cockroachdb/pebble/vfs/errorfs\"\n\t\"github.com/stretchr/testify/require\"\n)\n\n// TestIteratorErrors is a randomized test designed to ensure that errors\n// encountered by reads are properly propagated through to the user. It uses the\n// metamorphic tests configured with only write operations to first generate a\n// random database. It then uses the metamorphic tests to run a random set of\n// read operations against the generated database, randomly injecting errors at\n// the VFS layer. If an error is injected over the course of an operation, it\n// expects the error to surface to the operation output. If it doesn't, the test\n// fails.\nfunc TestIteratorErrors(t *testing.T) {\n\tseed := time.Now().UnixNano()\n\tt.Logf(\"Using seed %d\", seed)\n\trng := rand.New(rand.NewPCG(0, uint64(seed)))\n\n\t// Generate a random database by running the metamorphic test with the\n\t// WriteOpConfig. We'll perform ~10,000 random operations that mutate the\n\t// state of the database.\n\ttestOpts := metamorphic.RandomOptions(rng, nil /* custom opt parsers */)\n\t// With even a very small injection probability, it's relatively\n\t// unlikely that pebble.DebugCheckLevels will successfully complete\n\t// without being interrupted by an ErrInjected. Omit these checks.\n\t// TODO(jackson): Alternatively, we could wrap pebble.DebugCheckLevels,\n\t// mark the error value as having originated from CheckLevels, and retry\n\t// at most once. We would need to skip retrying on the second invocation\n\t// of DebugCheckLevels. It's all likely more trouble than it's worth.\n\ttestOpts.Opts.DebugCheck = nil\n\t// Disable the physical FS so we don't need to worry about paths down below.\n\tif fs := testOpts.Opts.FS; fs == nil || fs == vfs.Default {\n\t\ttestOpts.Opts.FS = vfs.NewMem()\n\t}\n\n\ttestOpts.Opts.Cache.Ref()\n\t{\n\t\ttest, err := metamorphic.New(\n\t\t\tmetamorphic.GenerateOps(rng, 10000, metamorphic.WriteOpConfig()),\n\t\t\ttestOpts, \"\" /* dir */, io.Discard)\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, metamorphic.Execute(test))\n\t}\n\tt.Log(\"Constructed test database state\")\n\t{\n\t\ttestOpts.Opts.DisableTableStats = true\n\t\ttestOpts.Opts.DisableAutomaticCompactions = true\n\n\t\t// Create an errorfs injector that injects ErrInjected on 5% of reads.\n\t\t// Wrap it in both a counter and a toggle so that we a) know whether an\n\t\t// error was injected over the course of an operation, and b) so that we\n\t\t// can disable error injection during Open.\n\t\tpredicate := errorfs.And(errorfs.Reads, errorfs.Randomly(0.50, seed))\n\t\tcounter := errorfs.Counter{Injector: errorfs.ErrInjected.If(predicate)}\n\t\ttoggle := errorfs.Toggle{Injector: &counter}\n\t\ttestOpts.Opts.FS = errorfs.Wrap(testOpts.Opts.FS, &toggle)\n\t\ttestOpts.Opts.ReadOnly = true\n\n\t\ttest, err := metamorphic.New(\n\t\t\tmetamorphic.GenerateOps(rng, 5000, metamorphic.ReadOpConfig()),\n\t\t\ttestOpts, \"\" /* dir */, &testWriter{t: t})\n\t\trequire.NoError(t, err)\n\n\t\tdefer func() {\n\t\t\tif r := recover(); r != nil {\n\t\t\t\tt.Errorf(\"last injected error: %+v\", counter.LastError())\n\t\t\t\tpanic(r)\n\t\t\t}\n\t\t}()\n\n\t\t// Begin injecting errors.\n\t\ttoggle.On()\n\n\t\tprevCount := counter.Load()\n\t\tmore := true\n\t\tfor i := 0; more; i++ {\n\t\t\tvar operationOutput string\n\t\t\tmore, operationOutput, err = test.Step()\n\t\t\t// test.Step returns an error if the test called Fatalf. Error\n\t\t\t// injection should NOT trigger calls to Fatalf.\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tnewCount := counter.Load()\n\t\t\tif diff := newCount - prevCount; diff > 0 {\n\t\t\t\tif !strings.Contains(operationOutput, errorfs.ErrInjected.Error()) {\n\t\t\t\t\tt.Fatalf(\"Injected %d errors in op %d but the operation output %q does not contain the injected error: %+v\",\n\t\t\t\t\t\tdiff, i, operationOutput, counter.LastError())\n\t\t\t\t}\n\t\t\t}\n\t\t\tprevCount = newCount\n\t\t}\n\t\tt.Logf(\"Injected %d errors over the course of the test.\", counter.Load())\n\t}\n}\n\ntype testWriter struct {\n\tt *testing.T\n}\n\nfunc (w *testWriter) Write(b []byte) (int, error) {\n\tw.t.Log(string(bytes.TrimSpace(b)))\n\treturn len(b), nil\n}\n"
        },
        {
          "name": "file_cache.go",
          "type": "blob",
          "size": 39.3173828125,
          "content": "// Copyright 2020 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"runtime/debug\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"unsafe\"\n\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/cache\"\n\t\"github.com/cockroachdb/pebble/internal/invariants\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan/keyspanimpl\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n\t\"github.com/cockroachdb/pebble/internal/sstableinternal\"\n\t\"github.com/cockroachdb/pebble/objstorage\"\n\t\"github.com/cockroachdb/pebble/objstorage/objstorageprovider/objiotracing\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n\t\"github.com/cockroachdb/pebble/sstable/valblk\"\n)\n\nvar emptyIter = &errorIter{err: nil}\nvar emptyKeyspanIter = &errorKeyspanIter{err: nil}\n\n// tableNewIters creates new iterators (point, range deletion and/or range key)\n// for the given file metadata. Which of the various iterator kinds the user is\n// requesting is specified with the iterKinds bitmap.\n//\n// On success, the requested subset of iters.{point,rangeDel,rangeKey} are\n// populated with iterators.\n//\n// If a point iterator is requested and the operation was successful,\n// iters.point is guaranteed to be non-nil and must be closed when the caller is\n// finished.\n//\n// If a range deletion or range key iterator is requested, the corresponding\n// iterator may be nil if the table does not contain any keys of the\n// corresponding kind. The returned iterSet type provides RangeDeletion() and\n// RangeKey() convenience methods that return non-nil empty iterators that may\n// be used if the caller requires a non-nil iterator.\n//\n// On error, all iterators are nil.\n//\n// The only (non-test) implementation of tableNewIters is\n// fileCacheContainer.newIters().\ntype tableNewIters func(\n\tctx context.Context,\n\tfile *manifest.FileMetadata,\n\topts *IterOptions,\n\tinternalOpts internalIterOpts,\n\tkinds iterKinds,\n) (iterSet, error)\n\n// tableNewRangeDelIter takes a tableNewIters and returns a TableNewSpanIter\n// for the rangedel iterator returned by tableNewIters.\nfunc tableNewRangeDelIter(newIters tableNewIters) keyspanimpl.TableNewSpanIter {\n\treturn func(ctx context.Context, file *manifest.FileMetadata, iterOptions keyspan.SpanIterOptions) (keyspan.FragmentIterator, error) {\n\t\titers, err := newIters(ctx, file, nil, internalIterOpts{}, iterRangeDeletions)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn iters.RangeDeletion(), nil\n\t}\n}\n\n// tableNewRangeKeyIter takes a tableNewIters and returns a TableNewSpanIter\n// for the range key iterator returned by tableNewIters.\nfunc tableNewRangeKeyIter(newIters tableNewIters) keyspanimpl.TableNewSpanIter {\n\treturn func(ctx context.Context, file *manifest.FileMetadata, iterOptions keyspan.SpanIterOptions) (keyspan.FragmentIterator, error) {\n\t\titers, err := newIters(ctx, file, nil, internalIterOpts{}, iterRangeKeys)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn iters.RangeKey(), nil\n\t}\n}\n\n// fileCacheOpts contains the db specific fields of a file cache. This is stored\n// in the fileCacheContainer along with the file cache.\n//\n// NB: It is important to make sure that the fields in this struct are\n// read-only. Since the fields here are shared by every single fileCacheShard,\n// if non read-only fields are updated, we could have unnecessary evictions of\n// those fields, and the surrounding fields from the CPU caches.\ntype fileCacheOpts struct {\n\t// iterCount keeps track of how many iterators are open. It is used to keep\n\t// track of leaked iterators on a per-db level.\n\titerCount *atomic.Int32\n\n\tloggerAndTracer   LoggerAndTracer\n\tcache             *cache.Cache\n\tcacheID           cache.ID\n\tobjProvider       objstorage.Provider\n\treaderOpts        sstable.ReaderOptions\n\tsstStatsCollector *sstable.CategoryStatsCollector\n}\n\n// fileCacheContainer contains the file cache and fields which are unique to the\n// DB.\ntype fileCacheContainer struct {\n\tfileCache *FileCache\n\n\t// dbOpts contains fields relevant to the file cache which are unique to\n\t// each DB.\n\tdbOpts fileCacheOpts\n}\n\n// newFileCacheContainer will panic if the underlying block cache in the file\n// cache doesn't match Options.Cache.\nfunc newFileCacheContainer(\n\tfc *FileCache,\n\tcacheID cache.ID,\n\tobjProvider objstorage.Provider,\n\topts *Options,\n\tsize int,\n\tsstStatsCollector *sstable.CategoryStatsCollector,\n) *fileCacheContainer {\n\t// We will release a ref to the file cache acquired here when\n\t// fileCacheContainer.close is called.\n\tif fc != nil {\n\t\tif fc.cache != opts.Cache {\n\t\t\tpanic(\"pebble: underlying cache for the file cache and db are different\")\n\t\t}\n\t\tfc.Ref()\n\t} else {\n\t\t// NewFileCache should create a ref to fc which the container should\n\t\t// drop whenever it is closed.\n\t\tfc = NewFileCache(opts.Cache, opts.Experimental.FileCacheShards, size)\n\t}\n\n\tt := &fileCacheContainer{}\n\tt.fileCache = fc\n\tt.dbOpts.loggerAndTracer = opts.LoggerAndTracer\n\tt.dbOpts.cache = opts.Cache\n\tt.dbOpts.cacheID = cacheID\n\tt.dbOpts.objProvider = objProvider\n\tt.dbOpts.readerOpts = opts.MakeReaderOptions()\n\tt.dbOpts.readerOpts.FilterMetricsTracker = &sstable.FilterMetricsTracker{}\n\tt.dbOpts.iterCount = new(atomic.Int32)\n\tt.dbOpts.sstStatsCollector = sstStatsCollector\n\treturn t\n}\n\n// Before calling close, make sure that there will be no further need\n// to access any of the files associated with the store.\nfunc (c *fileCacheContainer) close() error {\n\t// We want to do some cleanup work here. Check for leaked iterators\n\t// by the DB using this container. Note that we'll still perform cleanup\n\t// below in the case that there are leaked iterators.\n\tvar err error\n\tif v := c.dbOpts.iterCount.Load(); v > 0 {\n\t\terr = errors.Errorf(\"leaked iterators: %d\", errors.Safe(v))\n\t}\n\n\t// Release nodes here.\n\tfor _, shard := range c.fileCache.shards {\n\t\tif shard != nil {\n\t\t\tshard.removeDB(&c.dbOpts)\n\t\t}\n\t}\n\treturn firstError(err, c.fileCache.Unref())\n}\n\nfunc (c *fileCacheContainer) newIters(\n\tctx context.Context,\n\tfile *manifest.FileMetadata,\n\topts *IterOptions,\n\tinternalOpts internalIterOpts,\n\tkinds iterKinds,\n) (iterSet, error) {\n\treturn c.fileCache.getShard(file.FileBacking.DiskFileNum).newIters(ctx, file, opts, internalOpts, &c.dbOpts, kinds)\n}\n\n// getTableProperties returns the properties associated with the backing physical\n// table if the input metadata belongs to a virtual sstable.\nfunc (c *fileCacheContainer) getTableProperties(file *fileMetadata) (*sstable.Properties, error) {\n\treturn c.fileCache.getShard(file.FileBacking.DiskFileNum).getTableProperties(file, &c.dbOpts)\n}\n\nfunc (c *fileCacheContainer) evict(fileNum base.DiskFileNum) {\n\tc.fileCache.getShard(fileNum).evict(fileNum, &c.dbOpts, false)\n}\n\nfunc (c *fileCacheContainer) metrics() (CacheMetrics, FilterMetrics) {\n\tvar m CacheMetrics\n\tfor i := range c.fileCache.shards {\n\t\ts := c.fileCache.shards[i]\n\t\ts.mu.RLock()\n\t\tm.Count += int64(len(s.mu.nodes))\n\t\ts.mu.RUnlock()\n\t\tm.Hits += s.hits.Load()\n\t\tm.Misses += s.misses.Load()\n\t}\n\tm.Size = m.Count * int64(unsafe.Sizeof(fileCacheNode{})+unsafe.Sizeof(fileCacheValue{})+unsafe.Sizeof(sstable.Reader{}))\n\tf := c.dbOpts.readerOpts.FilterMetricsTracker.Load()\n\treturn m, f\n}\n\nfunc (c *fileCacheContainer) estimateSize(\n\tmeta *fileMetadata, lower, upper []byte,\n) (size uint64, err error) {\n\tc.withCommonReader(meta, func(cr sstable.CommonReader) error {\n\t\tsize, err = cr.EstimateDiskUsage(lower, upper)\n\t\treturn err\n\t})\n\treturn size, err\n}\n\n// createCommonReader creates a Reader for this file.\nfunc createCommonReader(v *fileCacheValue, file *fileMetadata) sstable.CommonReader {\n\t// TODO(bananabrick): We suffer an allocation if file is a virtual sstable.\n\tr := v.mustSSTableReader()\n\tvar cr sstable.CommonReader = r\n\tif file.Virtual {\n\t\tvirtualReader := sstable.MakeVirtualReader(\n\t\t\tr, file.VirtualMeta().VirtualReaderParams(v.isShared),\n\t\t)\n\t\tcr = &virtualReader\n\t}\n\treturn cr\n}\n\nfunc (c *fileCacheContainer) withCommonReader(\n\tmeta *fileMetadata, fn func(sstable.CommonReader) error,\n) error {\n\ts := c.fileCache.getShard(meta.FileBacking.DiskFileNum)\n\tv := s.findNode(context.TODO(), meta.FileBacking, &c.dbOpts)\n\tdefer s.unrefValue(v)\n\tif v.err != nil {\n\t\treturn v.err\n\t}\n\treturn fn(createCommonReader(v, meta))\n}\n\nfunc (c *fileCacheContainer) withReader(meta physicalMeta, fn func(*sstable.Reader) error) error {\n\ts := c.fileCache.getShard(meta.FileBacking.DiskFileNum)\n\tv := s.findNode(context.TODO(), meta.FileBacking, &c.dbOpts)\n\tdefer s.unrefValue(v)\n\tif v.err != nil {\n\t\treturn v.err\n\t}\n\treturn fn(v.reader.(*sstable.Reader))\n}\n\n// withVirtualReader fetches a VirtualReader associated with a virtual sstable.\nfunc (c *fileCacheContainer) withVirtualReader(\n\tmeta virtualMeta, fn func(sstable.VirtualReader) error,\n) error {\n\ts := c.fileCache.getShard(meta.FileBacking.DiskFileNum)\n\tv := s.findNode(context.TODO(), meta.FileBacking, &c.dbOpts)\n\tdefer s.unrefValue(v)\n\tif v.err != nil {\n\t\treturn v.err\n\t}\n\tprovider := c.dbOpts.objProvider\n\tobjMeta, err := provider.Lookup(fileTypeTable, meta.FileBacking.DiskFileNum)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn fn(sstable.MakeVirtualReader(v.mustSSTableReader(), meta.VirtualReaderParams(objMeta.IsShared())))\n}\n\nfunc (c *fileCacheContainer) iterCount() int64 {\n\treturn int64(c.dbOpts.iterCount.Load())\n}\n\n// FileCache is a shareable cache for open files. Open files are exclusively\n// sstable files today.\ntype FileCache struct {\n\trefs atomic.Int64\n\n\tcache  *Cache\n\tshards []*fileCacheShard\n}\n\n// Ref adds a reference to the file cache. Once a file cache is constructed, the\n// cache only remains valid if there is at least one reference to it.\nfunc (c *FileCache) Ref() {\n\tv := c.refs.Add(1)\n\t// We don't want the reference count to ever go from 0 -> 1,\n\t// cause a reference count of 0 implies that we've closed the cache.\n\tif v <= 1 {\n\t\tpanic(fmt.Sprintf(\"pebble: inconsistent reference count: %d\", v))\n\t}\n}\n\n// Unref removes a reference to the file cache.\nfunc (c *FileCache) Unref() error {\n\tv := c.refs.Add(-1)\n\tswitch {\n\tcase v < 0:\n\t\tpanic(fmt.Sprintf(\"pebble: inconsistent reference count: %d\", v))\n\tcase v == 0:\n\t\tvar err error\n\t\tfor i := range c.shards {\n\t\t\t// The cache shard is not allocated yet, nothing to close.\n\t\t\tif c.shards[i] == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\terr = firstError(err, c.shards[i].Close())\n\t\t}\n\n\t\t// Unref the cache which we create a reference to when the file cache is\n\t\t// first instantiated.\n\t\tc.cache.Unref()\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// NewFileCache will create a new file cache with one outstanding reference. It\n// is the callers responsibility to call Unref if they will no longer hold a\n// reference to the file cache.\nfunc NewFileCache(cache *Cache, numShards int, size int) *FileCache {\n\tif size == 0 {\n\t\tpanic(\"pebble: cannot create a file cache of size 0\")\n\t} else if numShards == 0 {\n\t\tpanic(\"pebble: cannot create a file cache with 0 shards\")\n\t}\n\n\tc := &FileCache{}\n\tc.cache = cache\n\tc.cache.Ref()\n\n\tc.shards = make([]*fileCacheShard, numShards)\n\tfor i := range c.shards {\n\t\tc.shards[i] = &fileCacheShard{}\n\t\tc.shards[i].init(size / len(c.shards))\n\t}\n\n\t// Hold a ref to the cache here.\n\tc.refs.Store(1)\n\n\treturn c\n}\n\nfunc (c *FileCache) getShard(fileNum base.DiskFileNum) *fileCacheShard {\n\treturn c.shards[uint64(fileNum)%uint64(len(c.shards))]\n}\n\ntype fileCacheKey struct {\n\tcacheID cache.ID\n\tfileNum base.DiskFileNum\n}\n\ntype fileCacheShard struct {\n\thits      atomic.Int64\n\tmisses    atomic.Int64\n\titerCount atomic.Int32\n\n\tsize int\n\n\tmu struct {\n\t\tsync.RWMutex\n\t\tnodes map[fileCacheKey]*fileCacheNode\n\t\t// The iters map is only created and populated in race builds.\n\t\titers map[io.Closer][]byte\n\n\t\thandHot  *fileCacheNode\n\t\thandCold *fileCacheNode\n\t\thandTest *fileCacheNode\n\n\t\tcoldTarget int\n\t\tsizeHot    int\n\t\tsizeCold   int\n\t\tsizeTest   int\n\t}\n\treleasing       sync.WaitGroup\n\treleasingCh     chan *fileCacheValue\n\treleaseLoopExit sync.WaitGroup\n}\n\nfunc (c *fileCacheShard) init(size int) {\n\tc.size = size\n\n\tc.mu.nodes = make(map[fileCacheKey]*fileCacheNode)\n\tc.mu.coldTarget = size\n\tc.releasingCh = make(chan *fileCacheValue, 100)\n\tc.releaseLoopExit.Add(1)\n\tgo c.releaseLoop()\n\n\tif invariants.RaceEnabled {\n\t\tc.mu.iters = make(map[io.Closer][]byte)\n\t}\n}\n\nfunc (c *fileCacheShard) releaseLoop() {\n\tdefer c.releaseLoopExit.Done()\n\tfor v := range c.releasingCh {\n\t\tv.release(c)\n\t}\n}\n\n// checkAndIntersectFilters checks the specific table and block property filters\n// for intersection with any available table and block-level properties. Returns\n// true for ok if this table should be read by this iterator.\nfunc checkAndIntersectFilters(\n\tr *sstable.Reader,\n\tblockPropertyFilters []BlockPropertyFilter,\n\tboundLimitedFilter sstable.BoundLimitedBlockPropertyFilter,\n\tsyntheticSuffix sstable.SyntheticSuffix,\n) (ok bool, filterer *sstable.BlockPropertiesFilterer, err error) {\n\tif boundLimitedFilter != nil || len(blockPropertyFilters) > 0 {\n\t\tfilterer, err = sstable.IntersectsTable(\n\t\t\tblockPropertyFilters,\n\t\t\tboundLimitedFilter,\n\t\t\tr.Properties.UserProperties,\n\t\t\tsyntheticSuffix,\n\t\t)\n\t\t// NB: IntersectsTable will return a nil filterer if the table-level\n\t\t// properties indicate there's no intersection with the provided filters.\n\t\tif filterer == nil || err != nil {\n\t\t\treturn false, nil, err\n\t\t}\n\t}\n\treturn true, filterer, nil\n}\n\nfunc (c *fileCacheShard) newIters(\n\tctx context.Context,\n\tfile *manifest.FileMetadata,\n\topts *IterOptions,\n\tinternalOpts internalIterOpts,\n\tdbOpts *fileCacheOpts,\n\tkinds iterKinds,\n) (iterSet, error) {\n\t// TODO(sumeer): constructing the Reader should also use a plumbed context,\n\t// since parts of the sstable are read during the construction. The Reader\n\t// should not remember that context since the Reader can be long-lived.\n\n\t// Calling findNode gives us the responsibility of decrementing v's\n\t// refCount. If opening the underlying table resulted in error, then we\n\t// decrement this straight away. Otherwise, we pass that responsibility to\n\t// the sstable iterator, which decrements when it is closed.\n\tv := c.findNode(ctx, file.FileBacking, dbOpts)\n\tif v.err != nil {\n\t\tdefer c.unrefValue(v)\n\t\treturn iterSet{}, v.err\n\t}\n\n\tr := v.mustSSTableReader()\n\t// Note: This suffers an allocation for virtual sstables.\n\tcr := createCommonReader(v, file)\n\tvar iters iterSet\n\tvar err error\n\tif kinds.RangeKey() && file.HasRangeKeys {\n\t\titers.rangeKey, err = newRangeKeyIter(ctx, r, file, cr, opts.SpanIterOptions())\n\t}\n\tif kinds.RangeDeletion() && file.HasPointKeys && err == nil {\n\t\titers.rangeDeletion, err = newRangeDelIter(ctx, file, cr, dbOpts)\n\t}\n\tif kinds.Point() && err == nil {\n\t\titers.point, err = c.newPointIter(ctx, v, file, cr, opts, internalOpts, dbOpts)\n\t}\n\tif err != nil {\n\t\t// NB: There's a subtlety here: Because the point iterator is the last\n\t\t// iterator we attempt to create, it's not possible for:\n\t\t//   err != nil && iters.point != nil\n\t\t// If it were possible, we'd need to account for it to avoid double\n\t\t// unref-ing here, once during CloseAll and once during `unrefValue`.\n\t\titers.CloseAll()\n\t\tc.unrefValue(v)\n\t\treturn iterSet{}, err\n\t}\n\t// Only point iterators ever require the reader stay pinned in the cache. If\n\t// we're not returning a point iterator to the caller, we need to unref v.\n\tif iters.point == nil {\n\t\tc.unrefValue(v)\n\t}\n\treturn iters, nil\n}\n\n// For flushable ingests, we decide whether to use the bloom filter base on\n// size.\nconst filterBlockSizeLimitForFlushableIngests = 64 * 1024\n\n// newPointIter is an internal helper that constructs a point iterator over a\n// sstable. This function is for internal use only, and callers should use\n// newIters instead.\nfunc (c *fileCacheShard) newPointIter(\n\tctx context.Context,\n\tv *fileCacheValue,\n\tfile *manifest.FileMetadata,\n\tcr sstable.CommonReader,\n\topts *IterOptions,\n\tinternalOpts internalIterOpts,\n\tdbOpts *fileCacheOpts,\n) (internalIterator, error) {\n\tvar (\n\t\thideObsoletePoints bool = false\n\t\tpointKeyFilters    []BlockPropertyFilter\n\t\tfilterer           *sstable.BlockPropertiesFilterer\n\t)\n\tr := v.mustSSTableReader()\n\tif opts != nil {\n\t\t// This code is appending (at most one filter) in-place to\n\t\t// opts.PointKeyFilters even though the slice is shared for iterators in\n\t\t// the same iterator tree. This is acceptable since all the following\n\t\t// properties are true:\n\t\t// - The iterator tree is single threaded, so the shared backing for the\n\t\t//   slice is being mutated in a single threaded manner.\n\t\t// - Each shallow copy of the slice has its own notion of length.\n\t\t// - The appended element is always the obsoleteKeyBlockPropertyFilter\n\t\t//   struct, which is stateless, so overwriting that struct when creating\n\t\t//   one sstable iterator is harmless to other sstable iterators that are\n\t\t//   relying on that struct.\n\t\t//\n\t\t// An alternative would be to have different slices for different sstable\n\t\t// iterators, but that requires more work to avoid allocations.\n\t\t//\n\t\t// TODO(bilal): for compaction reads of foreign sstables, we do hide\n\t\t// obsolete points (see sstable.Reader.newCompactionIter) but we don't\n\t\t// apply the obsolete block property filter. We could optimize this by\n\t\t// applying the filter.\n\t\thideObsoletePoints, pointKeyFilters =\n\t\t\tr.TryAddBlockPropertyFilterForHideObsoletePoints(\n\t\t\t\topts.snapshotForHideObsoletePoints, file.LargestSeqNum, opts.PointKeyFilters)\n\n\t\tvar ok bool\n\t\tvar err error\n\t\tok, filterer, err = checkAndIntersectFilters(r, pointKeyFilters,\n\t\t\tinternalOpts.boundLimitedFilter, file.SyntheticPrefixAndSuffix.Suffix())\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t} else if !ok {\n\t\t\t// No point keys within the table match the filters.\n\t\t\treturn nil, nil\n\t\t}\n\t}\n\n\tvar iter sstable.Iterator\n\tfilterBlockSizeLimit := sstable.AlwaysUseFilterBlock\n\tif opts != nil {\n\t\t// By default, we don't use block filters for L6 and restrict the size for\n\t\t// flushable ingests, as these blocks can be very big.\n\t\tif !opts.UseL6Filters {\n\t\t\tif opts.layer == manifest.Level(6) {\n\t\t\t\tfilterBlockSizeLimit = sstable.NeverUseFilterBlock\n\t\t\t} else if opts.layer.IsFlushableIngests() {\n\t\t\t\tfilterBlockSizeLimit = filterBlockSizeLimitForFlushableIngests\n\t\t\t}\n\t\t}\n\t\tif opts.layer.IsSet() && !opts.layer.IsFlushableIngests() {\n\t\t\tctx = objiotracing.WithLevel(ctx, opts.layer.Level())\n\t\t}\n\t}\n\ttableFormat, err := r.TableFormat()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif v.isShared && file.SyntheticSeqNum() != 0 {\n\t\tif tableFormat < sstable.TableFormatPebblev4 {\n\t\t\treturn nil, errors.New(\"pebble: shared ingested sstable has a lower table format than expected\")\n\t\t}\n\t\t// The table is shared and ingested.\n\t\thideObsoletePoints = true\n\t}\n\ttransforms := file.IterTransforms()\n\ttransforms.HideObsoletePoints = hideObsoletePoints\n\titerStatsAccum := internalOpts.iterStatsAccumulator\n\tif iterStatsAccum == nil && opts != nil && dbOpts.sstStatsCollector != nil {\n\t\titerStatsAccum = dbOpts.sstStatsCollector.Accumulator(\n\t\t\tuint64(uintptr(unsafe.Pointer(r))), opts.Category)\n\t}\n\tif internalOpts.compaction {\n\t\titer, err = cr.NewCompactionIter(transforms, iterStatsAccum, &v.readerProvider, internalOpts.bufferPool)\n\t} else {\n\t\titer, err = cr.NewPointIter(\n\t\t\tctx, transforms, opts.GetLowerBound(), opts.GetUpperBound(), filterer, filterBlockSizeLimit,\n\t\t\tinternalOpts.stats, iterStatsAccum, &v.readerProvider)\n\t}\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t// NB: v.closeHook takes responsibility for calling unrefValue(v) here. Take\n\t// care to avoid introducing an allocation here by adding a closure.\n\titer.SetCloseHook(v.closeHook)\n\tc.iterCount.Add(1)\n\tdbOpts.iterCount.Add(1)\n\tif invariants.RaceEnabled {\n\t\tc.mu.Lock()\n\t\tc.mu.iters[iter] = debug.Stack()\n\t\tc.mu.Unlock()\n\t}\n\treturn iter, nil\n}\n\n// newRangeDelIter is an internal helper that constructs an iterator over a\n// sstable's range deletions. This function is for table-cache internal use\n// only, and callers should use newIters instead.\nfunc newRangeDelIter(\n\tctx context.Context, file *manifest.FileMetadata, cr sstable.CommonReader, dbOpts *fileCacheOpts,\n) (keyspan.FragmentIterator, error) {\n\t// NB: range-del iterator does not maintain a reference to the table, nor\n\t// does it need to read from it after creation.\n\trangeDelIter, err := cr.NewRawRangeDelIter(ctx, file.FragmentIterTransforms())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t// Assert expected bounds in tests.\n\tif invariants.Sometimes(50) && rangeDelIter != nil {\n\t\tcmp := base.DefaultComparer.Compare\n\t\tif dbOpts.readerOpts.Comparer != nil {\n\t\t\tcmp = dbOpts.readerOpts.Comparer.Compare\n\t\t}\n\t\trangeDelIter = keyspan.AssertBounds(\n\t\t\trangeDelIter, file.SmallestPointKey, file.LargestPointKey.UserKey, cmp,\n\t\t)\n\t}\n\treturn rangeDelIter, nil\n}\n\n// newRangeKeyIter is an internal helper that constructs an iterator over a\n// sstable's range keys. This function is for table-cache internal use only, and\n// callers should use newIters instead.\nfunc newRangeKeyIter(\n\tctx context.Context,\n\tr *sstable.Reader,\n\tfile *fileMetadata,\n\tcr sstable.CommonReader,\n\topts keyspan.SpanIterOptions,\n) (keyspan.FragmentIterator, error) {\n\ttransforms := file.FragmentIterTransforms()\n\t// Don't filter a table's range keys if the file contains RANGEKEYDELs.\n\t// The RANGEKEYDELs may delete range keys in other levels. Skipping the\n\t// file's range key blocks may surface deleted range keys below. This is\n\t// done here, rather than deferring to the block-property collector in order\n\t// to maintain parity with point keys and the treatment of RANGEDELs.\n\tif r.Properties.NumRangeKeyDels == 0 && len(opts.RangeKeyFilters) > 0 {\n\t\tok, _, err := checkAndIntersectFilters(r, opts.RangeKeyFilters, nil, transforms.SyntheticSuffix())\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t} else if !ok {\n\t\t\treturn nil, nil\n\t\t}\n\t}\n\t// TODO(radu): wrap in an AssertBounds.\n\treturn cr.NewRawRangeKeyIter(ctx, transforms)\n}\n\n// tableCacheShardReaderProvider implements sstable.ReaderProvider for a\n// specific table.\ntype tableCacheShardReaderProvider struct {\n\tc              *fileCacheShard\n\tdbOpts         *fileCacheOpts\n\tbackingFileNum base.DiskFileNum\n\n\tmu struct {\n\t\tsync.Mutex\n\t\t// v is the result of findNode. Whenever it is not null, we hold a refcount\n\t\t// on the fileCacheValue.\n\t\tv *fileCacheValue\n\t\t// refCount is the number of GetReader() calls that have not received a\n\t\t// corresponding Close().\n\t\trefCount int\n\t}\n}\n\nvar _ valblk.ReaderProvider = &tableCacheShardReaderProvider{}\n\nfunc (rp *tableCacheShardReaderProvider) init(\n\tc *fileCacheShard, dbOpts *fileCacheOpts, backingFileNum base.DiskFileNum,\n) {\n\trp.c = c\n\trp.dbOpts = dbOpts\n\trp.backingFileNum = backingFileNum\n\trp.mu.v = nil\n\trp.mu.refCount = 0\n}\n\n// GetReader implements sstable.ReaderProvider. Note that it is not the\n// responsibility of tableCacheShardReaderProvider to ensure that the file\n// continues to exist. The ReaderProvider is used in iterators where the\n// top-level iterator is pinning the read state and preventing the files from\n// being deleted.\n//\n// The caller must call tableCacheShardReaderProvider.Close.\n//\n// Note that currently the Reader returned here is only used to read value\n// blocks. This reader shouldn't be used for other purposes like reading keys\n// outside of virtual sstable bounds.\n//\n// TODO(bananabrick): We could return a wrapper over the Reader to ensure\n// that the reader isn't used for other purposes.\nfunc (rp *tableCacheShardReaderProvider) GetReader(\n\tctx context.Context,\n) (valblk.ExternalBlockReader, error) {\n\trp.mu.Lock()\n\tdefer rp.mu.Unlock()\n\n\tif rp.mu.v != nil {\n\t\trp.mu.refCount++\n\t\treturn rp.mu.v.mustSSTableReader(), nil\n\t}\n\n\t// Calling findNodeInternal gives us the responsibility of decrementing v's\n\t// refCount. Note that if the table is no longer in the cache,\n\t// findNodeInternal will need to do IO to initialize a new Reader. We hold\n\t// rp.mu during this time so that concurrent GetReader calls block until the\n\t// Reader is created.\n\tv := rp.c.findNodeInternal(ctx, rp.backingFileNum, rp.dbOpts)\n\tif v.err != nil {\n\t\tdefer rp.c.unrefValue(v)\n\t\treturn nil, v.err\n\t}\n\trp.mu.v = v\n\trp.mu.refCount = 1\n\treturn v.mustSSTableReader(), nil\n}\n\n// Close implements sstable.ReaderProvider.\nfunc (rp *tableCacheShardReaderProvider) Close() {\n\trp.mu.Lock()\n\tdefer rp.mu.Unlock()\n\trp.mu.refCount--\n\tif rp.mu.refCount <= 0 {\n\t\tif rp.mu.refCount < 0 {\n\t\t\tpanic(\"pebble: sstable.ReaderProvider misuse\")\n\t\t}\n\t\trp.c.unrefValue(rp.mu.v)\n\t\trp.mu.v = nil\n\t}\n}\n\n// getTableProperties return sst table properties for target file.\nfunc (c *fileCacheShard) getTableProperties(\n\tfile *fileMetadata, dbOpts *fileCacheOpts,\n) (*sstable.Properties, error) {\n\t// Calling findNode gives us the responsibility of decrementing v's refCount here\n\tv := c.findNode(context.TODO(), file.FileBacking, dbOpts)\n\tdefer c.unrefValue(v)\n\n\tif v.err != nil {\n\t\treturn nil, v.err\n\t}\n\tr := v.mustSSTableReader()\n\treturn &r.Properties, nil\n}\n\n// releaseNode releases a node from the fileCacheShard.\n//\n// c.mu must be held when calling this.\nfunc (c *fileCacheShard) releaseNode(n *fileCacheNode) {\n\tc.unlinkNode(n)\n\tc.clearNode(n)\n}\n\n// unlinkNode removes a node from the fileCacheShard, leaving the shard\n// reference in place.\n//\n// c.mu must be held when calling this.\nfunc (c *fileCacheShard) unlinkNode(n *fileCacheNode) {\n\tkey := fileCacheKey{n.cacheID, n.fileNum}\n\tdelete(c.mu.nodes, key)\n\n\tswitch n.ptype {\n\tcase fileCacheNodeHot:\n\t\tc.mu.sizeHot--\n\tcase fileCacheNodeCold:\n\t\tc.mu.sizeCold--\n\tcase fileCacheNodeTest:\n\t\tc.mu.sizeTest--\n\t}\n\n\tif n == c.mu.handHot {\n\t\tc.mu.handHot = c.mu.handHot.prev()\n\t}\n\tif n == c.mu.handCold {\n\t\tc.mu.handCold = c.mu.handCold.prev()\n\t}\n\tif n == c.mu.handTest {\n\t\tc.mu.handTest = c.mu.handTest.prev()\n\t}\n\n\tif n.unlink() == n {\n\t\t// This was the last entry in the cache.\n\t\tc.mu.handHot = nil\n\t\tc.mu.handCold = nil\n\t\tc.mu.handTest = nil\n\t}\n\n\tn.links.prev = nil\n\tn.links.next = nil\n}\n\nfunc (c *fileCacheShard) clearNode(n *fileCacheNode) {\n\tif v := n.value; v != nil {\n\t\tn.value = nil\n\t\tc.unrefValue(v)\n\t}\n}\n\n// unrefValue decrements the reference count for the specified value, releasing\n// it if the reference count fell to 0. Note that the value has a reference if\n// it is present in fileCacheShard.mu.nodes, so a reference count of 0 means the\n// node has already been removed from that map.\nfunc (c *fileCacheShard) unrefValue(v *fileCacheValue) {\n\tif v.refCount.Add(-1) == 0 {\n\t\tc.releasing.Add(1)\n\t\tc.releasingCh <- v\n\t}\n}\n\n// findNode returns the node for the table with the given file number, creating\n// that node if it didn't already exist. The caller is responsible for\n// decrementing the returned node's refCount.\nfunc (c *fileCacheShard) findNode(\n\tctx context.Context, b *fileBacking, dbOpts *fileCacheOpts,\n) *fileCacheValue {\n\t// The backing must have a positive refcount (otherwise it could be deleted\n\t// at any time).\n\tb.MustHaveRefs()\n\t// Caution! Here b can be a physical or virtual sstable, or a blob file.\n\t// File cache sstable readers are associated with the physical backings. All\n\t// virtual tables with the same backing will use the same reader from the\n\t// cache; so no information that can differ among these virtual tables can\n\t// be passed to findNodeInternal.\n\tbackingFileNum := b.DiskFileNum\n\n\treturn c.findNodeInternal(ctx, backingFileNum, dbOpts)\n}\n\nfunc (c *fileCacheShard) findNodeInternal(\n\tctx context.Context, backingFileNum base.DiskFileNum, dbOpts *fileCacheOpts,\n) *fileCacheValue {\n\t// Fast-path for a hit in the cache.\n\tc.mu.RLock()\n\tkey := fileCacheKey{dbOpts.cacheID, backingFileNum}\n\tif n := c.mu.nodes[key]; n != nil && n.value != nil {\n\t\t// Fast-path hit.\n\t\t//\n\t\t// The caller is responsible for decrementing the refCount.\n\t\tv := n.value\n\t\tv.refCount.Add(1)\n\t\tc.mu.RUnlock()\n\t\tn.referenced.Store(true)\n\t\tc.hits.Add(1)\n\t\t<-v.loaded\n\t\treturn v\n\t}\n\tc.mu.RUnlock()\n\n\tc.mu.Lock()\n\n\tn := c.mu.nodes[key]\n\tswitch {\n\tcase n == nil:\n\t\t// Slow-path miss of a non-existent node.\n\t\tn = &fileCacheNode{\n\t\t\tfileNum: backingFileNum,\n\t\t\tptype:   fileCacheNodeCold,\n\t\t}\n\t\tc.addNode(n, dbOpts)\n\t\tc.mu.sizeCold++\n\n\tcase n.value != nil:\n\t\t// Slow-path hit of a hot or cold node.\n\t\t//\n\t\t// The caller is responsible for decrementing the refCount.\n\t\tv := n.value\n\t\tv.refCount.Add(1)\n\t\tn.referenced.Store(true)\n\t\tc.hits.Add(1)\n\t\tc.mu.Unlock()\n\t\t<-v.loaded\n\t\treturn v\n\n\tdefault:\n\t\t// Slow-path miss of a test node.\n\t\tc.unlinkNode(n)\n\t\tc.mu.coldTarget++\n\t\tif c.mu.coldTarget > c.size {\n\t\t\tc.mu.coldTarget = c.size\n\t\t}\n\n\t\tn.referenced.Store(false)\n\t\tn.ptype = fileCacheNodeHot\n\t\tc.addNode(n, dbOpts)\n\t\tc.mu.sizeHot++\n\t}\n\n\tc.misses.Add(1)\n\n\tv := &fileCacheValue{\n\t\tloaded: make(chan struct{}),\n\t}\n\tv.readerProvider.init(c, dbOpts, backingFileNum)\n\tv.refCount.Store(2)\n\t// Cache the closure invoked when an iterator is closed. This avoids an\n\t// allocation on every call to newIters.\n\tv.closeHook = func(i sstable.Iterator) error {\n\t\tif invariants.RaceEnabled {\n\t\t\tc.mu.Lock()\n\t\t\tdelete(c.mu.iters, i)\n\t\t\tc.mu.Unlock()\n\t\t}\n\t\tc.unrefValue(v)\n\t\tc.iterCount.Add(-1)\n\t\tdbOpts.iterCount.Add(-1)\n\t\treturn nil\n\t}\n\tn.value = v\n\n\tc.mu.Unlock()\n\n\t// Note adding to the cache lists must complete before we begin loading the\n\t// table as a failure during load will result in the node being unlinked.\n\tv.load(ctx, backingFileNum, c, dbOpts)\n\treturn v\n}\n\nfunc (c *fileCacheShard) addNode(n *fileCacheNode, dbOpts *fileCacheOpts) {\n\tc.evictNodes()\n\tn.cacheID = dbOpts.cacheID\n\tkey := fileCacheKey{n.cacheID, n.fileNum}\n\tc.mu.nodes[key] = n\n\n\tn.links.next = n\n\tn.links.prev = n\n\tif c.mu.handHot == nil {\n\t\t// First element.\n\t\tc.mu.handHot = n\n\t\tc.mu.handCold = n\n\t\tc.mu.handTest = n\n\t} else {\n\t\tc.mu.handHot.link(n)\n\t}\n\n\tif c.mu.handCold == c.mu.handHot {\n\t\tc.mu.handCold = c.mu.handCold.prev()\n\t}\n}\n\nfunc (c *fileCacheShard) evictNodes() {\n\tfor c.size <= c.mu.sizeHot+c.mu.sizeCold && c.mu.handCold != nil {\n\t\tc.runHandCold()\n\t}\n}\n\nfunc (c *fileCacheShard) runHandCold() {\n\tn := c.mu.handCold\n\tif n.ptype == fileCacheNodeCold {\n\t\tif n.referenced.Load() {\n\t\t\tn.referenced.Store(false)\n\t\t\tn.ptype = fileCacheNodeHot\n\t\t\tc.mu.sizeCold--\n\t\t\tc.mu.sizeHot++\n\t\t} else {\n\t\t\tc.clearNode(n)\n\t\t\tn.ptype = fileCacheNodeTest\n\t\t\tc.mu.sizeCold--\n\t\t\tc.mu.sizeTest++\n\t\t\tfor c.size < c.mu.sizeTest && c.mu.handTest != nil {\n\t\t\t\tc.runHandTest()\n\t\t\t}\n\t\t}\n\t}\n\n\tc.mu.handCold = c.mu.handCold.next()\n\n\tfor c.size-c.mu.coldTarget <= c.mu.sizeHot && c.mu.handHot != nil {\n\t\tc.runHandHot()\n\t}\n}\n\nfunc (c *fileCacheShard) runHandHot() {\n\tif c.mu.handHot == c.mu.handTest && c.mu.handTest != nil {\n\t\tc.runHandTest()\n\t\tif c.mu.handHot == nil {\n\t\t\treturn\n\t\t}\n\t}\n\n\tn := c.mu.handHot\n\tif n.ptype == fileCacheNodeHot {\n\t\tif n.referenced.Load() {\n\t\t\tn.referenced.Store(false)\n\t\t} else {\n\t\t\tn.ptype = fileCacheNodeCold\n\t\t\tc.mu.sizeHot--\n\t\t\tc.mu.sizeCold++\n\t\t}\n\t}\n\n\tc.mu.handHot = c.mu.handHot.next()\n}\n\nfunc (c *fileCacheShard) runHandTest() {\n\tif c.mu.sizeCold > 0 && c.mu.handTest == c.mu.handCold && c.mu.handCold != nil {\n\t\tc.runHandCold()\n\t\tif c.mu.handTest == nil {\n\t\t\treturn\n\t\t}\n\t}\n\n\tn := c.mu.handTest\n\tif n.ptype == fileCacheNodeTest {\n\t\tc.mu.coldTarget--\n\t\tif c.mu.coldTarget < 0 {\n\t\t\tc.mu.coldTarget = 0\n\t\t}\n\t\tc.unlinkNode(n)\n\t\tc.clearNode(n)\n\t}\n\n\tc.mu.handTest = c.mu.handTest.next()\n}\n\nfunc (c *fileCacheShard) evict(fileNum base.DiskFileNum, dbOpts *fileCacheOpts, allowLeak bool) {\n\tc.mu.Lock()\n\tkey := fileCacheKey{dbOpts.cacheID, fileNum}\n\tn := c.mu.nodes[key]\n\tvar v *fileCacheValue\n\tif n != nil {\n\t\t// NB: This is equivalent to fileCacheShard.releaseNode(), but we\n\t\t// perform the fileCacheShard.release() call synchronously below to\n\t\t// ensure the sstable file descriptor is closed before returning. Note\n\t\t// that fileCacheShard.releasing needs to be incremented while holding\n\t\t// fileCacheShard.mu in order to avoid a race with Close()\n\t\tc.unlinkNode(n)\n\t\tv = n.value\n\t\tif v != nil {\n\t\t\tif !allowLeak {\n\t\t\t\tif t := v.refCount.Add(-1); t != 0 {\n\t\t\t\t\tdbOpts.loggerAndTracer.Fatalf(\"sstable %s: refcount is not zero: %d\\n%s\", fileNum, t, debug.Stack())\n\t\t\t\t}\n\t\t\t}\n\t\t\tc.releasing.Add(1)\n\t\t}\n\t}\n\n\tc.mu.Unlock()\n\n\tif v != nil {\n\t\tv.release(c)\n\t}\n\n\tdbOpts.cache.EvictFile(dbOpts.cacheID, fileNum)\n}\n\n// removeDB evicts any nodes which have a reference to the DB\n// associated with dbOpts.cacheID. Make sure that there will\n// be no more accesses to the files associated with the DB.\nfunc (c *fileCacheShard) removeDB(dbOpts *fileCacheOpts) {\n\tvar fileNums []base.DiskFileNum\n\n\tc.mu.RLock()\n\t// Collect the fileNums which need to be cleaned.\n\tvar firstNode *fileCacheNode\n\tnode := c.mu.handHot\n\tfor node != firstNode {\n\t\tif firstNode == nil {\n\t\t\tfirstNode = node\n\t\t}\n\n\t\tif node.cacheID == dbOpts.cacheID {\n\t\t\tfileNums = append(fileNums, node.fileNum)\n\t\t}\n\t\tnode = node.next()\n\t}\n\tc.mu.RUnlock()\n\n\t// Evict all the nodes associated with the DB.\n\t// This should synchronously close all the files\n\t// associated with the DB.\n\tfor _, fileNum := range fileNums {\n\t\tc.evict(fileNum, dbOpts, true)\n\t}\n}\n\nfunc (c *fileCacheShard) Close() error {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\t// Check for leaked iterators. Note that we'll still perform cleanup below in\n\t// the case that there are leaked iterators.\n\tvar err error\n\tif v := c.iterCount.Load(); v > 0 {\n\t\tif !invariants.RaceEnabled {\n\t\t\terr = errors.Errorf(\"leaked iterators: %d\", errors.Safe(v))\n\t\t} else {\n\t\t\tvar buf bytes.Buffer\n\t\t\tfor _, stack := range c.mu.iters {\n\t\t\t\tfmt.Fprintf(&buf, \"%s\\n\", stack)\n\t\t\t}\n\t\t\terr = errors.Errorf(\"leaked iterators: %d\\n%s\", errors.Safe(v), buf.String())\n\t\t}\n\t}\n\n\tfor c.mu.handHot != nil {\n\t\tn := c.mu.handHot\n\t\tif n.value != nil {\n\t\t\tif n.value.refCount.Add(-1) == 0 {\n\t\t\t\tc.releasing.Add(1)\n\t\t\t\tc.releasingCh <- n.value\n\t\t\t}\n\t\t}\n\t\tc.unlinkNode(n)\n\t}\n\tc.mu.nodes = nil\n\tc.mu.handHot = nil\n\tc.mu.handCold = nil\n\tc.mu.handTest = nil\n\n\t// Only shutdown the releasing goroutine if there were no leaked\n\t// iterators. If there were leaked iterators, we leave the goroutine running\n\t// and the releasingCh open so that a subsequent iterator close can\n\t// complete. This behavior is used by iterator leak tests. Leaking the\n\t// goroutine for these tests is less bad not closing the iterator which\n\t// triggers other warnings about block cache handles not being released.\n\tif err != nil {\n\t\tc.releasing.Wait()\n\t\treturn err\n\t}\n\n\tclose(c.releasingCh)\n\tc.releasing.Wait()\n\tc.releaseLoopExit.Wait()\n\treturn err\n}\n\ntype fileCacheValue struct {\n\tcloseHook func(i sstable.Iterator) error\n\treader    io.Closer // *sstable.Reader\n\terr       error\n\tloaded    chan struct{}\n\t// Reference count for the value. The reader is closed when the reference\n\t// count drops to zero.\n\trefCount atomic.Int32\n\tisShared bool\n\n\t// readerProvider is embedded here so that we only allocate it once as long as\n\t// the table stays in the cache. Its state is not always logically tied to\n\t// this specific fileCacheShard - if a table goes out of the cache and then\n\t// comes back in, the readerProvider in a now-defunct fileCacheValue can\n\t// still be used and will internally refer to the new fileCacheValue.\n\treaderProvider tableCacheShardReaderProvider\n}\n\n// mustSSTable retrieves the value's *sstable.Reader. It panics if the cached\n// file is not a sstable (i.e., it is a blob file).\nfunc (v *fileCacheValue) mustSSTableReader() *sstable.Reader {\n\treturn v.reader.(*sstable.Reader)\n}\n\nfunc (v *fileCacheValue) load(\n\tctx context.Context, backingFileNum base.DiskFileNum, c *fileCacheShard, dbOpts *fileCacheOpts,\n) {\n\t// Try opening the file first.\n\tvar f objstorage.Readable\n\tvar r *sstable.Reader\n\tvar err error\n\tf, err = dbOpts.objProvider.OpenForReading(\n\t\tctx, fileTypeTable, backingFileNum, objstorage.OpenOptions{MustExist: true},\n\t)\n\tif err == nil {\n\t\to := dbOpts.readerOpts\n\t\to.SetInternalCacheOpts(sstableinternal.CacheOptions{\n\t\t\tCache:   dbOpts.cache,\n\t\t\tCacheID: dbOpts.cacheID,\n\t\t\tFileNum: backingFileNum,\n\t\t})\n\t\tr, err = sstable.NewReader(ctx, f, o)\n\t}\n\tif err == nil {\n\t\tv.reader = r\n\t\tvar objMeta objstorage.ObjectMetadata\n\t\tobjMeta, err = dbOpts.objProvider.Lookup(fileTypeTable, backingFileNum)\n\t\tv.isShared = objMeta.IsShared()\n\t}\n\tif err != nil {\n\t\tv.err = errors.Wrapf(\n\t\t\terr, \"pebble: backing file %s error\", backingFileNum)\n\t}\n\tif v.err != nil {\n\t\tc.mu.Lock()\n\t\tdefer c.mu.Unlock()\n\t\t// Lookup the node in the cache again as it might have already been\n\t\t// removed.\n\t\tkey := fileCacheKey{dbOpts.cacheID, backingFileNum}\n\t\tn := c.mu.nodes[key]\n\t\tif n != nil && n.value == v {\n\t\t\tc.releaseNode(n)\n\t\t}\n\t}\n\tclose(v.loaded)\n}\n\nfunc (v *fileCacheValue) release(c *fileCacheShard) {\n\t<-v.loaded\n\t// Nothing to be done about an error at this point. Close the reader if it is\n\t// open.\n\tif v.reader != nil {\n\t\t_ = v.reader.Close()\n\t}\n\tc.releasing.Done()\n}\n\ntype fileCacheNodeType int8\n\nconst (\n\tfileCacheNodeTest fileCacheNodeType = iota\n\tfileCacheNodeCold\n\tfileCacheNodeHot\n)\n\nfunc (p fileCacheNodeType) String() string {\n\tswitch p {\n\tcase fileCacheNodeTest:\n\t\treturn \"test\"\n\tcase fileCacheNodeCold:\n\t\treturn \"cold\"\n\tcase fileCacheNodeHot:\n\t\treturn \"hot\"\n\t}\n\treturn \"unknown\"\n}\n\ntype fileCacheNode struct {\n\tfileNum base.DiskFileNum\n\tvalue   *fileCacheValue\n\n\tlinks struct {\n\t\tnext *fileCacheNode\n\t\tprev *fileCacheNode\n\t}\n\tptype fileCacheNodeType\n\t// referenced is atomically set to indicate that this entry has been accessed\n\t// since the last time one of the clock hands swept it.\n\treferenced atomic.Bool\n\n\t// Storing the cache id associated with the DB instance here\n\t// avoids the need to thread the dbOpts struct through many functions.\n\tcacheID cache.ID\n}\n\nfunc (n *fileCacheNode) next() *fileCacheNode {\n\tif n == nil {\n\t\treturn nil\n\t}\n\treturn n.links.next\n}\n\nfunc (n *fileCacheNode) prev() *fileCacheNode {\n\tif n == nil {\n\t\treturn nil\n\t}\n\treturn n.links.prev\n}\n\nfunc (n *fileCacheNode) link(s *fileCacheNode) {\n\ts.links.prev = n.links.prev\n\ts.links.prev.links.next = s\n\ts.links.next = n\n\ts.links.next.links.prev = s\n}\n\nfunc (n *fileCacheNode) unlink() *fileCacheNode {\n\tnext := n.links.next\n\tn.links.prev.links.next = n.links.next\n\tn.links.next.links.prev = n.links.prev\n\tn.links.prev = n\n\tn.links.next = n\n\treturn next\n}\n\n// iterSet holds a set of iterators of various key kinds, all constructed over\n// the same data structure (eg, an sstable). A subset of the fields may be\n// populated depending on the `iterKinds` passed to newIters.\ntype iterSet struct {\n\tpoint         internalIterator\n\trangeDeletion keyspan.FragmentIterator\n\trangeKey      keyspan.FragmentIterator\n}\n\n// TODO(jackson): Consider adding methods for fast paths that check whether an\n// iterator of a particular kind is nil, so that these call sites don't need to\n// reach into the struct's fields directly.\n\n// Point returns the contained point iterator. If there is no point iterator,\n// Point returns a non-nil empty point iterator.\nfunc (s *iterSet) Point() internalIterator {\n\tif s.point == nil {\n\t\treturn emptyIter\n\t}\n\treturn s.point\n}\n\n// RangeDeletion returns the contained range deletion iterator. If there is no\n// range deletion iterator, RangeDeletion returns a non-nil empty keyspan\n// iterator.\nfunc (s *iterSet) RangeDeletion() keyspan.FragmentIterator {\n\tif s.rangeDeletion == nil {\n\t\treturn emptyKeyspanIter\n\t}\n\treturn s.rangeDeletion\n}\n\n// RangeKey returns the contained range key iterator. If there is no range key\n// iterator, RangeKey returns a non-nil empty keyspan iterator.\nfunc (s *iterSet) RangeKey() keyspan.FragmentIterator {\n\tif s.rangeKey == nil {\n\t\treturn emptyKeyspanIter\n\t}\n\treturn s.rangeKey\n}\n\n// CloseAll closes all of the held iterators. If CloseAll is called, then Close\n// must be not be called on the constituent iterators.\nfunc (s *iterSet) CloseAll() error {\n\tvar err error\n\tif s.point != nil {\n\t\terr = s.point.Close()\n\t\ts.point = nil\n\t}\n\tif s.rangeDeletion != nil {\n\t\ts.rangeDeletion.Close()\n\t\ts.rangeDeletion = nil\n\t}\n\tif s.rangeKey != nil {\n\t\ts.rangeKey.Close()\n\t\ts.rangeKey = nil\n\t}\n\treturn err\n}\n\n// iterKinds is a bitmap indicating a set of kinds of iterators. Callers may\n// bitwise-OR iterPointKeys, iterRangeDeletions and/or iterRangeKeys together to\n// represent a set of desired iterator kinds.\ntype iterKinds uint8\n\nfunc (t iterKinds) Point() bool         { return (t & iterPointKeys) != 0 }\nfunc (t iterKinds) RangeDeletion() bool { return (t & iterRangeDeletions) != 0 }\nfunc (t iterKinds) RangeKey() bool      { return (t & iterRangeKeys) != 0 }\n\nconst (\n\titerPointKeys iterKinds = 1 << iota\n\titerRangeDeletions\n\titerRangeKeys\n)\n"
        },
        {
          "name": "file_cache_test.go",
          "type": "blob",
          "size": 35.642578125,
          "content": "// Copyright 2013 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bufio\"\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"math/rand/v2\"\n\t\"os\"\n\t\"path\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n\t\"github.com/cockroachdb/pebble/internal/testkeys\"\n\t\"github.com/cockroachdb/pebble/objstorage\"\n\t\"github.com/cockroachdb/pebble/objstorage/objstorageprovider\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/stretchr/testify/require\"\n)\n\ntype fileCacheTestFile struct {\n\tvfs.File\n\tfs   *fileCacheTestFS\n\tname string\n}\n\nfunc (f *fileCacheTestFile) Close() error {\n\tf.fs.mu.Lock()\n\tif f.fs.closeCounts != nil {\n\t\tf.fs.closeCounts[f.name]++\n\t}\n\tf.fs.mu.Unlock()\n\treturn f.File.Close()\n}\n\ntype fileCacheTestFS struct {\n\tvfs.FS\n\n\tmu               sync.Mutex\n\topenCounts       map[string]int\n\tcloseCounts      map[string]int\n\topenErrorEnabled bool\n}\n\nfunc (fs *fileCacheTestFS) Open(name string, opts ...vfs.OpenOption) (vfs.File, error) {\n\tfs.mu.Lock()\n\tif fs.openErrorEnabled {\n\t\tfs.mu.Unlock()\n\t\treturn nil, errors.New(\"injected error\")\n\t}\n\tif fs.openCounts != nil {\n\t\tfs.openCounts[name]++\n\t}\n\tfs.mu.Unlock()\n\tf, err := fs.FS.Open(name, opts...)\n\tif len(opts) < 1 || opts[0] != vfs.RandomReadsOption {\n\t\treturn nil, errors.Errorf(\"sstable file %s not opened with random reads option\", name)\n\t}\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &fileCacheTestFile{f, fs, name}, nil\n}\n\nfunc (fs *fileCacheTestFS) validate(\n\tt *testing.T, c *fileCacheContainer, f func(i, gotO, gotC int) error,\n) {\n\tif err := fs.validateOpenTables(f); err != nil {\n\t\tt.Error(err)\n\t\treturn\n\t}\n\tc.close()\n\tif err := fs.validateNoneStillOpen(); err != nil {\n\t\tt.Error(err)\n\t\treturn\n\t}\n}\n\nfunc (fs *fileCacheTestFS) setOpenError(enabled bool) {\n\tfs.mu.Lock()\n\tdefer fs.mu.Unlock()\n\tfs.openErrorEnabled = enabled\n}\n\n// validateOpenTables validates that no tables in the cache are open twice, and\n// the number still open is no greater than fileCacheTestCacheSize.\nfunc (fs *fileCacheTestFS) validateOpenTables(f func(i, gotO, gotC int) error) error {\n\t// try backs off to let any clean-up goroutines do their work.\n\treturn try(100*time.Microsecond, 20*time.Second, func() error {\n\t\tfs.mu.Lock()\n\t\tdefer fs.mu.Unlock()\n\n\t\tnumStillOpen := 0\n\t\tfor i := 0; i < fileCacheTestNumTables; i++ {\n\t\t\tfilename := base.MakeFilepath(fs, \"\", fileTypeTable, base.DiskFileNum(i))\n\t\t\tgotO, gotC := fs.openCounts[filename], fs.closeCounts[filename]\n\t\t\tif gotO > gotC {\n\t\t\t\tnumStillOpen++\n\t\t\t}\n\t\t\tif gotC != gotO && gotC != gotO-1 {\n\t\t\t\treturn errors.Errorf(\"i=%d: table closed too many or too few times: opened %d times, closed %d times\",\n\t\t\t\t\ti, gotO, gotC)\n\t\t\t}\n\t\t\tif f != nil {\n\t\t\t\tif err := f(i, gotO, gotC); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif numStillOpen > fileCacheTestCacheSize {\n\t\t\treturn errors.Errorf(\"numStillOpen is %d, want <= %d\", numStillOpen, fileCacheTestCacheSize)\n\t\t}\n\t\treturn nil\n\t})\n}\n\n// validateNoneStillOpen validates that no tables in the cache are open.\nfunc (fs *fileCacheTestFS) validateNoneStillOpen() error {\n\t// try backs off to let any clean-up goroutines do their work.\n\treturn try(100*time.Microsecond, 20*time.Second, func() error {\n\t\tfs.mu.Lock()\n\t\tdefer fs.mu.Unlock()\n\n\t\tfor i := 0; i < fileCacheTestNumTables; i++ {\n\t\t\tfilename := base.MakeFilepath(fs, \"\", fileTypeTable, base.DiskFileNum(i))\n\t\t\tgotO, gotC := fs.openCounts[filename], fs.closeCounts[filename]\n\t\t\tif gotO != gotC {\n\t\t\t\treturn errors.Errorf(\"i=%d: opened %d times, closed %d times\", i, gotO, gotC)\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n}\n\nconst (\n\tfileCacheTestNumTables = 300\n\tfileCacheTestCacheSize = 100\n)\n\n// newFileCacheTest returns a shareable file cache to be used for tests.\n// It is the caller's responsibility to unref the file cache.\nfunc newFileCacheTest(size int64, fileCacheSize int, numShards int) *FileCache {\n\tcache := NewCache(size)\n\tdefer cache.Unref()\n\treturn NewFileCache(cache, numShards, fileCacheSize)\n}\n\nfunc newFileCacheContainerTest(\n\ttc *FileCache, dirname string,\n) (*fileCacheContainer, *fileCacheTestFS, error) {\n\txxx := bytes.Repeat([]byte(\"x\"), fileCacheTestNumTables)\n\tfs := &fileCacheTestFS{\n\t\tFS: vfs.NewMem(),\n\t}\n\tobjProvider, err := objstorageprovider.Open(objstorageprovider.DefaultSettings(fs, dirname))\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tdefer objProvider.Close()\n\n\tfor i := 0; i < fileCacheTestNumTables; i++ {\n\t\tw, _, err := objProvider.Create(context.Background(), fileTypeTable, base.DiskFileNum(i), objstorage.CreateOptions{})\n\t\tif err != nil {\n\t\t\treturn nil, nil, errors.Wrap(err, \"fs.Create\")\n\t\t}\n\t\ttw := sstable.NewWriter(w, sstable.WriterOptions{TableFormat: sstable.TableFormatPebblev2})\n\t\tik := base.ParseInternalKey(fmt.Sprintf(\"k.SET.%d\", i))\n\t\tif err := tw.Raw().AddWithForceObsolete(ik, xxx[:i], false); err != nil {\n\t\t\treturn nil, nil, errors.Wrap(err, \"tw.Set\")\n\t\t}\n\t\tif err := tw.RangeKeySet([]byte(\"k\"), []byte(\"l\"), nil, xxx[:i]); err != nil {\n\t\t\treturn nil, nil, errors.Wrap(err, \"tw.Set\")\n\t\t}\n\t\tif err := tw.Close(); err != nil {\n\t\t\treturn nil, nil, errors.Wrap(err, \"tw.Close\")\n\t\t}\n\t}\n\n\tfs.mu.Lock()\n\tfs.openCounts = map[string]int{}\n\tfs.closeCounts = map[string]int{}\n\tfs.mu.Unlock()\n\n\topts := &Options{}\n\topts.EnsureDefaults()\n\tif tc == nil {\n\t\topts.Cache = NewCache(8 << 20) // 8 MB\n\t\tdefer opts.Cache.Unref()\n\t} else {\n\t\topts.Cache = tc.cache\n\t}\n\n\tc := newFileCacheContainer(tc, opts.Cache.NewID(), objProvider, opts, fileCacheTestCacheSize,\n\t\t&sstable.CategoryStatsCollector{})\n\treturn c, fs, nil\n}\n\n// Test basic reference counting for the file cache.\nfunc TestFileCacheRefs(t *testing.T) {\n\tc := newFileCacheTest(8<<20, 10, 2)\n\n\tv := c.refs.Load()\n\tif v != 1 {\n\t\trequire.Equal(t, 1, v)\n\t}\n\n\tc.Ref()\n\tv = c.refs.Load()\n\tif v != 2 {\n\t\trequire.Equal(t, 2, v)\n\t}\n\n\tc.Unref()\n\tv = c.refs.Load()\n\tif v != 1 {\n\t\trequire.Equal(t, 1, v)\n\t}\n\n\tc.Unref()\n\tv = c.refs.Load()\n\tif v != 0 {\n\t\trequire.Equal(t, 0, v)\n\t}\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tif fmt.Sprint(r) != \"pebble: inconsistent reference count: -1\" {\n\t\t\t\tt.Fatalf(\"unexpected panic message\")\n\t\t\t}\n\t\t} else if r == nil {\n\t\t\tt.Fatalf(\"expected panic\")\n\t\t}\n\t}()\n\tc.Unref()\n}\n\n// Basic test to determine if reads through the file cache are wired correctly.\nfunc TestVirtualReadsWiring(t *testing.T) {\n\tvar d *DB\n\tvar err error\n\td, err = Open(\"\",\n\t\t&Options{\n\t\t\tFS:                 vfs.NewMem(),\n\t\t\tFormatMajorVersion: internalFormatNewest,\n\t\t\tComparer:           testkeys.Comparer,\n\t\t\t// Compactions which conflict with virtual sstable creation can be\n\t\t\t// picked by Pebble. We disable that.\n\t\t\tDisableAutomaticCompactions: true,\n\t\t})\n\trequire.NoError(t, err)\n\n\tb := newBatch(d)\n\t// Some combination of sets, range deletes, and range key sets/unsets, so\n\t// all of the file cache iterator functions are utilized.\n\trequire.NoError(t, b.Set([]byte{'a'}, []byte{'a'}, nil))                           // SeqNum start.\n\trequire.NoError(t, b.Set([]byte{'d'}, []byte{'d'}, nil))                           // SeqNum: +1\n\trequire.NoError(t, b.DeleteRange([]byte{'c'}, []byte{'e'}, nil))                   // SeqNum: +2\n\trequire.NoError(t, b.Set([]byte{'f'}, []byte{'f'}, nil))                           // SeqNum: +3\n\trequire.NoError(t, b.RangeKeySet([]byte{'f'}, []byte{'k'}, nil, []byte{'c'}, nil)) // SeqNum: +4\n\trequire.NoError(t, b.RangeKeyUnset([]byte{'j'}, []byte{'k'}, nil, nil))            // SeqNum: +5\n\trequire.NoError(t, b.Set([]byte{'z'}, []byte{'z'}, nil))                           // SeqNum: +6\n\trequire.NoError(t, d.Apply(b, nil))\n\trequire.NoError(t, d.Flush())\n\trequire.NoError(t, d.Compact([]byte{'a'}, []byte{'b'}, false))\n\trequire.Equal(t, 1, int(d.Metrics().Levels[6].NumFiles))\n\n\td.mu.Lock()\n\n\t// Virtualize the single sstable in the lsm.\n\n\tcurrVersion := d.mu.versions.currentVersion()\n\tl6 := currVersion.Levels[6]\n\tl6FileIter := l6.Iter()\n\tparentFile := l6FileIter.First()\n\tf1 := FileNum(d.mu.versions.nextFileNum.Load())\n\tf2 := f1 + 1\n\td.mu.versions.nextFileNum.Add(2)\n\n\tseqNumA := parentFile.Smallest.SeqNum()\n\t// See SeqNum comments above.\n\tseqNumCEDel := seqNumA + 2\n\tseqNumRangeSet := seqNumA + 4\n\tseqNumRangeUnset := seqNumA + 5\n\tseqNumZ := seqNumA + 6\n\n\tv1 := &manifest.FileMetadata{\n\t\tFileBacking:           parentFile.FileBacking,\n\t\tFileNum:               f1,\n\t\tCreationTime:          time.Now().Unix(),\n\t\tSize:                  parentFile.Size / 2,\n\t\tSmallestSeqNum:        parentFile.SmallestSeqNum,\n\t\tLargestSeqNum:         parentFile.LargestSeqNum,\n\t\tLargestSeqNumAbsolute: parentFile.LargestSeqNumAbsolute,\n\t\tSmallest:              base.MakeInternalKey([]byte{'a'}, seqNumA, InternalKeyKindSet),\n\t\tLargest:               base.MakeInternalKey([]byte{'a'}, seqNumA, InternalKeyKindSet),\n\t\tSmallestPointKey:      base.MakeInternalKey([]byte{'a'}, seqNumA, InternalKeyKindSet),\n\t\tLargestPointKey:       base.MakeInternalKey([]byte{'a'}, seqNumA, InternalKeyKindSet),\n\t\tHasPointKeys:          true,\n\t\tVirtual:               true,\n\t}\n\tv1.Stats.NumEntries = 1\n\n\tv2 := &manifest.FileMetadata{\n\t\tFileBacking:           parentFile.FileBacking,\n\t\tFileNum:               f2,\n\t\tCreationTime:          time.Now().Unix(),\n\t\tSize:                  parentFile.Size / 2,\n\t\tSmallestSeqNum:        parentFile.SmallestSeqNum,\n\t\tLargestSeqNum:         parentFile.LargestSeqNum,\n\t\tLargestSeqNumAbsolute: parentFile.LargestSeqNumAbsolute,\n\t\tSmallest:              base.MakeInternalKey([]byte{'d'}, seqNumCEDel, InternalKeyKindRangeDelete),\n\t\tLargest:               base.MakeInternalKey([]byte{'z'}, seqNumZ, InternalKeyKindSet),\n\t\tSmallestPointKey:      base.MakeInternalKey([]byte{'d'}, seqNumCEDel, InternalKeyKindRangeDelete),\n\t\tLargestPointKey:       base.MakeInternalKey([]byte{'z'}, seqNumZ, InternalKeyKindSet),\n\t\tSmallestRangeKey:      base.MakeInternalKey([]byte{'f'}, seqNumRangeSet, InternalKeyKindRangeKeySet),\n\t\tLargestRangeKey:       base.MakeInternalKey([]byte{'k'}, seqNumRangeUnset, InternalKeyKindRangeKeyUnset),\n\t\tHasPointKeys:          true,\n\t\tVirtual:               true,\n\t}\n\tv2.Stats.NumEntries = 6\n\n\tv1.LargestPointKey = v1.Largest\n\tv1.SmallestPointKey = v1.Smallest\n\n\tv2.LargestPointKey = v2.Largest\n\tv2.SmallestPointKey = v2.Smallest\n\n\tv1.ValidateVirtual(parentFile)\n\td.checkVirtualBounds(v1)\n\tv2.ValidateVirtual(parentFile)\n\td.checkVirtualBounds(v2)\n\n\t// Write the version edit.\n\tfileMetrics := func(ve *versionEdit) map[int]*LevelMetrics {\n\t\tmetrics := newFileMetrics(ve.NewFiles)\n\t\tfor de, f := range ve.DeletedFiles {\n\t\t\tlm := metrics[de.Level]\n\t\t\tif lm == nil {\n\t\t\t\tlm = &LevelMetrics{}\n\t\t\t\tmetrics[de.Level] = lm\n\t\t\t}\n\t\t\tmetrics[de.Level].NumFiles--\n\t\t\tmetrics[de.Level].Size -= int64(f.Size)\n\t\t}\n\t\treturn metrics\n\t}\n\n\tapplyVE := func(ve *versionEdit) error {\n\t\td.mu.versions.logLock()\n\t\tjobID := d.newJobIDLocked()\n\n\t\terr := d.mu.versions.logAndApply(jobID, ve, fileMetrics(ve), false, func() []compactionInfo {\n\t\t\treturn d.getInProgressCompactionInfoLocked(nil)\n\t\t})\n\t\td.updateReadStateLocked(nil)\n\t\treturn err\n\t}\n\n\tve := manifest.VersionEdit{}\n\td1 := manifest.DeletedFileEntry{Level: 6, FileNum: parentFile.FileNum}\n\tn1 := manifest.NewFileEntry{Level: 6, Meta: v1}\n\tn2 := manifest.NewFileEntry{Level: 6, Meta: v2}\n\n\tve.DeletedFiles = make(map[manifest.DeletedFileEntry]*manifest.FileMetadata)\n\tve.DeletedFiles[d1] = parentFile\n\tve.NewFiles = append(ve.NewFiles, n1)\n\tve.NewFiles = append(ve.NewFiles, n2)\n\tve.CreatedBackingTables = append(ve.CreatedBackingTables, parentFile.FileBacking)\n\n\trequire.NoError(t, applyVE(&ve))\n\n\tcurrVersion = d.mu.versions.currentVersion()\n\tl6 = currVersion.Levels[6]\n\tl6FileIter = l6.Iter()\n\tfor f := l6FileIter.First(); f != nil; f = l6FileIter.Next() {\n\t\trequire.Equal(t, true, f.Virtual)\n\t}\n\td.mu.Unlock()\n\n\t// Confirm that there were only 2 virtual sstables in L6.\n\trequire.Equal(t, 2, int(d.Metrics().Levels[6].NumFiles))\n\n\t// These reads will go through the file cache.\n\titer, _ := d.NewIter(nil)\n\texpected := []byte{'a', 'f', 'z'}\n\tfor i, x := 0, iter.First(); x; i, x = i+1, iter.Next() {\n\t\trequire.Equal(t, []byte{expected[i]}, iter.Value())\n\t}\n\titer.Close()\n\n\t// We don't defer this Close in case we get a panic while holding d.mu.\n\td.Close()\n}\n\n// The file cache shouldn't be usable after all the dbs close.\nfunc TestSharedFileCacheUseAfterAllFree(t *testing.T) {\n\tfc := newFileCacheTest(8<<20, 10, 1)\n\tdb1, err := Open(\"test\",\n\t\t&Options{\n\t\t\tFS:        vfs.NewMem(),\n\t\t\tCache:     fc.cache,\n\t\t\tFileCache: fc,\n\t\t})\n\trequire.NoError(t, err)\n\n\t// Release our reference, now that the db has a reference.\n\tfc.Unref()\n\n\tdb2, err := Open(\"test\",\n\t\t&Options{\n\t\t\tFS:        vfs.NewMem(),\n\t\t\tCache:     fc.cache,\n\t\t\tFileCache: fc,\n\t\t})\n\trequire.NoError(t, err)\n\n\trequire.NoError(t, db1.Close())\n\trequire.NoError(t, db2.Close())\n\n\tv := fc.refs.Load()\n\tif v != 0 {\n\t\tt.Fatalf(\"expected reference count %d, got %d\", 0, v)\n\t}\n\n\tdefer func() {\n\t\t// The cache ref gets incremented before the panic, so we should\n\t\t// decrement it to prevent the finalizer from detecting a leak.\n\t\tfc.cache.Unref()\n\n\t\tif r := recover(); r != nil {\n\t\t\tif fmt.Sprint(r) != \"pebble: inconsistent reference count: 1\" {\n\t\t\t\tt.Fatalf(\"unexpected panic message\")\n\t\t\t}\n\t\t} else if r == nil {\n\t\t\tt.Fatalf(\"expected panic\")\n\t\t}\n\t}()\n\n\tdb3, _ := Open(\"test\",\n\t\t&Options{\n\t\t\tFS:        vfs.NewMem(),\n\t\t\tCache:     fc.cache,\n\t\t\tFileCache: fc,\n\t\t})\n\t_ = db3\n}\n\n// TestSharedFileCacheUseAfterOneFree tests whether a shared file cache is\n// usable by a db, after one of the db's releases its reference.\nfunc TestSharedFileCacheUseAfterOneFree(t *testing.T) {\n\ttc := newFileCacheTest(8<<20, 10, 1)\n\tdb1, err := Open(\"test\",\n\t\t&Options{\n\t\t\tFS:        vfs.NewMem(),\n\t\t\tCache:     tc.cache,\n\t\t\tFileCache: tc,\n\t\t})\n\trequire.NoError(t, err)\n\n\t// Release our reference, now that the db has a reference.\n\ttc.Unref()\n\n\tdb2, err := Open(\"test\",\n\t\t&Options{\n\t\t\tFS:        vfs.NewMem(),\n\t\t\tCache:     tc.cache,\n\t\t\tFileCache: tc,\n\t\t})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db2.Close())\n\t}()\n\n\t// Make db1 release a reference to the cache. It should\n\t// still be usable by db2.\n\trequire.NoError(t, db1.Close())\n\tv := tc.refs.Load()\n\tif v != 1 {\n\t\tt.Fatalf(\"expected reference count %d, got %d\", 1, v)\n\t}\n\n\t// Check if db2 is still usable.\n\tstart := []byte(\"a\")\n\tend := []byte(\"d\")\n\trequire.NoError(t, db2.Set(start, nil, nil))\n\trequire.NoError(t, db2.Flush())\n\trequire.NoError(t, db2.DeleteRange(start, end, nil))\n\trequire.NoError(t, db2.Compact(start, end, false))\n}\n\n// TestSharedFileCacheUsable ensures that a shared file cache is usable by more\n// than one database at once.\nfunc TestSharedFileCacheUsable(t *testing.T) {\n\ttc := newFileCacheTest(8<<20, 10, 1)\n\tdb1, err := Open(\"test\",\n\t\t&Options{\n\t\t\tFS:        vfs.NewMem(),\n\t\t\tCache:     tc.cache,\n\t\t\tFileCache: tc,\n\t\t})\n\trequire.NoError(t, err)\n\n\t// Release our reference, now that the db has a reference.\n\ttc.Unref()\n\n\tdefer func() {\n\t\trequire.NoError(t, db1.Close())\n\t}()\n\n\tdb2, err := Open(\"test\",\n\t\t&Options{\n\t\t\tFS:        vfs.NewMem(),\n\t\t\tCache:     tc.cache,\n\t\t\tFileCache: tc,\n\t\t})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db2.Close())\n\t}()\n\n\tstart := []byte(\"a\")\n\tend := []byte(\"z\")\n\trequire.NoError(t, db1.Set(start, nil, nil))\n\trequire.NoError(t, db1.Flush())\n\trequire.NoError(t, db1.DeleteRange(start, end, nil))\n\trequire.NoError(t, db1.Compact(start, end, false))\n\n\tstart = []byte(\"x\")\n\tend = []byte(\"y\")\n\trequire.NoError(t, db2.Set(start, nil, nil))\n\trequire.NoError(t, db2.Flush())\n\trequire.NoError(t, db2.Set(start, []byte{'a'}, nil))\n\trequire.NoError(t, db2.Flush())\n\trequire.NoError(t, db2.DeleteRange(start, end, nil))\n\trequire.NoError(t, db2.Compact(start, end, false))\n}\n\nfunc TestSharedTableConcurrent(t *testing.T) {\n\ttc := newFileCacheTest(8<<20, 10, 1)\n\tdb1, err := Open(\"test\",\n\t\t&Options{\n\t\t\tFS:        vfs.NewMem(),\n\t\t\tCache:     tc.cache,\n\t\t\tFileCache: tc,\n\t\t})\n\trequire.NoError(t, err)\n\n\t// Release our reference, now that the db has a reference.\n\ttc.Unref()\n\n\tdefer func() {\n\t\trequire.NoError(t, db1.Close())\n\t}()\n\n\tdb2, err := Open(\"test\",\n\t\t&Options{\n\t\t\tFS:        vfs.NewMem(),\n\t\t\tCache:     tc.cache,\n\t\t\tFileCache: tc,\n\t\t})\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, db2.Close())\n\t}()\n\n\tvar wg sync.WaitGroup\n\twg.Add(2)\n\n\t// Now that both dbs have a reference to the file cache,\n\t// we'll run go routines which will use the DBs concurrently.\n\tconcFunc := func(db *DB) {\n\t\tfor i := 0; i < 1000; i++ {\n\t\t\tstart := []byte(\"a\")\n\t\t\tend := []byte(\"z\")\n\t\t\trequire.NoError(t, db.Set(start, nil, nil))\n\t\t\trequire.NoError(t, db.Flush())\n\t\t\trequire.NoError(t, db.DeleteRange(start, end, nil))\n\t\t\trequire.NoError(t, db.Compact(start, end, false))\n\t\t}\n\t\twg.Done()\n\t}\n\n\tgo concFunc(db1)\n\tgo concFunc(db2)\n\n\twg.Wait()\n}\n\nfunc testFileCacheRandomAccess(t *testing.T, concurrent bool) {\n\tconst N = 2000\n\tc, fs, err := newFileCacheContainerTest(nil, \"\")\n\trequire.NoError(t, err)\n\n\trngMu := sync.Mutex{}\n\trng := rand.New(rand.NewPCG(1, 1))\n\n\terrc := make(chan error, N)\n\tfor i := 0; i < N; i++ {\n\t\tgo func(i int) {\n\t\t\trngMu.Lock()\n\t\t\tfileNum, sleepTime := rng.IntN(fileCacheTestNumTables), rng.IntN(1000)\n\t\t\trngMu.Unlock()\n\t\t\tm := &fileMetadata{FileNum: FileNum(fileNum)}\n\t\t\tm.InitPhysicalBacking()\n\t\t\tm.FileBacking.Ref()\n\t\t\tdefer m.FileBacking.Unref()\n\t\t\titers, err := c.newIters(context.Background(), m, nil, internalIterOpts{}, iterPointKeys)\n\t\t\tif err != nil {\n\t\t\t\terrc <- errors.Errorf(\"i=%d, fileNum=%d: find: %v\", i, fileNum, err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\titer := iters.Point()\n\t\t\tkv := iter.SeekGE([]byte(\"k\"), base.SeekGEFlagsNone)\n\t\t\tif concurrent {\n\t\t\t\ttime.Sleep(time.Duration(sleepTime) * time.Microsecond)\n\t\t\t}\n\t\t\tif kv == nil {\n\t\t\t\terrc <- errors.Errorf(\"i=%d, fileNum=%d: valid.0: got false, want true\", i, fileNum)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tv, _, err := kv.Value(nil)\n\t\t\tif err != nil {\n\t\t\t\terrc <- errors.Errorf(\"i=%d, fileNum=%d: err extracting value: %v\", err)\n\t\t\t}\n\t\t\tif got := len(v); got != fileNum {\n\t\t\t\terrc <- errors.Errorf(\"i=%d, fileNum=%d: value: got %d bytes, want %d\", i, fileNum, got, fileNum)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif kv := iter.Next(); kv != nil {\n\t\t\t\terrc <- errors.Errorf(\"i=%d, fileNum=%d: next.1: got true, want false\", i, fileNum)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif err := iter.Close(); err != nil {\n\t\t\t\terrc <- errors.Wrapf(err, \"close error i=%d, fileNum=%dv\", i, fileNum)\n\t\t\t\treturn\n\t\t\t}\n\t\t\terrc <- nil\n\t\t}(i)\n\t\tif !concurrent {\n\t\t\trequire.NoError(t, <-errc)\n\t\t}\n\t}\n\tif concurrent {\n\t\tfor i := 0; i < N; i++ {\n\t\t\trequire.NoError(t, <-errc)\n\t\t}\n\t}\n\tfs.validate(t, c, nil)\n}\n\nfunc TestFileCacheRandomAccessSequential(t *testing.T) { testFileCacheRandomAccess(t, false) }\nfunc TestFileCacheRandomAccessConcurrent(t *testing.T) { testFileCacheRandomAccess(t, true) }\n\nfunc testFileCacheFrequentlyUsedInternal(t *testing.T, rangeIter bool) {\n\tconst (\n\t\tN       = 1000\n\t\tpinned0 = 7\n\t\tpinned1 = 11\n\t)\n\tc, fs, err := newFileCacheContainerTest(nil, \"\")\n\trequire.NoError(t, err)\n\n\tfor i := 0; i < N; i++ {\n\t\tfor _, j := range [...]int{pinned0, i % fileCacheTestNumTables, pinned1} {\n\t\t\tvar iters iterSet\n\t\t\tvar err error\n\t\t\tm := &fileMetadata{FileNum: FileNum(j)}\n\t\t\tm.InitPhysicalBacking()\n\t\t\tm.FileBacking.Ref()\n\t\t\tif rangeIter {\n\t\t\t\titers, err = c.newIters(context.Background(), m, nil, internalIterOpts{}, iterRangeKeys)\n\t\t\t} else {\n\t\t\t\titers, err = c.newIters(context.Background(), m, nil, internalIterOpts{}, iterPointKeys)\n\t\t\t}\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"i=%d, j=%d: find: %v\", i, j, err)\n\t\t\t}\n\t\t\tif err := iters.CloseAll(); err != nil {\n\t\t\t\tt.Fatalf(\"i=%d, j=%d: close: %v\", i, j, err)\n\t\t\t}\n\t\t}\n\t}\n\n\tfs.validate(t, c, func(i, gotO, gotC int) error {\n\t\tif i == pinned0 || i == pinned1 {\n\t\t\tif gotO != 1 || gotC != 0 {\n\t\t\t\treturn errors.Errorf(\"i=%d: pinned table: got %d, %d, want %d, %d\", i, gotO, gotC, 1, 0)\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n}\n\nfunc TestFileCacheFrequentlyUsed(t *testing.T) {\n\tfor i, iterType := range []string{\"point\", \"range\"} {\n\t\tt.Run(fmt.Sprintf(\"iter=%s\", iterType), func(t *testing.T) {\n\t\t\ttestFileCacheFrequentlyUsedInternal(t, i == 1)\n\t\t})\n\t}\n}\n\nfunc TestSharedFileCacheFrequentlyUsed(t *testing.T) {\n\tconst (\n\t\tN       = 1000\n\t\tpinned0 = 7\n\t\tpinned1 = 11\n\t)\n\ttc := newFileCacheTest(8<<20, 2*fileCacheTestCacheSize, 16)\n\tc1, fs1, err := newFileCacheContainerTest(tc, \"\")\n\trequire.NoError(t, err)\n\tc2, fs2, err := newFileCacheContainerTest(tc, \"\")\n\trequire.NoError(t, err)\n\ttc.Unref()\n\n\tfor i := 0; i < N; i++ {\n\t\tfor _, j := range [...]int{pinned0, i % fileCacheTestNumTables, pinned1} {\n\t\t\tm := &fileMetadata{FileNum: FileNum(j)}\n\t\t\tm.InitPhysicalBacking()\n\t\t\tm.FileBacking.Ref()\n\t\t\titers1, err := c1.newIters(context.Background(), m, nil, internalIterOpts{}, iterPointKeys)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"i=%d, j=%d: find: %v\", i, j, err)\n\t\t\t}\n\t\t\titers2, err := c2.newIters(context.Background(), m, nil, internalIterOpts{}, iterPointKeys)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"i=%d, j=%d: find: %v\", i, j, err)\n\t\t\t}\n\n\t\t\tif err := iters1.point.Close(); err != nil {\n\t\t\t\tt.Fatalf(\"i=%d, j=%d: close: %v\", i, j, err)\n\t\t\t}\n\t\t\tif err := iters2.point.Close(); err != nil {\n\t\t\t\tt.Fatalf(\"i=%d, j=%d: close: %v\", i, j, err)\n\t\t\t}\n\t\t}\n\t}\n\n\tfs1.validate(t, c1, func(i, gotO, gotC int) error {\n\t\tif i == pinned0 || i == pinned1 {\n\t\t\tif gotO != 1 || gotC != 0 {\n\t\t\t\treturn errors.Errorf(\"i=%d: pinned table: got %d, %d, want %d, %d\", i, gotO, gotC, 1, 0)\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n\n\tfs2.validate(t, c2, func(i, gotO, gotC int) error {\n\t\tif i == pinned0 || i == pinned1 {\n\t\t\tif gotO != 1 || gotC != 0 {\n\t\t\t\treturn errors.Errorf(\"i=%d: pinned table: got %d, %d, want %d, %d\", i, gotO, gotC, 1, 0)\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n}\n\nfunc testFileCacheEvictionsInternal(t *testing.T, rangeIter bool) {\n\tconst (\n\t\tN      = 1000\n\t\tlo, hi = 10, 20\n\t)\n\tc, fs, err := newFileCacheContainerTest(nil, \"\")\n\trequire.NoError(t, err)\n\n\trng := rand.New(rand.NewPCG(2, 2))\n\tfor i := 0; i < N; i++ {\n\t\tj := rng.IntN(fileCacheTestNumTables)\n\t\tvar iters iterSet\n\t\tvar err error\n\t\tm := &fileMetadata{FileNum: FileNum(j)}\n\t\tm.InitPhysicalBacking()\n\t\tm.FileBacking.Ref()\n\t\tif rangeIter {\n\t\t\titers, err = c.newIters(context.Background(), m, nil, internalIterOpts{}, iterRangeKeys)\n\t\t} else {\n\t\t\titers, err = c.newIters(context.Background(), m, nil, internalIterOpts{}, iterPointKeys)\n\t\t}\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"i=%d, j=%d: find: %v\", i, j, err)\n\t\t}\n\t\tif err := iters.CloseAll(); err != nil {\n\t\t\tt.Fatalf(\"i=%d, j=%d: close: %v\", i, j, err)\n\t\t}\n\n\t\tc.evict(base.DiskFileNum(lo + rng.Uint64N(hi-lo)))\n\t}\n\n\tsumEvicted, nEvicted := 0, 0\n\tsumSafe, nSafe := 0, 0\n\tfs.validate(t, c, func(i, gotO, gotC int) error {\n\t\tif lo <= i && i < hi {\n\t\t\tsumEvicted += gotO\n\t\t\tnEvicted++\n\t\t} else {\n\t\t\tsumSafe += gotO\n\t\t\tnSafe++\n\t\t}\n\t\treturn nil\n\t})\n\tfEvicted := float64(sumEvicted) / float64(nEvicted)\n\tfSafe := float64(sumSafe) / float64(nSafe)\n\t// The magic 1.25 number isn't derived from formal modeling. It's just a guess. For\n\t// (lo, hi, fileCacheTestCacheSize, fileCacheTestNumTables) = (10, 20, 100, 300),\n\t// the ratio seems to converge on roughly 1.5 for large N, compared to 1.0 if we do\n\t// not evict any cache entries.\n\tif ratio := fEvicted / fSafe; ratio < 1.25 {\n\t\tt.Errorf(\"evicted tables were opened %.3f times on average, safe tables %.3f, ratio %.3f < 1.250\",\n\t\t\tfEvicted, fSafe, ratio)\n\t}\n}\n\nfunc TestFileCacheEvictions(t *testing.T) {\n\tfor i, iterType := range []string{\"point\", \"range\"} {\n\t\tt.Run(fmt.Sprintf(\"iter=%s\", iterType), func(t *testing.T) {\n\t\t\ttestFileCacheEvictionsInternal(t, i == 1)\n\t\t})\n\t}\n}\n\nfunc TestSharedFileCacheEvictions(t *testing.T) {\n\tconst (\n\t\tN      = 1000\n\t\tlo, hi = 10, 20\n\t)\n\ttc := newFileCacheTest(8<<20, 2*fileCacheTestCacheSize, 16)\n\tc1, fs1, err := newFileCacheContainerTest(tc, \"\")\n\trequire.NoError(t, err)\n\tc2, fs2, err := newFileCacheContainerTest(tc, \"\")\n\trequire.NoError(t, err)\n\ttc.Unref()\n\n\t// TODO(radu): this test fails on most seeds.\n\trng := rand.New(rand.NewPCG(0, 0))\n\tfor i := 0; i < N; i++ {\n\t\tj := rng.IntN(fileCacheTestNumTables)\n\t\tm := &fileMetadata{FileNum: FileNum(j)}\n\t\tm.InitPhysicalBacking()\n\t\tm.FileBacking.Ref()\n\t\titers1, err := c1.newIters(context.Background(), m, nil, internalIterOpts{}, iterPointKeys)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"i=%d, j=%d: find: %v\", i, j, err)\n\t\t}\n\n\t\titers2, err := c2.newIters(context.Background(), m, nil, internalIterOpts{}, iterPointKeys)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"i=%d, j=%d: find: %v\", i, j, err)\n\t\t}\n\n\t\tif err := iters1.Point().Close(); err != nil {\n\t\t\tt.Fatalf(\"i=%d, j=%d: close: %v\", i, j, err)\n\t\t}\n\n\t\tif err := iters2.Point().Close(); err != nil {\n\t\t\tt.Fatalf(\"i=%d, j=%d: close: %v\", i, j, err)\n\t\t}\n\n\t\tc1.evict(base.DiskFileNum(lo + rng.Uint64N(hi-lo)))\n\t\tc2.evict(base.DiskFileNum(lo + rng.Uint64N(hi-lo)))\n\t}\n\n\tcheck := func(fs *fileCacheTestFS, c *fileCacheContainer) (float64, float64, float64) {\n\t\tsumEvicted, nEvicted := 0, 0\n\t\tsumSafe, nSafe := 0, 0\n\t\tfs.validate(t, c, func(i, gotO, gotC int) error {\n\t\t\tif lo <= i && i < hi {\n\t\t\t\tsumEvicted += gotO\n\t\t\t\tnEvicted++\n\t\t\t} else {\n\t\t\t\tsumSafe += gotO\n\t\t\t\tnSafe++\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\t\tfEvicted := float64(sumEvicted) / float64(nEvicted)\n\t\tfSafe := float64(sumSafe) / float64(nSafe)\n\n\t\treturn fEvicted, fSafe, fEvicted / fSafe\n\t}\n\n\t// The magic 1.25 number isn't derived from formal modeling. It's just a guess. For\n\t// (lo, hi, fileCacheTestCacheSize, fileCacheTestNumTables) = (10, 20, 100, 300),\n\t// the ratio seems to converge on roughly 1.5 for large N, compared to 1.0 if we do\n\t// not evict any cache entries.\n\tif fEvicted, fSafe, ratio := check(fs1, c1); ratio < 1.25 {\n\t\tt.Errorf(\n\t\t\t\"evicted tables were opened %.3f times on average, safe tables %.3f, ratio %.3f < 1.250\",\n\t\t\tfEvicted, fSafe, ratio,\n\t\t)\n\t}\n\n\tif fEvicted, fSafe, ratio := check(fs2, c2); ratio < 1.25 {\n\t\tt.Errorf(\n\t\t\t\"evicted tables were opened %.3f times on average, safe tables %.3f, ratio %.3f < 1.250\",\n\t\t\tfEvicted, fSafe, ratio,\n\t\t)\n\t}\n}\n\nfunc TestFileCacheIterLeak(t *testing.T) {\n\tc, _, err := newFileCacheContainerTest(nil, \"\")\n\trequire.NoError(t, err)\n\n\tm := &fileMetadata{FileNum: 0}\n\tm.InitPhysicalBacking()\n\tm.FileBacking.Ref()\n\tdefer m.FileBacking.Unref()\n\titers, err := c.newIters(context.Background(), m, nil, internalIterOpts{}, iterPointKeys)\n\trequire.NoError(t, err)\n\n\tif err := c.close(); err == nil {\n\t\tt.Fatalf(\"expected failure, but found success\")\n\t} else if !strings.HasPrefix(err.Error(), \"leaked iterators:\") {\n\t\tt.Fatalf(\"expected leaked iterators, but found %+v\", err)\n\t} else {\n\t\tt.Log(err.Error())\n\t}\n\trequire.NoError(t, iters.Point().Close())\n}\n\nfunc TestSharedFileCacheIterLeak(t *testing.T) {\n\ttc := newFileCacheTest(8<<20, 2*fileCacheTestCacheSize, 16)\n\tc1, _, err := newFileCacheContainerTest(tc, \"\")\n\trequire.NoError(t, err)\n\tc2, _, err := newFileCacheContainerTest(tc, \"\")\n\trequire.NoError(t, err)\n\tc3, _, err := newFileCacheContainerTest(tc, \"\")\n\trequire.NoError(t, err)\n\ttc.Unref()\n\n\tm := &fileMetadata{FileNum: 0}\n\tm.InitPhysicalBacking()\n\tm.FileBacking.Ref()\n\tdefer m.FileBacking.Unref()\n\titers, err := c1.newIters(context.Background(), m, nil, internalIterOpts{}, iterPointKeys)\n\trequire.NoError(t, err)\n\n\tif err := c1.close(); err == nil {\n\t\tt.Fatalf(\"expected failure, but found success\")\n\t} else if !strings.HasPrefix(err.Error(), \"leaked iterators:\") {\n\t\tt.Fatalf(\"expected leaked iterators, but found %+v\", err)\n\t} else {\n\t\tt.Log(err.Error())\n\t}\n\n\t// Closing c2 shouldn't error out since c2 isn't leaking any iterators.\n\trequire.NoError(t, c2.close())\n\n\t// Closing c3 should error out since c3 holds the last reference to the\n\t// FileCache, and when the FileCache closes, it will detect that there was a\n\t// leaked iterator.\n\tif err := c3.close(); err == nil {\n\t\tt.Fatalf(\"expected failure, but found success\")\n\t} else if !strings.HasPrefix(err.Error(), \"leaked iterators:\") {\n\t\tt.Fatalf(\"expected leaked iterators, but found %+v\", err)\n\t} else {\n\t\tt.Log(err.Error())\n\t}\n\n\trequire.NoError(t, iters.Point().Close())\n}\n\nfunc TestFileCacheRetryAfterFailure(t *testing.T) {\n\t// Test a retry can succeed after a failure, i.e., errors are not cached.\n\tc, fs, err := newFileCacheContainerTest(nil, \"\")\n\trequire.NoError(t, err)\n\n\tfs.setOpenError(true /* enabled */)\n\tm := &fileMetadata{FileNum: 0}\n\tm.InitPhysicalBacking()\n\tm.FileBacking.Ref()\n\tdefer m.FileBacking.Unref()\n\tif _, err = c.newIters(context.Background(), m, nil, internalIterOpts{}, iterPointKeys); err == nil {\n\t\tt.Fatalf(\"expected failure, but found success\")\n\t}\n\trequire.Equal(t, \"pebble: backing file 000000 error: injected error\", err.Error())\n\tfs.setOpenError(false /* enabled */)\n\tvar iters iterSet\n\titers, err = c.newIters(context.Background(), m, nil, internalIterOpts{}, iterPointKeys)\n\trequire.NoError(t, err)\n\trequire.NoError(t, iters.Point().Close())\n\tfs.validate(t, c, nil)\n}\n\nfunc TestFileCacheErrorBadMagicNumber(t *testing.T) {\n\tobj := &objstorage.MemObj{}\n\ttw := sstable.NewWriter(obj, sstable.WriterOptions{TableFormat: sstable.TableFormatPebblev2})\n\ttw.Set([]byte(\"a\"), nil)\n\trequire.NoError(t, tw.Close())\n\tbuf := obj.Data()\n\t// Bad magic number.\n\tbuf[len(buf)-1] = 0\n\tfs := &fileCacheTestFS{\n\t\tFS: vfs.NewMem(),\n\t}\n\tconst testFileNum = 3\n\tobjProvider, err := objstorageprovider.Open(objstorageprovider.DefaultSettings(fs, \"\"))\n\trequire.NoError(t, err)\n\tw, _, err := objProvider.Create(context.Background(), fileTypeTable, testFileNum, objstorage.CreateOptions{})\n\tw.Write(buf)\n\trequire.NoError(t, w.Finish())\n\topts := &Options{}\n\topts.EnsureDefaults()\n\topts.Cache = NewCache(8 << 20) // 8 MB\n\tdefer opts.Cache.Unref()\n\tc := newFileCacheContainer(nil, opts.Cache.NewID(), objProvider, opts, fileCacheTestCacheSize,\n\t\t&sstable.CategoryStatsCollector{})\n\trequire.NoError(t, err)\n\tdefer c.close()\n\n\tm := &fileMetadata{FileNum: testFileNum}\n\tm.InitPhysicalBacking()\n\tm.FileBacking.Ref()\n\tdefer m.FileBacking.Unref()\n\tif _, err = c.newIters(context.Background(), m, nil, internalIterOpts{}, iterPointKeys); err == nil {\n\t\tt.Fatalf(\"expected failure, but found success\")\n\t}\n\trequire.Equal(t,\n\t\t\"pebble: backing file 000003 error: pebble/table: invalid table 000003: (bad magic number: 0xf09faab3f09faa00)\",\n\t\terr.Error())\n}\n\nfunc TestFileCacheEvictClose(t *testing.T) {\n\terrs := make(chan error, 10)\n\tdb, err := Open(\"test\",\n\t\t&Options{\n\t\t\tFS: vfs.NewMem(),\n\t\t\tEventListener: &EventListener{\n\t\t\t\tTableDeleted: func(info TableDeleteInfo) {\n\t\t\t\t\terrs <- info.Err\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\trequire.NoError(t, err)\n\n\tstart := []byte(\"a\")\n\tend := []byte(\"z\")\n\trequire.NoError(t, db.Set(start, nil, nil))\n\trequire.NoError(t, db.Flush())\n\trequire.NoError(t, db.DeleteRange(start, end, nil))\n\trequire.NoError(t, db.Compact(start, end, false))\n\trequire.NoError(t, db.Close())\n\tclose(errs)\n\n\tfor err := range errs {\n\t\trequire.NoError(t, err)\n\t}\n}\n\nfunc TestFileCacheClockPro(t *testing.T) {\n\t// Test data was generated from the python code. See also\n\t// internal/cache/clockpro_test.go:TestCache.\n\tf, err := os.Open(\"internal/cache/testdata/cache\")\n\trequire.NoError(t, err)\n\n\tmem := vfs.NewMem()\n\tobjProvider, err := objstorageprovider.Open(objstorageprovider.DefaultSettings(mem, \"\"))\n\trequire.NoError(t, err)\n\tdefer objProvider.Close()\n\n\tmakeTable := func(dfn base.DiskFileNum) {\n\t\trequire.NoError(t, err)\n\t\tf, _, err := objProvider.Create(context.Background(), fileTypeTable, dfn, objstorage.CreateOptions{})\n\t\trequire.NoError(t, err)\n\t\tw := sstable.NewWriter(f, sstable.WriterOptions{})\n\t\trequire.NoError(t, w.Set([]byte(\"a\"), nil))\n\t\trequire.NoError(t, w.Close())\n\t}\n\n\topts := &Options{\n\t\tCache: NewCache(8 << 20), // 8 MB\n\t}\n\topts.EnsureDefaults()\n\tdefer opts.Cache.Unref()\n\n\tcache := &fileCacheShard{}\n\t// NB: The file cache size of 200 is required for the expected test values.\n\tcache.init(200)\n\tdbOpts := &fileCacheOpts{}\n\tdbOpts.loggerAndTracer = &base.LoggerWithNoopTracer{Logger: opts.Logger}\n\tdbOpts.cacheID = 0\n\tdbOpts.objProvider = objProvider\n\tdbOpts.readerOpts = opts.MakeReaderOptions()\n\n\tscanner := bufio.NewScanner(f)\n\ttables := make(map[int]bool)\n\tline := 1\n\n\tfor scanner.Scan() {\n\t\tfields := bytes.Fields(scanner.Bytes())\n\n\t\tkey, err := strconv.Atoi(string(fields[0]))\n\t\trequire.NoError(t, err)\n\n\t\t// Ensure that underlying sstables exist on disk, creating each table the\n\t\t// first time it is seen.\n\t\tif !tables[key] {\n\t\t\tmakeTable(base.DiskFileNum(key))\n\t\t\ttables[key] = true\n\t\t}\n\n\t\toldHits := cache.hits.Load()\n\t\tm := &fileMetadata{FileNum: FileNum(key)}\n\t\tm.InitPhysicalBacking()\n\t\tm.FileBacking.Ref()\n\t\tv := cache.findNode(context.Background(), m.FileBacking, dbOpts)\n\t\tcache.unrefValue(v)\n\n\t\thit := cache.hits.Load() != oldHits\n\t\twantHit := fields[1][0] == 'h'\n\t\tif hit != wantHit {\n\t\t\tt.Errorf(\"%d: cache hit mismatch: got %v, want %v\\n\", line, hit, wantHit)\n\t\t}\n\t\tline++\n\t\tm.FileBacking.Unref()\n\t}\n}\n\nfunc BenchmarkNewItersAlloc(b *testing.B) {\n\topts := &Options{\n\t\tFS:                 vfs.NewMem(),\n\t\tFormatMajorVersion: internalFormatNewest,\n\t}\n\td, err := Open(\"\", opts)\n\trequire.NoError(b, err)\n\tdefer func() { require.NoError(b, d.Close()) }()\n\n\trequire.NoError(b, d.Set([]byte{'a'}, []byte{'a'}, nil))\n\trequire.NoError(b, d.Flush())\n\trequire.NoError(b, d.Compact([]byte{'a'}, []byte{'z'}, false))\n\n\td.mu.Lock()\n\tcurrVersion := d.mu.versions.currentVersion()\n\tit := currVersion.Levels[6].Iter()\n\tm := it.First()\n\trequire.NotNil(b, m)\n\td.mu.Unlock()\n\n\t// Open once so that the Reader is cached.\n\titers, err := d.newIters(context.Background(), m, nil, internalIterOpts{}, iterPointKeys|iterRangeDeletions)\n\trequire.NoError(b, iters.CloseAll())\n\trequire.NoError(b, err)\n\n\tfor i := 0; i < b.N; i++ {\n\t\tb.StartTimer()\n\t\titers, err := d.newIters(context.Background(), m, nil, internalIterOpts{}, iterPointKeys|iterRangeDeletions)\n\t\tb.StopTimer()\n\t\trequire.NoError(b, err)\n\t\trequire.NoError(b, iters.CloseAll())\n\t}\n}\n\n// TestFileCacheNoSuchFileError verifies that when the file cache hits a \"no\n// such file\" error, it generates a useful fatal message.\nfunc TestFileCacheNoSuchFileError(t *testing.T) {\n\tconst dirname = \"test\"\n\tmem := vfs.NewMem()\n\tlogger := &catchFatalLogger{}\n\n\td, err := Open(dirname, &Options{\n\t\tFS:     mem,\n\t\tLogger: logger,\n\t})\n\trequire.NoError(t, err)\n\tdefer func() { _ = d.Close() }()\n\trequire.NoError(t, d.Set([]byte(\"a\"), []byte(\"val_a\"), nil))\n\trequire.NoError(t, d.Set([]byte(\"b\"), []byte(\"val_b\"), nil))\n\trequire.NoError(t, d.Flush())\n\tls, err := mem.List(dirname)\n\trequire.NoError(t, err)\n\n\t// Find the sst file.\n\tvar sst string\n\tfor _, file := range ls {\n\t\tif strings.HasSuffix(file, \".sst\") {\n\t\t\tif sst != \"\" {\n\t\t\t\tt.Fatalf(\"multiple SSTs found: %s, %s\", sst, file)\n\t\t\t}\n\t\t\tsst = file\n\t\t}\n\t}\n\tif sst == \"\" {\n\t\tt.Fatalf(\"no SST found after flush\")\n\t}\n\trequire.NoError(t, mem.Remove(path.Join(dirname, sst)))\n\n\t_, _, _ = d.Get([]byte(\"a\"))\n\trequire.NotZero(t, len(logger.fatalMsgs), \"no fatal message emitted\")\n\trequire.Equal(t, 1, len(logger.fatalMsgs), \"expected one fatal message; got: %v\", logger.fatalMsgs)\n\trequire.Contains(t, logger.fatalMsgs[0], \"directory contains 7 files, 2 unknown, 0 tables, 2 logs, 1 manifests\")\n}\n\nfunc BenchmarkFileCacheHotPath(b *testing.B) {\n\tmem := vfs.NewMem()\n\tobjProvider, err := objstorageprovider.Open(objstorageprovider.DefaultSettings(mem, \"\"))\n\trequire.NoError(b, err)\n\tdefer objProvider.Close()\n\n\tmakeTable := func(dfn base.DiskFileNum) {\n\t\trequire.NoError(b, err)\n\t\tf, _, err := objProvider.Create(context.Background(), fileTypeTable, dfn, objstorage.CreateOptions{})\n\t\trequire.NoError(b, err)\n\t\tw := sstable.NewWriter(f, sstable.WriterOptions{})\n\t\trequire.NoError(b, w.Set([]byte(\"a\"), nil))\n\t\trequire.NoError(b, w.Close())\n\t}\n\n\topts := &Options{\n\t\tCache: NewCache(8 << 20), // 8 MB\n\t}\n\topts.EnsureDefaults()\n\tdefer opts.Cache.Unref()\n\n\tcache := &fileCacheShard{}\n\tcache.init(2)\n\tdbOpts := &fileCacheOpts{}\n\tdbOpts.loggerAndTracer = &base.LoggerWithNoopTracer{Logger: opts.Logger}\n\tdbOpts.cacheID = 0\n\tdbOpts.objProvider = objProvider\n\tdbOpts.readerOpts = opts.MakeReaderOptions()\n\n\tmakeTable(1)\n\n\tm := &fileMetadata{FileNum: 1}\n\tm.InitPhysicalBacking()\n\tm.FileBacking.Ref()\n\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\tv := cache.findNode(context.Background(), m.FileBacking, dbOpts)\n\t\tcache.unrefValue(v)\n\t}\n}\n\ntype catchFatalLogger struct {\n\tfatalMsgs []string\n}\n\nvar _ Logger = (*catchFatalLogger)(nil)\n\nfunc (tl *catchFatalLogger) Infof(format string, args ...interface{})  {}\nfunc (tl *catchFatalLogger) Errorf(format string, args ...interface{}) {}\n\nfunc (tl *catchFatalLogger) Fatalf(format string, args ...interface{}) {\n\ttl.fatalMsgs = append(tl.fatalMsgs, fmt.Sprintf(format, args...))\n}\n"
        },
        {
          "name": "filenames.go",
          "type": "blob",
          "size": 0.6376953125,
          "content": "// Copyright 2012 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport \"github.com/cockroachdb/pebble/internal/base\"\n\ntype fileType = base.FileType\n\n// FileNum is an identifier for a file within a database.\ntype FileNum = base.FileNum\n\nconst (\n\tfileTypeLog      = base.FileTypeLog\n\tfileTypeLock     = base.FileTypeLock\n\tfileTypeTable    = base.FileTypeTable\n\tfileTypeManifest = base.FileTypeManifest\n\tfileTypeOptions  = base.FileTypeOptions\n\tfileTypeTemp     = base.FileTypeTemp\n\tfileTypeOldTemp  = base.FileTypeOldTemp\n)\n"
        },
        {
          "name": "filenames_test.go",
          "type": "blob",
          "size": 2.841796875,
          "content": "// Copyright 2020 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"testing\"\n\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/stretchr/testify/require\"\n)\n\n// TestSetCurrentFileCrash tests a crash that occurs during\n// a MANIFEST roll, leaving the temporary CURRENT file on\n// the filesystem. These temporary files should be cleaned\n// up on Open.\nfunc TestSetCurrentFileCrash(t *testing.T) {\n\tmem := vfs.NewMem()\n\n\t// Initialize a fresh database to write the initial MANIFEST.\n\t{\n\t\td, err := Open(\"\", &Options{FS: mem})\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, d.Close())\n\t}\n\n\t// Open the database again, this time with a FS that\n\t// errors on Rename and a tiny max manifest file size\n\t// to force manifest rolls.\n\t{\n\t\twantErr := errors.New(\"rename error\")\n\t\t_, err := Open(\"\", &Options{\n\t\t\tFS:                    renameErrorFS{FS: mem, err: wantErr},\n\t\t\tLogger:                noFatalLogger{t: t},\n\t\t\tMaxManifestFileSize:   1,\n\t\t\tL0CompactionThreshold: 10,\n\t\t})\n\t\t// Open should fail during a manifest roll,\n\t\t// leaving a temp dir on the filesystem.\n\t\tif !errors.Is(err, wantErr) {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\t// A temp file should be left on the filesystem\n\t// from the failed Rename of the CURRENT file.\n\tif temps := allTempFiles(t, mem); len(temps) == 0 {\n\t\tt.Fatal(\"no temp files on the filesystem\")\n\t}\n\n\t// Open the database a third time with a normal\n\t// filesystem again. It should clean up any temp\n\t// files on Open.\n\t{\n\t\td, err := Open(\"\", &Options{\n\t\t\tFS:                    mem,\n\t\t\tMaxManifestFileSize:   1,\n\t\t\tL0CompactionThreshold: 10,\n\t\t\tLogger:                testLogger{t},\n\t\t})\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, d.Close())\n\t\tif temps := allTempFiles(t, mem); len(temps) > 0 {\n\t\t\tt.Fatalf(\"temporary files still on disk: %#v\\n\", temps)\n\t\t}\n\t}\n}\n\nfunc allTempFiles(t *testing.T, fs vfs.FS) []string {\n\tvar files []string\n\tls, err := fs.List(\"\")\n\trequire.NoError(t, err)\n\tfor _, f := range ls {\n\t\tft, _, ok := base.ParseFilename(fs, f)\n\t\tif ok && ft == fileTypeTemp {\n\t\t\tfiles = append(files, f)\n\t\t}\n\t}\n\treturn files\n}\n\ntype renameErrorFS struct {\n\tvfs.FS\n\terr error\n}\n\nfunc (fs renameErrorFS) Rename(oldname string, newname string) error {\n\treturn fs.err\n}\n\n// noFatalLogger implements Logger, logging to the contained\n// *testing.T. Notably it does not panic on calls to Fatalf\n// to enable unit tests of fatal logic.\ntype noFatalLogger struct {\n\tt *testing.T\n}\n\nfunc (l noFatalLogger) Infof(format string, args ...interface{}) {\n\tl.t.Logf(format, args...)\n}\n\nfunc (l noFatalLogger) Errorf(format string, args ...interface{}) {\n\tl.t.Logf(format, args...)\n}\n\nfunc (l noFatalLogger) Fatalf(format string, args ...interface{}) {\n\tl.t.Logf(format, args...)\n}\n"
        },
        {
          "name": "flush_test.go",
          "type": "blob",
          "size": 2.76171875,
          "content": "// Copyright 2018 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/datadriven\"\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestManualFlush(t *testing.T) {\n\tgetOptions := func() *Options {\n\t\topts := &Options{\n\t\t\tFS:                    vfs.NewMem(),\n\t\t\tL0CompactionThreshold: 10,\n\t\t}\n\t\topts.DisableAutomaticCompactions = true\n\t\treturn opts\n\t}\n\td, err := Open(\"\", getOptions())\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, d.Close())\n\t}()\n\n\tdatadriven.RunTest(t, \"testdata/manual_flush\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"batch\":\n\t\t\tb := d.NewBatch()\n\t\t\tif err := runBatchDefineCmd(td, b); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tb.Commit(nil)\n\t\t\treturn \"\"\n\n\t\tcase \"flush\":\n\t\t\tif err := d.Flush(); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\n\t\t\td.mu.Lock()\n\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\td.mu.Unlock()\n\t\t\treturn s\n\n\t\tcase \"async-flush\":\n\t\t\td.mu.Lock()\n\t\t\tcur := d.mu.versions.currentVersion()\n\t\t\td.mu.Unlock()\n\n\t\t\tif _, err := d.AsyncFlush(); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\n\t\t\terr := try(100*time.Microsecond, 20*time.Second, func() error {\n\t\t\t\td.mu.Lock()\n\t\t\t\tdefer d.mu.Unlock()\n\t\t\t\tif cur == d.mu.versions.currentVersion() {\n\t\t\t\t\treturn errors.New(\"flush has not occurred\")\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\n\t\t\td.mu.Lock()\n\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\td.mu.Unlock()\n\t\t\treturn s\n\n\t\tcase \"reset\":\n\t\t\tif err := d.Close(); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\td, err = Open(\"\", getOptions())\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t}\n\t})\n}\n\n// TestFlushDelRangeEmptyKey tests flushing a range tombstone that begins with\n// an empty key. The empty key is a valid key but can be confused with nil.\nfunc TestFlushDelRangeEmptyKey(t *testing.T) {\n\td, err := Open(\"\", &Options{FS: vfs.NewMem()})\n\trequire.NoError(t, err)\n\trequire.NoError(t, d.DeleteRange([]byte{}, []byte(\"z\"), nil))\n\trequire.NoError(t, d.Flush())\n\trequire.NoError(t, d.Close())\n}\n\n// TestFlushEmptyKey tests that flushing an empty key does not trigger that key\n// order invariant assertions.\nfunc TestFlushEmptyKey(t *testing.T) {\n\td, err := Open(\"\", &Options{FS: vfs.NewMem()})\n\trequire.NoError(t, err)\n\trequire.NoError(t, d.Set(nil, []byte(\"hello\"), nil))\n\trequire.NoError(t, d.Flush())\n\tval, closer, err := d.Get(nil)\n\trequire.NoError(t, err)\n\trequire.Equal(t, val, []byte(\"hello\"))\n\trequire.NoError(t, closer.Close())\n\trequire.NoError(t, d.Close())\n}\n"
        },
        {
          "name": "flushable.go",
          "type": "blob",
          "size": 15.7724609375,
          "content": "// Copyright 2020 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/invariants\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan/keyspanimpl\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n)\n\n// flushable defines the interface for immutable memtables.\ntype flushable interface {\n\tnewIter(o *IterOptions) internalIterator\n\tnewFlushIter(o *IterOptions) internalIterator\n\tnewRangeDelIter(o *IterOptions) keyspan.FragmentIterator\n\tnewRangeKeyIter(o *IterOptions) keyspan.FragmentIterator\n\tcontainsRangeKeys() bool\n\t// inuseBytes returns the number of inuse bytes by the flushable.\n\tinuseBytes() uint64\n\t// totalBytes returns the total number of bytes allocated by the flushable.\n\ttotalBytes() uint64\n\t// readyForFlush returns true when the flushable is ready for flushing. See\n\t// memTable.readyForFlush for one implementation which needs to check whether\n\t// there are any outstanding write references.\n\treadyForFlush() bool\n\t// computePossibleOverlaps determines whether the flushable's keys overlap\n\t// with the bounds of any of the provided bounded items. If an item overlaps\n\t// or might overlap but it's not possible to determine overlap cheaply,\n\t// computePossibleOverlaps invokes the provided function with the object\n\t// that might overlap. computePossibleOverlaps must not perform any I/O and\n\t// implementations should invoke the provided function for items that would\n\t// require I/O to determine overlap.\n\tcomputePossibleOverlaps(overlaps func(bounded) shouldContinue, bounded ...bounded)\n}\n\ntype shouldContinue bool\n\nconst (\n\tcontinueIteration shouldContinue = true\n\tstopIteration                    = false\n)\n\ntype bounded interface {\n\tUserKeyBounds() base.UserKeyBounds\n}\n\nvar _ bounded = (*fileMetadata)(nil)\nvar _ bounded = KeyRange{}\n\nfunc sliceAsBounded[B bounded](s []B) []bounded {\n\tret := make([]bounded, len(s))\n\tfor i := 0; i < len(s); i++ {\n\t\tret[i] = s[i]\n\t}\n\treturn ret\n}\n\n// flushableEntry wraps a flushable and adds additional metadata and\n// functionality that is common to all flushables.\ntype flushableEntry struct {\n\tflushable\n\t// Channel which is closed when the flushable has been flushed.\n\tflushed chan struct{}\n\t// flushForced indicates whether a flush was forced on this memtable (either\n\t// manual, or due to ingestion). Protected by DB.mu.\n\tflushForced bool\n\t// delayedFlushForcedAt indicates whether a timer has been set to force a\n\t// flush on this memtable at some point in the future. Protected by DB.mu.\n\t// Holds the timestamp of when the flush will be issued.\n\tdelayedFlushForcedAt time.Time\n\t// logNum corresponds to the WAL that contains the records present in the\n\t// receiver.\n\tlogNum base.DiskFileNum\n\t// logSize is the size in bytes of the associated WAL. Protected by DB.mu.\n\tlogSize uint64\n\t// The current logSeqNum at the time the memtable was created. This is\n\t// guaranteed to be less than or equal to any seqnum stored in the memtable.\n\tlogSeqNum base.SeqNum\n\t// readerRefs tracks the read references on the flushable. The two sources of\n\t// reader references are DB.mu.mem.queue and readState.memtables. The memory\n\t// reserved by the flushable in the cache is released when the reader refs\n\t// drop to zero. If the flushable is referencing sstables, then the file\n\t// refount is also decreased once the reader refs drops to 0. If the\n\t// flushable is a memTable, when the reader refs drops to zero, the writer\n\t// refs will already be zero because the memtable will have been flushed and\n\t// that only occurs once the writer refs drops to zero.\n\treaderRefs atomic.Int32\n\t// Closure to invoke to release memory accounting.\n\treleaseMemAccounting func()\n\t// unrefFiles, if not nil, should be invoked to decrease the ref count of\n\t// files which are backing the flushable.\n\tunrefFiles func() []*fileBacking\n\t// deleteFnLocked should be called if the caller is holding DB.mu.\n\tdeleteFnLocked func(obsolete []*fileBacking)\n\t// deleteFn should be called if the caller is not holding DB.mu.\n\tdeleteFn func(obsolete []*fileBacking)\n}\n\nfunc (e *flushableEntry) readerRef() {\n\tswitch v := e.readerRefs.Add(1); {\n\tcase v <= 1:\n\t\tpanic(fmt.Sprintf(\"pebble: inconsistent reference count: %d\", v))\n\t}\n}\n\n// db.mu must not be held when this is called.\nfunc (e *flushableEntry) readerUnref(deleteFiles bool) {\n\te.readerUnrefHelper(deleteFiles, e.deleteFn)\n}\n\n// db.mu must be held when this is called.\nfunc (e *flushableEntry) readerUnrefLocked(deleteFiles bool) {\n\te.readerUnrefHelper(deleteFiles, e.deleteFnLocked)\n}\n\nfunc (e *flushableEntry) readerUnrefHelper(\n\tdeleteFiles bool, deleteFn func(obsolete []*fileBacking),\n) {\n\tswitch v := e.readerRefs.Add(-1); {\n\tcase v < 0:\n\t\tpanic(fmt.Sprintf(\"pebble: inconsistent reference count: %d\", v))\n\tcase v == 0:\n\t\tif e.releaseMemAccounting == nil {\n\t\t\tpanic(\"pebble: memtable reservation already released\")\n\t\t}\n\t\te.releaseMemAccounting()\n\t\te.releaseMemAccounting = nil\n\t\tif e.unrefFiles != nil {\n\t\t\tobsolete := e.unrefFiles()\n\t\t\te.unrefFiles = nil\n\t\t\tif deleteFiles {\n\t\t\t\tdeleteFn(obsolete)\n\t\t\t}\n\t\t}\n\t}\n}\n\ntype flushableList []*flushableEntry\n\n// ingestedFlushable is the implementation of the flushable interface for the\n// ingesting sstables which are added to the flushable list.\ntype ingestedFlushable struct {\n\t// files are non-overlapping and ordered (according to their bounds).\n\tfiles            []physicalMeta\n\tcomparer         *Comparer\n\tnewIters         tableNewIters\n\tnewRangeKeyIters keyspanimpl.TableNewSpanIter\n\n\t// Since the level slice is immutable, we construct and set it once. It\n\t// should be safe to read from slice in future reads.\n\tslice manifest.LevelSlice\n\t// hasRangeKeys is set on ingestedFlushable construction.\n\thasRangeKeys bool\n\t// exciseSpan is populated if an excise operation should be performed during\n\t// flush.\n\texciseSpan   KeyRange\n\texciseSeqNum base.SeqNum\n}\n\nfunc newIngestedFlushable(\n\tfiles []*fileMetadata,\n\tcomparer *Comparer,\n\tnewIters tableNewIters,\n\tnewRangeKeyIters keyspanimpl.TableNewSpanIter,\n\texciseSpan KeyRange,\n\tseqNum base.SeqNum,\n) *ingestedFlushable {\n\tif invariants.Enabled {\n\t\tfor i := 1; i < len(files); i++ {\n\t\t\tprev := files[i-1].UserKeyBounds()\n\t\t\tthis := files[i].UserKeyBounds()\n\t\t\tif prev.End.IsUpperBoundFor(comparer.Compare, this.Start) {\n\t\t\t\tpanic(errors.AssertionFailedf(\"ingested flushable files overlap: %s %s\", prev, this))\n\t\t\t}\n\t\t}\n\t}\n\tvar physicalFiles []physicalMeta\n\tvar hasRangeKeys bool\n\tfor _, f := range files {\n\t\tif f.HasRangeKeys {\n\t\t\thasRangeKeys = true\n\t\t}\n\t\tphysicalFiles = append(physicalFiles, f.PhysicalMeta())\n\t}\n\n\tret := &ingestedFlushable{\n\t\tfiles:            physicalFiles,\n\t\tcomparer:         comparer,\n\t\tnewIters:         newIters,\n\t\tnewRangeKeyIters: newRangeKeyIters,\n\t\t// slice is immutable and can be set once and used many times.\n\t\tslice:        manifest.NewLevelSliceKeySorted(comparer.Compare, files),\n\t\thasRangeKeys: hasRangeKeys,\n\t\texciseSpan:   exciseSpan,\n\t\texciseSeqNum: seqNum,\n\t}\n\n\treturn ret\n}\n\n// TODO(sumeer): ingestedFlushable iters also need to plumb context for\n// tracing.\n\n// newIter is part of the flushable interface.\nfunc (s *ingestedFlushable) newIter(o *IterOptions) internalIterator {\n\tvar opts IterOptions\n\tif o != nil {\n\t\topts = *o\n\t}\n\treturn newLevelIter(\n\t\tcontext.Background(), opts, s.comparer, s.newIters, s.slice.Iter(), manifest.FlushableIngestsLayer(),\n\t\tinternalIterOpts{},\n\t)\n}\n\n// newFlushIter is part of the flushable interface.\nfunc (s *ingestedFlushable) newFlushIter(*IterOptions) internalIterator {\n\t// newFlushIter is only used for writing memtables to disk as sstables.\n\t// Since ingested sstables are already present on disk, they don't need to\n\t// make use of a flush iter.\n\tpanic(\"pebble: not implemented\")\n}\n\nfunc (s *ingestedFlushable) constructRangeDelIter(\n\tctx context.Context, file *manifest.FileMetadata, _ keyspan.SpanIterOptions,\n) (keyspan.FragmentIterator, error) {\n\titers, err := s.newIters(ctx, file, nil, internalIterOpts{}, iterRangeDeletions)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn iters.RangeDeletion(), nil\n}\n\n// newRangeDelIter is part of the flushable interface.\n// TODO(bananabrick): Using a level iter instead of a keyspan level iter to\n// surface range deletes is more efficient.\n//\n// TODO(sumeer): *IterOptions are being ignored, so the index block load for\n// the point iterator in constructRangeDeIter is not tracked.\nfunc (s *ingestedFlushable) newRangeDelIter(_ *IterOptions) keyspan.FragmentIterator {\n\tliter := keyspanimpl.NewLevelIter(\n\t\tcontext.TODO(),\n\t\tkeyspan.SpanIterOptions{}, s.comparer.Compare,\n\t\ts.constructRangeDelIter, s.slice.Iter(), manifest.FlushableIngestsLayer(),\n\t\tmanifest.KeyTypePoint,\n\t)\n\tif !s.exciseSpan.Valid() {\n\t\treturn liter\n\t}\n\t// We have an excise span to weave into the rangedel iterators.\n\t//\n\t// TODO(bilal): should this be pooled?\n\tmiter := &keyspanimpl.MergingIter{}\n\trdel := keyspan.Span{\n\t\tStart: s.exciseSpan.Start,\n\t\tEnd:   s.exciseSpan.End,\n\t\tKeys:  []keyspan.Key{{Trailer: base.MakeTrailer(s.exciseSeqNum, base.InternalKeyKindRangeDelete)}},\n\t}\n\trdelIter := keyspan.NewIter(s.comparer.Compare, []keyspan.Span{rdel})\n\tmiter.Init(s.comparer, keyspan.NoopTransform, new(keyspanimpl.MergingBuffers), liter, rdelIter)\n\treturn miter\n}\n\n// newRangeKeyIter is part of the flushable interface.\nfunc (s *ingestedFlushable) newRangeKeyIter(o *IterOptions) keyspan.FragmentIterator {\n\tvar rkeydelIter keyspan.FragmentIterator\n\tif s.exciseSpan.Valid() {\n\t\t// We have an excise span to weave into the rangekey iterators.\n\t\trkeydel := keyspan.Span{\n\t\t\tStart: s.exciseSpan.Start,\n\t\t\tEnd:   s.exciseSpan.End,\n\t\t\tKeys:  []keyspan.Key{{Trailer: base.MakeTrailer(s.exciseSeqNum, base.InternalKeyKindRangeKeyDelete)}},\n\t\t}\n\t\trkeydelIter = keyspan.NewIter(s.comparer.Compare, []keyspan.Span{rkeydel})\n\t}\n\n\tif !s.hasRangeKeys {\n\t\tif rkeydelIter == nil {\n\t\t\t// NB: we have to return the nil literal as opposed to the nil\n\t\t\t// value of rkeydelIter, otherwise callers of this function will\n\t\t\t// have the return value fail == nil checks.\n\t\t\treturn nil\n\t\t}\n\t\treturn rkeydelIter\n\t}\n\n\tliter := keyspanimpl.NewLevelIter(\n\t\tcontext.TODO(),\n\t\tkeyspan.SpanIterOptions{}, s.comparer.Compare, s.newRangeKeyIters,\n\t\ts.slice.Iter(), manifest.FlushableIngestsLayer(), manifest.KeyTypeRange,\n\t)\n\tif rkeydelIter == nil {\n\t\treturn liter\n\t}\n\t// TODO(bilal): should this be pooled?\n\tmiter := &keyspanimpl.MergingIter{}\n\tmiter.Init(s.comparer, keyspan.NoopTransform, new(keyspanimpl.MergingBuffers), liter, rkeydelIter)\n\treturn miter\n}\n\n// containsRangeKeys is part of the flushable interface.\nfunc (s *ingestedFlushable) containsRangeKeys() bool {\n\treturn s.hasRangeKeys || s.exciseSpan.Valid()\n}\n\n// inuseBytes is part of the flushable interface.\nfunc (s *ingestedFlushable) inuseBytes() uint64 {\n\t// inuseBytes is only used when memtables are flushed to disk as sstables.\n\tpanic(\"pebble: not implemented\")\n}\n\n// totalBytes is part of the flushable interface.\nfunc (s *ingestedFlushable) totalBytes() uint64 {\n\t// We don't allocate additional bytes for the ingestedFlushable.\n\treturn 0\n}\n\n// readyForFlush is part of the flushable interface.\nfunc (s *ingestedFlushable) readyForFlush() bool {\n\t// ingestedFlushable should always be ready to flush. However, note that\n\t// memtables before the ingested sstables in the memtable queue must be\n\t// flushed before an ingestedFlushable can be flushed. This is because the\n\t// ingested sstables need an updated view of the Version to\n\t// determine where to place the files in the lsm.\n\treturn true\n}\n\n// computePossibleOverlaps is part of the flushable interface.\nfunc (s *ingestedFlushable) computePossibleOverlaps(\n\tfn func(bounded) shouldContinue, bounded ...bounded,\n) {\n\tfor _, b := range bounded {\n\t\tif s.anyFileOverlaps(b.UserKeyBounds()) {\n\t\t\t// Some file overlaps in key boundaries. The file doesn't necessarily\n\t\t\t// contain any keys within the key range, but we would need to perform I/O\n\t\t\t// to know for sure. The flushable interface dictates that we're not\n\t\t\t// permitted to perform I/O here, so err towards assuming overlap.\n\t\t\tif !fn(b) {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n}\n\n// anyFileBoundsOverlap returns true if there is at least a file in s.files with\n// bounds that overlap the given bounds.\nfunc (s *ingestedFlushable) anyFileOverlaps(bounds base.UserKeyBounds) bool {\n\t// Note that s.files are non-overlapping and sorted.\n\tfor _, f := range s.files {\n\t\tfileBounds := f.UserKeyBounds()\n\t\tif !fileBounds.End.IsUpperBoundFor(s.comparer.Compare, bounds.Start) {\n\t\t\t// The file ends before the bounds start. Go to the next file.\n\t\t\tcontinue\n\t\t}\n\t\tif !bounds.End.IsUpperBoundFor(s.comparer.Compare, fileBounds.Start) {\n\t\t\t// The file starts after the bounds end. There is no overlap, and\n\t\t\t// further files will not overlap either (the files are sorted).\n\t\t\tbreak\n\t\t}\n\t\t// There is overlap. Note that UserKeyBounds.Overlaps() performs exactly the\n\t\t// checks above.\n\t\treturn true\n\t}\n\tif s.exciseSpan.Valid() {\n\t\tuk := s.exciseSpan.UserKeyBounds()\n\t\treturn uk.Overlaps(s.comparer.Compare, &bounds)\n\t}\n\treturn false\n}\n\n// computePossibleOverlapsGenericImpl is an implementation of the flushable\n// interface's computePossibleOverlaps function for flushable implementations\n// with only in-memory state that do not have special requirements and should\n// read through the ordinary flushable iterators.\n//\n// This function must only be used with implementations that are infallible (eg,\n// memtable iterators) and will panic if an error is encountered.\nfunc computePossibleOverlapsGenericImpl[F flushable](\n\tf F, cmp Compare, fn func(bounded) shouldContinue, bounded []bounded,\n) {\n\titer := f.newIter(nil)\n\trangeDelIter := f.newRangeDelIter(nil)\n\trangeKeyIter := f.newRangeKeyIter(nil)\n\tfor _, b := range bounded {\n\t\toverlap, err := determineOverlapAllIters(cmp, b.UserKeyBounds(), iter, rangeDelIter, rangeKeyIter)\n\t\tif invariants.Enabled && err != nil {\n\t\t\tpanic(errors.AssertionFailedf(\"expected iterator to be infallible: %v\", err))\n\t\t}\n\t\tif overlap {\n\t\t\tif !fn(b) {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\n\tif iter != nil {\n\t\tif err := iter.Close(); err != nil {\n\t\t\t// This implementation must be used in circumstances where\n\t\t\t// reading through the iterator is infallible.\n\t\t\tpanic(err)\n\t\t}\n\t}\n\tif rangeDelIter != nil {\n\t\trangeDelIter.Close()\n\t}\n\tif rangeKeyIter != nil {\n\t\trangeKeyIter.Close()\n\t}\n}\n\n// determineOverlapAllIters checks for overlap in a point iterator, range\n// deletion iterator and range key iterator.\nfunc determineOverlapAllIters(\n\tcmp base.Compare,\n\tbounds base.UserKeyBounds,\n\tpointIter base.InternalIterator,\n\trangeDelIter, rangeKeyIter keyspan.FragmentIterator,\n) (bool, error) {\n\tif pointIter != nil {\n\t\tif pointOverlap, err := determineOverlapPointIterator(cmp, bounds, pointIter); pointOverlap || err != nil {\n\t\t\treturn pointOverlap, err\n\t\t}\n\t}\n\tif rangeDelIter != nil {\n\t\tif rangeDelOverlap, err := determineOverlapKeyspanIterator(cmp, bounds, rangeDelIter); rangeDelOverlap || err != nil {\n\t\t\treturn rangeDelOverlap, err\n\t\t}\n\t}\n\tif rangeKeyIter != nil {\n\t\treturn determineOverlapKeyspanIterator(cmp, bounds, rangeKeyIter)\n\t}\n\treturn false, nil\n}\n\nfunc determineOverlapPointIterator(\n\tcmp base.Compare, bounds base.UserKeyBounds, iter internalIterator,\n) (bool, error) {\n\tkv := iter.SeekGE(bounds.Start, base.SeekGEFlagsNone)\n\tif kv == nil {\n\t\treturn false, iter.Error()\n\t}\n\treturn bounds.End.IsUpperBoundForInternalKey(cmp, kv.K), nil\n}\n\nfunc determineOverlapKeyspanIterator(\n\tcmp base.Compare, bounds base.UserKeyBounds, iter keyspan.FragmentIterator,\n) (bool, error) {\n\t// NB: The spans surfaced by the fragment iterator are non-overlapping.\n\tspan, err := iter.SeekGE(bounds.Start)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tfor ; span != nil; span, err = iter.Next() {\n\t\tif !bounds.End.IsUpperBoundFor(cmp, span.Start) {\n\t\t\t// The span starts after our bounds.\n\t\t\treturn false, nil\n\t\t}\n\t\tif !span.Empty() {\n\t\t\treturn true, nil\n\t\t}\n\t}\n\treturn false, err\n}\n"
        },
        {
          "name": "flushable_test.go",
          "type": "blob",
          "size": 5.359375,
          "content": "package pebble\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/cockroachdb/datadriven\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/stretchr/testify/require\"\n)\n\n// Simple sanity tests for the flushable interface implementation for ingested\n// sstables.\nfunc TestIngestedSSTFlushableAPI(t *testing.T) {\n\tvar mem vfs.FS\n\tvar d *DB\n\tdefer func() {\n\t\trequire.NoError(t, d.Close())\n\t}()\n\tvar flushable flushable\n\n\treset := func() {\n\t\tif d != nil {\n\t\t\trequire.NoError(t, d.Close())\n\t\t}\n\n\t\tmem = vfs.NewMem()\n\t\trequire.NoError(t, mem.MkdirAll(\"ext\", 0755))\n\t\topts := &Options{\n\t\t\tFS:                    mem,\n\t\t\tL0CompactionThreshold: 100,\n\t\t\tL0StopWritesThreshold: 100,\n\t\t\tDebugCheck:            DebugCheckLevels,\n\t\t\tFormatMajorVersion:    internalFormatNewest,\n\t\t}\n\t\t// Disable automatic compactions because otherwise we'll race with\n\t\t// delete-only compactions triggered by ingesting range tombstones.\n\t\topts.DisableAutomaticCompactions = true\n\n\t\tvar err error\n\t\td, err = Open(\"\", opts)\n\t\trequire.NoError(t, err)\n\t\tflushable = nil\n\t}\n\treset()\n\n\tloadFileMeta := func(paths []string, exciseSpan KeyRange, seqNum base.SeqNum) []*fileMetadata {\n\t\tpendingOutputs := make([]base.FileNum, len(paths))\n\t\tfor i := range paths {\n\t\t\tpendingOutputs[i] = d.mu.versions.getNextFileNum()\n\t\t}\n\t\tjobID := d.newJobID()\n\n\t\t// We can reuse the ingestLoad function for this test even if we're\n\t\t// not actually ingesting a file.\n\t\tlr, err := ingestLoad(context.Background(), d.opts, d.FormatMajorVersion(), paths, nil, nil, d.cacheID, pendingOutputs)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tmeta := make([]*fileMetadata, len(lr.local))\n\t\tif exciseSpan.Valid() {\n\t\t\tseqNum++\n\t\t}\n\t\tfor i := range meta {\n\t\t\tmeta[i] = lr.local[i].fileMetadata\n\t\t\tif err := setSeqNumInMetadata(meta[i], seqNum+base.SeqNum(i), d.cmp, d.opts.Comparer.FormatKey); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t}\n\t\tif len(meta) == 0 {\n\t\t\t// All of the sstables to be ingested were empty. Nothing to do.\n\t\t\tpanic(\"empty sstable\")\n\t\t}\n\t\t// The file cache requires the *fileMetadata to have a positive\n\t\t// reference count. Fake a reference before we try to load the file.\n\t\tfor _, f := range meta {\n\t\t\tf.FileBacking.Ref()\n\t\t}\n\n\t\t// Verify the sstables do not overlap.\n\t\tif err := ingestSortAndVerify(d.cmp, lr, exciseSpan); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Hard link the sstables into the DB directory. Since the sstables aren't\n\t\t// referenced by a version, they won't be used. If the hard linking fails\n\t\t// (e.g. because the files reside on a different filesystem), ingestLink will\n\t\t// fall back to copying, and if that fails we undo our work and return an\n\t\t// error.\n\t\tif err := ingestLinkLocal(context.Background(), jobID, d.opts, d.objProvider, lr.local); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// Fsync the directory we added the tables to. We need to do this at some\n\t\t// point before we update the MANIFEST (via logAndApply), otherwise a crash\n\t\t// can have the tables referenced in the MANIFEST, but not present in the\n\t\t// directory.\n\t\tif err := d.dataDir.Sync(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\treturn meta\n\t}\n\n\tvar seqNum uint64\n\tdatadriven.RunTest(t, \"testdata/ingested_flushable_api\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"reset\":\n\t\t\treset()\n\t\t\treturn \"\"\n\t\tcase \"build\":\n\t\t\tif err := runBuildCmd(td, d, mem); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\t\tcase \"flushable\":\n\t\t\t// Creates an ingestedFlushable over the input files.\n\t\t\tpaths := make([]string, 0, len(td.CmdArgs))\n\t\t\tvar exciseSpan KeyRange\n\t\t\tstartSeqNum := base.SeqNum(seqNum)\n\t\t\tfor _, arg := range td.CmdArgs {\n\t\t\t\tswitch arg.Key {\n\t\t\t\tcase \"excise\":\n\t\t\t\t\tparts := strings.Split(arg.Vals[0], \"-\")\n\t\t\t\t\tif len(parts) != 2 {\n\t\t\t\t\t\treturn fmt.Sprintf(\"invalid excise range: %s\", arg.Vals[0])\n\t\t\t\t\t}\n\t\t\t\t\texciseSpan.Start = []byte(parts[0])\n\t\t\t\t\texciseSpan.End = []byte(parts[1])\n\t\t\t\t\tseqNum++\n\t\t\t\tdefault:\n\t\t\t\t\tpaths = append(paths, arg.String())\n\t\t\t\t\tseqNum++\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tmeta := loadFileMeta(paths, exciseSpan, startSeqNum)\n\t\t\tflushable = newIngestedFlushable(meta, d.opts.Comparer, d.newIters, d.tableNewRangeKeyIter, exciseSpan, base.SeqNum(startSeqNum))\n\t\t\treturn \"\"\n\t\tcase \"iter\":\n\t\t\titer := flushable.newIter(nil)\n\t\t\tvar buf bytes.Buffer\n\t\t\tfor x := iter.First(); x != nil; x = iter.Next() {\n\t\t\t\tbuf.WriteString(x.K.String())\n\t\t\t\tbuf.WriteString(\"\\n\")\n\t\t\t}\n\t\t\titer.Close()\n\t\t\treturn buf.String()\n\t\tcase \"rangekeyIter\":\n\t\t\titer := flushable.newRangeKeyIter(nil)\n\t\t\tvar buf bytes.Buffer\n\t\t\tif iter != nil {\n\t\t\t\tspan, err := iter.First()\n\t\t\t\tfor ; span != nil; span, err = iter.Next() {\n\t\t\t\t\tbuf.WriteString(span.String())\n\t\t\t\t\tbuf.WriteString(\"\\n\")\n\t\t\t\t}\n\t\t\t\titer.Close()\n\t\t\t\tif err != nil {\n\t\t\t\t\tfmt.Fprintf(&buf, \"err=%q\", err.Error())\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn buf.String()\n\t\tcase \"rangedelIter\":\n\t\t\titer := flushable.newRangeDelIter(nil)\n\t\t\tvar buf bytes.Buffer\n\t\t\tif iter != nil {\n\t\t\t\tspan, err := iter.First()\n\t\t\t\tfor ; span != nil; span, err = iter.Next() {\n\t\t\t\t\tbuf.WriteString(span.String())\n\t\t\t\t\tbuf.WriteString(\"\\n\")\n\t\t\t\t}\n\t\t\t\titer.Close()\n\t\t\t\tif err != nil {\n\t\t\t\t\tfmt.Fprintf(&buf, \"err=%q\", err.Error())\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn buf.String()\n\t\tcase \"readyForFlush\":\n\t\t\tif flushable.readyForFlush() {\n\t\t\t\treturn \"true\"\n\t\t\t}\n\t\t\treturn \"false\"\n\t\tcase \"containsRangeKey\":\n\t\t\tif flushable.containsRangeKeys() {\n\t\t\t\treturn \"true\"\n\t\t\t}\n\t\t\treturn \"false\"\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t}\n\t})\n}\n"
        },
        {
          "name": "format_major_version.go",
          "type": "blob",
          "size": 23.24609375,
          "content": "// Copyright 2021 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"fmt\"\n\t\"strconv\"\n\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/cockroachdb/pebble/vfs/atomicfs\"\n)\n\n// FormatMajorVersion is a constant controlling the format of persisted\n// data. Backwards incompatible changes to durable formats are gated\n// behind new format major versions.\n//\n// At any point, a database's format major version may be bumped.\n// However, once a database's format major version is increased,\n// previous versions of Pebble will refuse to open the database.\n//\n// The zero value format is the FormatDefault constant. The exact\n// FormatVersion that the default corresponds to may change with time.\ntype FormatMajorVersion uint64\n\n// SafeValue implements redact.SafeValue.\nfunc (v FormatMajorVersion) SafeValue() {}\n\n// String implements fmt.Stringer.\nfunc (v FormatMajorVersion) String() string {\n\t// NB: This must not change. It's used as the value for the on-disk\n\t// version marker file.\n\t//\n\t// Specifically, this value must always parse as a base 10 integer\n\t// that fits in a uint64. We format it as zero-padded, 3-digit\n\t// number today, but the padding may change.\n\treturn fmt.Sprintf(\"%03d\", v)\n}\n\nconst (\n\t// FormatDefault leaves the format version unspecified. When used to create a\n\t// new store, Pebble will choose the earliest format version it supports.\n\tFormatDefault FormatMajorVersion = iota\n\n\t// 21.2 versions.\n\n\t// FormatMostCompatible maintains the most backwards compatibility,\n\t// maintaining bi-directional compatibility with RocksDB 6.2.1 in\n\t// the particular configuration described in the Pebble README.\n\t// Deprecated.\n\t_ // FormatMostCompatible\n\n\t// formatVersionedManifestMarker is the first\n\t// backwards-incompatible change made to Pebble, introducing the\n\t// format-version marker file for handling backwards-incompatible\n\t// changes more broadly, and replacing the `CURRENT` file with a\n\t// marker file.\n\t//\n\t// This format version is intended as an intermediary version state.\n\t// It is deliberately unexported to discourage direct use of this\n\t// format major version.  Clients should use FormatVersioned which\n\t// also ensures earlier versions of Pebble fail to open a database\n\t// written in a future format major version.\n\t// Deprecated.\n\t_ // formatVersionedManifestMarker\n\n\t// FormatVersioned is a new format major version that replaces the\n\t// old `CURRENT` file with a new 'marker' file scheme.  Previous\n\t// Pebble versions will be unable to open the database unless\n\t// they're aware of format versions.\n\t// Deprecated.\n\t_ // FormatVersioned\n\n\t// FormatSetWithDelete is a format major version that introduces a new key\n\t// kind, base.InternalKeyKindSetWithDelete. Previous Pebble versions will be\n\t// unable to open this database.\n\t// Deprecated.\n\t_ // FormatSetWithDelete\n\n\t// 22.1 versions.\n\n\t// FormatBlockPropertyCollector is a format major version that introduces\n\t// BlockPropertyCollectors.\n\t// Deprecated.\n\t_ // FormatBlockPropertyCollector\n\n\t// FormatSplitUserKeysMarked is a format major version that guarantees that\n\t// all files that share user keys with neighbors are marked for compaction\n\t// in the manifest. Ratcheting to FormatSplitUserKeysMarked will block\n\t// (without holding mutexes) until the scan of the LSM is complete and the\n\t// manifest has been rotated.\n\t// Deprecated.\n\t_ // FormatSplitUserKeysMarked\n\n\t// 22.2 versions.\n\n\t// FormatSplitUserKeysMarkedCompacted is a format major version that\n\t// guarantees that all files explicitly marked for compaction in the manifest\n\t// have been compacted. Combined with the FormatSplitUserKeysMarked format\n\t// major version, this version guarantees that there are no user keys split\n\t// across multiple files within a level L1+. Ratcheting to this format version\n\t// will block (without holding mutexes) until all necessary compactions for\n\t// files marked for compaction are complete.\n\t// Deprecated.\n\t_ // FormatSplitUserKeysMarkedCompacted\n\n\t// FormatRangeKeys is a format major version that introduces range keys.\n\t// Deprecated.\n\t_ // FormatRangeKeys\n\n\t// FormatMinTableFormatPebblev1 is a format major version that guarantees that\n\t// tables created by or ingested into the DB at or above this format major\n\t// version will have a table format version of at least Pebblev1 (Block\n\t// Properties).\n\t// Deprecated.\n\t_ // FormatMinTableFormatPebblev1\n\n\t// FormatPrePebblev1Marked is a format major version that guarantees that all\n\t// sstables with a table format version pre-Pebblev1 (i.e. those that are\n\t// guaranteed to not contain block properties) are marked for compaction in\n\t// the manifest. Ratcheting to FormatPrePebblev1Marked will block (without\n\t// holding mutexes) until the scan of the LSM is complete and the manifest has\n\t// been rotated.\n\t// Deprecated.\n\t_ // FormatPrePebblev1Marked\n\n\t// 23.1 versions.\n\n\t// formatUnusedPrePebblev1MarkedCompacted is an unused format major version.\n\t// This format major version was originally intended to ship in the 23.1\n\t// release. It was later decided that this should be deferred until a\n\t// subsequent release. The original ordering is preserved so as not to\n\t// introduce breaking changes in Cockroach.\n\t_ // formatUnusedPrePebblev1MarkedCompacted\n\n\t// FormatSSTableValueBlocks is a format major version that adds support for\n\t// storing values in value blocks in the sstable. Value block support is not\n\t// necessarily enabled when writing sstables, when running with this format\n\t// major version.\n\t_ // FormatSSTableValueBlocks\n\n\t// FormatFlushableIngest is a format major version that enables lazy\n\t// addition of ingested sstables into the LSM structure. When an ingest\n\t// overlaps with a memtable, a record of the ingest is written to the WAL\n\t// without waiting for a flush. Subsequent reads treat the ingested files as\n\t// a level above the overlapping memtable. Once the memtable is flushed, the\n\t// ingested files are moved into the lowest possible levels.\n\t//\n\t// This feature is behind a format major version because it required\n\t// breaking changes to the WAL format.\n\tFormatFlushableIngest\n\n\t// 23.2 versions.\n\n\t// FormatPrePebblev1MarkedCompacted is a format major version that guarantees\n\t// that all sstables explicitly marked for compaction in the manifest (see\n\t// FormatPrePebblev1Marked) have been compacted. Ratcheting to this format\n\t// version will block (without holding mutexes) until all necessary\n\t// compactions for files marked for compaction are complete.\n\tFormatPrePebblev1MarkedCompacted\n\n\t// FormatDeleteSizedAndObsolete is a format major version that adds support\n\t// for deletion tombstones that encode the size of the value they're\n\t// expected to delete. This format major version is required before the\n\t// associated key kind may be committed through batch applications or\n\t// ingests. It also adds support for keys that are marked obsolete (see\n\t// sstable/format.go for details).\n\tFormatDeleteSizedAndObsolete\n\n\t// FormatVirtualSSTables is a format major version that adds support for\n\t// virtual sstables that can reference a sub-range of keys in an underlying\n\t// physical sstable. This information is persisted through new,\n\t// backward-incompatible fields in the Manifest, and therefore requires\n\t// a format major version.\n\tFormatVirtualSSTables\n\n\t// FormatSyntheticPrefixSuffix is a format major version that adds support for\n\t// sstables to have their content exposed in a different prefix or suffix of\n\t// keyspace than the actual prefix/suffix persisted in the keys in such\n\t// sstables. The prefix and suffix replacement information is stored in new\n\t// fields in the Manifest and thus requires a format major version.\n\tFormatSyntheticPrefixSuffix\n\n\t// FormatFlushableIngestExcises is a format major version that adds support for\n\t// having excises unconditionally being written as flushable ingestions. This\n\t// is implemented through adding a new key kind that can go in the same batches\n\t// as flushable ingested sstables.\n\tFormatFlushableIngestExcises\n\n\t// FormatColumnarBlocks is a format major version enabling use of the\n\t// TableFormatPebblev5 table format, that encodes sstable data blocks, index\n\t// blocks and keyspan blocks by organizing the KVs into columns within the\n\t// block.\n\tFormatColumnarBlocks\n\n\t// -- Add new versions here --\n\n\t// FormatNewest is the most recent format major version.\n\tFormatNewest FormatMajorVersion = iota - 1\n\n\t// Experimental versions, which are excluded by FormatNewest (but can be used\n\t// in tests) can be defined here.\n\n\t// -- Add experimental versions here --\n\n\t// internalFormatNewest is the most recent, possibly experimental format major\n\t// version.\n\tinternalFormatNewest FormatMajorVersion = iota - 2\n)\n\n// FormatMinSupported is the minimum format version that is supported by this\n// Pebble version.\nconst FormatMinSupported = FormatFlushableIngest\n\n// FormatMinForSharedObjects it the minimum format version that supports shared\n// objects (see CreateOnShared option).\nconst FormatMinForSharedObjects = FormatVirtualSSTables\n\n// IsSupported returns true if the version is supported by the current Pebble\n// version.\nfunc (v FormatMajorVersion) IsSupported() bool {\n\treturn v == FormatDefault && v >= FormatMinSupported && v <= internalFormatNewest\n}\n\n// MaxTableFormat returns the maximum sstable.TableFormat that can be used at\n// this FormatMajorVersion.\nfunc (v FormatMajorVersion) MaxTableFormat() sstable.TableFormat {\n\tswitch v {\n\tcase FormatDefault, FormatFlushableIngest, FormatPrePebblev1MarkedCompacted:\n\t\treturn sstable.TableFormatPebblev3\n\tcase FormatDeleteSizedAndObsolete, FormatVirtualSSTables, FormatSyntheticPrefixSuffix,\n\t\tFormatFlushableIngestExcises:\n\t\treturn sstable.TableFormatPebblev4\n\tcase FormatColumnarBlocks:\n\t\treturn sstable.TableFormatPebblev5\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"pebble: unsupported format major version: %s\", v))\n\t}\n}\n\n// MinTableFormat returns the minimum sstable.TableFormat that can be used at\n// this FormatMajorVersion.\nfunc (v FormatMajorVersion) MinTableFormat() sstable.TableFormat {\n\tswitch v {\n\tcase FormatDefault, FormatFlushableIngest, FormatPrePebblev1MarkedCompacted,\n\t\tFormatDeleteSizedAndObsolete, FormatVirtualSSTables, FormatSyntheticPrefixSuffix,\n\t\tFormatFlushableIngestExcises, FormatColumnarBlocks:\n\t\treturn sstable.TableFormatPebblev1\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"pebble: unsupported format major version: %s\", v))\n\t}\n}\n\n// formatMajorVersionMigrations defines the migrations from one format\n// major version to the next. Each migration is defined as a closure\n// which will be invoked on the database before the new format major\n// version is committed. Migrations must be idempotent. Migrations are\n// invoked with d.mu locked.\n//\n// Each migration is responsible for invoking finalizeFormatVersUpgrade\n// to set the new format major version.  RatchetFormatMajorVersion will\n// panic if a migration returns a nil error but fails to finalize the\n// new format major version.\nvar formatMajorVersionMigrations = map[FormatMajorVersion]func(*DB) error{\n\tFormatFlushableIngest: func(d *DB) error { return nil },\n\tFormatPrePebblev1MarkedCompacted: func(d *DB) error {\n\t\t// Before finalizing the format major version, rewrite any sstables\n\t\t// still marked for compaction. Note all format major versions\n\t\t// migrations are invoked with DB.mu locked.\n\t\tif err := d.compactMarkedFilesLocked(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn d.finalizeFormatVersUpgrade(FormatPrePebblev1MarkedCompacted)\n\t},\n\tFormatDeleteSizedAndObsolete: func(d *DB) error {\n\t\treturn d.finalizeFormatVersUpgrade(FormatDeleteSizedAndObsolete)\n\t},\n\tFormatVirtualSSTables: func(d *DB) error {\n\t\treturn d.finalizeFormatVersUpgrade(FormatVirtualSSTables)\n\t},\n\tFormatSyntheticPrefixSuffix: func(d *DB) error {\n\t\treturn d.finalizeFormatVersUpgrade(FormatSyntheticPrefixSuffix)\n\t},\n\tFormatFlushableIngestExcises: func(d *DB) error {\n\t\treturn d.finalizeFormatVersUpgrade(FormatFlushableIngestExcises)\n\t},\n\tFormatColumnarBlocks: func(d *DB) error {\n\t\treturn d.finalizeFormatVersUpgrade(FormatColumnarBlocks)\n\t},\n}\n\nconst formatVersionMarkerName = `format-version`\n\n// lookupFormatMajorVersion retrieves the format version from the format version\n// marker file.\n//\n// If such a file does not exist, returns FormatDefault. Note that this case is\n// only acceptable if we are creating a new store (we no longer support\n// FormatMostCompatible which is the only one with no version marker file).\nfunc lookupFormatMajorVersion(\n\tfs vfs.FS, dirname string, ls []string,\n) (FormatMajorVersion, *atomicfs.Marker, error) {\n\tm, versString, err := atomicfs.LocateMarkerInListing(fs, dirname, formatVersionMarkerName, ls)\n\tif err != nil {\n\t\treturn 0, nil, err\n\t}\n\tif versString == \"\" {\n\t\treturn FormatDefault, m, nil\n\t}\n\tv, err := strconv.ParseUint(versString, 10, 64)\n\tif err != nil {\n\t\treturn 0, nil, errors.Wrap(err, \"parsing format major version\")\n\t}\n\tvers := FormatMajorVersion(v)\n\tif vers == FormatDefault {\n\t\treturn 0, nil, errors.Newf(\"pebble: default format major version should not persisted\", vers)\n\t}\n\tif vers > internalFormatNewest {\n\t\treturn 0, nil, errors.Newf(\"pebble: database %q written in unknown format major version %d\", dirname, vers)\n\t}\n\tif vers < FormatMinSupported {\n\t\treturn 0, nil, errors.Newf(\"pebble: database %q written in format major version %d which is no longer supported\", dirname, vers)\n\t}\n\treturn vers, m, nil\n}\n\n// FormatMajorVersion returns the database's active format major\n// version. The format major version may be higher than the one\n// provided in Options when the database was opened if the existing\n// database was written with a higher format version.\nfunc (d *DB) FormatMajorVersion() FormatMajorVersion {\n\treturn FormatMajorVersion(d.mu.formatVers.vers.Load())\n}\n\n// TableFormat returns the TableFormat that the database is currently using when\n// writing sstables. The table format is determined by the database's format\n// major version, as well as experimental settings like EnableValueBlocks and\n// EnableColumnarBlocks.\nfunc (d *DB) TableFormat() sstable.TableFormat {\n\t// The table is typically written at the maximum allowable format implied by\n\t// the current format major version of the DB.\n\tf := d.FormatMajorVersion().MaxTableFormat()\n\tswitch f {\n\tcase sstable.TableFormatPebblev3:\n\t\t// In format major versions with maximum table formats of Pebblev3,\n\t\t// value blocks were conditional on an experimental setting. In format\n\t\t// major versions with maximum table formats of Pebblev4 and higher,\n\t\t// value blocks are always enabled.\n\t\tif d.opts.Experimental.EnableValueBlocks == nil || !d.opts.Experimental.EnableValueBlocks() {\n\t\t\tf = sstable.TableFormatPebblev2\n\t\t}\n\tcase sstable.TableFormatPebblev5:\n\t\tif d.opts.Experimental.EnableColumnarBlocks == nil || !d.opts.Experimental.EnableColumnarBlocks() {\n\t\t\tf = sstable.TableFormatPebblev4\n\t\t}\n\t}\n\treturn f\n}\n\n// RatchetFormatMajorVersion ratchets the opened database's format major\n// version to the provided version. It errors if the provided format\n// major version is below the database's current version. Once a\n// database's format major version is upgraded, previous Pebble versions\n// that do not know of the format version will be unable to open the\n// database.\nfunc (d *DB) RatchetFormatMajorVersion(fmv FormatMajorVersion) error {\n\tif err := d.closed.Load(); err != nil {\n\t\tpanic(err)\n\t}\n\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\treturn d.ratchetFormatMajorVersionLocked(fmv)\n}\n\nfunc (d *DB) ratchetFormatMajorVersionLocked(formatVers FormatMajorVersion) error {\n\tif d.opts.ReadOnly {\n\t\treturn ErrReadOnly\n\t}\n\tif formatVers > internalFormatNewest {\n\t\t// Guard against accidentally forgetting to update internalFormatNewest.\n\t\treturn errors.Errorf(\"pebble: unknown format version %d\", formatVers)\n\t}\n\tif currentVers := d.FormatMajorVersion(); currentVers > formatVers {\n\t\treturn errors.Newf(\"pebble: database already at format major version %d; cannot reduce to %d\",\n\t\t\tcurrentVers, formatVers)\n\t}\n\tif d.mu.formatVers.ratcheting {\n\t\treturn errors.Newf(\"pebble: database format major version upgrade is in-progress\")\n\t}\n\td.mu.formatVers.ratcheting = true\n\tdefer func() { d.mu.formatVers.ratcheting = false }()\n\n\tfor nextVers := d.FormatMajorVersion() + 1; nextVers <= formatVers; nextVers++ {\n\t\tif err := formatMajorVersionMigrations[nextVers](d); err != nil {\n\t\t\treturn errors.Wrapf(err, \"migrating to version %d\", nextVers)\n\t\t}\n\n\t\t// NB: The migration is responsible for calling\n\t\t// finalizeFormatVersUpgrade to finalize the upgrade. This\n\t\t// structure is necessary because some migrations may need to\n\t\t// update in-memory state (without ever dropping locks) after\n\t\t// the upgrade is finalized. Here we assert that the upgrade\n\t\t// did occur.\n\t\tif d.FormatMajorVersion() != nextVers {\n\t\t\td.opts.Logger.Fatalf(\"pebble: successful migration to format version %d never finalized the upgrade\", nextVers)\n\t\t}\n\t}\n\treturn nil\n}\n\n// finalizeFormatVersUpgrade is typically only be called from within a\n// format major version migration.\n//\n// See formatMajorVersionMigrations.\nfunc (d *DB) finalizeFormatVersUpgrade(formatVers FormatMajorVersion) error {\n\tif err := d.writeFormatVersionMarker(formatVers); err != nil {\n\t\treturn err\n\t}\n\td.mu.formatVers.vers.Store(uint64(formatVers))\n\td.opts.EventListener.FormatUpgrade(formatVers)\n\treturn nil\n}\n\nfunc (d *DB) writeFormatVersionMarker(formatVers FormatMajorVersion) error {\n\t// We use the marker to encode the active format version in the\n\t// marker filename. Unlike other uses of the atomic marker, there is\n\t// no file with the filename `formatVers.String()` on the\n\t// filesystem.\n\treturn d.mu.formatVers.marker.Move(formatVers.String())\n}\n\n// compactMarkedFilesLocked performs a migration that schedules rewrite\n// compactions to compact away any sstables marked for compaction.\n// compactMarkedFilesLocked is run while ratcheting the database's format major\n// version to FormatSplitUserKeysMarkedCompacted.\n//\n// Note that while this method is called with the DB.mu held, and will not\n// return until all marked files have been compacted, the mutex is dropped while\n// waiting for compactions to complete (or for slots to free up).\nfunc (d *DB) compactMarkedFilesLocked() error {\n\tcurr := d.mu.versions.currentVersion()\n\tfor curr.Stats.MarkedForCompaction > 0 {\n\t\t// Attempt to schedule a compaction to rewrite a file marked for\n\t\t// compaction.\n\t\td.maybeScheduleCompactionPicker(func(picker compactionPicker, env compactionEnv) *pickedCompaction {\n\t\t\treturn picker.pickRewriteCompaction(env)\n\t\t})\n\n\t\t// The above attempt might succeed and schedule a rewrite compaction. Or\n\t\t// there might not be available compaction concurrency to schedule the\n\t\t// compaction.  Or compaction of the file might have already been in\n\t\t// progress. In any scenario, wait until there's some change in the\n\t\t// state of active compactions.\n\n\t\t// Before waiting, check that the database hasn't been closed. Trying to\n\t\t// schedule the compaction may have dropped d.mu while waiting for a\n\t\t// manifest write to complete. In that dropped interim, the database may\n\t\t// have been closed.\n\t\tif err := d.closed.Load(); err != nil {\n\t\t\treturn err.(error)\n\t\t}\n\n\t\t// Some flush or compaction may have scheduled or completed while we waited\n\t\t// for the manifest lock in maybeScheduleCompactionPicker. Get the latest\n\t\t// Version before waiting on a compaction.\n\t\tcurr = d.mu.versions.currentVersion()\n\n\t\t// Only wait on compactions if there are files still marked for compaction.\n\t\t// NB: Waiting on this condition variable drops d.mu while blocked.\n\t\tif curr.Stats.MarkedForCompaction > 0 {\n\t\t\tif d.mu.compact.compactingCount == 0 {\n\t\t\t\tpanic(\"expected a compaction of marked files in progress\")\n\t\t\t}\n\t\t\td.mu.compact.cond.Wait()\n\t\t\t// Refresh the current version again.\n\t\t\tcurr = d.mu.versions.currentVersion()\n\t\t}\n\t}\n\treturn nil\n}\n\n// findFilesFunc scans the LSM for files, returning true if at least one\n// file was found. The returned array contains the matched files, if any, per\n// level.\ntype findFilesFunc func(v *version) (found bool, files [numLevels][]*fileMetadata, _ error)\n\n// This method is not used currently, but it will be useful the next time we need\n// to mark files for compaction.\nvar _ = (*DB)(nil).markFilesLocked\n\n// markFilesLocked durably marks the files that match the given findFilesFunc for\n// compaction.\nfunc (d *DB) markFilesLocked(findFn findFilesFunc) error {\n\tjobID := d.newJobIDLocked()\n\n\t// Acquire a read state to have a view of the LSM and a guarantee that none\n\t// of the referenced files will be deleted until we've unreferenced the read\n\t// state. Some findFilesFuncs may read the files, requiring they not be\n\t// deleted.\n\trs := d.loadReadState()\n\tvar (\n\t\tfound bool\n\t\tfiles [numLevels][]*fileMetadata\n\t\terr   error\n\t)\n\tfunc() {\n\t\tdefer rs.unrefLocked()\n\t\t// Note the unusual locking: unlock, defer Lock(). The scan of the files in\n\t\t// the version does not need to block other operations that require the\n\t\t// DB.mu. Drop it for the scan, before re-acquiring it.\n\t\td.mu.Unlock()\n\t\tdefer d.mu.Lock()\n\t\tfound, files, err = findFn(rs.current)\n\t}()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// The database lock has been acquired again by the defer within the above\n\t// anonymous function.\n\tif !found {\n\t\t// Nothing to do.\n\t\treturn nil\n\t}\n\n\t// After scanning, if we found files to mark, we fetch the current state of\n\t// the LSM (which may have changed) and set MarkedForCompaction on the files,\n\t// and update the version's Stats.MarkedForCompaction count, which are both\n\t// protected by d.mu.\n\n\t// Lock the manifest for a coherent view of the LSM. The database lock has\n\t// been re-acquired by the defer within the above anonymous function.\n\td.mu.versions.logLock()\n\tvers := d.mu.versions.currentVersion()\n\tfor l, filesToMark := range files {\n\t\tif len(filesToMark) == 0 {\n\t\t\tcontinue\n\t\t}\n\t\tfor _, f := range filesToMark {\n\t\t\t// Ignore files to be marked that have already been compacted or marked.\n\t\t\tif f.CompactionState == manifest.CompactionStateCompacted ||\n\t\t\t\tf.MarkedForCompaction {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// Else, mark the file for compaction in this version.\n\t\t\tvers.Stats.MarkedForCompaction++\n\t\t\tf.MarkedForCompaction = true\n\t\t}\n\t\t// The compaction picker uses the markedForCompactionAnnotator to\n\t\t// quickly find files marked for compaction, or to quickly determine\n\t\t// that there are no such files marked for compaction within a level.\n\t\t// A b-tree node may be annotated with an annotation recording that\n\t\t// there are no files marked for compaction within the node's subtree,\n\t\t// based on the assumption that it's static.\n\t\t//\n\t\t// Since we're marking files for compaction, these b-tree nodes'\n\t\t// annotations will be out of date. Clear the compaction-picking\n\t\t// annotation, so that it's recomputed the next time the compaction\n\t\t// picker looks for a file marked for compaction.\n\t\tmarkedForCompactionAnnotator.InvalidateLevelAnnotation(vers.Levels[l])\n\t}\n\n\t// The 'marked-for-compaction' bit is persisted in the MANIFEST file\n\t// metadata. We've already modified the in-memory file metadata, but the\n\t// manifest hasn't been updated. Force rotation to a new MANIFEST file,\n\t// which will write every file metadata to the new manifest file and ensure\n\t// that the now marked-for-compaction file metadata are persisted as marked.\n\t// NB: This call to logAndApply will unlockthe MANIFEST, which we locked up\n\t// above before obtaining `vers`.\n\treturn d.mu.versions.logAndApply(\n\t\tjobID,\n\t\t&manifest.VersionEdit{},\n\t\tmap[int]*LevelMetrics{},\n\t\ttrue, /* forceRotation */\n\t\tfunc() []compactionInfo { return d.getInProgressCompactionInfoLocked(nil) })\n}\n"
        },
        {
          "name": "format_major_version_test.go",
          "type": "blob",
          "size": 8.630859375,
          "content": "// Copyright 2021 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/cockroachdb/pebble/sstable\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/cockroachdb/pebble/vfs/atomicfs\"\n\t\"github.com/stretchr/testify/require\"\n)\n\n// TestFormatMajorVersionValues checks that we don't accidentally change the\n// numbers of format versions.\nfunc TestFormatMajorVersionStableValues(t *testing.T) {\n\trequire.Equal(t, FormatDefault, FormatMajorVersion(0))\n\n\trequire.Equal(t, FormatFlushableIngest, FormatMajorVersion(13))\n\trequire.Equal(t, FormatPrePebblev1MarkedCompacted, FormatMajorVersion(14))\n\trequire.Equal(t, FormatDeleteSizedAndObsolete, FormatMajorVersion(15))\n\trequire.Equal(t, FormatVirtualSSTables, FormatMajorVersion(16))\n\trequire.Equal(t, FormatSyntheticPrefixSuffix, FormatMajorVersion(17))\n\trequire.Equal(t, FormatFlushableIngestExcises, FormatMajorVersion(18))\n\n\t// When we add a new version, we should add a check for the new version in\n\t// addition to updating these expected values.\n\trequire.Equal(t, FormatNewest, FormatMajorVersion(19))\n\trequire.Equal(t, internalFormatNewest, FormatMajorVersion(19))\n}\n\nfunc TestFormatMajorVersion_MigrationDefined(t *testing.T) {\n\tfor v := FormatMinSupported; v <= FormatNewest; v++ {\n\t\tif _, ok := formatMajorVersionMigrations[v]; !ok {\n\t\t\tt.Errorf(\"format major version %d has no migration defined\", v)\n\t\t}\n\t}\n}\n\nfunc TestRatchetFormat(t *testing.T) {\n\tfs := vfs.NewMem()\n\td, err := Open(\"\", (&Options{FS: fs}).WithFSDefaults())\n\trequire.NoError(t, err)\n\trequire.NoError(t, d.Set([]byte(\"foo\"), []byte(\"bar\"), Sync))\n\trequire.Equal(t, FormatFlushableIngest, d.FormatMajorVersion())\n\trequire.NoError(t, d.RatchetFormatMajorVersion(FormatPrePebblev1MarkedCompacted))\n\trequire.Equal(t, FormatPrePebblev1MarkedCompacted, d.FormatMajorVersion())\n\trequire.NoError(t, d.RatchetFormatMajorVersion(FormatDeleteSizedAndObsolete))\n\trequire.Equal(t, FormatDeleteSizedAndObsolete, d.FormatMajorVersion())\n\trequire.NoError(t, d.RatchetFormatMajorVersion(FormatVirtualSSTables))\n\trequire.Equal(t, FormatVirtualSSTables, d.FormatMajorVersion())\n\trequire.NoError(t, d.RatchetFormatMajorVersion(FormatSyntheticPrefixSuffix))\n\trequire.Equal(t, FormatSyntheticPrefixSuffix, d.FormatMajorVersion())\n\trequire.NoError(t, d.RatchetFormatMajorVersion(FormatFlushableIngestExcises))\n\trequire.Equal(t, FormatFlushableIngestExcises, d.FormatMajorVersion())\n\trequire.NoError(t, d.RatchetFormatMajorVersion(FormatColumnarBlocks))\n\trequire.Equal(t, FormatColumnarBlocks, d.FormatMajorVersion())\n\n\trequire.NoError(t, d.Close())\n\n\t// If we Open the database again, leaving the default format, the\n\t// database should Open using the persisted FormatNewest.\n\td, err = Open(\"\", (&Options{FS: fs, Logger: testLogger{t}}).WithFSDefaults())\n\trequire.NoError(t, err)\n\trequire.Equal(t, internalFormatNewest, d.FormatMajorVersion())\n\trequire.NoError(t, d.Close())\n\n\t// Move the marker to a version that does not exist.\n\tm, _, err := atomicfs.LocateMarker(fs, \"\", formatVersionMarkerName)\n\trequire.NoError(t, err)\n\trequire.NoError(t, m.Move(\"999999\"))\n\trequire.NoError(t, m.Close())\n\n\t_, err = Open(\"\", (&Options{\n\t\tFS:                 fs,\n\t\tFormatMajorVersion: FormatMinSupported,\n\t}).WithFSDefaults())\n\trequire.Error(t, err)\n\trequire.EqualError(t, err, `pebble: database \"\" written in unknown format major version 999999`)\n}\n\nfunc testBasicDB(d *DB) error {\n\tkey := []byte(\"a\")\n\tvalue := []byte(\"b\")\n\tif err := d.Set(key, value, nil); err != nil {\n\t\treturn err\n\t}\n\tif err := d.Flush(); err != nil {\n\t\treturn err\n\t}\n\tif err := d.Compact(nil, []byte(\"\\xff\"), false); err != nil {\n\t\treturn err\n\t}\n\n\titer, _ := d.NewIter(nil)\n\tfor valid := iter.First(); valid; valid = iter.Next() {\n\t}\n\tif err := iter.Close(); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc TestFormatMajorVersions(t *testing.T) {\n\tfor vers := FormatMinSupported; vers <= FormatNewest; vers++ {\n\t\tt.Run(fmt.Sprintf(\"vers=%03d\", vers), func(t *testing.T) {\n\t\t\tfs := vfs.NewCrashableMem()\n\t\t\topts := (&Options{\n\t\t\t\tFS:                 fs,\n\t\t\t\tFormatMajorVersion: vers,\n\t\t\t\tLogger:             testLogger{t},\n\t\t\t}).WithFSDefaults()\n\n\t\t\t// Create a database at this format major version and perform\n\t\t\t// some very basic operations.\n\t\t\td, err := Open(\"\", opts)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.NoError(t, testBasicDB(d))\n\t\t\trequire.NoError(t, d.Close())\n\n\t\t\t// Re-open the database at this format major version, and again\n\t\t\t// perform some basic operations.\n\t\t\td, err = Open(\"\", opts)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.NoError(t, testBasicDB(d))\n\t\t\trequire.NoError(t, d.Close())\n\n\t\t\tt.Run(\"upgrade-at-open\", func(t *testing.T) {\n\t\t\t\tfor upgradeVers := vers + 1; upgradeVers <= FormatNewest; upgradeVers++ {\n\t\t\t\t\tt.Run(fmt.Sprintf(\"upgrade-vers=%03d\", upgradeVers), func(t *testing.T) {\n\t\t\t\t\t\t// We use vfs.MemFS's CrashClone to perform an upgrade without\n\t\t\t\t\t\t// affecting the original filesystem.\n\t\t\t\t\t\topts := opts.Clone()\n\t\t\t\t\t\topts.FS = fs.CrashClone(vfs.CrashCloneCfg{UnsyncedDataPercent: 0})\n\n\t\t\t\t\t\t// Re-open the database, passing a higher format\n\t\t\t\t\t\t// major version in the Options to automatically\n\t\t\t\t\t\t// ratchet the format major version. Ensure some\n\t\t\t\t\t\t// basic operations pass.\n\t\t\t\t\t\topts.FormatMajorVersion = upgradeVers\n\t\t\t\t\t\td, err = Open(\"\", opts)\n\t\t\t\t\t\trequire.NoError(t, err)\n\t\t\t\t\t\trequire.Equal(t, upgradeVers, d.FormatMajorVersion())\n\t\t\t\t\t\trequire.NoError(t, testBasicDB(d))\n\t\t\t\t\t\trequire.NoError(t, d.Close())\n\n\t\t\t\t\t\t// Re-open to ensure the upgrade persisted.\n\t\t\t\t\t\td, err = Open(\"\", opts)\n\t\t\t\t\t\trequire.NoError(t, err)\n\t\t\t\t\t\trequire.Equal(t, upgradeVers, d.FormatMajorVersion())\n\t\t\t\t\t\trequire.NoError(t, testBasicDB(d))\n\t\t\t\t\t\trequire.NoError(t, d.Close())\n\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t})\n\n\t\t\tt.Run(\"upgrade-while-open\", func(t *testing.T) {\n\t\t\t\tfor upgradeVers := vers + 1; upgradeVers <= FormatNewest; upgradeVers++ {\n\t\t\t\t\tt.Run(fmt.Sprintf(\"upgrade-vers=%03d\", upgradeVers), func(t *testing.T) {\n\t\t\t\t\t\t// Ensure the previous tests don't overwrite our\n\t\t\t\t\t\t// options.\n\t\t\t\t\t\trequire.Equal(t, vers, opts.FormatMajorVersion)\n\n\t\t\t\t\t\t// We use vfs.MemFS's CrashClone to perform an upgrade without\n\t\t\t\t\t\t// affecting the original filesystem.\n\t\t\t\t\t\topts := opts.Clone()\n\t\t\t\t\t\topts.FS = fs.CrashClone(vfs.CrashCloneCfg{UnsyncedDataPercent: 0})\n\n\t\t\t\t\t\t// Re-open the database, still at the current format\n\t\t\t\t\t\t// major version. Perform some basic operations,\n\t\t\t\t\t\t// ratchet the format version up, and perform\n\t\t\t\t\t\t// additional basic operations.\n\t\t\t\t\t\td, err = Open(\"\", opts)\n\t\t\t\t\t\trequire.NoError(t, err)\n\t\t\t\t\t\trequire.NoError(t, testBasicDB(d))\n\t\t\t\t\t\trequire.Equal(t, vers, d.FormatMajorVersion())\n\t\t\t\t\t\trequire.NoError(t, d.RatchetFormatMajorVersion(upgradeVers))\n\t\t\t\t\t\trequire.Equal(t, upgradeVers, d.FormatMajorVersion())\n\t\t\t\t\t\trequire.NoError(t, testBasicDB(d))\n\t\t\t\t\t\trequire.NoError(t, d.Close())\n\n\t\t\t\t\t\t// Re-open to ensure the upgrade persisted.\n\t\t\t\t\t\td, err = Open(\"\", opts)\n\t\t\t\t\t\trequire.NoError(t, err)\n\t\t\t\t\t\trequire.Equal(t, upgradeVers, d.FormatMajorVersion())\n\t\t\t\t\t\trequire.NoError(t, testBasicDB(d))\n\t\t\t\t\t\trequire.NoError(t, d.Close())\n\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t})\n\t\t})\n\t}\n}\n\nfunc TestFormatMajorVersions_TableFormat(t *testing.T) {\n\t// NB: This test is intended to validate the mapping between every\n\t// FormatMajorVersion and sstable.TableFormat exhaustively. This serves as a\n\t// sanity check that new versions have a corresponding mapping. The test\n\t// fixture is intentionally verbose.\n\n\tm := map[FormatMajorVersion][2]sstable.TableFormat{\n\t\tFormatDefault:                    {sstable.TableFormatPebblev1, sstable.TableFormatPebblev3},\n\t\tFormatFlushableIngest:            {sstable.TableFormatPebblev1, sstable.TableFormatPebblev3},\n\t\tFormatPrePebblev1MarkedCompacted: {sstable.TableFormatPebblev1, sstable.TableFormatPebblev3},\n\t\tFormatDeleteSizedAndObsolete:     {sstable.TableFormatPebblev1, sstable.TableFormatPebblev4},\n\t\tFormatVirtualSSTables:            {sstable.TableFormatPebblev1, sstable.TableFormatPebblev4},\n\t\tFormatSyntheticPrefixSuffix:      {sstable.TableFormatPebblev1, sstable.TableFormatPebblev4},\n\t\tFormatFlushableIngestExcises:     {sstable.TableFormatPebblev1, sstable.TableFormatPebblev4},\n\t\tFormatColumnarBlocks:             {sstable.TableFormatPebblev1, sstable.TableFormatPebblev5},\n\t}\n\n\t// Valid versions.\n\tfor fmv := FormatMinSupported; fmv <= internalFormatNewest; fmv++ {\n\t\tgot := [2]sstable.TableFormat{fmv.MinTableFormat(), fmv.MaxTableFormat()}\n\t\trequire.Equalf(t, m[fmv], got, \"got %s; want %s\", got, m[fmv])\n\t\trequire.True(t, got[0] <= got[1] /* min <= max */)\n\t}\n\n\t// Invalid versions.\n\tfmv := internalFormatNewest + 1\n\trequire.Panics(t, func() { _ = fmv.MaxTableFormat() })\n\trequire.Panics(t, func() { _ = fmv.MinTableFormat() })\n}\n"
        },
        {
          "name": "get_iter.go",
          "type": "blob",
          "size": 8.9853515625,
          "content": "// Copyright 2018 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n\t\"github.com/cockroachdb/pebble/internal/treeprinter\"\n)\n\n// getIter is an internal iterator used to perform gets. It iterates through\n// the values for a particular key, level by level. It is not a general purpose\n// internalIterator, but specialized for Get operations so that it loads data\n// lazily.\ntype getIter struct {\n\tcomparer *Comparer\n\tnewIters tableNewIters\n\tsnapshot base.SeqNum\n\titerOpts IterOptions\n\tkey      []byte\n\tprefix   []byte\n\titer     internalIterator\n\tlevel    int\n\tbatch    *Batch\n\tmem      flushableList\n\tl0       []manifest.LevelSlice\n\tversion  *version\n\titerKV   *base.InternalKV\n\t// tombstoned and tombstonedSeqNum track whether the key has been deleted by\n\t// a range delete tombstone. The first visible (at getIter.snapshot) range\n\t// deletion encounterd transitions tombstoned to true. The tombstonedSeqNum\n\t// field is updated to hold the sequence number of the tombstone.\n\ttombstoned       bool\n\ttombstonedSeqNum base.SeqNum\n\terr              error\n}\n\n// TODO(sumeer): CockroachDB code doesn't use getIter, but, for completeness,\n// make this implement InternalIteratorWithStats.\n\n// getIter implements the base.InternalIterator interface.\nvar _ base.InternalIterator = (*getIter)(nil)\n\nfunc (g *getIter) String() string {\n\treturn fmt.Sprintf(\"len(l0)=%d, len(mem)=%d, level=%d\", len(g.l0), len(g.mem), g.level)\n}\n\nfunc (g *getIter) SeekGE(key []byte, flags base.SeekGEFlags) *base.InternalKV {\n\tpanic(\"pebble: SeekGE unimplemented\")\n}\n\nfunc (g *getIter) SeekPrefixGE(prefix, key []byte, flags base.SeekGEFlags) *base.InternalKV {\n\treturn g.SeekPrefixGEStrict(prefix, key, flags)\n}\n\nfunc (g *getIter) SeekPrefixGEStrict(prefix, key []byte, flags base.SeekGEFlags) *base.InternalKV {\n\tpanic(\"pebble: SeekPrefixGE unimplemented\")\n}\n\nfunc (g *getIter) SeekLT(key []byte, flags base.SeekLTFlags) *base.InternalKV {\n\tpanic(\"pebble: SeekLT unimplemented\")\n}\n\nfunc (g *getIter) First() *base.InternalKV {\n\treturn g.Next()\n}\n\nfunc (g *getIter) Last() *base.InternalKV {\n\tpanic(\"pebble: Last unimplemented\")\n}\n\nfunc (g *getIter) Next() *base.InternalKV {\n\t// If g.iter != nil, we're already iterating through a level. Next. Note\n\t// that it's possible the next key within the level is still relevant (eg,\n\t// MERGE keys written in the presence of an LSM snapshot).\n\t//\n\t// NB: We can't perform this Next below, in the for loop, because when we\n\t// open an iterator into the next level, we need to seek to the key.\n\tif g.iter != nil {\n\t\tg.iterKV = g.iter.Next()\n\t\tif err := g.iter.Error(); err != nil {\n\t\t\tg.err = err\n\t\t\treturn nil\n\t\t}\n\t}\n\n\t// This for loop finds the next internal key in the LSM that is equal to\n\t// g.key, visible at g.snapshot and not shadowed by a range deletion. If it\n\t// exhausts a level, it initializes iterators for the next level.\n\tfor {\n\t\tif g.iter != nil {\n\t\t\tif g.iterKV != nil {\n\t\t\t\t// Check if the current KV pair is deleted by a range deletion.\n\t\t\t\tif g.tombstoned && g.tombstonedSeqNum > g.iterKV.SeqNum() {\n\t\t\t\t\t// We have a range tombstone covering this key. Rather than\n\t\t\t\t\t// return a point or range deletion here, we return nil and\n\t\t\t\t\t// close our internal iterator stopping iteration.\n\t\t\t\t\tg.err = g.iter.Close()\n\t\t\t\t\tg.iter = nil\n\t\t\t\t\treturn nil\n\t\t\t\t}\n\n\t\t\t\t// Is this the correct user key?\n\t\t\t\tif g.comparer.Equal(g.key, g.iterKV.K.UserKey) {\n\t\t\t\t\t// If the KV pair is not visible at the get's snapshot,\n\t\t\t\t\t// Next. The level may still contain older keys with the\n\t\t\t\t\t// same user key that are visible.\n\t\t\t\t\tif !g.iterKV.Visible(g.snapshot, base.SeqNumMax) {\n\t\t\t\t\t\tg.iterKV = g.iter.Next()\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\treturn g.iterKV\n\t\t\t\t}\n\t\t\t}\n\t\t\t// We've advanced the iterator passed the desired key. Move on to the\n\t\t\t// next memtable / level.\n\t\t\tg.err = g.iter.Close()\n\t\t\tg.iter = nil\n\t\t\tif g.err != nil {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\t\t// g.iter == nil; we need to initialize the next iterator.\n\t\tif !g.initializeNextIterator() {\n\t\t\treturn nil\n\t\t}\n\t\tg.iterKV = g.iter.SeekPrefixGE(g.prefix, g.key, base.SeekGEFlagsNone)\n\t}\n}\n\nfunc (g *getIter) Prev() *base.InternalKV {\n\tpanic(\"pebble: Prev unimplemented\")\n}\n\nfunc (g *getIter) NextPrefix([]byte) *base.InternalKV {\n\tpanic(\"pebble: NextPrefix unimplemented\")\n}\n\nfunc (g *getIter) Error() error {\n\treturn g.err\n}\n\nfunc (g *getIter) Close() error {\n\tif g.iter != nil {\n\t\tif err := g.iter.Close(); err != nil && g.err == nil {\n\t\t\tg.err = err\n\t\t}\n\t\tg.iter = nil\n\t}\n\treturn g.err\n}\n\nfunc (g *getIter) SetBounds(lower, upper []byte) {\n\tpanic(\"pebble: SetBounds unimplemented\")\n}\n\nfunc (g *getIter) SetContext(_ context.Context) {}\n\n// DebugTree is part of the InternalIterator interface.\nfunc (g *getIter) DebugTree(tp treeprinter.Node) {\n\tn := tp.Childf(\"%T(%p)\", g, g)\n\tif g.iter != nil {\n\t\tg.iter.DebugTree(n)\n\t}\n}\n\nfunc (g *getIter) initializeNextIterator() (ok bool) {\n\t// A batch's keys shadow all other keys, so we visit the batch first.\n\tif g.batch != nil {\n\t\tif g.batch.index == nil {\n\t\t\tg.err = ErrNotIndexed\n\t\t\tg.iterKV = nil\n\t\t\treturn false\n\t\t}\n\t\tg.iter = g.batch.newInternalIter(nil)\n\t\tif !g.maybeSetTombstone(g.batch.newRangeDelIter(nil,\n\t\t\t// Get always reads the entirety of the batch's history, so no\n\t\t\t// batch keys should be filtered.\n\t\t\tbase.SeqNumMax,\n\t\t)) {\n\t\t\treturn false\n\t\t}\n\t\tg.batch = nil\n\t\treturn true\n\t}\n\n\t// If we're trying to initialize the next level of the iterator stack but\n\t// have a tombstone from a previous level, it is guaranteed to delete keys\n\t// in lower levels. This key is deleted.\n\tif g.tombstoned {\n\t\treturn false\n\t}\n\n\t// Create iterators from memtables from newest to oldest.\n\tif n := len(g.mem); n > 0 {\n\t\tm := g.mem[n-1]\n\t\tg.iter = m.newIter(nil)\n\t\tif !g.maybeSetTombstone(m.newRangeDelIter(nil)) {\n\t\t\treturn false\n\t\t}\n\t\tg.mem = g.mem[:n-1]\n\t\treturn true\n\t}\n\n\t// Visit each sublevel of L0 individually, so that we only need to read\n\t// at most one file per sublevel.\n\tif g.level == 0 {\n\t\t// Create iterators from L0 from newest to oldest.\n\t\tif n := len(g.l0); n > 0 {\n\t\t\tfiles := g.l0[n-1].Iter()\n\t\t\tg.l0 = g.l0[:n-1]\n\n\t\t\titer, rangeDelIter, err := g.getSSTableIterators(files, manifest.L0Sublevel(n))\n\t\t\tif err != nil {\n\t\t\t\tg.err = firstError(g.err, err)\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tif !g.maybeSetTombstone(rangeDelIter) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tg.iter = iter\n\t\t\treturn true\n\t\t}\n\t\t// We've exhausted all the sublevels of L0. Progress to L1.\n\t\tg.level++\n\t}\n\tfor g.level < numLevels {\n\t\tif g.version.Levels[g.level].Empty() {\n\t\t\tg.level++\n\t\t\tcontinue\n\t\t}\n\t\t// Open the next level of the LSM.\n\t\titer, rangeDelIter, err := g.getSSTableIterators(g.version.Levels[g.level].Iter(), manifest.Level(g.level))\n\t\tif err != nil {\n\t\t\tg.err = firstError(g.err, err)\n\t\t\treturn false\n\t\t}\n\t\tif !g.maybeSetTombstone(rangeDelIter) {\n\t\t\treturn false\n\t\t}\n\t\tg.level++\n\t\tg.iter = iter\n\t\treturn true\n\t}\n\t// We've exhausted all levels of the LSM.\n\treturn false\n}\n\n// getSSTableIterators returns a point iterator and a range deletion iterator\n// for the sstable in files that overlaps with the key g.key. Pebble does not\n// split user keys across adjacent sstables within a level, ensuring that at\n// most one sstable overlaps g.key.\nfunc (g *getIter) getSSTableIterators(\n\tfiles manifest.LevelIterator, level manifest.Layer,\n) (internalIterator, keyspan.FragmentIterator, error) {\n\tfiles = files.Filter(manifest.KeyTypePoint)\n\tm := files.SeekGE(g.comparer.Compare, g.key)\n\tif m == nil {\n\t\treturn emptyIter, nil, nil\n\t}\n\t// m is now positioned at the file containing the first point key ≥ `g.key`.\n\t// Does it exist and possibly contain point keys with the user key 'g.key'?\n\tif m == nil || !m.HasPointKeys || g.comparer.Compare(m.SmallestPointKey.UserKey, g.key) > 0 {\n\t\treturn emptyIter, nil, nil\n\t}\n\t// m may possibly contain point (or range deletion) keys relevant to g.key.\n\tg.iterOpts.layer = level\n\titers, err := g.newIters(context.Background(), m, &g.iterOpts, internalIterOpts{}, iterPointKeys|iterRangeDeletions)\n\tif err != nil {\n\t\treturn emptyIter, nil, err\n\t}\n\treturn iters.Point(), iters.RangeDeletion(), nil\n}\n\n// maybeSetTombstone updates g.tombstoned[SeqNum] to reflect the presence of a\n// range deletion covering g.key, if there are any. It returns true if\n// successful, or false if an error occurred and the caller should abort\n// iteration.\nfunc (g *getIter) maybeSetTombstone(rangeDelIter keyspan.FragmentIterator) (ok bool) {\n\tif rangeDelIter == nil {\n\t\t// Nothing to do.\n\t\treturn true\n\t}\n\t// Find the range deletion that covers the sought key, if any.\n\tt, err := keyspan.Get(g.comparer.Compare, rangeDelIter, g.key)\n\tif err != nil {\n\t\tg.err = firstError(g.err, err)\n\t\treturn false\n\t}\n\t// Find the most recent visible range deletion's sequence number. We only\n\t// care about the most recent range deletion that's visible because it's the\n\t// \"most powerful.\"\n\tg.tombstonedSeqNum, g.tombstoned = t.LargestVisibleSeqNum(g.snapshot)\n\trangeDelIter.Close()\n\treturn true\n}\n"
        },
        {
          "name": "get_iter_test.go",
          "type": "blob",
          "size": 11.759765625,
          "content": "// Copyright 2018 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"context\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n\t\"github.com/cockroachdb/pebble/internal/testkeys\"\n)\n\nfunc TestGetIter(t *testing.T) {\n\t// testTable is a table to insert into a version.\n\t// Each element of data is a string of the form \"internalKey value\".\n\ttype testTable struct {\n\t\tlevel   int\n\t\tfileNum FileNum\n\t\tdata    []string\n\t}\n\n\ttestCases := []struct {\n\t\tdescription string\n\t\t// badOrdering is whether this test case has a table ordering violation.\n\t\tbadOrdering bool\n\t\t// tables are the tables to populate the version with.\n\t\ttables []testTable\n\t\t// queries are the queries to run against the version. Each element has\n\t\t// the form \"internalKey wantedValue\". The internalKey is passed to the\n\t\t// version.get method, wantedValue may be \"ErrNotFound\" if the query\n\t\t// should return that error.\n\t\tqueries []string\n\t}{\n\t\t{\n\t\t\tdescription: \"empty: an empty version\",\n\t\t\tqueries: []string{\n\t\t\t\t\"abc.SEPARATOR.101 ErrNotFound\",\n\t\t\t},\n\t\t},\n\n\t\t{\n\t\t\tdescription: \"single-0: one level-0 table\",\n\t\t\ttables: []testTable{\n\t\t\t\t{\n\t\t\t\t\tlevel:   0,\n\t\t\t\t\tfileNum: 10,\n\t\t\t\t\tdata: []string{\n\t\t\t\t\t\t\"the.SET.101 a\",\n\t\t\t\t\t\t\"cat.SET.102 b\",\n\t\t\t\t\t\t\"on_.SET.103 c\",\n\t\t\t\t\t\t\"the.SET.104 d\",\n\t\t\t\t\t\t\"mat.SET.105 e\",\n\t\t\t\t\t\t\"the.DEL.106 \",\n\t\t\t\t\t\t\"the.MERGE.107 g\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\tqueries: []string{\n\t\t\t\t\"aaa.SEPARATOR.105 ErrNotFound\",\n\t\t\t\t\"cat.SEPARATOR.105 b\",\n\t\t\t\t\"hat.SEPARATOR.105 ErrNotFound\",\n\t\t\t\t\"mat.SEPARATOR.105 e\",\n\t\t\t\t\"the.SEPARATOR.108 g\",\n\t\t\t\t\"the.SEPARATOR.107 g\",\n\t\t\t\t\"the.SEPARATOR.106 ErrNotFound\",\n\t\t\t\t\"the.SEPARATOR.105 d\",\n\t\t\t\t\"the.SEPARATOR.104 d\",\n\t\t\t\t\"the.SEPARATOR.104 d\",\n\t\t\t\t\"the.SEPARATOR.103 a\",\n\t\t\t\t\"the.SEPARATOR.102 a\",\n\t\t\t\t\"the.SEPARATOR.101 a\",\n\t\t\t\t\"the.SEPARATOR.100 ErrNotFound\",\n\t\t\t\t\"zzz.SEPARATOR.105 ErrNotFound\",\n\t\t\t},\n\t\t},\n\n\t\t{\n\t\t\tdescription: \"triple-0: three level-0 tables\",\n\t\t\ttables: []testTable{\n\t\t\t\t{\n\t\t\t\t\tlevel:   0,\n\t\t\t\t\tfileNum: 10,\n\t\t\t\t\tdata: []string{\n\t\t\t\t\t\t\"the.SET.101 a\",\n\t\t\t\t\t\t\"cat.SET.102 b\",\n\t\t\t\t\t\t\"on_.SET.103 c\",\n\t\t\t\t\t\t\"the.SET.104 d\",\n\t\t\t\t\t\t\"mat.SET.105 e\",\n\t\t\t\t\t\t\"the.DEL.106 \",\n\t\t\t\t\t\t\"the.SET.107 g\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tlevel:   0,\n\t\t\t\t\tfileNum: 11,\n\t\t\t\t\tdata: []string{\n\t\t\t\t\t\t\"awk.SET.111 w\",\n\t\t\t\t\t\t\"cat.SET.112 x\",\n\t\t\t\t\t\t\"man.SET.113 y\",\n\t\t\t\t\t\t\"sed.SET.114 z\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tlevel:   0,\n\t\t\t\t\tfileNum: 12,\n\t\t\t\t\tdata: []string{\n\t\t\t\t\t\t\"the.DEL.121 \",\n\t\t\t\t\t\t\"cat.DEL.122 \",\n\t\t\t\t\t\t\"man.DEL.123 \",\n\t\t\t\t\t\t\"was.SET.124 D\",\n\t\t\t\t\t\t\"not.SET.125 E\",\n\t\t\t\t\t\t\"the.SET.126 F\",\n\t\t\t\t\t\t\"man.SET.127 G\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\tqueries: []string{\n\t\t\t\t\"aaa.SEPARATOR.105 ErrNotFound\",\n\t\t\t\t\"awk.SEPARATOR.135 w\",\n\t\t\t\t\"awk.SEPARATOR.125 w\",\n\t\t\t\t\"awk.SEPARATOR.115 w\",\n\t\t\t\t\"awk.SEPARATOR.105 ErrNotFound\",\n\t\t\t\t\"cat.SEPARATOR.135 ErrNotFound\",\n\t\t\t\t\"cat.SEPARATOR.125 ErrNotFound\",\n\t\t\t\t\"cat.SEPARATOR.115 x\",\n\t\t\t\t\"cat.SEPARATOR.105 b\",\n\t\t\t\t\"man.SEPARATOR.135 G\",\n\t\t\t\t\"man.SEPARATOR.125 ErrNotFound\",\n\t\t\t\t\"man.SEPARATOR.115 y\",\n\t\t\t\t\"man.SEPARATOR.105 ErrNotFound\",\n\t\t\t\t\"on_.SEPARATOR.135 c\",\n\t\t\t\t\"on_.SEPARATOR.125 c\",\n\t\t\t\t\"on_.SEPARATOR.115 c\",\n\t\t\t\t\"on_.SEPARATOR.105 c\",\n\t\t\t\t\"the.SEPARATOR.135 F\",\n\t\t\t\t\"the.SEPARATOR.127 F\",\n\t\t\t\t\"the.SEPARATOR.126 F\",\n\t\t\t\t\"the.SEPARATOR.125 ErrNotFound\",\n\t\t\t\t\"the.SEPARATOR.122 ErrNotFound\",\n\t\t\t\t\"the.SEPARATOR.121 ErrNotFound\",\n\t\t\t\t\"the.SEPARATOR.120 g\",\n\t\t\t\t\"the.SEPARATOR.115 g\",\n\t\t\t\t\"the.SEPARATOR.114 g\",\n\t\t\t\t\"the.SEPARATOR.111 g\",\n\t\t\t\t\"the.SEPARATOR.110 g\",\n\t\t\t\t\"the.SEPARATOR.108 g\",\n\t\t\t\t\"the.SEPARATOR.107 g\",\n\t\t\t\t\"the.SEPARATOR.106 ErrNotFound\",\n\t\t\t\t\"the.SEPARATOR.105 d\",\n\t\t\t\t\"the.SEPARATOR.104 d\",\n\t\t\t\t\"the.SEPARATOR.104 d\",\n\t\t\t\t\"the.SEPARATOR.103 a\",\n\t\t\t\t\"the.SEPARATOR.102 a\",\n\t\t\t\t\"the.SEPARATOR.101 a\",\n\t\t\t\t\"the.SEPARATOR.100 ErrNotFound\",\n\t\t\t\t\"zzz.SEPARATOR.105 ErrNotFound\",\n\t\t\t},\n\t\t},\n\n\t\t{\n\t\t\tdescription: \"complex: many tables at many levels\",\n\t\t\ttables: []testTable{\n\t\t\t\t{\n\t\t\t\t\tlevel:   0,\n\t\t\t\t\tfileNum: 50,\n\t\t\t\t\tdata: []string{\n\t\t\t\t\t\t\"alfalfa__.SET.501 p1\",\n\t\t\t\t\t\t\"asparagus.SET.502 p2\",\n\t\t\t\t\t\t\"cabbage__.DEL.503 \",\n\t\t\t\t\t\t\"spinach__.MERGE.504 p3\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tlevel:   0,\n\t\t\t\t\tfileNum: 51,\n\t\t\t\t\tdata: []string{\n\t\t\t\t\t\t\"asparagus.SET.511 q1\",\n\t\t\t\t\t\t\"asparagus.SET.512 q2\",\n\t\t\t\t\t\t\"asparagus.SET.513 q3\",\n\t\t\t\t\t\t\"beans____.SET.514 q4\",\n\t\t\t\t\t\t\"broccoli_.SET.515 q5\",\n\t\t\t\t\t\t\"cabbage__.SET.516 q6\",\n\t\t\t\t\t\t\"celery___.SET.517 q7\",\n\t\t\t\t\t\t\"spinach__.MERGE.518 q8\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tlevel:   1,\n\t\t\t\t\tfileNum: 40,\n\t\t\t\t\tdata: []string{\n\t\t\t\t\t\t\"alfalfa__.SET.410 r1\",\n\t\t\t\t\t\t\"asparagus.SET.420 r2\",\n\t\t\t\t\t\t\"arugula__.SET.430 r3\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tlevel:   1,\n\t\t\t\t\tfileNum: 41,\n\t\t\t\t\tdata: []string{\n\t\t\t\t\t\t\"beans____.SET.411 s1\",\n\t\t\t\t\t\t\"beans____.SET.421 s2\",\n\t\t\t\t\t\t\"bokchoy__.DEL.431 \",\n\t\t\t\t\t\t\"broccoli_.SET.441 s4\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tlevel:   1,\n\t\t\t\t\tfileNum: 42,\n\t\t\t\t\tdata: []string{\n\t\t\t\t\t\t\"cabbage__.SET.412 t1\",\n\t\t\t\t\t\t\"corn_____.DEL.422 \",\n\t\t\t\t\t\t\"spinach__.MERGE.432 t2\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tlevel:   2,\n\t\t\t\t\tfileNum: 30,\n\t\t\t\t\tdata: []string{\n\t\t\t\t\t\t\"alfalfa__.SET.310 u1\",\n\t\t\t\t\t\t\"bokchoy__.SET.320 u2\",\n\t\t\t\t\t\t\"celery___.SET.330 u3\",\n\t\t\t\t\t\t\"corn_____.SET.340 u4\",\n\t\t\t\t\t\t\"spinach__.MERGE.350 u5\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\tqueries: []string{\n\t\t\t\t\"a________.SEPARATOR.999 ErrNotFound\",\n\t\t\t\t\"alfalfa__.SEPARATOR.520 p1\",\n\t\t\t\t\"alfalfa__.SEPARATOR.510 p1\",\n\t\t\t\t\"alfalfa__.SEPARATOR.500 r1\",\n\t\t\t\t\"alfalfa__.SEPARATOR.400 u1\",\n\t\t\t\t\"alfalfa__.SEPARATOR.300 ErrNotFound\",\n\t\t\t\t\"asparagus.SEPARATOR.520 q3\",\n\t\t\t\t\"asparagus.SEPARATOR.510 p2\",\n\t\t\t\t\"asparagus.SEPARATOR.500 r2\",\n\t\t\t\t\"asparagus.SEPARATOR.400 ErrNotFound\",\n\t\t\t\t\"asparagus.SEPARATOR.300 ErrNotFound\",\n\t\t\t\t\"arugula__.SEPARATOR.520 r3\",\n\t\t\t\t\"arugula__.SEPARATOR.510 r3\",\n\t\t\t\t\"arugula__.SEPARATOR.500 r3\",\n\t\t\t\t\"arugula__.SEPARATOR.400 ErrNotFound\",\n\t\t\t\t\"arugula__.SEPARATOR.300 ErrNotFound\",\n\t\t\t\t\"beans____.SEPARATOR.520 q4\",\n\t\t\t\t\"beans____.SEPARATOR.510 s2\",\n\t\t\t\t\"beans____.SEPARATOR.500 s2\",\n\t\t\t\t\"beans____.SEPARATOR.400 ErrNotFound\",\n\t\t\t\t\"beans____.SEPARATOR.300 ErrNotFound\",\n\t\t\t\t\"bokchoy__.SEPARATOR.520 ErrNotFound\",\n\t\t\t\t\"bokchoy__.SEPARATOR.510 ErrNotFound\",\n\t\t\t\t\"bokchoy__.SEPARATOR.500 ErrNotFound\",\n\t\t\t\t\"bokchoy__.SEPARATOR.400 u2\",\n\t\t\t\t\"bokchoy__.SEPARATOR.300 ErrNotFound\",\n\t\t\t\t\"broccoli_.SEPARATOR.520 q5\",\n\t\t\t\t\"broccoli_.SEPARATOR.510 s4\",\n\t\t\t\t\"broccoli_.SEPARATOR.500 s4\",\n\t\t\t\t\"broccoli_.SEPARATOR.400 ErrNotFound\",\n\t\t\t\t\"broccoli_.SEPARATOR.300 ErrNotFound\",\n\t\t\t\t\"cabbage__.SEPARATOR.520 q6\",\n\t\t\t\t\"cabbage__.SEPARATOR.510 ErrNotFound\",\n\t\t\t\t\"cabbage__.SEPARATOR.500 t1\",\n\t\t\t\t\"cabbage__.SEPARATOR.400 ErrNotFound\",\n\t\t\t\t\"cabbage__.SEPARATOR.300 ErrNotFound\",\n\t\t\t\t\"celery___.SEPARATOR.520 q7\",\n\t\t\t\t\"celery___.SEPARATOR.510 u3\",\n\t\t\t\t\"celery___.SEPARATOR.500 u3\",\n\t\t\t\t\"celery___.SEPARATOR.400 u3\",\n\t\t\t\t\"celery___.SEPARATOR.300 ErrNotFound\",\n\t\t\t\t\"corn_____.SEPARATOR.520 ErrNotFound\",\n\t\t\t\t\"corn_____.SEPARATOR.510 ErrNotFound\",\n\t\t\t\t\"corn_____.SEPARATOR.500 ErrNotFound\",\n\t\t\t\t\"corn_____.SEPARATOR.400 u4\",\n\t\t\t\t\"corn_____.SEPARATOR.300 ErrNotFound\",\n\t\t\t\t\"d________.SEPARATOR.999 ErrNotFound\",\n\t\t\t\t\"spinach__.SEPARATOR.999 u5t2p3q8\",\n\t\t\t\t\"spinach__.SEPARATOR.518 u5t2p3q8\",\n\t\t\t\t\"spinach__.SEPARATOR.517 u5t2p3\",\n\t\t\t\t\"spinach__.SEPARATOR.504 u5t2p3\",\n\t\t\t\t\"spinach__.SEPARATOR.503 u5t2\",\n\t\t\t\t\"spinach__.SEPARATOR.432 u5t2\",\n\t\t\t\t\"spinach__.SEPARATOR.431 u5\",\n\t\t\t\t\"spinach__.SEPARATOR.350 u5\",\n\t\t\t\t\"spinach__.SEPARATOR.349 ErrNotFound\",\n\t\t\t},\n\t\t},\n\n\t\t{\n\t\t\tdescription: \"broken invariants 0: non-increasing level 0 sequence numbers\",\n\t\t\tbadOrdering: true,\n\t\t\ttables: []testTable{\n\t\t\t\t{\n\t\t\t\t\tlevel:   0,\n\t\t\t\t\tfileNum: 19,\n\t\t\t\t\tdata: []string{\n\t\t\t\t\t\t\"a.SET.101 a\",\n\t\t\t\t\t\t\"b.SET.102 b\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tlevel:   0,\n\t\t\t\t\tfileNum: 20,\n\t\t\t\t\tdata: []string{\n\t\t\t\t\t\t\"c.SET.101 c\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\n\t\t{\n\t\t\tdescription: \"broken invariants 1: non-increasing level 0 sequence numbers\",\n\t\t\tbadOrdering: true,\n\t\t\ttables: []testTable{\n\t\t\t\t{\n\t\t\t\t\tlevel:   0,\n\t\t\t\t\tfileNum: 19,\n\t\t\t\t\tdata: []string{\n\t\t\t\t\t\t\"a.SET.101 a\",\n\t\t\t\t\t\t\"b.SET.102 b\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tlevel:   0,\n\t\t\t\t\tfileNum: 20,\n\t\t\t\t\tdata: []string{\n\t\t\t\t\t\t\"c.SET.100 c\",\n\t\t\t\t\t\t\"d.SET.101 d\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\n\t\t{\n\t\t\tdescription: \"broken invariants 2: matching level 0 sequence numbers, considered acceptable\",\n\t\t\tbadOrdering: false,\n\t\t\ttables: []testTable{\n\t\t\t\t{\n\t\t\t\t\tlevel:   0,\n\t\t\t\t\tfileNum: 19,\n\t\t\t\t\tdata: []string{\n\t\t\t\t\t\t\"a.SET.101 a\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tlevel:   0,\n\t\t\t\t\tfileNum: 20,\n\t\t\t\t\tdata: []string{\n\t\t\t\t\t\t\"a.SET.101 a\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\n\t\t{\n\t\t\tdescription: \"broken invariants 3: level non-0 overlapping internal key ranges\",\n\t\t\tbadOrdering: true,\n\t\t\ttables: []testTable{\n\t\t\t\t{\n\t\t\t\t\tlevel:   5,\n\t\t\t\t\tfileNum: 11,\n\t\t\t\t\tdata: []string{\n\t\t\t\t\t\t\"bat.SET.101 xxx\",\n\t\t\t\t\t\t\"dog.SET.102 xxx\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tlevel:   5,\n\t\t\t\t\tfileNum: 12,\n\t\t\t\t\tdata: []string{\n\t\t\t\t\t\t\"cow.SET.103 xxx\",\n\t\t\t\t\t\t\"pig.SET.104 xxx\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\n\tcmp := testkeys.Comparer\n\tfor _, tc := range testCases {\n\t\tdesc := tc.description[:strings.Index(tc.description, \":\")]\n\n\t\t// m is a map from file numbers to DBs.\n\t\tm := map[FileNum]*memTable{}\n\t\tnewIter := func(\n\t\t\t_ context.Context, file *manifest.FileMetadata, _ *IterOptions, _ internalIterOpts, _ iterKinds,\n\t\t) (iterSet, error) {\n\t\t\td, ok := m[file.FileNum]\n\t\t\tif !ok {\n\t\t\t\treturn iterSet{}, errors.New(\"no such file\")\n\t\t\t}\n\t\t\treturn iterSet{point: d.newIter(nil)}, nil\n\t\t}\n\n\t\tvar files [numLevels][]*fileMetadata\n\t\tfor _, tt := range tc.tables {\n\t\t\td := newMemTable(memTableOptions{})\n\t\t\tm[tt.fileNum] = d\n\n\t\t\tmeta := &fileMetadata{\n\t\t\t\tFileNum: tt.fileNum,\n\t\t\t}\n\t\t\tmeta.InitPhysicalBacking()\n\t\t\tfor i, datum := range tt.data {\n\t\t\t\ts := strings.Split(datum, \" \")\n\t\t\t\tikey := base.ParseInternalKey(s[0])\n\t\t\t\terr := d.set(ikey, []byte(s[1]))\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatalf(\"desc=%q: memtable Set: %v\", desc, err)\n\t\t\t\t}\n\n\t\t\t\tmeta.ExtendPointKeyBounds(cmp.Compare, ikey, ikey)\n\t\t\t\tif i == 0 {\n\t\t\t\t\tmeta.SmallestSeqNum = ikey.SeqNum()\n\t\t\t\t\tmeta.LargestSeqNum = ikey.SeqNum()\n\t\t\t\t} else {\n\t\t\t\t\tif meta.SmallestSeqNum > ikey.SeqNum() {\n\t\t\t\t\t\tmeta.SmallestSeqNum = ikey.SeqNum()\n\t\t\t\t\t}\n\t\t\t\t\tif meta.LargestSeqNum < ikey.SeqNum() {\n\t\t\t\t\t\tmeta.LargestSeqNum = ikey.SeqNum()\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tmeta.LargestSeqNumAbsolute = meta.LargestSeqNum\n\t\t\t}\n\n\t\t\tfiles[tt.level] = append(files[tt.level], meta)\n\t\t}\n\t\tv := manifest.NewVersion(cmp, 10<<20, files)\n\t\terr := v.CheckOrdering()\n\t\tif tc.badOrdering && err == nil {\n\t\t\tt.Errorf(\"desc=%q: want bad ordering, got nil error\", desc)\n\t\t\tcontinue\n\t\t} else if !tc.badOrdering && err != nil {\n\t\t\tt.Errorf(\"desc=%q: bad ordering: %v\", desc, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tget := func(v *version, ikey InternalKey) ([]byte, error) {\n\t\t\tvar buf struct {\n\t\t\t\tdbi Iterator\n\t\t\t\tget getIter\n\t\t\t}\n\n\t\t\tget := &buf.get\n\t\t\tget.comparer = testkeys.Comparer\n\t\t\tget.newIters = newIter\n\t\t\tget.key = ikey.UserKey\n\t\t\tget.prefix = ikey.UserKey[:testkeys.Comparer.Split(ikey.UserKey)]\n\t\t\tget.l0 = v.L0SublevelFiles\n\t\t\tget.version = v\n\t\t\tget.snapshot = ikey.SeqNum() + 1\n\t\t\tget.iterOpts = IterOptions{\n\t\t\t\tCategory:                      categoryGet,\n\t\t\t\tlogger:                        testLogger{t},\n\t\t\t\tsnapshotForHideObsoletePoints: get.snapshot,\n\t\t\t}\n\n\t\t\ti := &buf.dbi\n\t\t\ti.comparer = *testkeys.Comparer\n\t\t\ti.merge = DefaultMerger.Merge\n\t\t\ti.iter = get\n\n\t\t\tdefer i.Close()\n\t\t\tif !i.First() {\n\t\t\t\terr := i.Error()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\treturn nil, ErrNotFound\n\t\t\t}\n\t\t\treturn i.Value(), nil\n\t\t}\n\n\t\tfor _, query := range tc.queries {\n\t\t\ts := strings.Split(query, \" \")\n\t\t\tikey := base.ParseInternalKey(s[0])\n\t\t\tvalue, err := get(v, ikey)\n\t\t\tgot, want := \"\", s[1]\n\t\t\tif err != nil {\n\t\t\t\tif err != ErrNotFound {\n\t\t\t\t\tt.Errorf(\"desc=%q: query=%q: %v\", desc, s[0], err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tgot = \"ErrNotFound\"\n\t\t\t} else {\n\t\t\t\tgot = string(value)\n\t\t\t}\n\t\t\tif got != want {\n\t\t\t\tt.Errorf(\"desc=%q: query=%q: got %q, want %q\", desc, s[0], got, want)\n\t\t\t}\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "go.mod",
          "type": "blob",
          "size": 2.2333984375,
          "content": "module github.com/cockroachdb/pebble\n\nrequire (\n\tgithub.com/DataDog/zstd v1.5.6-0.20230824185856-869dae002e5e\n\tgithub.com/HdrHistogram/hdrhistogram-go v1.1.2\n\tgithub.com/cespare/xxhash/v2 v2.2.0\n\tgithub.com/cockroachdb/crlib v0.0.0-20241112164430-1264a2edc35b\n\tgithub.com/cockroachdb/datadriven v1.0.3-0.20240530155848-7682d40af056\n\tgithub.com/cockroachdb/errors v1.11.3\n\tgithub.com/cockroachdb/metamorphic v0.0.0-20231108215700-4ba948b56895\n\tgithub.com/cockroachdb/redact v1.1.5\n\tgithub.com/cockroachdb/swiss v0.0.0-20240612210725-f4de07ae6964\n\tgithub.com/cockroachdb/tokenbucket v0.0.0-20230807174530-cc333fc44b06\n\tgithub.com/ghemawat/stream v0.0.0-20171120220530-696b145b53b9\n\tgithub.com/golang/snappy v0.0.5-0.20231225225746-43d5d4cd4e0e\n\tgithub.com/guptarohit/asciigraph v0.5.5\n\tgithub.com/klauspost/compress v1.16.7\n\tgithub.com/kr/pretty v0.3.1\n\tgithub.com/olekukonko/tablewriter v0.0.5\n\tgithub.com/pkg/errors v0.9.1\n\tgithub.com/pmezard/go-difflib v1.0.0\n\tgithub.com/prometheus/client_golang v1.12.0\n\tgithub.com/prometheus/client_model v0.2.1-0.20210607210712-147c58e9608a\n\tgithub.com/spf13/cobra v1.0.0\n\tgithub.com/stretchr/testify v1.9.0\n\tgolang.org/x/exp v0.0.0-20230626212559-97b1e661b5df\n\tgolang.org/x/perf v0.0.0-20230113213139-801c7ef9e5c5\n\tgolang.org/x/sync v0.7.0\n\tgolang.org/x/sys v0.18.0\n)\n\nrequire (\n\tgithub.com/aclements/go-moremath v0.0.0-20210112150236-f10218a38794 // indirect\n\tgithub.com/beorn7/perks v1.0.1 // indirect\n\tgithub.com/cockroachdb/logtags v0.0.0-20230118201751-21c54148d20b // indirect\n\tgithub.com/davecgh/go-spew v1.1.1 // indirect\n\tgithub.com/getsentry/sentry-go v0.27.0 // indirect\n\tgithub.com/gogo/protobuf v1.3.2 // indirect\n\tgithub.com/golang/protobuf v1.5.3 // indirect\n\tgithub.com/inconshreveable/mousetrap v1.0.0 // indirect\n\tgithub.com/kr/text v0.2.0 // indirect\n\tgithub.com/mattn/go-runewidth v0.0.9 // indirect\n\tgithub.com/matttproud/golang_protobuf_extensions v1.0.2-0.20181231171920-c182affec369 // indirect\n\tgithub.com/prometheus/common v0.32.1 // indirect\n\tgithub.com/prometheus/procfs v0.7.3 // indirect\n\tgithub.com/rogpeppe/go-internal v1.9.0 // indirect\n\tgithub.com/spf13/pflag v1.0.5 // indirect\n\tgolang.org/x/text v0.14.0 // indirect\n\tgoogle.golang.org/protobuf v1.33.0 // indirect\n\tgopkg.in/yaml.v3 v3.0.1 // indirect\n)\n\ngo 1.22\n"
        },
        {
          "name": "go.sum",
          "type": "blob",
          "size": 66.2060546875,
          "content": "cloud.google.com/go v0.0.0-20170206221025-ce650573d812/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=\ncloud.google.com/go v0.26.0/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=\ncloud.google.com/go v0.34.0/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=\ncloud.google.com/go v0.38.0/go.mod h1:990N+gfupTy94rShfmMCWGDn0LpTmnzTp2qbd1dvSRU=\ncloud.google.com/go v0.44.1/go.mod h1:iSa0KzasP4Uvy3f1mN/7PiObzGgflwredwwASm/v6AU=\ncloud.google.com/go v0.44.2/go.mod h1:60680Gw3Yr4ikxnPRS/oxxkBccT6SA1yMk63TGekxKY=\ncloud.google.com/go v0.45.1/go.mod h1:RpBamKRgapWJb87xiFSdk4g1CME7QZg3uwTez+TSTjc=\ncloud.google.com/go v0.46.3/go.mod h1:a6bKKbmY7er1mI7TEI4lsAkts/mkhTSZK8w33B4RAg0=\ncloud.google.com/go v0.50.0/go.mod h1:r9sluTvynVuxRIOHXQEHMFffphuXHOMZMycpNR5e6To=\ncloud.google.com/go v0.52.0/go.mod h1:pXajvRH/6o3+F9jDHZWQ5PbGhn+o8w9qiu/CffaVdO4=\ncloud.google.com/go v0.53.0/go.mod h1:fp/UouUEsRkN6ryDKNW/Upv/JBKnv6WDthjR6+vze6M=\ncloud.google.com/go v0.54.0/go.mod h1:1rq2OEkV3YMf6n/9ZvGWI3GWw0VoqH/1x2nd8Is/bPc=\ncloud.google.com/go v0.56.0/go.mod h1:jr7tqZxxKOVYizybht9+26Z/gUq7tiRzu+ACVAMbKVk=\ncloud.google.com/go v0.57.0/go.mod h1:oXiQ6Rzq3RAkkY7N6t3TcE6jE+CIBBbA36lwQ1JyzZs=\ncloud.google.com/go v0.62.0/go.mod h1:jmCYTdRCQuc1PHIIJ/maLInMho30T/Y0M4hTdTShOYc=\ncloud.google.com/go v0.65.0/go.mod h1:O5N8zS7uWy9vkA9vayVHs65eM1ubvY4h553ofrNHObY=\ncloud.google.com/go/bigquery v1.0.1/go.mod h1:i/xbL2UlR5RvWAURpBYZTtm/cXjCha9lbfbpx4poX+o=\ncloud.google.com/go/bigquery v1.3.0/go.mod h1:PjpwJnslEMmckchkHFfq+HTD2DmtT67aNFKH1/VBDHE=\ncloud.google.com/go/bigquery v1.4.0/go.mod h1:S8dzgnTigyfTmLBfrtrhyYhwRxG72rYxvftPBK2Dvzc=\ncloud.google.com/go/bigquery v1.5.0/go.mod h1:snEHRnqQbz117VIFhE8bmtwIDY80NLUZUMb4Nv6dBIg=\ncloud.google.com/go/bigquery v1.7.0/go.mod h1://okPTzCYNXSlb24MZs83e2Do+h+VXtc4gLoIoXIAPc=\ncloud.google.com/go/bigquery v1.8.0/go.mod h1:J5hqkt3O0uAFnINi6JXValWIb1v0goeZM77hZzJN/fQ=\ncloud.google.com/go/datastore v1.0.0/go.mod h1:LXYbyblFSglQ5pkeyhO+Qmw7ukd3C+pD7TKLgZqpHYE=\ncloud.google.com/go/datastore v1.1.0/go.mod h1:umbIZjpQpHh4hmRpGhH4tLFup+FVzqBi1b3c64qFpCk=\ncloud.google.com/go/pubsub v1.0.1/go.mod h1:R0Gpsv3s54REJCy4fxDixWD93lHJMoZTyQ2kNxGRt3I=\ncloud.google.com/go/pubsub v1.1.0/go.mod h1:EwwdRX2sKPjnvnqCa270oGRyludottCI76h+R3AArQw=\ncloud.google.com/go/pubsub v1.2.0/go.mod h1:jhfEVHT8odbXTkndysNHCcx0awwzvfOlguIAii9o8iA=\ncloud.google.com/go/pubsub v1.3.1/go.mod h1:i+ucay31+CNRpDW4Lu78I4xXG+O1r/MAHgjpRVR+TSU=\ncloud.google.com/go/storage v1.0.0/go.mod h1:IhtSnM/ZTZV8YYJWCY8RULGVqBDmpoyjwiyrjsg+URw=\ncloud.google.com/go/storage v1.5.0/go.mod h1:tpKbwo567HUNpVclU5sGELwQWBDZ8gh0ZeosJ0Rtdos=\ncloud.google.com/go/storage v1.6.0/go.mod h1:N7U0C8pVQ/+NIKOBQyamJIeKQKkZ+mxpohlUTyfDhBk=\ncloud.google.com/go/storage v1.8.0/go.mod h1:Wv1Oy7z6Yz3DshWRJFhqM/UCfaWIRTdp0RXyy7KQOVs=\ncloud.google.com/go/storage v1.10.0/go.mod h1:FLPqc6j+Ki4BU591ie1oL6qBQGu2Bl/tZ9ullr3+Kg0=\ndmitri.shuralyov.com/gpu/mtl v0.0.0-20190408044501-666a987793e9/go.mod h1:H6x//7gZCb22OMCxBHrMx7a5I7Hp++hsVxbQ4BYO7hU=\ngioui.org v0.0.0-20210308172011-57750fc8a0a6/go.mod h1:RSH6KIUZ0p2xy5zHDxgAM4zumjgTw83q2ge/PI+yyw8=\ngithub.com/BurntSushi/toml v0.3.1/go.mod h1:xHWCNGjB5oqiDr8zfno3MHue2Ht5sIBksp03qcyfWMU=\ngithub.com/BurntSushi/xgb v0.0.0-20160522181843-27f122750802/go.mod h1:IVnqGOEym/WlBOVXweHU+Q+/VP0lqqI8lqeDx9IjBqo=\ngithub.com/DataDog/zstd v1.5.6-0.20230824185856-869dae002e5e h1:ZIWapoIRN1VqT8GR8jAwb1Ie9GyehWjVcGh32Y2MznE=\ngithub.com/DataDog/zstd v1.5.6-0.20230824185856-869dae002e5e/go.mod h1:g4AWEaM3yOg3HYfnJ3YIawPnVdXJh9QME85blwSAmyw=\ngithub.com/GoogleCloudPlatform/cloudsql-proxy v0.0.0-20190129172621-c8b1d7a94ddf/go.mod h1:aJ4qN3TfrelA6NZ6AXsXRfmEVaYin3EDbSPJrKS8OXo=\ngithub.com/HdrHistogram/hdrhistogram-go v1.1.2 h1:5IcZpTvzydCQeHzK4Ef/D5rrSqwxob0t8PQPMybUNFM=\ngithub.com/HdrHistogram/hdrhistogram-go v1.1.2/go.mod h1:yDgFjdqOqDEKOvasDdhWNXYg9BVp4O+o5f6V/ehm6Oo=\ngithub.com/OneOfOne/xxhash v1.2.2/go.mod h1:HSdplMjZKSmBqAxg5vPj2TmRDmfkzw+cTzAElWljhcU=\ngithub.com/aclements/go-gg v0.0.0-20170118225347-6dbb4e4fefb0/go.mod h1:55qNq4vcpkIuHowELi5C8e+1yUHtoLoOUR9QU5j7Tes=\ngithub.com/aclements/go-moremath v0.0.0-20210112150236-f10218a38794 h1:xlwdaKcTNVW4PtpQb8aKA4Pjy0CdJHEqvFbAnvR5m2g=\ngithub.com/aclements/go-moremath v0.0.0-20210112150236-f10218a38794/go.mod h1:7e+I0LQFUI9AXWxOfsQROs9xPhoJtbsyWcjJqDd4KPY=\ngithub.com/aclements/go-perfevent v0.0.0-20240301234650-f7843625020f h1:JjxwchlOepwsUWcQwD2mLUAGE9aCp0/ehy6yCHFBOvo=\ngithub.com/aclements/go-perfevent v0.0.0-20240301234650-f7843625020f/go.mod h1:tMDTce/yLLN/SK8gMOxQfnyeMeCg8KGzp0D1cbECEeo=\ngithub.com/ajstarks/svgo v0.0.0-20180226025133-644b8db467af/go.mod h1:K08gAheRH3/J6wwsYMMT4xOr94bZjxIelGM0+d/wbFw=\ngithub.com/ajstarks/svgo v0.0.0-20210923152817-c3b6e2f0c527/go.mod h1:K08gAheRH3/J6wwsYMMT4xOr94bZjxIelGM0+d/wbFw=\ngithub.com/alecthomas/template v0.0.0-20160405071501-a0175ee3bccc/go.mod h1:LOuyumcjzFXgccqObfd/Ljyb9UuFJ6TxHnclSeseNhc=\ngithub.com/alecthomas/template v0.0.0-20190718012654-fb15b899a751/go.mod h1:LOuyumcjzFXgccqObfd/Ljyb9UuFJ6TxHnclSeseNhc=\ngithub.com/alecthomas/units v0.0.0-20151022065526-2efee857e7cf/go.mod h1:ybxpYRFXyAe+OPACYpWeL0wqObRcbAqCMya13uyzqw0=\ngithub.com/alecthomas/units v0.0.0-20190717042225-c3de453c63f4/go.mod h1:ybxpYRFXyAe+OPACYpWeL0wqObRcbAqCMya13uyzqw0=\ngithub.com/alecthomas/units v0.0.0-20190924025748-f65c72e2690d/go.mod h1:rBZYJk541a8SKzHPHnH3zbiI+7dagKZ0cgpgrD7Fyho=\ngithub.com/armon/consul-api v0.0.0-20180202201655-eb2c6b5be1b6/go.mod h1:grANhF5doyWs3UAsr3K4I6qtAmlQcZDesFNEHPZAzj8=\ngithub.com/beorn7/perks v0.0.0-20180321164747-3a771d992973/go.mod h1:Dwedo/Wpr24TaqPxmxbtue+5NUziq4I4S80YR8gNf3Q=\ngithub.com/beorn7/perks v1.0.0/go.mod h1:KWe93zE9D1o94FZ5RNwFwVgaQK1VOXiVxmqh+CedLV8=\ngithub.com/beorn7/perks v1.0.1 h1:VlbKKnNfV8bJzeqoa4cOKqO6bYr3WgKZxO8Z16+hsOM=\ngithub.com/beorn7/perks v1.0.1/go.mod h1:G2ZrVWU2WbWT9wwq4/hrbKbnv/1ERSJQ0ibhJ6rlkpw=\ngithub.com/boombuler/barcode v1.0.0/go.mod h1:paBWMcWSl3LHKBqUq+rly7CNSldXjb2rDl3JlRe0mD8=\ngithub.com/boombuler/barcode v1.0.1/go.mod h1:paBWMcWSl3LHKBqUq+rly7CNSldXjb2rDl3JlRe0mD8=\ngithub.com/census-instrumentation/opencensus-proto v0.2.1/go.mod h1:f6KPmirojxKA12rnyqOA5BBL4O983OfeGPqjHWSTneU=\ngithub.com/cespare/xxhash v1.1.0/go.mod h1:XrSqR1VqqWfGrhpAt58auRo0WTKS1nRRg3ghfAqPWnc=\ngithub.com/cespare/xxhash/v2 v2.1.1/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XLFGgcrjCOs=\ngithub.com/cespare/xxhash/v2 v2.1.2/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XLFGgcrjCOs=\ngithub.com/cespare/xxhash/v2 v2.2.0 h1:DC2CZ1Ep5Y4k3ZQ899DldepgrayRUGE6BBZ/cd9Cj44=\ngithub.com/cespare/xxhash/v2 v2.2.0/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XLFGgcrjCOs=\ngithub.com/chzyer/logex v1.1.10/go.mod h1:+Ywpsq7O8HXn0nuIou7OrIPyXbp3wmkHB+jjWRnGsAI=\ngithub.com/chzyer/readline v0.0.0-20180603132655-2972be24d48e/go.mod h1:nSuG5e5PlCu98SY8svDHJxuZscDgtXS6KTTbou5AhLI=\ngithub.com/chzyer/test v0.0.0-20180213035817-a1ea475d72b1/go.mod h1:Q3SI9o4m/ZMnBNeIyt5eFwwo7qiLfzFZmjNmxjkiQlU=\ngithub.com/client9/misspell v0.3.4/go.mod h1:qj6jICC3Q7zFZvVWo7KLAzC3yx5G7kyvSDkc90ppPyw=\ngithub.com/cncf/udpa/go v0.0.0-20191209042840-269d4d468f6f/go.mod h1:M8M6+tZqaGXZJjfX53e64911xZQV5JYwmTeXPW+k8Sc=\ngithub.com/cockroachdb/crlib v0.0.0-20241112164430-1264a2edc35b h1:SHlYZ/bMx7frnmeqCu+xm0TCxXLzX3jQIVuFbnFGtFU=\ngithub.com/cockroachdb/crlib v0.0.0-20241112164430-1264a2edc35b/go.mod h1:Gq51ZeKaFCXk6QwuGM0w1dnaOqc/F5zKT2zA9D6Xeac=\ngithub.com/cockroachdb/datadriven v1.0.3-0.20240530155848-7682d40af056 h1:slXychO2uDM6hYRu4c0pD0udNI8uObfeKN6UInWViS8=\ngithub.com/cockroachdb/datadriven v1.0.3-0.20240530155848-7682d40af056/go.mod h1:a9RdTaap04u637JoCzcUoIcDmvwSUtcUFtT/C3kJlTU=\ngithub.com/cockroachdb/errors v1.11.3 h1:5bA+k2Y6r+oz/6Z/RFlNeVCesGARKuC6YymtcDrbC/I=\ngithub.com/cockroachdb/errors v1.11.3/go.mod h1:m4UIW4CDjx+R5cybPsNrRbreomiFqt8o1h1wUVazSd8=\ngithub.com/cockroachdb/logtags v0.0.0-20230118201751-21c54148d20b h1:r6VH0faHjZeQy818SGhaone5OnYfxFR/+AzdY3sf5aE=\ngithub.com/cockroachdb/logtags v0.0.0-20230118201751-21c54148d20b/go.mod h1:Vz9DsVWQQhf3vs21MhPMZpMGSht7O/2vFW2xusFUVOs=\ngithub.com/cockroachdb/metamorphic v0.0.0-20231108215700-4ba948b56895 h1:XANOgPYtvELQ/h4IrmPAohXqe2pWA8Bwhejr3VQoZsA=\ngithub.com/cockroachdb/metamorphic v0.0.0-20231108215700-4ba948b56895/go.mod h1:aPd7gM9ov9M8v32Yy5NJrDyOcD8z642dqs+F0CeNXfA=\ngithub.com/cockroachdb/redact v1.1.5 h1:u1PMllDkdFfPWaNGMyLD1+so+aq3uUItthCFqzwPJ30=\ngithub.com/cockroachdb/redact v1.1.5/go.mod h1:BVNblN9mBWFyMyqK1k3AAiSxhvhfK2oOZZ2lK+dpvRg=\ngithub.com/cockroachdb/swiss v0.0.0-20240612210725-f4de07ae6964 h1:Ew0znI2JatzKy52N1iS5muUsHkf2UJuhocH7uFW7jjs=\ngithub.com/cockroachdb/swiss v0.0.0-20240612210725-f4de07ae6964/go.mod h1:yBRu/cnL4ks9bgy4vAASdjIW+/xMlFwuHKqtmh3GZQg=\ngithub.com/cockroachdb/tokenbucket v0.0.0-20230807174530-cc333fc44b06 h1:zuQyyAKVxetITBuuhv3BI9cMrmStnpT18zmgmTxunpo=\ngithub.com/cockroachdb/tokenbucket v0.0.0-20230807174530-cc333fc44b06/go.mod h1:7nc4anLGjupUW/PeY5qiNYsdNXj7zopG+eqsS7To5IQ=\ngithub.com/coreos/bbolt v1.3.2/go.mod h1:iRUV2dpdMOn7Bo10OQBFzIJO9kkE559Wcmn+qkEiiKk=\ngithub.com/coreos/etcd v3.3.10+incompatible/go.mod h1:uF7uidLiAD3TWHmW31ZFd/JWoc32PjwdhPthX9715RE=\ngithub.com/coreos/go-semver v0.2.0/go.mod h1:nnelYz7RCh+5ahJtPPxZlU+153eP4D4r3EedlOD2RNk=\ngithub.com/coreos/go-systemd v0.0.0-20190321100706-95778dfbb74e/go.mod h1:F5haX7vjVVG0kc13fIWeqUViNPyEJxv/OmvnBo0Yme4=\ngithub.com/coreos/pkg v0.0.0-20180928190104-399ea9e2e55f/go.mod h1:E3G3o1h8I7cfcXa63jLwjI0eiQQMgzzUDFVpN/nH/eA=\ngithub.com/cpuguy83/go-md2man/v2 v2.0.0/go.mod h1:maD7wRr/U5Z6m/iR4s+kqSMx2CaBsrgA7czyZG/E6dU=\ngithub.com/creack/pty v1.1.9/go.mod h1:oKZEueFk5CKHvIhNR5MUki03XCEU+Q6VDXinZuGJ33E=\ngithub.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=\ngithub.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/dgrijalva/jwt-go v3.2.0+incompatible/go.mod h1:E3ru+11k8xSBh+hMPgOLZmtrrCbhqsmaPHjLKYnJCaQ=\ngithub.com/dgryski/go-sip13 v0.0.0-20181026042036-e10d5fee7954/go.mod h1:vAd38F8PWV+bWy6jNmig1y/TA+kYO4g3RSRF0IAv0no=\ngithub.com/envoyproxy/go-control-plane v0.9.0/go.mod h1:YTl/9mNaCwkRvm6d1a2C3ymFceY/DCBVvsKhRF0iEA4=\ngithub.com/envoyproxy/go-control-plane v0.9.1-0.20191026205805-5f8ba28d4473/go.mod h1:YTl/9mNaCwkRvm6d1a2C3ymFceY/DCBVvsKhRF0iEA4=\ngithub.com/envoyproxy/go-control-plane v0.9.4/go.mod h1:6rpuAdCZL397s3pYoYcLgu1mIlRU8Am5FuJP05cCM98=\ngithub.com/envoyproxy/protoc-gen-validate v0.1.0/go.mod h1:iSmxcyjqTsJpI2R4NaDN7+kN2VEUnK/pcBlmesArF7c=\ngithub.com/fogleman/gg v1.2.1-0.20190220221249-0403632d5b90/go.mod h1:R/bRT+9gY/C5z7JzPU0zXsXHKM4/ayA+zqcVNZzPa1k=\ngithub.com/fogleman/gg v1.3.0/go.mod h1:R/bRT+9gY/C5z7JzPU0zXsXHKM4/ayA+zqcVNZzPa1k=\ngithub.com/fsnotify/fsnotify v1.4.7/go.mod h1:jwhsz4b93w/PPRr/qN1Yymfu8t87LnFCMoQvtojpjFo=\ngithub.com/getsentry/sentry-go v0.27.0 h1:Pv98CIbtB3LkMWmXi4Joa5OOcwbmnX88sF5qbK3r3Ps=\ngithub.com/getsentry/sentry-go v0.27.0/go.mod h1:lc76E2QywIyW8WuBnwl8Lc4bkmQH4+w1gwTf25trprY=\ngithub.com/ghemawat/stream v0.0.0-20171120220530-696b145b53b9 h1:r5GgOLGbza2wVHRzK7aAj6lWZjfbAwiu/RDCVOKjRyM=\ngithub.com/ghemawat/stream v0.0.0-20171120220530-696b145b53b9/go.mod h1:106OIgooyS7OzLDOpUGgm9fA3bQENb/cFSyyBmMoJDs=\ngithub.com/ghodss/yaml v1.0.0/go.mod h1:4dBDuWmgqj2HViK6kFavaiC9ZROes6MMH2rRYeMEF04=\ngithub.com/go-errors/errors v1.4.2 h1:J6MZopCL4uSllY1OfXM374weqZFFItUbrImctkmUxIA=\ngithub.com/go-errors/errors v1.4.2/go.mod h1:sIVyrIiJhuEF+Pj9Ebtd6P/rEYROXFi3BopGUQ5a5Og=\ngithub.com/go-fonts/dejavu v0.1.0/go.mod h1:4Wt4I4OU2Nq9asgDCteaAaWZOV24E+0/Pwo0gppep4g=\ngithub.com/go-fonts/latin-modern v0.2.0/go.mod h1:rQVLdDMK+mK1xscDwsqM5J8U2jrRa3T0ecnM9pNujks=\ngithub.com/go-fonts/liberation v0.1.1/go.mod h1:K6qoJYypsmfVjWg8KOVDQhLc8UDgIK2HYqyqAO9z7GY=\ngithub.com/go-fonts/liberation v0.2.0/go.mod h1:K6qoJYypsmfVjWg8KOVDQhLc8UDgIK2HYqyqAO9z7GY=\ngithub.com/go-fonts/stix v0.1.0/go.mod h1:w/c1f0ldAUlJmLBvlbkvVXLAD+tAMqobIIQpmnUIzUY=\ngithub.com/go-gl/glfw v0.0.0-20190409004039-e6da0acd62b1/go.mod h1:vR7hzQXu2zJy9AVAgeJqvqgH9Q5CA+iKCZ2gyEVpxRU=\ngithub.com/go-gl/glfw/v3.3/glfw v0.0.0-20191125211704-12ad95a8df72/go.mod h1:tQ2UAYgL5IevRw8kRxooKSPJfGvJ9fJQFa0TUsXzTg8=\ngithub.com/go-gl/glfw/v3.3/glfw v0.0.0-20200222043503-6f7a984d4dc4/go.mod h1:tQ2UAYgL5IevRw8kRxooKSPJfGvJ9fJQFa0TUsXzTg8=\ngithub.com/go-kit/kit v0.8.0/go.mod h1:xBxKIO96dXMWWy0MnWVtmwkA9/13aqxPnvrjFYMA2as=\ngithub.com/go-kit/kit v0.9.0/go.mod h1:xBxKIO96dXMWWy0MnWVtmwkA9/13aqxPnvrjFYMA2as=\ngithub.com/go-kit/log v0.1.0/go.mod h1:zbhenjAZHb184qTLMA9ZjW7ThYL0H2mk7Q6pNt4vbaY=\ngithub.com/go-latex/latex v0.0.0-20210118124228-b3d85cf34e07/go.mod h1:CO1AlKB2CSIqUrmQPqA0gdRIlnLEY0gK5JGjh37zN5U=\ngithub.com/go-latex/latex v0.0.0-20210823091927-c0d11ff05a81/go.mod h1:SX0U8uGpxhq9o2S/CELCSUxEWWAuoCUcVCQWv7G2OCk=\ngithub.com/go-logfmt/logfmt v0.3.0/go.mod h1:Qt1PoO58o5twSAckw1HlFXLmHsOX5/0LbT9GBnD5lWE=\ngithub.com/go-logfmt/logfmt v0.4.0/go.mod h1:3RMwSq7FuexP4Kalkev3ejPJsZTpXXBr9+V4qmtdjCk=\ngithub.com/go-logfmt/logfmt v0.5.0/go.mod h1:wCYkCAKZfumFQihp8CzCvQ3paCTfi41vtzG1KdI/P7A=\ngithub.com/go-pdf/fpdf v0.5.0/go.mod h1:HzcnA+A23uwogo0tp9yU+l3V+KXhiESpt1PMayhOh5M=\ngithub.com/go-sql-driver/mysql v1.4.1/go.mod h1:zAC/RDZ24gD3HViQzih4MyKcchzm+sOG5ZlKdlhCg5w=\ngithub.com/go-stack/stack v1.8.0/go.mod h1:v0f6uXyyMGvRgIKkXu+yp6POWl0qKG85gN/melR3HDY=\ngithub.com/gogo/protobuf v1.1.1/go.mod h1:r8qH/GZQm5c6nD/R0oafs1akxWv10x8SbQlK7atdtwQ=\ngithub.com/gogo/protobuf v1.2.1/go.mod h1:hp+jE20tsWTFYpLwKvXlhS1hjn+gTNwPg2I6zVXpSg4=\ngithub.com/gogo/protobuf v1.3.2 h1:Ov1cvc58UF3b5XjBnZv7+opcTcQFZebYjWzi34vdm4Q=\ngithub.com/gogo/protobuf v1.3.2/go.mod h1:P1XiOD3dCwIKUDQYPy72D8LYyHL2YPYrpS2s69NZV8Q=\ngithub.com/golang/freetype v0.0.0-20170609003504-e2365dfdc4a0/go.mod h1:E/TSTwGwJL78qG/PmXZO1EjYhfJinVAhrmmHX6Z8B9k=\ngithub.com/golang/glog v0.0.0-20160126235308-23def4e6c14b/go.mod h1:SBH7ygxi8pfUlaOkMMuAQtPIUF8ecWP5IEl/CR7VP2Q=\ngithub.com/golang/groupcache v0.0.0-20190129154638-5b532d6fd5ef/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=\ngithub.com/golang/groupcache v0.0.0-20190702054246-869f871628b6/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=\ngithub.com/golang/groupcache v0.0.0-20191227052852-215e87163ea7/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=\ngithub.com/golang/groupcache v0.0.0-20200121045136-8c9f03a8e57e/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=\ngithub.com/golang/mock v1.1.1/go.mod h1:oTYuIxOrZwtPieC+H1uAHpcLFnEyAGVDL/k47Jfbm0A=\ngithub.com/golang/mock v1.2.0/go.mod h1:oTYuIxOrZwtPieC+H1uAHpcLFnEyAGVDL/k47Jfbm0A=\ngithub.com/golang/mock v1.3.1/go.mod h1:sBzyDLLjw3U8JLTeZvSv8jJB+tU5PVekmnlKIyFUx0Y=\ngithub.com/golang/mock v1.4.0/go.mod h1:UOMv5ysSaYNkG+OFQykRIcU/QvvxJf3p21QfJ2Bt3cw=\ngithub.com/golang/mock v1.4.1/go.mod h1:UOMv5ysSaYNkG+OFQykRIcU/QvvxJf3p21QfJ2Bt3cw=\ngithub.com/golang/mock v1.4.3/go.mod h1:UOMv5ysSaYNkG+OFQykRIcU/QvvxJf3p21QfJ2Bt3cw=\ngithub.com/golang/mock v1.4.4/go.mod h1:l3mdAwkq5BuhzHwde/uurv3sEJeZMXNpwsxVWU71h+4=\ngithub.com/golang/protobuf v1.2.0/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.3.1/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.3.2/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=\ngithub.com/golang/protobuf v1.3.3/go.mod h1:vzj43D7+SQXF/4pzW/hwtAqwc6iTitCiVSaWz5lYuqw=\ngithub.com/golang/protobuf v1.3.4/go.mod h1:vzj43D7+SQXF/4pzW/hwtAqwc6iTitCiVSaWz5lYuqw=\ngithub.com/golang/protobuf v1.3.5/go.mod h1:6O5/vntMXwX2lRkT1hjjk0nAC1IDOTvTlVgjlRvqsdk=\ngithub.com/golang/protobuf v1.4.0-rc.1/go.mod h1:ceaxUfeHdC40wWswd/P6IGgMaK3YpKi5j83Wpe3EHw8=\ngithub.com/golang/protobuf v1.4.0-rc.1.0.20200221234624-67d41d38c208/go.mod h1:xKAWHe0F5eneWXFV3EuXVDTCmh+JuBKY0li0aMyXATA=\ngithub.com/golang/protobuf v1.4.0-rc.2/go.mod h1:LlEzMj4AhA7rCAGe4KMBDvJI+AwstrUpVNzEA03Pprs=\ngithub.com/golang/protobuf v1.4.0-rc.4.0.20200313231945-b860323f09d0/go.mod h1:WU3c8KckQ9AFe+yFwt9sWVRKCVIyN9cPHBJSNnbL67w=\ngithub.com/golang/protobuf v1.4.0/go.mod h1:jodUvKwWbYaEsadDk5Fwe5c77LiNKVO9IDvqG2KuDX0=\ngithub.com/golang/protobuf v1.4.1/go.mod h1:U8fpvMrcmy5pZrNK1lt4xCsGvpyWQ/VVv6QDs8UjoX8=\ngithub.com/golang/protobuf v1.4.2/go.mod h1:oDoupMAO8OvCJWAcko0GGGIgR6R6ocIYbsSw735rRwI=\ngithub.com/golang/protobuf v1.4.3/go.mod h1:oDoupMAO8OvCJWAcko0GGGIgR6R6ocIYbsSw735rRwI=\ngithub.com/golang/protobuf v1.5.0/go.mod h1:FsONVRAS9T7sI+LIUmWTfcYkHO4aIWwzhcaSAoJOfIk=\ngithub.com/golang/protobuf v1.5.2/go.mod h1:XVQd3VNwM+JqD3oG2Ue2ip4fOMUkwXdXDdiuN0vRsmY=\ngithub.com/golang/protobuf v1.5.3 h1:KhyjKVUg7Usr/dYsdSqoFveMYd5ko72D+zANwlG1mmg=\ngithub.com/golang/protobuf v1.5.3/go.mod h1:XVQd3VNwM+JqD3oG2Ue2ip4fOMUkwXdXDdiuN0vRsmY=\ngithub.com/golang/snappy v0.0.5-0.20231225225746-43d5d4cd4e0e h1:4bw4WeyTYPp0smaXiJZCNnLrvVBqirQVreixayXezGc=\ngithub.com/golang/snappy v0.0.5-0.20231225225746-43d5d4cd4e0e/go.mod h1:/XxbfmMg8lxefKM7IXC3fBNl/7bRcc72aCRzEWrmP2Q=\ngithub.com/gonum/blas v0.0.0-20181208220705-f22b278b28ac/go.mod h1:P32wAyui1PQ58Oce/KYkOqQv8cVw1zAapXOl+dRFGbc=\ngithub.com/gonum/floats v0.0.0-20181209220543-c233463c7e82/go.mod h1:PxC8OnwL11+aosOB5+iEPoV3picfs8tUpkVd0pDo+Kg=\ngithub.com/gonum/internal v0.0.0-20181124074243-f884aa714029/go.mod h1:Pu4dmpkhSyOzRwuXkOgAvijx4o+4YMUJJo9OvPYMkks=\ngithub.com/gonum/lapack v0.0.0-20181123203213-e4cdc5a0bff9/go.mod h1:XA3DeT6rxh2EAE789SSiSJNqxPaC0aE9J8NTOI0Jo/A=\ngithub.com/gonum/matrix v0.0.0-20181209220409-c518dec07be9/go.mod h1:0EXg4mc1CNP0HCqCz+K4ts155PXIlUywf0wqN+GfPZw=\ngithub.com/google/btree v0.0.0-20180813153112-4030bb1f1f0c/go.mod h1:lNA+9X1NB3Zf8V7Ke586lFgjr2dZNuvo3lPJSGZ5JPQ=\ngithub.com/google/btree v1.0.0/go.mod h1:lNA+9X1NB3Zf8V7Ke586lFgjr2dZNuvo3lPJSGZ5JPQ=\ngithub.com/google/go-cmp v0.2.0/go.mod h1:oXzfMopK8JAjlY9xF4vHSVASa0yLyX7SntLO5aqRK0M=\ngithub.com/google/go-cmp v0.3.0/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\ngithub.com/google/go-cmp v0.3.1/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=\ngithub.com/google/go-cmp v0.4.0/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.4.1/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.0/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.1/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.4/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.5/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/go-cmp v0.5.9 h1:O2Tfq5qg4qc4AmwVlvv0oLiVAGB7enBSJ2x2DqQFi38=\ngithub.com/google/go-cmp v0.5.9/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=\ngithub.com/google/gofuzz v1.0.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=\ngithub.com/google/martian v2.1.0+incompatible/go.mod h1:9I4somxYTbIHy5NJKHRl3wXiIaQGbYVAs8BPL6v8lEs=\ngithub.com/google/martian/v3 v3.0.0/go.mod h1:y5Zk1BBys9G+gd6Jrk0W3cC1+ELVxBWuIGO+w/tUAp0=\ngithub.com/google/pprof v0.0.0-20181206194817-3ea8567a2e57/go.mod h1:zfwlbNMJ+OItoe0UupaVj+oy1omPYYDuagoSzA8v9mc=\ngithub.com/google/pprof v0.0.0-20190515194954-54271f7e092f/go.mod h1:zfwlbNMJ+OItoe0UupaVj+oy1omPYYDuagoSzA8v9mc=\ngithub.com/google/pprof v0.0.0-20191218002539-d4f498aebedc/go.mod h1:ZgVRPoUq/hfqzAqh7sHMqb3I9Rq5C59dIz2SbBwJ4eM=\ngithub.com/google/pprof v0.0.0-20200212024743-f11f1df84d12/go.mod h1:ZgVRPoUq/hfqzAqh7sHMqb3I9Rq5C59dIz2SbBwJ4eM=\ngithub.com/google/pprof v0.0.0-20200229191704-1ebb73c60ed3/go.mod h1:ZgVRPoUq/hfqzAqh7sHMqb3I9Rq5C59dIz2SbBwJ4eM=\ngithub.com/google/pprof v0.0.0-20200430221834-fc25d7d30c6d/go.mod h1:ZgVRPoUq/hfqzAqh7sHMqb3I9Rq5C59dIz2SbBwJ4eM=\ngithub.com/google/pprof v0.0.0-20200708004538-1a94d8640e99/go.mod h1:ZgVRPoUq/hfqzAqh7sHMqb3I9Rq5C59dIz2SbBwJ4eM=\ngithub.com/google/renameio v0.1.0/go.mod h1:KWCgfxg9yswjAJkECMjeO8J8rahYeXnNhOm40UhjYkI=\ngithub.com/google/safehtml v0.0.2/go.mod h1:L4KWwDsUJdECRAEpZoBn3O64bQaywRscowZjJAzjHnU=\ngithub.com/googleapis/gax-go v0.0.0-20161107002406-da06d194a00e/go.mod h1:SFVmujtThgffbyetf+mdk2eWhX2bMyUtNHzFKcPA9HY=\ngithub.com/googleapis/gax-go/v2 v2.0.4/go.mod h1:0Wqv26UfaUD9n4G6kQubkQ+KchISgw+vpHVxEJEs9eg=\ngithub.com/googleapis/gax-go/v2 v2.0.5/go.mod h1:DWXyrwAJ9X0FpwwEdw+IPEYBICEFu5mhpdKc/us6bOk=\ngithub.com/gorilla/websocket v1.4.0/go.mod h1:E7qHFY5m1UJ88s3WnNqhKjPHQ0heANvMoAMk2YaljkQ=\ngithub.com/grpc-ecosystem/go-grpc-middleware v1.0.0/go.mod h1:FiyG127CGDf3tlThmgyCl78X/SZQqEOJBCDaAfeWzPs=\ngithub.com/grpc-ecosystem/go-grpc-prometheus v1.2.0/go.mod h1:8NvIoxWQoOIhqOTXgfV/d3M/q6VIi02HzZEHgUlZvzk=\ngithub.com/grpc-ecosystem/grpc-gateway v1.9.0/go.mod h1:vNeuVxBJEsws4ogUvrchl83t/GYV9WGTSLVdBhOQFDY=\ngithub.com/guptarohit/asciigraph v0.5.5 h1:ccFnUF8xYIOUPPY3tmdvRyHqmn1MYI9iv1pLKX+/ZkQ=\ngithub.com/guptarohit/asciigraph v0.5.5/go.mod h1:dYl5wwK4gNsnFf9Zp+l06rFiDZ5YtXM6x7SRWZ3KGag=\ngithub.com/hashicorp/golang-lru v0.5.0/go.mod h1:/m3WP610KZHVQ1SGc6re/UDhFvYD7pJ4Ao+sR/qLZy8=\ngithub.com/hashicorp/golang-lru v0.5.1/go.mod h1:/m3WP610KZHVQ1SGc6re/UDhFvYD7pJ4Ao+sR/qLZy8=\ngithub.com/hashicorp/hcl v1.0.0/go.mod h1:E5yfLk+7swimpb2L/Alb/PJmXilQ/rhwaUYs4T20WEQ=\ngithub.com/ianlancetaylor/demangle v0.0.0-20181102032728-5e5cf60278f6/go.mod h1:aSSvb/t6k1mPoxDqO4vJh6VOCGPwU4O0C2/Eqndh1Sc=\ngithub.com/inconshreveable/mousetrap v1.0.0 h1:Z8tu5sraLXCXIcARxBp/8cbvlwVa7Z1NHg9XEKhtSvM=\ngithub.com/inconshreveable/mousetrap v1.0.0/go.mod h1:PxqpIevigyE2G7u3NXJIT2ANytuPF1OarO4DADm73n8=\ngithub.com/jonboulle/clockwork v0.1.0/go.mod h1:Ii8DK3G1RaLaWxj9trq07+26W01tbo22gdxWY5EU2bo=\ngithub.com/jpillora/backoff v1.0.0/go.mod h1:J/6gKK9jxlEcS3zixgDgUAsiuZ7yrSoa/FX5e0EB2j4=\ngithub.com/json-iterator/go v1.1.6/go.mod h1:+SdeFBvtyEkXs7REEP0seUULqWtbJapLOCVDaaPEHmU=\ngithub.com/json-iterator/go v1.1.10/go.mod h1:KdQUCv79m/52Kvf8AW2vK1V8akMuk1QjK/uOdHXbAo4=\ngithub.com/json-iterator/go v1.1.11/go.mod h1:KdQUCv79m/52Kvf8AW2vK1V8akMuk1QjK/uOdHXbAo4=\ngithub.com/json-iterator/go v1.1.12/go.mod h1:e30LSqwooZae/UwlEbR2852Gd8hjQvJoHmT4TnhNGBo=\ngithub.com/jstemmer/go-junit-report v0.0.0-20190106144839-af01ea7f8024/go.mod h1:6v2b51hI/fHJwM22ozAgKL4VKDeJcHhJFhtBdhmNjmU=\ngithub.com/jstemmer/go-junit-report v0.9.1/go.mod h1:Brl9GWCQeLvo8nXZwPNNblvFj/XSXhF0NWZEnDohbsk=\ngithub.com/julienschmidt/httprouter v1.2.0/go.mod h1:SYymIcj16QtmaHHD7aYtjjsJG7VTCxuUUipMqKk8s4w=\ngithub.com/julienschmidt/httprouter v1.3.0/go.mod h1:JR6WtHb+2LUe8TCKY3cZOxFyyO8IZAc4RVcycCCAKdM=\ngithub.com/jung-kurt/gofpdf v1.0.0/go.mod h1:7Id9E/uU8ce6rXgefFLlgrJj/GYY22cpxn+r32jIOes=\ngithub.com/jung-kurt/gofpdf v1.0.3-0.20190309125859-24315acbbda5/go.mod h1:7Id9E/uU8ce6rXgefFLlgrJj/GYY22cpxn+r32jIOes=\ngithub.com/kisielk/errcheck v1.1.0/go.mod h1:EZBBE59ingxPouuu3KfxchcWSUPOHkagtvWXihfKN4Q=\ngithub.com/kisielk/errcheck v1.5.0/go.mod h1:pFxgyoBC7bSaBwPgfKdkLd5X25qrDl4LWUI2bnpBCr8=\ngithub.com/kisielk/gotool v1.0.0/go.mod h1:XhKaO+MFFWcvkIS/tQcRk01m1F5IRFswLeQ+oQHNcck=\ngithub.com/klauspost/compress v1.16.7 h1:2mk3MPGNzKyxErAw8YaohYh69+pa4sIQSC0fPGCFR9I=\ngithub.com/klauspost/compress v1.16.7/go.mod h1:ntbaceVETuRiXiv4DpjP66DpAtAGkEQskQzEyD//IeE=\ngithub.com/konsorten/go-windows-terminal-sequences v1.0.1/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ=\ngithub.com/konsorten/go-windows-terminal-sequences v1.0.3/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ=\ngithub.com/kr/logfmt v0.0.0-20140226030751-b84e30acd515/go.mod h1:+0opPa2QZZtGFBFZlji/RkVcI2GknAs/DXo4wKdlNEc=\ngithub.com/kr/pretty v0.1.0/go.mod h1:dAy3ld7l9f0ibDNOQOHHMYYIIbhfbHSm3C4ZsoJORNo=\ngithub.com/kr/pretty v0.3.1 h1:flRD4NNwYAUpkphVc1HcthR4KEIFJ65n8Mw5qdRn3LE=\ngithub.com/kr/pretty v0.3.1/go.mod h1:hoEshYVHaxMs3cyo3Yncou5ZscifuDolrwPKZanG3xk=\ngithub.com/kr/pty v1.1.1/go.mod h1:pFQYn66WHrOpPYNljwOMqo10TkYh1fy3cYio2l3bCsQ=\ngithub.com/kr/text v0.1.0/go.mod h1:4Jbv+DJW3UT/LiOwJeYQe1efqtUx/iVham/4vfdArNI=\ngithub.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=\ngithub.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=\ngithub.com/magiconair/properties v1.8.0/go.mod h1:PppfXfuXeibc/6YijjN8zIbojt8czPbwD3XqdrwzmxQ=\ngithub.com/mattn/go-runewidth v0.0.9 h1:Lm995f3rfxdpd6TSmuVCHVb/QhupuXlYr8sCI/QdE+0=\ngithub.com/mattn/go-runewidth v0.0.9/go.mod h1:H031xJmbD/WCDINGzjvQ9THkh0rPKHF+m2gUSrubnMI=\ngithub.com/mattn/go-sqlite3 v1.14.5/go.mod h1:WVKg1VTActs4Qso6iwGbiFih2UIHo0ENGwNd0Lj+XmI=\ngithub.com/matttproud/golang_protobuf_extensions v1.0.1/go.mod h1:D8He9yQNgCq6Z5Ld7szi9bcBfOoFv/3dc6xSMkL2PC0=\ngithub.com/matttproud/golang_protobuf_extensions v1.0.2-0.20181231171920-c182affec369 h1:I0XW9+e1XWDxdcEniV4rQAIOPUGDq67JSCiRCgGCZLI=\ngithub.com/matttproud/golang_protobuf_extensions v1.0.2-0.20181231171920-c182affec369/go.mod h1:BSXmuO+STAnVfrANrmjBb36TMTDstsz7MSK+HVaYKv4=\ngithub.com/mitchellh/go-homedir v1.1.0/go.mod h1:SfyaCUpYCn1Vlf4IUYiD9fPX4A5wJrkLzIz1N1q0pr0=\ngithub.com/mitchellh/mapstructure v1.1.2/go.mod h1:FVVH3fgwuzCH5S8UJGiWEs2h04kUh9fWfEaFds41c1Y=\ngithub.com/modern-go/concurrent v0.0.0-20180228061459-e0a39a4cb421/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=\ngithub.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=\ngithub.com/modern-go/reflect2 v0.0.0-20180701023420-4b7aa43c6742/go.mod h1:bx2lNnkwVCuqBIxFjflWJWanXIb3RllmbCylyMrvgv0=\ngithub.com/modern-go/reflect2 v1.0.1/go.mod h1:bx2lNnkwVCuqBIxFjflWJWanXIb3RllmbCylyMrvgv0=\ngithub.com/modern-go/reflect2 v1.0.2/go.mod h1:yWuevngMOJpCy52FWWMvUC8ws7m/LJsjYzDa0/r8luk=\ngithub.com/mwitkow/go-conntrack v0.0.0-20161129095857-cc309e4a2223/go.mod h1:qRWi+5nqEBWmkhHvq77mSJWrCKwh8bxhgT7d/eI7P4U=\ngithub.com/mwitkow/go-conntrack v0.0.0-20190716064945-2f068394615f/go.mod h1:qRWi+5nqEBWmkhHvq77mSJWrCKwh8bxhgT7d/eI7P4U=\ngithub.com/niemeyer/pretty v0.0.0-20200227124842-a10e7caefd8e/go.mod h1:zD1mROLANZcx1PVRCS0qkT7pwLkGfwJo4zjcN/Tysno=\ngithub.com/oklog/ulid v1.3.1/go.mod h1:CirwcVhetQ6Lv90oh/F+FBtV6XMibvdAFo93nm5qn4U=\ngithub.com/olekukonko/tablewriter v0.0.5 h1:P2Ga83D34wi1o9J6Wh1mRuqd4mF/x/lgBS7N7AbDhec=\ngithub.com/olekukonko/tablewriter v0.0.5/go.mod h1:hPp6KlRPjbx+hW8ykQs1w3UBbZlj6HuIJcUGPhkA7kY=\ngithub.com/pelletier/go-toml v1.2.0/go.mod h1:5z9KED0ma1S8pY6P1sdut58dfprrGBbd/94hg7ilaic=\ngithub.com/phpdave11/gofpdf v1.4.2/go.mod h1:zpO6xFn9yxo3YLyMvW8HcKWVdbNqgIfOOp2dXMnm1mY=\ngithub.com/phpdave11/gofpdi v1.0.12/go.mod h1:vBmVV0Do6hSBHC8uKUQ71JGW+ZGQq74llk/7bXwjDoI=\ngithub.com/phpdave11/gofpdi v1.0.13/go.mod h1:vBmVV0Do6hSBHC8uKUQ71JGW+ZGQq74llk/7bXwjDoI=\ngithub.com/pingcap/errors v0.11.4 h1:lFuQV/oaUMGcD2tqt+01ROSmJs75VG1ToEOkZIZ4nE4=\ngithub.com/pingcap/errors v0.11.4/go.mod h1:Oi8TUi2kEtXXLMJk9l1cGmz20kV3TaQ0usTwv5KuLY8=\ngithub.com/pkg/diff v0.0.0-20210226163009-20ebb0f2a09e/go.mod h1:pJLUxLENpZxwdsKMEsNbx1VGcRFpLqf3715MtcvvzbA=\ngithub.com/pkg/errors v0.8.0/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\ngithub.com/pkg/errors v0.8.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\ngithub.com/pkg/errors v0.9.1 h1:FEBLx1zS214owpjy7qsBeixbURkuhQAwrK5UwLGTwt4=\ngithub.com/pkg/errors v0.9.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=\ngithub.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\ngithub.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\ngithub.com/prometheus/client_golang v0.9.1/go.mod h1:7SWBe2y4D6OKWSNQJUaRYU/AaXPKyh/dDVn+NZz0KFw=\ngithub.com/prometheus/client_golang v0.9.3/go.mod h1:/TN21ttK/J9q6uSwhBd54HahCDft0ttaMvbicHlPoso=\ngithub.com/prometheus/client_golang v1.0.0/go.mod h1:db9x61etRT2tGnBNRi70OPL5FsnadC4Ky3P0J6CfImo=\ngithub.com/prometheus/client_golang v1.7.1/go.mod h1:PY5Wy2awLA44sXw4AOSfFBetzPP4j5+D6mVACh+pe2M=\ngithub.com/prometheus/client_golang v1.11.0/go.mod h1:Z6t4BnS23TR94PD6BsDNk8yVqroYurpAkEiz0P2BEV0=\ngithub.com/prometheus/client_golang v1.12.0 h1:C+UIj/QWtmqY13Arb8kwMt5j34/0Z2iKamrJ+ryC0Gg=\ngithub.com/prometheus/client_golang v1.12.0/go.mod h1:3Z9XVyYiZYEO+YQWt3RD2R3jrbd179Rt297l4aS6nDY=\ngithub.com/prometheus/client_model v0.0.0-20180712105110-5c3871d89910/go.mod h1:MbSGuTsp3dbXC40dX6PRTWyKYBIrTGTE9sqQNg2J8bo=\ngithub.com/prometheus/client_model v0.0.0-20190129233127-fd36f4220a90/go.mod h1:xMI15A0UPsDsEKsMN9yxemIoYk6Tm2C1GtYGdfGttqA=\ngithub.com/prometheus/client_model v0.0.0-20190812154241-14fe0d1b01d4/go.mod h1:xMI15A0UPsDsEKsMN9yxemIoYk6Tm2C1GtYGdfGttqA=\ngithub.com/prometheus/client_model v0.2.0/go.mod h1:xMI15A0UPsDsEKsMN9yxemIoYk6Tm2C1GtYGdfGttqA=\ngithub.com/prometheus/client_model v0.2.1-0.20210607210712-147c58e9608a h1:CmF68hwI0XsOQ5UwlBopMi2Ow4Pbg32akc4KIVCOm+Y=\ngithub.com/prometheus/client_model v0.2.1-0.20210607210712-147c58e9608a/go.mod h1:LDGWKZIo7rky3hgvBe+caln+Dr3dPggB5dvjtD7w9+w=\ngithub.com/prometheus/common v0.0.0-20181113130724-41aa239b4cce/go.mod h1:daVV7qP5qjZbuso7PdcryaAu0sAZbrN9i7WWcTMWvro=\ngithub.com/prometheus/common v0.4.0/go.mod h1:TNfzLD0ON7rHzMJeJkieUDPYmFC7Snx/y86RQel1bk4=\ngithub.com/prometheus/common v0.4.1/go.mod h1:TNfzLD0ON7rHzMJeJkieUDPYmFC7Snx/y86RQel1bk4=\ngithub.com/prometheus/common v0.10.0/go.mod h1:Tlit/dnDKsSWFlCLTWaA1cyBgKHSMdTB80sz/V91rCo=\ngithub.com/prometheus/common v0.26.0/go.mod h1:M7rCNAaPfAosfx8veZJCuw84e35h3Cfd9VFqTh1DIvc=\ngithub.com/prometheus/common v0.32.1 h1:hWIdL3N2HoUx3B8j3YN9mWor0qhY/NlEKZEaXxuIRh4=\ngithub.com/prometheus/common v0.32.1/go.mod h1:vu+V0TpY+O6vW9J44gczi3Ap/oXXR10b+M/gUGO4Hls=\ngithub.com/prometheus/procfs v0.0.0-20181005140218-185b4288413d/go.mod h1:c3At6R/oaqEKCNdg8wHV1ftS6bRYblBhIjjI8uT2IGk=\ngithub.com/prometheus/procfs v0.0.0-20190507164030-5867b95ac084/go.mod h1:TjEm7ze935MbeOT/UhFTIMYKhuLP4wbCsTZCD3I8kEA=\ngithub.com/prometheus/procfs v0.0.2/go.mod h1:TjEm7ze935MbeOT/UhFTIMYKhuLP4wbCsTZCD3I8kEA=\ngithub.com/prometheus/procfs v0.1.3/go.mod h1:lV6e/gmhEcM9IjHGsFOCxxuZ+z1YqCvr4OA4YeYWdaU=\ngithub.com/prometheus/procfs v0.6.0/go.mod h1:cz+aTbrPOrUb4q7XlbU9ygM+/jj0fzG6c1xBZuNvfVA=\ngithub.com/prometheus/procfs v0.7.3 h1:4jVXhlkAyzOScmCkXBTOLRLTz8EeU+eyjrwB/EPq0VU=\ngithub.com/prometheus/procfs v0.7.3/go.mod h1:cz+aTbrPOrUb4q7XlbU9ygM+/jj0fzG6c1xBZuNvfVA=\ngithub.com/prometheus/tsdb v0.7.1/go.mod h1:qhTCs0VvXwvX/y3TZrWD7rabWM+ijKTux40TwIPHuXU=\ngithub.com/rogpeppe/fastuuid v0.0.0-20150106093220-6724a57986af/go.mod h1:XWv6SoW27p1b0cqNHllgS5HIMJraePCO15w5zCzIWYg=\ngithub.com/rogpeppe/go-internal v1.3.0/go.mod h1:M8bDsm7K2OlrFYOpmOWEs/qY81heoFRclV5y23lUDJ4=\ngithub.com/rogpeppe/go-internal v1.9.0 h1:73kH8U+JUqXU8lRuOHeVHaa/SZPifC7BkcraZVejAe8=\ngithub.com/rogpeppe/go-internal v1.9.0/go.mod h1:WtVeX8xhTBvf0smdhujwtBcq4Qrzq/fJaraNFVN+nFs=\ngithub.com/russross/blackfriday/v2 v2.0.1/go.mod h1:+Rmxgy9KzJVeS9/2gXHxylqXiyQDYRxCVz55jmeOWTM=\ngithub.com/ruudk/golang-pdf417 v0.0.0-20181029194003-1af4ab5afa58/go.mod h1:6lfFZQK844Gfx8o5WFuvpxWRwnSoipWe/p622j1v06w=\ngithub.com/ruudk/golang-pdf417 v0.0.0-20201230142125-a7e3863a1245/go.mod h1:pQAZKsJ8yyVxGRWYNEm9oFB8ieLgKFnamEyDmSA0BRk=\ngithub.com/shurcooL/sanitized_anchor_name v1.0.0/go.mod h1:1NzhyTcUVG4SuEtjjoZeVRXNmyL/1OwPU0+IJeTBvfc=\ngithub.com/sirupsen/logrus v1.2.0/go.mod h1:LxeOpSwHxABJmUn/MG1IvRgCAasNZTLOkJPxbbu5VWo=\ngithub.com/sirupsen/logrus v1.4.2/go.mod h1:tLMulIdttU9McNUspp0xgXVQah82FyeX6MwdIuYE2rE=\ngithub.com/sirupsen/logrus v1.6.0/go.mod h1:7uNnSEd1DgxDLC74fIahvMZmmYsHGZGEOFrfsX/uA88=\ngithub.com/soheilhy/cmux v0.1.4/go.mod h1:IM3LyeVVIOuxMH7sFAkER9+bJ4dT7Ms6E4xg4kGIyLM=\ngithub.com/spaolacci/murmur3 v0.0.0-20180118202830-f09979ecbc72/go.mod h1:JwIasOWyU6f++ZhiEuf87xNszmSA2myDM2Kzu9HwQUA=\ngithub.com/spf13/afero v1.1.2/go.mod h1:j4pytiNVoe2o6bmDsKpLACNPDBIoEAkihy7loJ1B0CQ=\ngithub.com/spf13/cast v1.3.0/go.mod h1:Qx5cxh0v+4UWYiBimWS+eyWzqEqokIECu5etghLkUJE=\ngithub.com/spf13/cobra v1.0.0 h1:6m/oheQuQ13N9ks4hubMG6BnvwOeaJrqSPLahSnczz8=\ngithub.com/spf13/cobra v1.0.0/go.mod h1:/6GTrnGXV9HjY+aR4k0oJ5tcvakLuG6EuKReYlHNrgE=\ngithub.com/spf13/jwalterweatherman v1.0.0/go.mod h1:cQK4TGJAtQXfYWX+Ddv3mKDzgVb68N+wFjFa4jdeBTo=\ngithub.com/spf13/pflag v1.0.3/go.mod h1:DYY7MBk1bdzusC3SYhjObp+wFpr4gzcvqqNjLnInEg4=\ngithub.com/spf13/pflag v1.0.5 h1:iy+VFUOCP1a+8yFto/drg2CJ5u0yRoB7fZw3DKv/JXA=\ngithub.com/spf13/pflag v1.0.5/go.mod h1:McXfInJRrz4CZXVZOBLb0bTZqETkiAhM9Iw0y3An2Bg=\ngithub.com/spf13/viper v1.4.0/go.mod h1:PTJ7Z/lr49W6bUbkmS1V3by4uWynFiR9p7+dSq/yZzE=\ngithub.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\ngithub.com/stretchr/objx v0.1.1/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\ngithub.com/stretchr/testify v1.2.2/go.mod h1:a8OnRcib4nhh0OaRAV+Yts87kKdq0PP7pXfy6kDkUVs=\ngithub.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=\ngithub.com/stretchr/testify v1.4.0/go.mod h1:j7eGeouHqKxXV5pUuKE4zz7dFj8WfuZ+81PSLYec5m4=\ngithub.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\ngithub.com/stretchr/testify v1.9.0 h1:HtqpIVDClZ4nwg75+f6Lvsy/wHu+3BoSGCbBAcpTsTg=\ngithub.com/stretchr/testify v1.9.0/go.mod h1:r2ic/lqez/lEtzL7wO/rwa5dbSLXVDPFyf8C91i36aY=\ngithub.com/tmc/grpc-websocket-proxy v0.0.0-20190109142713-0ad062ec5ee5/go.mod h1:ncp9v5uamzpCO7NfCPTXjqaC+bZgJeR0sMTm6dMHP7U=\ngithub.com/ugorji/go v1.1.4/go.mod h1:uQMGLiO92mf5W77hV/PUCpI3pbzQx3CRekS0kk+RGrc=\ngithub.com/xiang90/probing v0.0.0-20190116061207-43a291ad63a2/go.mod h1:UETIi67q53MR2AWcXfiuqkDkRtnGDLqkBTpCHuJHxtU=\ngithub.com/xordataexchange/crypt v0.0.3-0.20170626215501-b2862e3d0a77/go.mod h1:aYKd//L2LvnjZzWKhF00oedf4jCCReLcmhLdhm1A27Q=\ngithub.com/yuin/goldmark v1.1.25/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=\ngithub.com/yuin/goldmark v1.1.27/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=\ngithub.com/yuin/goldmark v1.1.32/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=\ngithub.com/yuin/goldmark v1.2.1/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=\ngo.etcd.io/bbolt v1.3.2/go.mod h1:IbVyRI1SCnLcuJnV2u8VeU0CEYM7e686BmAb1XKL+uU=\ngo.opencensus.io v0.21.0/go.mod h1:mSImk1erAIZhrmZN+AvHh14ztQfjbGwt4TtuofqLduU=\ngo.opencensus.io v0.22.0/go.mod h1:+kGneAE2xo2IficOXnaByMWTGM9T73dGwxeWcUqIpI8=\ngo.opencensus.io v0.22.2/go.mod h1:yxeiOL68Rb0Xd1ddK5vPZ/oVn4vY4Ynel7k9FzqtOIw=\ngo.opencensus.io v0.22.3/go.mod h1:yxeiOL68Rb0Xd1ddK5vPZ/oVn4vY4Ynel7k9FzqtOIw=\ngo.opencensus.io v0.22.4/go.mod h1:yxeiOL68Rb0Xd1ddK5vPZ/oVn4vY4Ynel7k9FzqtOIw=\ngo.uber.org/atomic v1.4.0/go.mod h1:gD2HeocX3+yG+ygLZcrzQJaqmWj9AIm7n08wl/qW/PE=\ngo.uber.org/multierr v1.1.0/go.mod h1:wR5kodmAFQ0UK8QlbwjlSNy0Z68gJhDJUG5sjR94q/0=\ngo.uber.org/zap v1.10.0/go.mod h1:vwi/ZaCAaUcBkycHslxD9B2zi4UTXhF60s6SWpuDF0Q=\ngolang.org/x/crypto v0.0.0-20180904163835-0709b304e793/go.mod h1:6SG95UA2DQfeDnfUPMdvaQW0Q7yPrPDi9nlGo2tz2b4=\ngolang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=\ngolang.org/x/crypto v0.0.0-20190510104115-cbcb75029529/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\ngolang.org/x/crypto v0.0.0-20190605123033-f99c8df09eb5/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\ngolang.org/x/crypto v0.0.0-20191011191535-87dc89f01550/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\ngolang.org/x/crypto v0.0.0-20200622213623-75b288015ac9/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=\ngolang.org/x/exp v0.0.0-20180321215751-8460e604b9de/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\ngolang.org/x/exp v0.0.0-20180807140117-3d87b88a115f/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\ngolang.org/x/exp v0.0.0-20190121172915-509febef88a4/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\ngolang.org/x/exp v0.0.0-20190125153040-c74c464bbbf2/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\ngolang.org/x/exp v0.0.0-20190306152737-a1d7652674e8/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=\ngolang.org/x/exp v0.0.0-20190510132918-efd6b22b2522/go.mod h1:ZjyILWgesfNpC6sMxTJOJm9Kp84zZh5NQWvqDGG3Qr8=\ngolang.org/x/exp v0.0.0-20190829153037-c13cbed26979/go.mod h1:86+5VVa7VpoJ4kLfm080zCjGlMRFzhUhsZKEZO7MGek=\ngolang.org/x/exp v0.0.0-20191002040644-a1355ae1e2c3/go.mod h1:NOZ3BPKG0ec/BKJQgnvsSFpcKLM5xXVWnvZS97DWHgE=\ngolang.org/x/exp v0.0.0-20191030013958-a1ab85dbe136/go.mod h1:JXzH8nQsPlswgeRAPE3MuO9GYsAcnJvJ4vnMwN/5qkY=\ngolang.org/x/exp v0.0.0-20191129062945-2f5052295587/go.mod h1:2RIsYlXP63K8oxa1u096TMicItID8zy7Y6sNkU49FU4=\ngolang.org/x/exp v0.0.0-20191227195350-da58074b4299/go.mod h1:2RIsYlXP63K8oxa1u096TMicItID8zy7Y6sNkU49FU4=\ngolang.org/x/exp v0.0.0-20200119233911-0405dc783f0a/go.mod h1:2RIsYlXP63K8oxa1u096TMicItID8zy7Y6sNkU49FU4=\ngolang.org/x/exp v0.0.0-20200207192155-f17229e696bd/go.mod h1:J/WKrq2StrnmMY6+EHIKF9dgMWnmCNThgcyBT1FY9mM=\ngolang.org/x/exp v0.0.0-20200224162631-6cc2880d07d6/go.mod h1:3jZMyOhIsHpP37uCMkUooju7aAi5cS1Q23tOzKc+0MU=\ngolang.org/x/exp v0.0.0-20230626212559-97b1e661b5df h1:UA2aFVmmsIlefxMk29Dp2juaUSth8Pyn3Tq5Y5mJGME=\ngolang.org/x/exp v0.0.0-20230626212559-97b1e661b5df/go.mod h1:FXUEEKJgO7OQYeo8N01OfiKP8RXMtf6e8aTskBGqWdc=\ngolang.org/x/image v0.0.0-20180708004352-c73c2afc3b81/go.mod h1:ux5Hcp/YLpHSI86hEcLt0YII63i6oz57MZXIpbrjZUs=\ngolang.org/x/image v0.0.0-20190227222117-0694c2d4d067/go.mod h1:kZ7UVZpmo3dzQBMxlp+ypCbDeSB+sBbTgSJuh5dn5js=\ngolang.org/x/image v0.0.0-20190802002840-cff245a6509b/go.mod h1:FeLwcggjj3mMvU+oOTbSwawSJRM1uh48EjtB4UJZlP0=\ngolang.org/x/image v0.0.0-20190910094157-69e4b8554b2a/go.mod h1:FeLwcggjj3mMvU+oOTbSwawSJRM1uh48EjtB4UJZlP0=\ngolang.org/x/image v0.0.0-20200119044424-58c23975cae1/go.mod h1:FeLwcggjj3mMvU+oOTbSwawSJRM1uh48EjtB4UJZlP0=\ngolang.org/x/image v0.0.0-20200430140353-33d19683fad8/go.mod h1:FeLwcggjj3mMvU+oOTbSwawSJRM1uh48EjtB4UJZlP0=\ngolang.org/x/image v0.0.0-20200618115811-c13761719519/go.mod h1:FeLwcggjj3mMvU+oOTbSwawSJRM1uh48EjtB4UJZlP0=\ngolang.org/x/image v0.0.0-20201208152932-35266b937fa6/go.mod h1:FeLwcggjj3mMvU+oOTbSwawSJRM1uh48EjtB4UJZlP0=\ngolang.org/x/image v0.0.0-20210216034530-4410531fe030/go.mod h1:FeLwcggjj3mMvU+oOTbSwawSJRM1uh48EjtB4UJZlP0=\ngolang.org/x/image v0.0.0-20210607152325-775e3b0c77b9/go.mod h1:023OzeP/+EPmXeapQh35lcL3II3LrY8Ic+EFFKVhULM=\ngolang.org/x/image v0.0.0-20210628002857-a66eb6448b8d/go.mod h1:023OzeP/+EPmXeapQh35lcL3II3LrY8Ic+EFFKVhULM=\ngolang.org/x/lint v0.0.0-20181026193005-c67002cb31c3/go.mod h1:UVdnD1Gm6xHRNCYTkRU2/jEulfH38KcIWyp/GAMgvoE=\ngolang.org/x/lint v0.0.0-20190227174305-5b3e6a55c961/go.mod h1:wehouNa3lNwaWXcvxsM5YxQ5yQlVC4a0KAMCusXpPoU=\ngolang.org/x/lint v0.0.0-20190301231843-5614ed5bae6f/go.mod h1:UVdnD1Gm6xHRNCYTkRU2/jEulfH38KcIWyp/GAMgvoE=\ngolang.org/x/lint v0.0.0-20190313153728-d0100b6bd8b3/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\ngolang.org/x/lint v0.0.0-20190409202823-959b441ac422/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\ngolang.org/x/lint v0.0.0-20190909230951-414d861bb4ac/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\ngolang.org/x/lint v0.0.0-20190930215403-16217165b5de/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=\ngolang.org/x/lint v0.0.0-20191125180803-fdd1cda4f05f/go.mod h1:5qLYkcX4OjUUV8bRuDixDT3tpyyb+LUpUlRWLxfhWrs=\ngolang.org/x/lint v0.0.0-20200130185559-910be7a94367/go.mod h1:3xt1FjdF8hUf6vQPIChWIBhFzV8gjjsPE/fR3IyQdNY=\ngolang.org/x/lint v0.0.0-20200302205851-738671d3881b/go.mod h1:3xt1FjdF8hUf6vQPIChWIBhFzV8gjjsPE/fR3IyQdNY=\ngolang.org/x/mobile v0.0.0-20190312151609-d3739f865fa6/go.mod h1:z+o9i4GpDbdi3rU15maQ/Ox0txvL9dWGYEHz965HBQE=\ngolang.org/x/mobile v0.0.0-20190719004257-d2bd2a29d028/go.mod h1:E/iHnbuqvinMTCcRqshq8CkpyQDoeVncDDYHnLhea+o=\ngolang.org/x/mod v0.0.0-20190513183733-4bf6d317e70e/go.mod h1:mXi4GBBbnImb6dmsKGUJ2LatrhH/nqhxcFungHvyanc=\ngolang.org/x/mod v0.1.0/go.mod h1:0QHyrYULN0/3qlju5TqG8bIK38QM8yzMo5ekMj3DlcY=\ngolang.org/x/mod v0.1.1-0.20191105210325-c90efee705ee/go.mod h1:QqPTAvyqsEbceGzBzNggFXnrqF1CaUcvgkdR5Ot7KZg=\ngolang.org/x/mod v0.1.1-0.20191107180719-034126e5016b/go.mod h1:QqPTAvyqsEbceGzBzNggFXnrqF1CaUcvgkdR5Ot7KZg=\ngolang.org/x/mod v0.2.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=\ngolang.org/x/mod v0.3.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=\ngolang.org/x/net v0.0.0-20180724234803-3673e40ba225/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20180826012351-8a410e7b638d/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20181114220301-adae6a3d119a/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20181220203305-927f97764cc3/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20190108225652-1e06a53dbb7e/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20190213061140-3a22650c66bd/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=\ngolang.org/x/net v0.0.0-20190311183353-d8887717615a/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\ngolang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\ngolang.org/x/net v0.0.0-20190501004415-9ce7a6920f09/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\ngolang.org/x/net v0.0.0-20190503192946-f4e77d36d62c/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=\ngolang.org/x/net v0.0.0-20190522155817-f3200d17e092/go.mod h1:HSz+uSET+XFnRR8LxR5pz3Of3rY3CfYBVs4xY44aLks=\ngolang.org/x/net v0.0.0-20190603091049-60506f45cf65/go.mod h1:HSz+uSET+XFnRR8LxR5pz3Of3rY3CfYBVs4xY44aLks=\ngolang.org/x/net v0.0.0-20190613194153-d28f0bde5980/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20190628185345-da137c7871d7/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20190724013045-ca1201d0de80/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20191209160850-c0dbc17a3553/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200114155413-6afb5195e5aa/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200202094626-16171245cfb2/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200222125558-5a598a2470a0/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200226121028-0de0cce0169b/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200301022130-244492dfa37a/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\ngolang.org/x/net v0.0.0-20200324143707-d3edc9973b7e/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\ngolang.org/x/net v0.0.0-20200501053045-e0ff5e5a1de5/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\ngolang.org/x/net v0.0.0-20200506145744-7e3656a0809f/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\ngolang.org/x/net v0.0.0-20200513185701-a91f0712d120/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\ngolang.org/x/net v0.0.0-20200520182314-0ba52f642ac2/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=\ngolang.org/x/net v0.0.0-20200625001655-4c5254603344/go.mod h1:/O7V0waA8r7cgGh81Ro3o1hOxt32SMVPicZroKQ2sZA=\ngolang.org/x/net v0.0.0-20200707034311-ab3426394381/go.mod h1:/O7V0waA8r7cgGh81Ro3o1hOxt32SMVPicZroKQ2sZA=\ngolang.org/x/net v0.0.0-20200822124328-c89045814202/go.mod h1:/O7V0waA8r7cgGh81Ro3o1hOxt32SMVPicZroKQ2sZA=\ngolang.org/x/net v0.0.0-20201021035429-f5854403a974/go.mod h1:sp8m0HH+o8qH0wwXwYZr8TS3Oi6o0r6Gce1SSxlDquU=\ngolang.org/x/net v0.0.0-20210525063256-abc453219eb5/go.mod h1:9nx3DQGgdP8bBQD5qxJ1jj9UTztislL4KSBs9R2vV5Y=\ngolang.org/x/oauth2 v0.0.0-20170207211851-4464e7848382/go.mod h1:N/0e6XlmueqKjAGxoOufVs8QHGRruUQn6yWY3a++T0U=\ngolang.org/x/oauth2 v0.0.0-20180821212333-d2e6202438be/go.mod h1:N/0e6XlmueqKjAGxoOufVs8QHGRruUQn6yWY3a++T0U=\ngolang.org/x/oauth2 v0.0.0-20190226205417-e64efc72b421/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\ngolang.org/x/oauth2 v0.0.0-20190604053449-0f29369cfe45/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\ngolang.org/x/oauth2 v0.0.0-20191202225959-858c2ad4c8b6/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\ngolang.org/x/oauth2 v0.0.0-20200107190931-bf48bf16ab8d/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=\ngolang.org/x/oauth2 v0.0.0-20210514164344-f6687ab2804c/go.mod h1:KelEdhl1UZF7XfJ4dDtk6s++YSgaE7mD/BuKKDLBl4A=\ngolang.org/x/perf v0.0.0-20230113213139-801c7ef9e5c5 h1:ObuXPmIgI4ZMyQLIz48cJYgSyWdjUXc2SZAdyJMwEAU=\ngolang.org/x/perf v0.0.0-20230113213139-801c7ef9e5c5/go.mod h1:UBKtEnL8aqnd+0JHqZ+2qoMDwtuy6cYhhKNoHLBiTQc=\ngolang.org/x/sync v0.0.0-20180314180146-1d60e4601c6f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20181108010431-42b317875d0f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20181221193216-37e7f081c4d4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20190227155943-e225da77a7e6/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20190911185100-cd5d95a43a6e/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20200317015054-43a5402ce75a/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20200625203802-6e8e738ad208/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20201020160332-67f06af15bc9/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.0.0-20201207232520-09787c993a3a/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\ngolang.org/x/sync v0.7.0 h1:YsImfSBoP9QPYL0xyKJPq0gcaJdG3rInoqxTWbfQu9M=\ngolang.org/x/sync v0.7.0/go.mod h1:Czt+wKu1gCyEFDUtn0jG5QVvpJ6rzVqr5aXyt9drQfk=\ngolang.org/x/sys v0.0.0-20180830151530-49385e6e1522/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20180905080454-ebe1bf3edb33/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20181107165924-66b7b1311ac8/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20181116152217-5ac8a444bdc5/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=\ngolang.org/x/sys v0.0.0-20190312061237-fead79001313/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190412213103-97732733099d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190422165155-953cdadca894/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190502145724-3ef323f4f1fd/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190507160741-ecd444e8653b/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190606165138-5da285871e9c/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190624142023-c5567b49c5d0/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20190726091711-fc99dfbffb4e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20191001151750-bb3f8db39f24/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20191204072324-ce4227a45e2e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20191228213918-04cbcbbfeed8/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200106162015-b016eb3dc98e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200113162924-86b910548bc1/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200122134326-e047566fdf82/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200202164722-d101bd2416d5/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200212091648-12a6c2dcc1e4/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200223170610-d5e6a3e2c0ae/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200302150141-5c8b2ff67527/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200323222414-85ca7c5b95cd/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200331124033-c3d80250170d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200501052902-10377860bb8e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200511232937-7e40ca221e25/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200515095857-1151b9dac4a9/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200523222454-059865788121/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200615200032-f1bc736245b1/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200625212154-ddb9806d33ae/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200803210538-64077c9b5642/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20200930185726-fdedc70b468f/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20201119102817-f84b799fce68/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210124154548-22da62e12c0c/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210304124612-50617c2ba197/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210423082822-04245dca01da/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=\ngolang.org/x/sys v0.0.0-20210603081109-ebe580a85c40/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.0.0-20220114195835-da31bd327af9/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.18.0 h1:DBdB3niSjOA/O0blCZBqDefyWNYveAYMNF1Wum0DYQ4=\ngolang.org/x/sys v0.18.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=\ngolang.org/x/term v0.0.0-20201126162022-7de9c90e9dd1/go.mod h1:bj7SfCRtBDWHUb9snDiAeCFNEtKQo2Wmx5Cou7ajbmo=\ngolang.org/x/text v0.0.0-20170915032832-14c0d48ead0c/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\ngolang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\ngolang.org/x/text v0.3.1-0.20180807135948-17ff2d5776d2/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\ngolang.org/x/text v0.3.2/go.mod h1:bEr9sfX3Q8Zfm5fL9x+3itogRgK3+ptLWKqgva+5dAk=\ngolang.org/x/text v0.3.3/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\ngolang.org/x/text v0.3.5/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\ngolang.org/x/text v0.3.6/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\ngolang.org/x/text v0.14.0 h1:ScX5w1eTa3QqT8oi6+ziP7dTV1S2+ALU0bI+0zXKWiQ=\ngolang.org/x/text v0.14.0/go.mod h1:18ZOQIKpY8NJVqYksKHtTdi31H5itFRjB5/qKTNYzSU=\ngolang.org/x/time v0.0.0-20181108054448-85acf8d2951c/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\ngolang.org/x/time v0.0.0-20190308202827-9d24e82272b4/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\ngolang.org/x/time v0.0.0-20191024005414-555d28b269f0/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=\ngolang.org/x/tools v0.0.0-20180221164845-07fd8470d635/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20180525024113-a5b4c53f6e8b/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20190114222345-bf090417da8b/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20190206041539-40960b6deb8e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\ngolang.org/x/tools v0.0.0-20190226205152-f727befe758c/go.mod h1:9Yl7xja0Znq3iFh3HoIrodX9oNMXvdceNzlUR8zjMvY=\ngolang.org/x/tools v0.0.0-20190311212946-11955173bddd/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\ngolang.org/x/tools v0.0.0-20190312151545-0bb0c0a6e846/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\ngolang.org/x/tools v0.0.0-20190312170243-e65039ee4138/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=\ngolang.org/x/tools v0.0.0-20190425150028-36563e24a262/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=\ngolang.org/x/tools v0.0.0-20190506145303-2d16b83fe98c/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=\ngolang.org/x/tools v0.0.0-20190524140312-2c0ae7006135/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=\ngolang.org/x/tools v0.0.0-20190606124116-d0a3d012864b/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=\ngolang.org/x/tools v0.0.0-20190621195816-6e04913cbbac/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=\ngolang.org/x/tools v0.0.0-20190628153133-6cdbf07be9d0/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=\ngolang.org/x/tools v0.0.0-20190816200558-6889da9d5479/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20190911174233-4f2ddba30aff/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20190927191325-030b2cf1153e/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191012152004-8de300cfc20a/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191113191852-77e3bb0ad9e7/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191115202509-3a792d9c32b2/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191119224855-298f0cb1881e/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191125144606-a911d9008d1f/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191130070609-6e064ea0cf2d/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\ngolang.org/x/tools v0.0.0-20191216173652-a0e659d51361/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20191227053925-7b8e75db28f4/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200117161641-43d50277825c/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200122220014-bf1340f18c4a/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200130002326-2f3ba24bd6e7/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200204074204-1cc6d1ef6c74/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200207183749-b753a1ba74fa/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200212150539-ea181f53ac56/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200224181240-023911ca70b2/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200227222343-706bc42d1f0d/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=\ngolang.org/x/tools v0.0.0-20200304193943-95d2e580d8eb/go.mod h1:o4KQGtdN14AW+yjsvvwRTJJuXz8XRtIHtEnmAXLyFUw=\ngolang.org/x/tools v0.0.0-20200312045724-11d5b4c81c7d/go.mod h1:o4KQGtdN14AW+yjsvvwRTJJuXz8XRtIHtEnmAXLyFUw=\ngolang.org/x/tools v0.0.0-20200331025713-a30bf2db82d4/go.mod h1:Sl4aGygMT6LrqrWclx+PTx3U+LnKx/seiNR+3G19Ar8=\ngolang.org/x/tools v0.0.0-20200501065659-ab2804fb9c9d/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=\ngolang.org/x/tools v0.0.0-20200512131952-2bc93b1c0c88/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=\ngolang.org/x/tools v0.0.0-20200515010526-7d3b6ebf133d/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=\ngolang.org/x/tools v0.0.0-20200618134242-20370b0cb4b2/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=\ngolang.org/x/tools v0.0.0-20200619180055-7c47624df98f/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=\ngolang.org/x/tools v0.0.0-20200729194436-6467de6f59a7/go.mod h1:njjCfa9FT2d7l9Bc6FUM5FLjQPp3cFF28FI3qnDFljA=\ngolang.org/x/tools v0.0.0-20200804011535-6c149bb5ef0d/go.mod h1:njjCfa9FT2d7l9Bc6FUM5FLjQPp3cFF28FI3qnDFljA=\ngolang.org/x/tools v0.0.0-20200825202427-b303f430e36d/go.mod h1:njjCfa9FT2d7l9Bc6FUM5FLjQPp3cFF28FI3qnDFljA=\ngolang.org/x/tools v0.0.0-20210106214847-113979e3529a/go.mod h1:emZCQorbCU4vsT4fOWvOPXz4eW1wZW4PmDk9uLelYpA=\ngolang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20191011141410-1b5146add898/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngolang.org/x/xerrors v0.0.0-20200804184101-5ec99f83aff1/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngonum.org/v1/gonum v0.0.0-20180816165407-929014505bf4/go.mod h1:Y+Yx5eoAFn32cQvJDxZx5Dpnq+c3wtXuadVZAcxbbBo=\ngonum.org/v1/gonum v0.8.2/go.mod h1:oe/vMfY3deqTw+1EZJhuvEW2iwGF1bW9wwu7XCu0+v0=\ngonum.org/v1/gonum v0.9.3 h1:DnoIG+QAMaF5NvxnGe/oKsgKcAc6PcUyl8q0VetfQ8s=\ngonum.org/v1/gonum v0.9.3/go.mod h1:TZumC3NeyVQskjXqmyWt4S3bINhy7B4eYwW69EbyX+0=\ngonum.org/v1/netlib v0.0.0-20190313105609-8cb42192e0e0/go.mod h1:wa6Ws7BG/ESfp6dHfk7C6KdzKA7wR7u/rKwOGE66zvw=\ngonum.org/v1/plot v0.0.0-20190515093506-e2840ee46a6b/go.mod h1:Wt8AAjI+ypCyYX3nZBvf6cAIx93T+c/OS2HFAYskSZc=\ngonum.org/v1/plot v0.9.0/go.mod h1:3Pcqqmp6RHvJI72kgb8fThyUnav364FOsdDo2aGW5lY=\ngonum.org/v1/plot v0.10.0/go.mod h1:JWIHJ7U20drSQb/aDpTetJzfC1KlAPldJLpkSy88dvQ=\ngoogle.golang.org/api v0.0.0-20170206182103-3d017632ea10/go.mod h1:4mhQ8q/RsB7i+udVvVy5NUi08OU8ZlA0gRVgrF7VFY0=\ngoogle.golang.org/api v0.4.0/go.mod h1:8k5glujaEP+g9n7WNsDg8QP6cUVNI86fCNMcbazEtwE=\ngoogle.golang.org/api v0.7.0/go.mod h1:WtwebWUNSVBH/HAw79HIFXZNqEvBhG+Ra+ax0hx3E3M=\ngoogle.golang.org/api v0.8.0/go.mod h1:o4eAsZoiT+ibD93RtjEohWalFOjRDx6CVaqeizhEnKg=\ngoogle.golang.org/api v0.9.0/go.mod h1:o4eAsZoiT+ibD93RtjEohWalFOjRDx6CVaqeizhEnKg=\ngoogle.golang.org/api v0.13.0/go.mod h1:iLdEw5Ide6rF15KTC1Kkl0iskquN2gFfn9o9XIsbkAI=\ngoogle.golang.org/api v0.14.0/go.mod h1:iLdEw5Ide6rF15KTC1Kkl0iskquN2gFfn9o9XIsbkAI=\ngoogle.golang.org/api v0.15.0/go.mod h1:iLdEw5Ide6rF15KTC1Kkl0iskquN2gFfn9o9XIsbkAI=\ngoogle.golang.org/api v0.17.0/go.mod h1:BwFmGc8tA3vsd7r/7kR8DY7iEEGSU04BFxCo5jP/sfE=\ngoogle.golang.org/api v0.18.0/go.mod h1:BwFmGc8tA3vsd7r/7kR8DY7iEEGSU04BFxCo5jP/sfE=\ngoogle.golang.org/api v0.19.0/go.mod h1:BwFmGc8tA3vsd7r/7kR8DY7iEEGSU04BFxCo5jP/sfE=\ngoogle.golang.org/api v0.20.0/go.mod h1:BwFmGc8tA3vsd7r/7kR8DY7iEEGSU04BFxCo5jP/sfE=\ngoogle.golang.org/api v0.22.0/go.mod h1:BwFmGc8tA3vsd7r/7kR8DY7iEEGSU04BFxCo5jP/sfE=\ngoogle.golang.org/api v0.24.0/go.mod h1:lIXQywCXRcnZPGlsd8NbLnOjtAoL6em04bJ9+z0MncE=\ngoogle.golang.org/api v0.28.0/go.mod h1:lIXQywCXRcnZPGlsd8NbLnOjtAoL6em04bJ9+z0MncE=\ngoogle.golang.org/api v0.29.0/go.mod h1:Lcubydp8VUV7KeIHD9z2Bys/sm/vGKnG1UHuDBSrHWM=\ngoogle.golang.org/api v0.30.0/go.mod h1:QGmEvQ87FHZNiUVJkT14jQNYJ4ZJjdRF23ZXz5138Fc=\ngoogle.golang.org/appengine v1.1.0/go.mod h1:EbEs0AVv82hx2wNQdGPgUI5lhzA/G0D9YwlJXL52JkM=\ngoogle.golang.org/appengine v1.4.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=\ngoogle.golang.org/appengine v1.5.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=\ngoogle.golang.org/appengine v1.6.1/go.mod h1:i06prIuMbXzDqacNJfV5OdTW448YApPu5ww/cMBSeb0=\ngoogle.golang.org/appengine v1.6.5/go.mod h1:8WjMMxjGQR8xUklV/ARdw2HLXBOI7O7uCIDZVag1xfc=\ngoogle.golang.org/appengine v1.6.6/go.mod h1:8WjMMxjGQR8xUklV/ARdw2HLXBOI7O7uCIDZVag1xfc=\ngoogle.golang.org/genproto v0.0.0-20180817151627-c66870c02cf8/go.mod h1:JiN7NxoALGmiZfu7CAH4rXhgtRTLTxftemlI0sWmxmc=\ngoogle.golang.org/genproto v0.0.0-20190307195333-5fe7a883aa19/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\ngoogle.golang.org/genproto v0.0.0-20190418145605-e7d98fc518a7/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\ngoogle.golang.org/genproto v0.0.0-20190425155659-357c62f0e4bb/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\ngoogle.golang.org/genproto v0.0.0-20190502173448-54afdca5d873/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=\ngoogle.golang.org/genproto v0.0.0-20190801165951-fa694d86fc64/go.mod h1:DMBHOl98Agz4BDEuKkezgsaosCRResVns1a3J2ZsMNc=\ngoogle.golang.org/genproto v0.0.0-20190819201941-24fa4b261c55/go.mod h1:DMBHOl98Agz4BDEuKkezgsaosCRResVns1a3J2ZsMNc=\ngoogle.golang.org/genproto v0.0.0-20190911173649-1774047e7e51/go.mod h1:IbNlFCBrqXvoKpeg0TB2l7cyZUmoaFKYIwrEpbDKLA8=\ngoogle.golang.org/genproto v0.0.0-20191108220845-16a3f7862a1a/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\ngoogle.golang.org/genproto v0.0.0-20191115194625-c23dd37a84c9/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\ngoogle.golang.org/genproto v0.0.0-20191216164720-4f79533eabd1/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\ngoogle.golang.org/genproto v0.0.0-20191230161307-f3c370f40bfb/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\ngoogle.golang.org/genproto v0.0.0-20200115191322-ca5a22157cba/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\ngoogle.golang.org/genproto v0.0.0-20200122232147-0452cf42e150/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=\ngoogle.golang.org/genproto v0.0.0-20200204135345-fa8e72b47b90/go.mod h1:GmwEX6Z4W5gMy59cAlVYjN9JhxgbQH6Gn+gFDQe2lzA=\ngoogle.golang.org/genproto v0.0.0-20200212174721-66ed5ce911ce/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200224152610-e50cd9704f63/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200228133532-8c2c7df3a383/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200305110556-506484158171/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200312145019-da6875a35672/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200331122359-1ee6d9798940/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200430143042-b979b6f78d84/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200511104702-f5ebc3bea380/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=\ngoogle.golang.org/genproto v0.0.0-20200515170657-fc4c6c6a6587/go.mod h1:YsZOwe1myG/8QRHRsmBRE1LrgQY60beZKjly0O1fX9U=\ngoogle.golang.org/genproto v0.0.0-20200526211855-cb27e3aa2013/go.mod h1:NbSheEEYHJ7i3ixzK3sjbqSGDJWnxyFXZblF3eUsNvo=\ngoogle.golang.org/genproto v0.0.0-20200618031413-b414f8b61790/go.mod h1:jDfRM7FcilCzHH/e9qn6dsT145K34l5v+OpcnNgKAAA=\ngoogle.golang.org/genproto v0.0.0-20200729003335-053ba62fc06f/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\ngoogle.golang.org/genproto v0.0.0-20200804131852-c06518451d9c/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\ngoogle.golang.org/genproto v0.0.0-20200825200019-8632dd797987/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=\ngoogle.golang.org/grpc v0.0.0-20170208002647-2a6bf6142e96/go.mod h1:yo6s7OP7yaDglbqo1J04qKzAhqBH6lvTonzMVmEdcZw=\ngoogle.golang.org/grpc v1.19.0/go.mod h1:mqu4LbDTu4XGKhr4mRzUsmM4RtVoemTSY81AxZiDr8c=\ngoogle.golang.org/grpc v1.20.1/go.mod h1:10oTOabMzJvdu6/UiuZezV6QK5dSlG84ov/aaiqXj38=\ngoogle.golang.org/grpc v1.21.0/go.mod h1:oYelfM1adQP15Ek0mdvEgi9Df8B9CZIaU1084ijfRaM=\ngoogle.golang.org/grpc v1.21.1/go.mod h1:oYelfM1adQP15Ek0mdvEgi9Df8B9CZIaU1084ijfRaM=\ngoogle.golang.org/grpc v1.23.0/go.mod h1:Y5yQAOtifL1yxbo5wqy6BxZv8vAUGQwXBOALyacEbxg=\ngoogle.golang.org/grpc v1.25.1/go.mod h1:c3i+UQWmh7LiEpx4sFZnkU36qjEYZ0imhYfXVyQciAY=\ngoogle.golang.org/grpc v1.26.0/go.mod h1:qbnxyOmOxrQa7FizSgH+ReBfzJrCY1pSN7KXBS8abTk=\ngoogle.golang.org/grpc v1.27.0/go.mod h1:qbnxyOmOxrQa7FizSgH+ReBfzJrCY1pSN7KXBS8abTk=\ngoogle.golang.org/grpc v1.27.1/go.mod h1:qbnxyOmOxrQa7FizSgH+ReBfzJrCY1pSN7KXBS8abTk=\ngoogle.golang.org/grpc v1.28.0/go.mod h1:rpkK4SK4GF4Ach/+MFLZUBavHOvF2JJB5uozKKal+60=\ngoogle.golang.org/grpc v1.29.1/go.mod h1:itym6AZVZYACWQqET3MqgPpjcuV5QH3BxFS3IjizoKk=\ngoogle.golang.org/grpc v1.30.0/go.mod h1:N36X2cJ7JwdamYAgDz+s+rVMFjt3numwzf/HckM8pak=\ngoogle.golang.org/grpc v1.31.0/go.mod h1:N36X2cJ7JwdamYAgDz+s+rVMFjt3numwzf/HckM8pak=\ngoogle.golang.org/protobuf v0.0.0-20200109180630-ec00e32a8dfd/go.mod h1:DFci5gLYBciE7Vtevhsrf46CRTquxDuWsQurQQe4oz8=\ngoogle.golang.org/protobuf v0.0.0-20200221191635-4d8936d0db64/go.mod h1:kwYJMbMJ01Woi6D6+Kah6886xMZcty6N08ah7+eCXa0=\ngoogle.golang.org/protobuf v0.0.0-20200228230310-ab0ca4ff8a60/go.mod h1:cfTl7dwQJ+fmap5saPgwCLgHXTUD7jkjRqWcaiX5VyM=\ngoogle.golang.org/protobuf v1.20.1-0.20200309200217-e05f789c0967/go.mod h1:A+miEFZTKqfCUM6K7xSMQL9OKL/b6hQv+e19PK+JZNE=\ngoogle.golang.org/protobuf v1.21.0/go.mod h1:47Nbq4nVaFHyn7ilMalzfO3qCViNmqZ2kzikPIcrTAo=\ngoogle.golang.org/protobuf v1.22.0/go.mod h1:EGpADcykh3NcUnDUJcl1+ZksZNG86OlYog2l/sGQquU=\ngoogle.golang.org/protobuf v1.23.0/go.mod h1:EGpADcykh3NcUnDUJcl1+ZksZNG86OlYog2l/sGQquU=\ngoogle.golang.org/protobuf v1.23.1-0.20200526195155-81db48ad09cc/go.mod h1:EGpADcykh3NcUnDUJcl1+ZksZNG86OlYog2l/sGQquU=\ngoogle.golang.org/protobuf v1.24.0/go.mod h1:r/3tXBNzIEhYS9I1OUVjXDlt8tc493IdKGjtUeSXeh4=\ngoogle.golang.org/protobuf v1.25.0/go.mod h1:9JNX74DMeImyA3h4bdi1ymwjUzf21/xIlbajtzgsN7c=\ngoogle.golang.org/protobuf v1.26.0-rc.1/go.mod h1:jlhhOSvTdKEhbULTjvd4ARK9grFBp09yW+WbY/TyQbw=\ngoogle.golang.org/protobuf v1.26.0/go.mod h1:9q0QmTI4eRPtz6boOQmLYwt+qCgq0jsYwAQnmE0givc=\ngoogle.golang.org/protobuf v1.33.0 h1:uNO2rsAINq/JlFpSdYEKIZ0uKD/R9cpdv0T+yoGwGmI=\ngoogle.golang.org/protobuf v1.33.0/go.mod h1:c6P6GXX6sHbq/GpV6MGZEdwhWPcYBgnhAHhKbcUYpos=\ngopkg.in/alecthomas/kingpin.v2 v2.2.6/go.mod h1:FMv+mEhP44yOT+4EoQTLFTRgOQ1FBLkstjWtayDeSgw=\ngopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/check.v1 v1.0.0-20180628173108-788fd7840127/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/check.v1 v1.0.0-20190902080502-41f04d3bba15/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/check.v1 v1.0.0-20200227125254-8fa46927fb4f/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=\ngopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c/go.mod h1:JHkPIbrfpd72SG/EVd6muEfDQjcINNoR0C8j2r3qZ4Q=\ngopkg.in/errgo.v2 v2.1.0/go.mod h1:hNsd1EY+bozCKY1Ytp96fpM3vjJbqLJn88ws8XvfDNI=\ngopkg.in/resty.v1 v1.12.0/go.mod h1:mDo4pnntr5jdWRML875a/NmxYqAlA73dVijT2AXvQQo=\ngopkg.in/yaml.v2 v2.0.0-20170812160011-eb3733d160e7/go.mod h1:JAlM8MvJe8wmxCU4Bli9HhUf9+ttbYbLASfIpnQbh74=\ngopkg.in/yaml.v2 v2.2.1/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.2.2/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.2.4/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.2.5/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.3.0/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=\ngopkg.in/yaml.v2 v2.4.0/go.mod h1:RDklbk79AGWmwhnvt/jBztapEOGDOx6ZbXqjP6csGnQ=\ngopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\ngopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=\ngopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\nhonnef.co/go/tools v0.0.0-20190102054323-c2f93a96b099/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\nhonnef.co/go/tools v0.0.0-20190106161140-3f1c8253044a/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\nhonnef.co/go/tools v0.0.0-20190418001031-e561f6794a2a/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\nhonnef.co/go/tools v0.0.0-20190523083050-ea95bdfd59fc/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=\nhonnef.co/go/tools v0.0.1-2019.2.3/go.mod h1:a3bituU0lyd329TUQxRnasdCoJDkEUEAqEt0JzvZhAg=\nhonnef.co/go/tools v0.0.1-2020.1.3/go.mod h1:X/FiERA/W4tHapMX5mGpAtMSVEeEUOyHaw9vFzvIQ3k=\nhonnef.co/go/tools v0.0.1-2020.1.4/go.mod h1:X/FiERA/W4tHapMX5mGpAtMSVEeEUOyHaw9vFzvIQ3k=\nrsc.io/binaryregexp v0.2.0/go.mod h1:qTv7/COck+e2FymRvadv62gMdZztPaShugOCi3I+8D8=\nrsc.io/pdf v0.1.1/go.mod h1:n8OzWcQ6Sp37PL01nO98y4iUCRdTGarVfzxY20ICaU4=\nrsc.io/quote/v3 v3.1.0/go.mod h1:yEA65RcK8LyAZtP9Kv3t0HmxON59tX3rD+tICJqUlj0=\nrsc.io/sampler v1.3.0/go.mod h1:T1hPZKmBbMNahiBKFy5HrXp6adAjACjK9JXDnKaTXpA=\n"
        },
        {
          "name": "ingest.go",
          "type": "blob",
          "size": 99.4638671875,
          "content": "// Copyright 2018 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"slices\"\n\t\"sort\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/cache\"\n\t\"github.com/cockroachdb/pebble/internal/invariants\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n\t\"github.com/cockroachdb/pebble/internal/overlap\"\n\t\"github.com/cockroachdb/pebble/internal/sstableinternal\"\n\t\"github.com/cockroachdb/pebble/objstorage\"\n\t\"github.com/cockroachdb/pebble/objstorage/remote\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n)\n\nfunc sstableKeyCompare(userCmp Compare, a, b InternalKey) int {\n\tc := userCmp(a.UserKey, b.UserKey)\n\tif c != 0 {\n\t\treturn c\n\t}\n\tif a.IsExclusiveSentinel() {\n\t\tif !b.IsExclusiveSentinel() {\n\t\t\treturn -1\n\t\t}\n\t} else if b.IsExclusiveSentinel() {\n\t\treturn +1\n\t}\n\treturn 0\n}\n\n// KeyRange encodes a key range in user key space. A KeyRange's Start is\n// inclusive while its End is exclusive.\n//\n// KeyRange is equivalent to base.UserKeyBounds with exclusive end.\ntype KeyRange struct {\n\tStart, End []byte\n}\n\n// Valid returns true if the KeyRange is defined.\nfunc (k *KeyRange) Valid() bool {\n\treturn k.Start != nil && k.End != nil\n}\n\n// Contains returns whether the specified key exists in the KeyRange.\nfunc (k *KeyRange) Contains(cmp base.Compare, key InternalKey) bool {\n\tv := cmp(key.UserKey, k.End)\n\treturn (v < 0 || (v == 0 && key.IsExclusiveSentinel())) && cmp(k.Start, key.UserKey) <= 0\n}\n\n// UserKeyBounds returns the KeyRange as UserKeyBounds. Also implements the internal `bounded` interface.\nfunc (k KeyRange) UserKeyBounds() base.UserKeyBounds {\n\treturn base.UserKeyBoundsEndExclusive(k.Start, k.End)\n}\n\n// OverlapsInternalKeyRange checks if the specified internal key range has an\n// overlap with the KeyRange. Note that we aren't checking for full containment\n// of smallest-largest within k, rather just that there's some intersection\n// between the two ranges.\nfunc (k *KeyRange) OverlapsInternalKeyRange(cmp base.Compare, smallest, largest InternalKey) bool {\n\tukb := k.UserKeyBounds()\n\tb := base.UserKeyBoundsFromInternal(smallest, largest)\n\treturn ukb.Overlaps(cmp, &b)\n}\n\n// Overlaps checks if the specified file has an overlap with the KeyRange.\n// Note that we aren't checking for full containment of m within k, rather just\n// that there's some intersection between m and k's bounds.\nfunc (k *KeyRange) Overlaps(cmp base.Compare, m *fileMetadata) bool {\n\tb := k.UserKeyBounds()\n\treturn m.Overlaps(cmp, &b)\n}\n\n// OverlapsKeyRange checks if this span overlaps with the provided KeyRange.\n// Note that we aren't checking for full containment of either span in the other,\n// just that there's a key x that is in both key ranges.\nfunc (k *KeyRange) OverlapsKeyRange(cmp Compare, span KeyRange) bool {\n\treturn cmp(k.Start, span.End) < 0 && cmp(k.End, span.Start) > 0\n}\n\nfunc ingestValidateKey(opts *Options, key *InternalKey) error {\n\tif key.Kind() == InternalKeyKindInvalid {\n\t\treturn base.CorruptionErrorf(\"pebble: external sstable has corrupted key: %s\",\n\t\t\tkey.Pretty(opts.Comparer.FormatKey))\n\t}\n\tif key.SeqNum() != 0 {\n\t\treturn base.CorruptionErrorf(\"pebble: external sstable has non-zero seqnum: %s\",\n\t\t\tkey.Pretty(opts.Comparer.FormatKey))\n\t}\n\treturn nil\n}\n\n// ingestSynthesizeShared constructs a fileMetadata for one shared sstable owned\n// or shared by another node.\nfunc ingestSynthesizeShared(\n\topts *Options, sm SharedSSTMeta, fileNum base.FileNum,\n) (*fileMetadata, error) {\n\tif sm.Size == 0 {\n\t\t// Disallow 0 file sizes\n\t\treturn nil, errors.New(\"pebble: cannot ingest shared file with size 0\")\n\t}\n\t// Don't load table stats. Doing a round trip to shared storage, one SST\n\t// at a time is not worth it as it slows down ingestion.\n\tmeta := &fileMetadata{\n\t\tFileNum:      fileNum,\n\t\tCreationTime: time.Now().Unix(),\n\t\tVirtual:      true,\n\t\tSize:         sm.Size,\n\t}\n\t// For simplicity, we use the same number for both the FileNum and the\n\t// DiskFileNum (even though this is a virtual sstable). Pass the underlying\n\t// FileBacking's size to the same size as the virtualized view of the sstable.\n\t// This ensures that we don't over-prioritize this sstable for compaction just\n\t// yet, as we do not have a clear sense of what parts of this sstable are\n\t// referenced by other nodes.\n\tmeta.InitProviderBacking(base.DiskFileNum(fileNum), sm.Size)\n\n\tif sm.LargestPointKey.Valid() && sm.LargestPointKey.UserKey != nil {\n\t\t// Initialize meta.{HasPointKeys,Smallest,Largest}, etc.\n\t\t//\n\t\t// NB: We create new internal keys and pass them into ExtendPointKeyBounds\n\t\t// so that we can sub a zero sequence number into the bounds. We can set\n\t\t// the sequence number to anything here; it'll be reset in ingestUpdateSeqNum\n\t\t// anyway. However, we do need to use the same sequence number across all\n\t\t// bound keys at this step so that we end up with bounds that are consistent\n\t\t// across point/range keys.\n\t\t//\n\t\t// Because of the sequence number rewriting, we cannot use the Kind of\n\t\t// sm.SmallestPointKey. For example, the original SST might start with\n\t\t// a.SET.2 and a.RANGEDEL.1 (with a.SET.2 being the smallest key); after\n\t\t// rewriting the sequence numbers, these keys become a.SET.100 and\n\t\t// a.RANGEDEL.100, with a.RANGEDEL.100 being the smallest key. To create a\n\t\t// correct bound, we just use the maximum key kind (which sorts first).\n\t\t// Similarly, we use the smallest key kind for the largest key.\n\t\tsmallestPointKey := base.MakeInternalKey(sm.SmallestPointKey.UserKey, 0, base.InternalKeyKindMaxForSSTable)\n\t\tlargestPointKey := base.MakeInternalKey(sm.LargestPointKey.UserKey, 0, 0)\n\t\tif sm.LargestPointKey.IsExclusiveSentinel() {\n\t\t\tlargestPointKey = base.MakeRangeDeleteSentinelKey(sm.LargestPointKey.UserKey)\n\t\t}\n\t\tif opts.Comparer.Equal(smallestPointKey.UserKey, largestPointKey.UserKey) &&\n\t\t\tsmallestPointKey.Trailer < largestPointKey.Trailer {\n\t\t\t// We get kinds from the sender, however we substitute our own sequence\n\t\t\t// numbers. This can result in cases where an sstable [b#5,SET-b#4,DELSIZED]\n\t\t\t// becomes [b#0,SET-b#0,DELSIZED] when we synthesize it here, but the\n\t\t\t// kinds need to be reversed now because DelSized > Set.\n\t\t\tsmallestPointKey, largestPointKey = largestPointKey, smallestPointKey\n\t\t}\n\t\tmeta.ExtendPointKeyBounds(opts.Comparer.Compare, smallestPointKey, largestPointKey)\n\t}\n\tif sm.LargestRangeKey.Valid() && sm.LargestRangeKey.UserKey != nil {\n\t\t// Initialize meta.{HasRangeKeys,Smallest,Largest}, etc.\n\t\t//\n\t\t// See comment above on why we use a zero sequence number and these key\n\t\t// kinds here.\n\t\tsmallestRangeKey := base.MakeInternalKey(sm.SmallestRangeKey.UserKey, 0, base.InternalKeyKindRangeKeyMax)\n\t\tlargestRangeKey := base.MakeExclusiveSentinelKey(base.InternalKeyKindRangeKeyMin, sm.LargestRangeKey.UserKey)\n\t\tmeta.ExtendRangeKeyBounds(opts.Comparer.Compare, smallestRangeKey, largestRangeKey)\n\t}\n\tif err := meta.Validate(opts.Comparer.Compare, opts.Comparer.FormatKey); err != nil {\n\t\treturn nil, err\n\t}\n\treturn meta, nil\n}\n\n// ingestLoad1External loads the fileMetadata for one external sstable.\n// Sequence number and target level calculation happens during prepare/apply.\nfunc ingestLoad1External(\n\topts *Options, e ExternalFile, fileNum base.FileNum,\n) (*fileMetadata, error) {\n\tif e.Size == 0 {\n\t\treturn nil, errors.New(\"pebble: cannot ingest external file with size 0\")\n\t}\n\tif !e.HasRangeKey && !e.HasPointKey {\n\t\treturn nil, errors.New(\"pebble: cannot ingest external file with no point or range keys\")\n\t}\n\n\tif opts.Comparer.Compare(e.StartKey, e.EndKey) > 0 {\n\t\treturn nil, errors.Newf(\"pebble: external file bounds [%q, %q) are invalid\", e.StartKey, e.EndKey)\n\t}\n\tif opts.Comparer.Compare(e.StartKey, e.EndKey) == 0 && !e.EndKeyIsInclusive {\n\t\treturn nil, errors.Newf(\"pebble: external file bounds [%q, %q) are invalid\", e.StartKey, e.EndKey)\n\t}\n\tif n := opts.Comparer.Split(e.StartKey); n != len(e.StartKey) {\n\t\treturn nil, errors.Newf(\"pebble: external file bounds start key %q has suffix\", e.StartKey)\n\t}\n\tif n := opts.Comparer.Split(e.EndKey); n != len(e.EndKey) {\n\t\treturn nil, errors.Newf(\"pebble: external file bounds end key %q has suffix\", e.EndKey)\n\t}\n\n\t// Don't load table stats. Doing a round trip to shared storage, one SST\n\t// at a time is not worth it as it slows down ingestion.\n\tmeta := &fileMetadata{\n\t\tFileNum:      fileNum,\n\t\tCreationTime: time.Now().Unix(),\n\t\tVirtual:      true,\n\t\tSize:         e.Size,\n\t}\n\n\t// In the name of keeping this ingestion as fast as possible, we avoid\n\t// *all* existence checks and synthesize a file metadata with smallest/largest\n\t// keys that overlap whatever the passed-in span was.\n\tsmallestCopy := slices.Clone(e.StartKey)\n\tlargestCopy := slices.Clone(e.EndKey)\n\tif e.HasPointKey {\n\t\t// Sequence numbers are updated later by\n\t\t// ingestUpdateSeqNum, applying a squence number that\n\t\t// is applied to all keys in the sstable.\n\t\tif e.EndKeyIsInclusive {\n\t\t\tmeta.ExtendPointKeyBounds(\n\t\t\t\topts.Comparer.Compare,\n\t\t\t\tbase.MakeInternalKey(smallestCopy, 0, base.InternalKeyKindMaxForSSTable),\n\t\t\t\tbase.MakeInternalKey(largestCopy, 0, 0))\n\t\t} else {\n\t\t\tmeta.ExtendPointKeyBounds(\n\t\t\t\topts.Comparer.Compare,\n\t\t\t\tbase.MakeInternalKey(smallestCopy, 0, base.InternalKeyKindMaxForSSTable),\n\t\t\t\tbase.MakeRangeDeleteSentinelKey(largestCopy))\n\t\t}\n\t}\n\tif e.HasRangeKey {\n\t\tmeta.ExtendRangeKeyBounds(\n\t\t\topts.Comparer.Compare,\n\t\t\tbase.MakeInternalKey(smallestCopy, 0, InternalKeyKindRangeKeyMax),\n\t\t\tbase.MakeExclusiveSentinelKey(InternalKeyKindRangeKeyMin, largestCopy),\n\t\t)\n\t}\n\n\tmeta.SyntheticPrefixAndSuffix = sstable.MakeSyntheticPrefixAndSuffix(e.SyntheticPrefix, e.SyntheticSuffix)\n\n\treturn meta, nil\n}\n\ntype rangeKeyIngestValidator struct {\n\t// lastRangeKey is the last range key seen in the previous file.\n\tlastRangeKey keyspan.Span\n\t// comparer, if unset, disables range key validation.\n\tcomparer *base.Comparer\n}\n\nfunc disableRangeKeyChecks() rangeKeyIngestValidator {\n\treturn rangeKeyIngestValidator{}\n}\n\nfunc validateSuffixedBoundaries(\n\tcmp *base.Comparer, lastRangeKey keyspan.Span,\n) rangeKeyIngestValidator {\n\treturn rangeKeyIngestValidator{\n\t\tlastRangeKey: lastRangeKey,\n\t\tcomparer:     cmp,\n\t}\n}\n\n// Validate valides if the stored state of this rangeKeyIngestValidator allows for\n// a file with the given nextFileSmallestKey to be ingested, such that the stored\n// last file's largest range key defragments cleanly with the next file's smallest\n// key if it was suffixed. If a value of nil is passed in for nextFileSmallestKey,\n// that denotes the next file does not have a range key or there is no next file.\nfunc (r *rangeKeyIngestValidator) Validate(nextFileSmallestKey *keyspan.Span) error {\n\tif r.comparer == nil {\n\t\treturn nil\n\t}\n\tif r.lastRangeKey.Valid() {\n\t\tif r.comparer.Split.HasSuffix(r.lastRangeKey.End) {\n\t\t\tif nextFileSmallestKey == nil || !r.comparer.Equal(r.lastRangeKey.End, nextFileSmallestKey.Start) {\n\t\t\t\t// The last range key has a suffix, and it doesn't defragment cleanly with this range key.\n\t\t\t\treturn errors.AssertionFailedf(\"pebble: ingest sstable has suffixed largest range key that does not match the start key of the next sstable: %s\",\n\t\t\t\t\tr.comparer.FormatKey(r.lastRangeKey.End))\n\t\t\t} else if !keyspan.DefragmentInternal.ShouldDefragment(r.comparer.CompareRangeSuffixes, &r.lastRangeKey, nextFileSmallestKey) {\n\t\t\t\t// The last range key has a suffix, and it doesn't defragment cleanly with this range key.\n\t\t\t\treturn errors.AssertionFailedf(\"pebble: ingest sstable has suffixed range key that won't defragment with next sstable: %s\",\n\t\t\t\t\tr.comparer.FormatKey(r.lastRangeKey.End))\n\t\t\t}\n\t\t}\n\t} else if nextFileSmallestKey != nil && r.comparer.Split.HasSuffix(nextFileSmallestKey.Start) {\n\t\treturn errors.Newf(\"pebble: ingest sstable has suffixed range key start that won't defragment: %s\",\n\t\t\tr.comparer.FormatKey(nextFileSmallestKey.Start))\n\t}\n\treturn nil\n}\n\n// ingestLoad1 creates the FileMetadata for one file. This file will be owned\n// by this store.\n//\n// prevLastRangeKey is the last range key from the previous file. It is used to\n// ensure that the range keys defragment cleanly across files. These checks\n// are disabled if disableRangeKeyChecks is true.\nfunc ingestLoad1(\n\tctx context.Context,\n\topts *Options,\n\tfmv FormatMajorVersion,\n\treadable objstorage.Readable,\n\tcacheID cache.ID,\n\tfileNum base.FileNum,\n\trangeKeyValidator rangeKeyIngestValidator,\n) (meta *fileMetadata, lastRangeKey keyspan.Span, err error) {\n\to := opts.MakeReaderOptions()\n\to.SetInternalCacheOpts(sstableinternal.CacheOptions{\n\t\tCache:   opts.Cache,\n\t\tCacheID: cacheID,\n\t\tFileNum: base.PhysicalTableDiskFileNum(fileNum),\n\t})\n\tr, err := sstable.NewReader(ctx, readable, o)\n\tif err != nil {\n\t\treturn nil, keyspan.Span{}, err\n\t}\n\tdefer r.Close()\n\n\t// Avoid ingesting tables with format versions this DB doesn't support.\n\ttf, err := r.TableFormat()\n\tif err != nil {\n\t\treturn nil, keyspan.Span{}, err\n\t}\n\tif tf < fmv.MinTableFormat() || tf > fmv.MaxTableFormat() {\n\t\treturn nil, keyspan.Span{}, errors.Newf(\n\t\t\t\"pebble: table format %s is not within range supported at DB format major version %d, (%s,%s)\",\n\t\t\ttf, fmv, fmv.MinTableFormat(), fmv.MaxTableFormat(),\n\t\t)\n\t}\n\tif tf.BlockColumnar() {\n\t\tif _, ok := opts.KeySchemas[r.Properties.KeySchemaName]; !ok {\n\t\t\treturn nil, keyspan.Span{}, errors.Newf(\n\t\t\t\t\"pebble: table uses key schema %q unknown to the database\",\n\t\t\t\tr.Properties.KeySchemaName)\n\t\t}\n\t}\n\n\tmeta = &fileMetadata{}\n\tmeta.FileNum = fileNum\n\tmeta.Size = uint64(readable.Size())\n\tmeta.CreationTime = time.Now().Unix()\n\tmeta.InitPhysicalBacking()\n\n\t// Avoid loading into the file cache for collecting stats if we\n\t// don't need to. If there are no range deletions, we have all the\n\t// information to compute the stats here.\n\t//\n\t// This is helpful in tests for avoiding awkwardness around deletion of\n\t// ingested files from MemFS. MemFS implements the Windows semantics of\n\t// disallowing removal of an open file. Under MemFS, if we don't populate\n\t// meta.Stats here, the file will be loaded into the file cache for\n\t// calculating stats before we can remove the original link.\n\tmaybeSetStatsFromProperties(meta.PhysicalMeta(), &r.Properties)\n\n\t{\n\t\titer, err := r.NewIter(sstable.NoTransforms, nil /* lower */, nil /* upper */)\n\t\tif err != nil {\n\t\t\treturn nil, keyspan.Span{}, err\n\t\t}\n\t\tdefer iter.Close()\n\t\tvar smallest InternalKey\n\t\tif kv := iter.First(); kv != nil {\n\t\t\tif err := ingestValidateKey(opts, &kv.K); err != nil {\n\t\t\t\treturn nil, keyspan.Span{}, err\n\t\t\t}\n\t\t\tsmallest = kv.K.Clone()\n\t\t}\n\t\tif err := iter.Error(); err != nil {\n\t\t\treturn nil, keyspan.Span{}, err\n\t\t}\n\t\tif kv := iter.Last(); kv != nil {\n\t\t\tif err := ingestValidateKey(opts, &kv.K); err != nil {\n\t\t\t\treturn nil, keyspan.Span{}, err\n\t\t\t}\n\t\t\tmeta.ExtendPointKeyBounds(opts.Comparer.Compare, smallest, kv.K.Clone())\n\t\t}\n\t\tif err := iter.Error(); err != nil {\n\t\t\treturn nil, keyspan.Span{}, err\n\t\t}\n\t}\n\n\titer, err := r.NewRawRangeDelIter(ctx, sstable.NoFragmentTransforms)\n\tif err != nil {\n\t\treturn nil, keyspan.Span{}, err\n\t}\n\tif iter != nil {\n\t\tdefer iter.Close()\n\t\tvar smallest InternalKey\n\t\tif s, err := iter.First(); err != nil {\n\t\t\treturn nil, keyspan.Span{}, err\n\t\t} else if s != nil {\n\t\t\tkey := s.SmallestKey()\n\t\t\tif err := ingestValidateKey(opts, &key); err != nil {\n\t\t\t\treturn nil, keyspan.Span{}, err\n\t\t\t}\n\t\t\tsmallest = key.Clone()\n\t\t}\n\t\tif s, err := iter.Last(); err != nil {\n\t\t\treturn nil, keyspan.Span{}, err\n\t\t} else if s != nil {\n\t\t\tk := s.SmallestKey()\n\t\t\tif err := ingestValidateKey(opts, &k); err != nil {\n\t\t\t\treturn nil, keyspan.Span{}, err\n\t\t\t}\n\t\t\tlargest := s.LargestKey().Clone()\n\t\t\tmeta.ExtendPointKeyBounds(opts.Comparer.Compare, smallest, largest)\n\t\t}\n\t}\n\n\t// Update the range-key bounds for the table.\n\t{\n\t\titer, err := r.NewRawRangeKeyIter(ctx, sstable.NoFragmentTransforms)\n\t\tif err != nil {\n\t\t\treturn nil, keyspan.Span{}, err\n\t\t}\n\t\tif iter != nil {\n\t\t\tdefer iter.Close()\n\t\t\tvar smallest InternalKey\n\t\t\tif s, err := iter.First(); err != nil {\n\t\t\t\treturn nil, keyspan.Span{}, err\n\t\t\t} else if s != nil {\n\t\t\t\tkey := s.SmallestKey()\n\t\t\t\tif err := ingestValidateKey(opts, &key); err != nil {\n\t\t\t\t\treturn nil, keyspan.Span{}, err\n\t\t\t\t}\n\t\t\t\tsmallest = key.Clone()\n\t\t\t\t// Range keys need some additional validation as we need to ensure they\n\t\t\t\t// defragment cleanly with the lastRangeKey from the previous file.\n\t\t\t\tif err := rangeKeyValidator.Validate(s); err != nil {\n\t\t\t\t\treturn nil, keyspan.Span{}, err\n\t\t\t\t}\n\t\t\t}\n\t\t\tlastRangeKey = keyspan.Span{}\n\t\t\tif s, err := iter.Last(); err != nil {\n\t\t\t\treturn nil, keyspan.Span{}, err\n\t\t\t} else if s != nil {\n\t\t\t\tk := s.SmallestKey()\n\t\t\t\tif err := ingestValidateKey(opts, &k); err != nil {\n\t\t\t\t\treturn nil, keyspan.Span{}, err\n\t\t\t\t}\n\t\t\t\t// As range keys are fragmented, the end key of the last range key in\n\t\t\t\t// the table provides the upper bound for the table.\n\t\t\t\tlargest := s.LargestKey().Clone()\n\t\t\t\tmeta.ExtendRangeKeyBounds(opts.Comparer.Compare, smallest, largest)\n\t\t\t\tlastRangeKey = s.Clone()\n\t\t\t} else {\n\t\t\t\t// s == nil.\n\t\t\t\trangeKeyValidator.Validate(nil /* nextFileSmallestKey */)\n\t\t\t}\n\t\t} else {\n\t\t\trangeKeyValidator.Validate(nil /* nextFileSmallestKey */)\n\t\t\tlastRangeKey = keyspan.Span{}\n\t\t}\n\t}\n\n\tif !meta.HasPointKeys && !meta.HasRangeKeys {\n\t\treturn nil, keyspan.Span{}, nil\n\t}\n\n\t// Sanity check that the various bounds on the file were set consistently.\n\tif err := meta.Validate(opts.Comparer.Compare, opts.Comparer.FormatKey); err != nil {\n\t\treturn nil, keyspan.Span{}, err\n\t}\n\n\treturn meta, lastRangeKey, nil\n}\n\ntype ingestLoadResult struct {\n\tlocal    []ingestLocalMeta\n\tshared   []ingestSharedMeta\n\texternal []ingestExternalMeta\n\n\texternalFilesHaveLevel bool\n}\n\ntype ingestLocalMeta struct {\n\t*fileMetadata\n\tpath string\n}\n\ntype ingestSharedMeta struct {\n\t*fileMetadata\n\tshared SharedSSTMeta\n}\n\ntype ingestExternalMeta struct {\n\t*fileMetadata\n\texternal ExternalFile\n\t// usedExistingBacking is true if the external file is reusing a backing\n\t// that existed before this ingestion. In this case, we called\n\t// VirtualBackings.Protect() on that backing; we will need to call\n\t// Unprotect() after the ingestion.\n\tusedExistingBacking bool\n}\n\nfunc (r *ingestLoadResult) fileCount() int {\n\treturn len(r.local) + len(r.shared) + len(r.external)\n}\n\nfunc ingestLoad(\n\tctx context.Context,\n\topts *Options,\n\tfmv FormatMajorVersion,\n\tpaths []string,\n\tshared []SharedSSTMeta,\n\texternal []ExternalFile,\n\tcacheID cache.ID,\n\tpending []base.FileNum,\n) (ingestLoadResult, error) {\n\tlocalFileNums := pending[:len(paths)]\n\tsharedFileNums := pending[len(paths) : len(paths)+len(shared)]\n\texternalFileNums := pending[len(paths)+len(shared) : len(paths)+len(shared)+len(external)]\n\n\tvar result ingestLoadResult\n\tresult.local = make([]ingestLocalMeta, 0, len(paths))\n\tvar lastRangeKey keyspan.Span\n\t// NB: we disable range key boundary assertions if we have shared or external files\n\t// present in this ingestion. This is because a suffixed range key in a local file\n\t// can possibly defragment with a suffixed range key in a shared or external file.\n\t// We also disable range key boundary assertions if we have CreateOnShared set to\n\t// true, as that means we could have suffixed RangeKeyDels or Unsets in the local\n\t// files that won't ever be surfaced, even if there are no shared or external files\n\t// in the ingestion.\n\tshouldDisableRangeKeyChecks := len(shared) > 0 || len(external) > 0 || opts.Experimental.CreateOnShared != remote.CreateOnSharedNone\n\tfor i := range paths {\n\t\tf, err := opts.FS.Open(paths[i])\n\t\tif err != nil {\n\t\t\treturn ingestLoadResult{}, err\n\t\t}\n\n\t\treadable, err := sstable.NewSimpleReadable(f)\n\t\tif err != nil {\n\t\t\treturn ingestLoadResult{}, err\n\t\t}\n\t\tvar m *fileMetadata\n\t\trangeKeyValidator := disableRangeKeyChecks()\n\t\tif !shouldDisableRangeKeyChecks {\n\t\t\trangeKeyValidator = validateSuffixedBoundaries(opts.Comparer, lastRangeKey)\n\t\t}\n\t\tm, lastRangeKey, err = ingestLoad1(ctx, opts, fmv, readable, cacheID, localFileNums[i], rangeKeyValidator)\n\t\tif err != nil {\n\t\t\treturn ingestLoadResult{}, err\n\t\t}\n\t\tif m != nil {\n\t\t\tresult.local = append(result.local, ingestLocalMeta{\n\t\t\t\tfileMetadata: m,\n\t\t\t\tpath:         paths[i],\n\t\t\t})\n\t\t}\n\t}\n\n\tif !shouldDisableRangeKeyChecks {\n\t\trangeKeyValidator := validateSuffixedBoundaries(opts.Comparer, lastRangeKey)\n\t\tif err := rangeKeyValidator.Validate(nil /* nextFileSmallestKey */); err != nil {\n\t\t\treturn ingestLoadResult{}, err\n\t\t}\n\t}\n\n\t// Sort the shared files according to level.\n\tsort.Sort(sharedByLevel(shared))\n\n\tresult.shared = make([]ingestSharedMeta, 0, len(shared))\n\tfor i := range shared {\n\t\tm, err := ingestSynthesizeShared(opts, shared[i], sharedFileNums[i])\n\t\tif err != nil {\n\t\t\treturn ingestLoadResult{}, err\n\t\t}\n\t\tif shared[i].Level < sharedLevelsStart {\n\t\t\treturn ingestLoadResult{}, errors.New(\"cannot ingest shared file in level below sharedLevelsStart\")\n\t\t}\n\t\tresult.shared = append(result.shared, ingestSharedMeta{\n\t\t\tfileMetadata: m,\n\t\t\tshared:       shared[i],\n\t\t})\n\t}\n\tresult.external = make([]ingestExternalMeta, 0, len(external))\n\tfor i := range external {\n\t\tm, err := ingestLoad1External(opts, external[i], externalFileNums[i])\n\t\tif err != nil {\n\t\t\treturn ingestLoadResult{}, err\n\t\t}\n\t\tresult.external = append(result.external, ingestExternalMeta{\n\t\t\tfileMetadata: m,\n\t\t\texternal:     external[i],\n\t\t})\n\t\tif external[i].Level > 0 {\n\t\t\tif i != 0 && !result.externalFilesHaveLevel {\n\t\t\t\treturn ingestLoadResult{}, base.AssertionFailedf(\"pebble: external sstables must all have level set or unset\")\n\t\t\t}\n\t\t\tresult.externalFilesHaveLevel = true\n\t\t} else if result.externalFilesHaveLevel {\n\t\t\treturn ingestLoadResult{}, base.AssertionFailedf(\"pebble: external sstables must all have level set or unset\")\n\t\t}\n\t}\n\treturn result, nil\n}\n\nfunc ingestSortAndVerify(cmp Compare, lr ingestLoadResult, exciseSpan KeyRange) error {\n\t// Verify that all the shared files (i.e. files in sharedMeta)\n\t// fit within the exciseSpan.\n\tfor _, f := range lr.shared {\n\t\tif !exciseSpan.Contains(cmp, f.Smallest) || !exciseSpan.Contains(cmp, f.Largest) {\n\t\t\treturn errors.Newf(\"pebble: shared file outside of excise span, span [%s-%s), file = %s\", exciseSpan.Start, exciseSpan.End, f.String())\n\t\t}\n\t}\n\n\tif lr.externalFilesHaveLevel {\n\t\tfor _, f := range lr.external {\n\t\t\tif !exciseSpan.Contains(cmp, f.Smallest) || !exciseSpan.Contains(cmp, f.Largest) {\n\t\t\t\treturn base.AssertionFailedf(\"pebble: external file outside of excise span, span [%s-%s), file = %s\", exciseSpan.Start, exciseSpan.End, f.String())\n\t\t\t}\n\t\t}\n\t}\n\n\tif len(lr.external) > 0 {\n\t\tif len(lr.shared) > 0 {\n\t\t\t// If external files are present alongside shared files,\n\t\t\t// return an error.\n\t\t\treturn base.AssertionFailedf(\"pebble: external files cannot be ingested atomically alongside shared files\")\n\t\t}\n\n\t\t// Sort according to the smallest key.\n\t\tslices.SortFunc(lr.external, func(a, b ingestExternalMeta) int {\n\t\t\treturn cmp(a.Smallest.UserKey, b.Smallest.UserKey)\n\t\t})\n\t\tfor i := 1; i < len(lr.external); i++ {\n\t\t\tif sstableKeyCompare(cmp, lr.external[i-1].Largest, lr.external[i].Smallest) >= 0 {\n\t\t\t\treturn errors.Newf(\"pebble: external sstables have overlapping ranges\")\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}\n\tif len(lr.local) <= 1 {\n\t\treturn nil\n\t}\n\n\t// Sort according to the smallest key.\n\tslices.SortFunc(lr.local, func(a, b ingestLocalMeta) int {\n\t\treturn cmp(a.Smallest.UserKey, b.Smallest.UserKey)\n\t})\n\n\tfor i := 1; i < len(lr.local); i++ {\n\t\tif sstableKeyCompare(cmp, lr.local[i-1].Largest, lr.local[i].Smallest) >= 0 {\n\t\t\treturn errors.Newf(\"pebble: local ingestion sstables have overlapping ranges\")\n\t\t}\n\t}\n\tif len(lr.shared) == 0 {\n\t\treturn nil\n\t}\n\tfilesInLevel := make([]*fileMetadata, 0, len(lr.shared))\n\tfor l := sharedLevelsStart; l < numLevels; l++ {\n\t\tfilesInLevel = filesInLevel[:0]\n\t\tfor i := range lr.shared {\n\t\t\tif lr.shared[i].shared.Level == uint8(l) {\n\t\t\t\tfilesInLevel = append(filesInLevel, lr.shared[i].fileMetadata)\n\t\t\t}\n\t\t}\n\t\tfor i := range lr.external {\n\t\t\tif lr.external[i].external.Level == uint8(l) {\n\t\t\t\tfilesInLevel = append(filesInLevel, lr.external[i].fileMetadata)\n\t\t\t}\n\t\t}\n\t\tslices.SortFunc(filesInLevel, func(a, b *fileMetadata) int {\n\t\t\treturn cmp(a.Smallest.UserKey, b.Smallest.UserKey)\n\t\t})\n\t\tfor i := 1; i < len(filesInLevel); i++ {\n\t\t\tif sstableKeyCompare(cmp, filesInLevel[i-1].Largest, filesInLevel[i].Smallest) >= 0 {\n\t\t\t\treturn base.AssertionFailedf(\"pebble: external shared sstables have overlapping ranges\")\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc ingestCleanup(objProvider objstorage.Provider, meta []ingestLocalMeta) error {\n\tvar firstErr error\n\tfor i := range meta {\n\t\tif err := objProvider.Remove(fileTypeTable, meta[i].FileBacking.DiskFileNum); err != nil {\n\t\t\tfirstErr = firstError(firstErr, err)\n\t\t}\n\t}\n\treturn firstErr\n}\n\n// ingestLinkLocal creates new objects which are backed by either hardlinks to or\n// copies of the ingested files.\nfunc ingestLinkLocal(\n\tctx context.Context,\n\tjobID JobID,\n\topts *Options,\n\tobjProvider objstorage.Provider,\n\tlocalMetas []ingestLocalMeta,\n) error {\n\tfor i := range localMetas {\n\t\tobjMeta, err := objProvider.LinkOrCopyFromLocal(\n\t\t\tctx, opts.FS, localMetas[i].path, fileTypeTable, localMetas[i].FileBacking.DiskFileNum,\n\t\t\tobjstorage.CreateOptions{PreferSharedStorage: true},\n\t\t)\n\t\tif err != nil {\n\t\t\tif err2 := ingestCleanup(objProvider, localMetas[:i]); err2 != nil {\n\t\t\t\topts.Logger.Errorf(\"ingest cleanup failed: %v\", err2)\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\t\tif opts.EventListener.TableCreated != nil {\n\t\t\topts.EventListener.TableCreated(TableCreateInfo{\n\t\t\t\tJobID:   int(jobID),\n\t\t\t\tReason:  \"ingesting\",\n\t\t\t\tPath:    objProvider.Path(objMeta),\n\t\t\t\tFileNum: base.PhysicalTableDiskFileNum(localMetas[i].FileNum),\n\t\t\t})\n\t\t}\n\t}\n\treturn nil\n}\n\n// ingestAttachRemote attaches remote objects to the storage provider.\n//\n// For external objects, we reuse existing FileBackings from the current version\n// when possible.\n//\n// ingestUnprotectExternalBackings() must be called after this function (even in\n// error cases).\nfunc (d *DB) ingestAttachRemote(jobID JobID, lr ingestLoadResult) error {\n\tremoteObjs := make([]objstorage.RemoteObjectToAttach, 0, len(lr.shared)+len(lr.external))\n\tfor i := range lr.shared {\n\t\tbacking, err := lr.shared[i].shared.Backing.Get()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tremoteObjs = append(remoteObjs, objstorage.RemoteObjectToAttach{\n\t\t\tFileNum:  lr.shared[i].FileBacking.DiskFileNum,\n\t\t\tFileType: fileTypeTable,\n\t\t\tBacking:  backing,\n\t\t})\n\t}\n\n\td.findExistingBackingsForExternalObjects(lr.external)\n\n\tnewFileBackings := make(map[remote.ObjectKey]*fileBacking, len(lr.external))\n\tfor i := range lr.external {\n\t\tmeta := lr.external[i].fileMetadata\n\t\tif meta.FileBacking != nil {\n\t\t\t// The backing was filled in by findExistingBackingsForExternalObjects().\n\t\t\tcontinue\n\t\t}\n\t\tkey := remote.MakeObjectKey(lr.external[i].external.Locator, lr.external[i].external.ObjName)\n\t\tif backing, ok := newFileBackings[key]; ok {\n\t\t\t// We already created the same backing in this loop.\n\t\t\tmeta.FileBacking = backing\n\t\t\tcontinue\n\t\t}\n\t\tproviderBacking, err := d.objProvider.CreateExternalObjectBacking(key.Locator, key.ObjectName)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// We have to attach the remote object (and assign it a DiskFileNum). For\n\t\t// simplicity, we use the same number for both the FileNum and the\n\t\t// DiskFileNum (even though this is a virtual sstable).\n\t\tmeta.InitProviderBacking(base.DiskFileNum(meta.FileNum), lr.external[i].external.Size)\n\n\t\t// Set the underlying FileBacking's size to the same size as the virtualized\n\t\t// view of the sstable. This ensures that we don't over-prioritize this\n\t\t// sstable for compaction just yet, as we do not have a clear sense of\n\t\t// what parts of this sstable are referenced by other nodes.\n\t\tmeta.FileBacking.Size = lr.external[i].external.Size\n\t\tnewFileBackings[key] = meta.FileBacking\n\n\t\tremoteObjs = append(remoteObjs, objstorage.RemoteObjectToAttach{\n\t\t\tFileNum:  meta.FileBacking.DiskFileNum,\n\t\t\tFileType: fileTypeTable,\n\t\t\tBacking:  providerBacking,\n\t\t})\n\t}\n\n\tfor i := range lr.external {\n\t\tif err := lr.external[i].Validate(d.opts.Comparer.Compare, d.opts.Comparer.FormatKey); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tremoteObjMetas, err := d.objProvider.AttachRemoteObjects(remoteObjs)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tfor i := range lr.shared {\n\t\t// One corner case around file sizes we need to be mindful of, is that\n\t\t// if one of the shareObjs was initially created by us (and has boomeranged\n\t\t// back from another node), we'll need to update the FileBacking's size\n\t\t// to be the true underlying size. Otherwise, we could hit errors when we\n\t\t// open the db again after a crash/restart (see checkConsistency in open.go),\n\t\t// plus it more accurately allows us to prioritize compactions of files\n\t\t// that were originally created by us.\n\t\tif remoteObjMetas[i].IsShared() && !d.objProvider.IsSharedForeign(remoteObjMetas[i]) {\n\t\t\tsize, err := d.objProvider.Size(remoteObjMetas[i])\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tlr.shared[i].FileBacking.Size = uint64(size)\n\t\t}\n\t}\n\n\tif d.opts.EventListener.TableCreated != nil {\n\t\tfor i := range remoteObjMetas {\n\t\t\td.opts.EventListener.TableCreated(TableCreateInfo{\n\t\t\t\tJobID:   int(jobID),\n\t\t\t\tReason:  \"ingesting\",\n\t\t\t\tPath:    d.objProvider.Path(remoteObjMetas[i]),\n\t\t\t\tFileNum: remoteObjMetas[i].DiskFileNum,\n\t\t\t})\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// findExistingBackingsForExternalObjects populates the FileBacking for external\n// files which are already in use by the current version.\n//\n// We take a Ref and LatestRef on populated backings.\nfunc (d *DB) findExistingBackingsForExternalObjects(metas []ingestExternalMeta) {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\n\tfor i := range metas {\n\t\tdiskFileNums := d.objProvider.GetExternalObjects(metas[i].external.Locator, metas[i].external.ObjName)\n\t\t// We cross-check against fileBackings in the current version because it is\n\t\t// possible that the external object is referenced by an sstable which only\n\t\t// exists in a previous version. In that case, that object could be removed\n\t\t// at any time so we cannot reuse it.\n\t\tfor _, n := range diskFileNums {\n\t\t\tif backing, ok := d.mu.versions.virtualBackings.Get(n); ok {\n\t\t\t\t// Protect this backing from being removed from the latest version. We\n\t\t\t\t// will unprotect in ingestUnprotectExternalBackings.\n\t\t\t\td.mu.versions.virtualBackings.Protect(n)\n\t\t\t\tmetas[i].usedExistingBacking = true\n\t\t\t\tmetas[i].FileBacking = backing\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n}\n\n// ingestUnprotectExternalBackings unprotects the file backings that were reused\n// for external objects when the ingestion fails.\nfunc (d *DB) ingestUnprotectExternalBackings(lr ingestLoadResult) {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\n\tfor _, meta := range lr.external {\n\t\tif meta.usedExistingBacking {\n\t\t\t// If the backing is not use anywhere else and the ingest failed (or the\n\t\t\t// ingested tables were already compacted away), this call will cause in\n\t\t\t// the next version update to remove the backing.\n\t\t\td.mu.versions.virtualBackings.Unprotect(meta.FileBacking.DiskFileNum)\n\t\t}\n\t}\n}\n\nfunc setSeqNumInMetadata(\n\tm *fileMetadata, seqNum base.SeqNum, cmp Compare, format base.FormatKey,\n) error {\n\tsetSeqFn := func(k base.InternalKey) base.InternalKey {\n\t\treturn base.MakeInternalKey(k.UserKey, seqNum, k.Kind())\n\t}\n\t// NB: we set the fields directly here, rather than via their Extend*\n\t// methods, as we are updating sequence numbers.\n\tif m.HasPointKeys {\n\t\tm.SmallestPointKey = setSeqFn(m.SmallestPointKey)\n\t}\n\tif m.HasRangeKeys {\n\t\tm.SmallestRangeKey = setSeqFn(m.SmallestRangeKey)\n\t}\n\tm.Smallest = setSeqFn(m.Smallest)\n\t// Only update the seqnum for the largest key if that key is not an\n\t// \"exclusive sentinel\" (i.e. a range deletion sentinel or a range key\n\t// boundary), as doing so effectively drops the exclusive sentinel (by\n\t// lowering the seqnum from the max value), and extends the bounds of the\n\t// table.\n\t// NB: as the largest range key is always an exclusive sentinel, it is never\n\t// updated.\n\tif m.HasPointKeys && !m.LargestPointKey.IsExclusiveSentinel() {\n\t\tm.LargestPointKey = setSeqFn(m.LargestPointKey)\n\t}\n\tif !m.Largest.IsExclusiveSentinel() {\n\t\tm.Largest = setSeqFn(m.Largest)\n\t}\n\t// Setting smallestSeqNum == largestSeqNum triggers the setting of\n\t// Properties.GlobalSeqNum when an sstable is loaded.\n\tm.SmallestSeqNum = seqNum\n\tm.LargestSeqNum = seqNum\n\tm.LargestSeqNumAbsolute = seqNum\n\t// Ensure the new bounds are consistent.\n\tif err := m.Validate(cmp, format); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc ingestUpdateSeqNum(\n\tcmp Compare, format base.FormatKey, seqNum base.SeqNum, loadResult ingestLoadResult,\n) error {\n\t// Shared sstables are required to be sorted by level ascending. We then\n\t// iterate the shared sstables in reverse, assigning the lower sequence\n\t// numbers to the shared sstables that will be ingested into the lower\n\t// (larger numbered) levels first. This ensures sequence number shadowing is\n\t// correct.\n\tfor i := len(loadResult.shared) - 1; i >= 0; i-- {\n\t\tif i-1 >= 0 && loadResult.shared[i-1].shared.Level > loadResult.shared[i].shared.Level {\n\t\t\tpanic(errors.AssertionFailedf(\"shared files %s, %s out of order\", loadResult.shared[i-1], loadResult.shared[i]))\n\t\t}\n\t\tif err := setSeqNumInMetadata(loadResult.shared[i].fileMetadata, seqNum, cmp, format); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tseqNum++\n\t}\n\tfor i := range loadResult.external {\n\t\tif err := setSeqNumInMetadata(loadResult.external[i].fileMetadata, seqNum, cmp, format); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tseqNum++\n\t}\n\tfor i := range loadResult.local {\n\t\tif err := setSeqNumInMetadata(loadResult.local[i].fileMetadata, seqNum, cmp, format); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tseqNum++\n\t}\n\treturn nil\n}\n\n// ingestTargetLevel returns the target level for a file being ingested.\n// If suggestSplit is true, it accounts for ingest-time splitting as part of\n// its target level calculation, and if a split candidate is found, that file\n// is returned as the splitFile.\nfunc ingestTargetLevel(\n\tctx context.Context,\n\tcmp base.Compare,\n\tlsmOverlap overlap.WithLSM,\n\tbaseLevel int,\n\tcompactions map[*compaction]struct{},\n\tmeta *fileMetadata,\n\tsuggestSplit bool,\n) (targetLevel int, splitFile *fileMetadata, err error) {\n\t// Find the lowest level which does not have any files which overlap meta. We\n\t// search from L0 to L6 looking for whether there are any files in the level\n\t// which overlap meta. We want the \"lowest\" level (where lower means\n\t// increasing level number) in order to reduce write amplification.\n\t//\n\t// There are 2 kinds of overlap we need to check for: file boundary overlap\n\t// and data overlap. Data overlap implies file boundary overlap. Note that it\n\t// is always possible to ingest into L0.\n\t//\n\t// To place meta at level i where i > 0:\n\t// - there must not be any data overlap with levels <= i, since that will\n\t//   violate the sequence number invariant.\n\t// - no file boundary overlap with level i, since that will violate the\n\t//   invariant that files do not overlap in levels i > 0.\n\t//   - if there is only a file overlap at a given level, and no data overlap,\n\t//     we can still slot a file at that level. We return the fileMetadata with\n\t//     which we have file boundary overlap (must be only one file, as sstable\n\t//     bounds are usually tight on user keys) and the caller is expected to split\n\t//     that sstable into two virtual sstables, allowing this file to go into that\n\t//     level. Note that if we have file boundary overlap with two files, which\n\t//     should only happen on rare occasions, we treat it as data overlap and\n\t//     don't use this optimization.\n\t//\n\t// The file boundary overlap check is simpler to conceptualize. Consider the\n\t// following example, in which the ingested file lies completely before or\n\t// after the file being considered.\n\t//\n\t//   |--|           |--|  ingested file: [a,b] or [f,g]\n\t//         |-----|        existing file: [c,e]\n\t//  _____________________\n\t//   a  b  c  d  e  f  g\n\t//\n\t// In both cases the ingested file can move to considering the next level.\n\t//\n\t// File boundary overlap does not necessarily imply data overlap. The check\n\t// for data overlap is a little more nuanced. Consider the following examples:\n\t//\n\t//  1. No data overlap:\n\t//\n\t//          |-|   |--|    ingested file: [cc-d] or [ee-ff]\n\t//  |*--*--*----*------*| existing file: [a-g], points: [a, b, c, dd, g]\n\t//  _____________________\n\t//   a  b  c  d  e  f  g\n\t//\n\t// In this case the ingested files can \"fall through\" this level. The checks\n\t// continue at the next level.\n\t//\n\t//  2. Data overlap:\n\t//\n\t//            |--|        ingested file: [d-e]\n\t//  |*--*--*----*------*| existing file: [a-g], points: [a, b, c, dd, g]\n\t//  _____________________\n\t//   a  b  c  d  e  f  g\n\t//\n\t// In this case the file cannot be ingested into this level as the point 'dd'\n\t// is in the way.\n\t//\n\t// It is worth noting that the check for data overlap is only approximate. In\n\t// the previous example, the ingested table [d-e] could contain only the\n\t// points 'd' and 'e', in which case the table would be eligible for\n\t// considering lower levels. However, such a fine-grained check would need to\n\t// be exhaustive (comparing points and ranges in both the ingested existing\n\t// tables) and such a check is prohibitively expensive. Thus Pebble treats any\n\t// existing point that falls within the ingested table bounds as being \"data\n\t// overlap\".\n\n\tif lsmOverlap[0].Result == overlap.Data {\n\t\treturn 0, nil, nil\n\t}\n\ttargetLevel = 0\n\tsplitFile = nil\n\tfor level := baseLevel; level < numLevels; level++ {\n\t\tvar candidateSplitFile *fileMetadata\n\t\tswitch lsmOverlap[level].Result {\n\t\tcase overlap.Data:\n\t\t\t// We cannot ingest into or under this level; return the best target level\n\t\t\t// so far.\n\t\t\treturn targetLevel, splitFile, nil\n\n\t\tcase overlap.OnlyBoundary:\n\t\t\tif !suggestSplit || lsmOverlap[level].SplitFile == nil {\n\t\t\t\t// We can ingest under this level, but not into this level.\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// We can ingest into this level if we split this file.\n\t\t\tcandidateSplitFile = lsmOverlap[level].SplitFile\n\n\t\tcase overlap.None:\n\t\t// We can ingest into this level.\n\n\t\tdefault:\n\t\t\treturn 0, nil, base.AssertionFailedf(\"unexpected WithLevel.Result: %v\", lsmOverlap[level].Result)\n\t\t}\n\n\t\t// Check boundary overlap with any ongoing compactions. We consider an\n\t\t// overlapping compaction that's writing files to an output level as\n\t\t// equivalent to boundary overlap with files in that output level.\n\t\t//\n\t\t// We cannot check for data overlap with the new SSTs compaction will produce\n\t\t// since compaction hasn't been done yet. However, there's no need to check\n\t\t// since all keys in them will be from levels in [c.startLevel,\n\t\t// c.outputLevel], and all those levels have already had their data overlap\n\t\t// tested negative (else we'd have returned earlier).\n\t\t//\n\t\t// An alternative approach would be to cancel these compactions and proceed\n\t\t// with an ingest-time split on this level if necessary. However, compaction\n\t\t// cancellation can result in significant wasted effort and is best avoided\n\t\t// unless necessary.\n\t\toverlaps := false\n\t\tfor c := range compactions {\n\t\t\tif c.outputLevel == nil || level != c.outputLevel.level {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif cmp(meta.Smallest.UserKey, c.largest.UserKey) <= 0 &&\n\t\t\t\tcmp(meta.Largest.UserKey, c.smallest.UserKey) >= 0 {\n\t\t\t\toverlaps = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif !overlaps {\n\t\t\ttargetLevel = level\n\t\t\tsplitFile = candidateSplitFile\n\t\t}\n\t}\n\treturn targetLevel, splitFile, nil\n}\n\n// Ingest ingests a set of sstables into the DB. Ingestion of the files is\n// atomic and semantically equivalent to creating a single batch containing all\n// of the mutations in the sstables. Ingestion may require the memtable to be\n// flushed. The ingested sstable files are moved into the DB and must reside on\n// the same filesystem as the DB. Sstables can be created for ingestion using\n// sstable.Writer. On success, Ingest removes the input paths.\n//\n// Two types of sstables are accepted for ingestion(s): one is sstables present\n// in the instance's vfs.FS and can be referenced locally. The other is sstables\n// present in remote.Storage, referred to as shared or foreign sstables. These\n// shared sstables can be linked through objstorageprovider.Provider, and do not\n// need to already be present on the local vfs.FS. Foreign sstables must all fit\n// in an excise span, and are destined for a level specified in SharedSSTMeta.\n//\n// All sstables *must* be Sync()'d by the caller after all bytes are written\n// and before its file handle is closed; failure to do so could violate\n// durability or lead to corrupted on-disk state. This method cannot, in a\n// platform-and-FS-agnostic way, ensure that all sstables in the input are\n// properly synced to disk. Opening new file handles and Sync()-ing them\n// does not always guarantee durability; see the discussion here on that:\n// https://github.com/cockroachdb/pebble/pull/835#issuecomment-663075379\n//\n// Ingestion loads each sstable into the lowest level of the LSM which it\n// doesn't overlap (see ingestTargetLevel). If an sstable overlaps a memtable,\n// ingestion forces the memtable to flush, and then waits for the flush to\n// occur. In some cases, such as with no foreign sstables and no excise span,\n// ingestion that gets blocked on a memtable can join the flushable queue and\n// finish even before the memtable has been flushed.\n//\n// The steps for ingestion are:\n//\n//  1. Allocate file numbers for every sstable being ingested.\n//  2. Load the metadata for all sstables being ingested.\n//  3. Sort the sstables by smallest key, verifying non overlap (for local\n//     sstables).\n//  4. Hard link (or copy) the local sstables into the DB directory.\n//  5. Allocate a sequence number to use for all of the entries in the\n//     local sstables. This is the step where overlap with memtables is\n//     determined. If there is overlap, we remember the most recent memtable\n//     that overlaps.\n//  6. Update the sequence number in the ingested local sstables. (Remote\n//     sstables get fixed sequence numbers that were determined at load time.)\n//  7. Wait for the most recent memtable that overlaps to flush (if any).\n//  8. Add the ingested sstables to the version (DB.ingestApply).\n//     8.1.  If an excise span was specified, figure out what sstables in the\n//     current version overlap with the excise span, and create new virtual\n//     sstables out of those sstables that exclude the excised span (DB.excise).\n//  9. Publish the ingestion sequence number.\n//\n// Note that if the mutable memtable overlaps with ingestion, a flush of the\n// memtable is forced equivalent to DB.Flush. Additionally, subsequent\n// mutations that get sequence numbers larger than the ingestion sequence\n// number get queued up behind the ingestion waiting for it to complete. This\n// can produce a noticeable hiccup in performance. See\n// https://github.com/cockroachdb/pebble/issues/25 for an idea for how to fix\n// this hiccup.\nfunc (d *DB) Ingest(ctx context.Context, paths []string) error {\n\tif err := d.closed.Load(); err != nil {\n\t\tpanic(err)\n\t}\n\tif d.opts.ReadOnly {\n\t\treturn ErrReadOnly\n\t}\n\t_, err := d.ingest(ctx, paths, nil /* shared */, KeyRange{}, nil /* external */)\n\treturn err\n}\n\n// IngestOperationStats provides some information about where in the LSM the\n// bytes were ingested.\ntype IngestOperationStats struct {\n\t// Bytes is the total bytes in the ingested sstables.\n\tBytes uint64\n\t// ApproxIngestedIntoL0Bytes is the approximate number of bytes ingested\n\t// into L0. This value is approximate when flushable ingests are active and\n\t// an ingest overlaps an entry in the flushable queue. Currently, this\n\t// approximation is very rough, only including tables that overlapped the\n\t// memtable. This estimate may be improved with #2112.\n\tApproxIngestedIntoL0Bytes uint64\n\t// MemtableOverlappingFiles is the count of ingested sstables\n\t// that overlapped keys in the memtables.\n\tMemtableOverlappingFiles int\n}\n\n// ExternalFile are external sstables that can be referenced through\n// objprovider and ingested as remote files that will not be refcounted or\n// cleaned up. For use with online restore. Note that the underlying sstable\n// could contain keys outside the [Smallest,Largest) bounds; however Pebble\n// is expected to only read the keys within those bounds.\ntype ExternalFile struct {\n\t// Locator is the shared.Locator that can be used with objProvider to\n\t// resolve a reference to this external sstable.\n\tLocator remote.Locator\n\n\t// ObjName is the unique name of this sstable on Locator.\n\tObjName string\n\n\t// Size of the referenced proportion of the virtualized sstable. An estimate\n\t// is acceptable in lieu of the backing file size.\n\tSize uint64\n\n\t// StartKey and EndKey define the bounds of the sstable; the ingestion\n\t// of this file will only result in keys within [StartKey, EndKey) if\n\t// EndKeyIsInclusive is false or [StartKey, EndKey] if it is true.\n\t// These bounds are loose i.e. it's possible for keys to not span the\n\t// entirety of this range.\n\t//\n\t// StartKey and EndKey user keys must not have suffixes.\n\t//\n\t// Multiple ExternalFiles in one ingestion must all have non-overlapping\n\t// bounds.\n\tStartKey, EndKey []byte\n\n\t// EndKeyIsInclusive is true if EndKey should be treated as inclusive.\n\tEndKeyIsInclusive bool\n\n\t// HasPointKey and HasRangeKey denote whether this file contains point keys\n\t// or range keys. If both structs are false, an error is returned during\n\t// ingestion.\n\tHasPointKey, HasRangeKey bool\n\n\t// SyntheticPrefix will prepend this suffix to all keys in the file during\n\t// iteration. Note that the backing file itself is not modified.\n\t//\n\t// SyntheticPrefix must be a prefix of both Bounds.Start and Bounds.End.\n\tSyntheticPrefix []byte\n\n\t// SyntheticSuffix will replace the suffix of every key in the file during\n\t// iteration. Note that the file itself is not modified, rather, every key\n\t// returned by an iterator will have the synthetic suffix.\n\t//\n\t// SyntheticSuffix can only be used under the following conditions:\n\t//  - the synthetic suffix must sort before any non-empty suffixes in the\n\t//    backing sst (the entire sst, not just the part restricted to Bounds).\n\t//  - the backing sst must not contain multiple keys with the same prefix.\n\tSyntheticSuffix []byte\n\n\t// Level denotes the level at which this file was present at read time\n\t// if the external file was returned by a scan of an existing Pebble\n\t// instance. If Level is 0, this field is ignored.\n\tLevel uint8\n}\n\n// IngestWithStats does the same as Ingest, and additionally returns\n// IngestOperationStats.\nfunc (d *DB) IngestWithStats(ctx context.Context, paths []string) (IngestOperationStats, error) {\n\tif err := d.closed.Load(); err != nil {\n\t\tpanic(err)\n\t}\n\tif d.opts.ReadOnly {\n\t\treturn IngestOperationStats{}, ErrReadOnly\n\t}\n\treturn d.ingest(ctx, paths, nil, KeyRange{}, nil)\n}\n\n// IngestExternalFiles does the same as IngestWithStats, and additionally\n// accepts external files (with locator info that can be resolved using\n// d.opts.SharedStorage). These files must also be non-overlapping with\n// each other, and must be resolvable through d.objProvider.\nfunc (d *DB) IngestExternalFiles(\n\tctx context.Context, external []ExternalFile,\n) (IngestOperationStats, error) {\n\tif err := d.closed.Load(); err != nil {\n\t\tpanic(err)\n\t}\n\n\tif d.opts.ReadOnly {\n\t\treturn IngestOperationStats{}, ErrReadOnly\n\t}\n\tif d.opts.Experimental.RemoteStorage == nil {\n\t\treturn IngestOperationStats{}, errors.New(\"pebble: cannot ingest external files without shared storage configured\")\n\t}\n\treturn d.ingest(ctx, nil, nil, KeyRange{}, external)\n}\n\n// IngestAndExcise does the same as IngestWithStats, and additionally accepts a\n// list of shared files to ingest that can be read from a remote.Storage through\n// a Provider. All the shared files must live within exciseSpan, and any existing\n// keys in exciseSpan are deleted by turning existing sstables into virtual\n// sstables (if not virtual already) and shrinking their spans to exclude\n// exciseSpan. See the comment at Ingest for a more complete picture of the\n// ingestion process.\n//\n// Panics if this DB instance was not instantiated with a remote.Storage and\n// shared sstables are present.\nfunc (d *DB) IngestAndExcise(\n\tctx context.Context,\n\tpaths []string,\n\tshared []SharedSSTMeta,\n\texternal []ExternalFile,\n\texciseSpan KeyRange,\n) (IngestOperationStats, error) {\n\tif err := d.closed.Load(); err != nil {\n\t\tpanic(err)\n\t}\n\tif d.opts.ReadOnly {\n\t\treturn IngestOperationStats{}, ErrReadOnly\n\t}\n\tif invariants.Enabled {\n\t\t// Excise is only supported on prefix keys.\n\t\tif d.opts.Comparer.Split(exciseSpan.Start) != len(exciseSpan.Start) {\n\t\t\tpanic(\"IngestAndExcise called with suffixed start key\")\n\t\t}\n\t\tif d.opts.Comparer.Split(exciseSpan.End) != len(exciseSpan.End) {\n\t\t\tpanic(\"IngestAndExcise called with suffixed end key\")\n\t\t}\n\t}\n\tif v := d.FormatMajorVersion(); v < FormatMinForSharedObjects {\n\t\treturn IngestOperationStats{}, errors.Errorf(\n\t\t\t\"store has format major version %d; IngestAndExcise requires at least %d\",\n\t\t\tv, FormatMinForSharedObjects,\n\t\t)\n\t}\n\treturn d.ingest(ctx, paths, shared, exciseSpan, external)\n}\n\n// Both DB.mu and commitPipeline.mu must be held while this is called.\nfunc (d *DB) newIngestedFlushableEntry(\n\tmeta []*fileMetadata, seqNum base.SeqNum, logNum base.DiskFileNum, exciseSpan KeyRange,\n) (*flushableEntry, error) {\n\t// If there's an excise being done atomically with the same ingest, we\n\t// assign the lowest sequence number in the set of sequence numbers for this\n\t// ingestion to the excise. Note that we've already allocated fileCount+1\n\t// sequence numbers in this case.\n\t//\n\t// This mimics the behaviour in the non-flushable ingest case (see the callsite\n\t// for ingestUpdateSeqNum).\n\tfileSeqNumStart := seqNum\n\tif exciseSpan.Valid() {\n\t\tfileSeqNumStart = seqNum + 1 // the first seqNum is reserved for the excise.\n\t}\n\t// Update the sequence number for all of the sstables in the\n\t// metadata. Writing the metadata to the manifest when the\n\t// version edit is applied is the mechanism that persists the\n\t// sequence number. The sstables themselves are left unmodified.\n\t// In this case, a version edit will only be written to the manifest\n\t// when the flushable is eventually flushed. If Pebble restarts in that\n\t// time, then we'll lose the ingest sequence number information. But this\n\t// information will also be reconstructed on node restart.\n\tfor i, m := range meta {\n\t\tif err := setSeqNumInMetadata(m, fileSeqNumStart+base.SeqNum(i), d.cmp, d.opts.Comparer.FormatKey); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tf := newIngestedFlushable(meta, d.opts.Comparer, d.newIters, d.tableNewRangeKeyIter, exciseSpan, seqNum)\n\n\t// NB: The logNum/seqNum are the WAL number which we're writing this entry\n\t// to and the sequence number within the WAL which we'll write this entry\n\t// to.\n\tentry := d.newFlushableEntry(f, logNum, seqNum)\n\t// The flushable entry starts off with a single reader ref, so increment\n\t// the FileMetadata.Refs.\n\tfor _, file := range f.files {\n\t\tfile.FileBacking.Ref()\n\t}\n\tentry.unrefFiles = func() []*fileBacking {\n\t\tvar obsolete []*fileBacking\n\t\tfor _, file := range f.files {\n\t\t\tif file.FileBacking.Unref() == 0 {\n\t\t\t\tobsolete = append(obsolete, file.FileMetadata.FileBacking)\n\t\t\t}\n\t\t}\n\t\treturn obsolete\n\t}\n\n\tentry.flushForced = true\n\tentry.releaseMemAccounting = func() {}\n\treturn entry, nil\n}\n\n// Both DB.mu and commitPipeline.mu must be held while this is called. Since\n// we're holding both locks, the order in which we rotate the memtable or\n// recycle the WAL in this function is irrelevant as long as the correct log\n// numbers are assigned to the appropriate flushable.\nfunc (d *DB) handleIngestAsFlushable(\n\tmeta []*fileMetadata, seqNum base.SeqNum, exciseSpan KeyRange,\n) error {\n\tb := d.NewBatch()\n\tif exciseSpan.Valid() {\n\t\tb.excise(exciseSpan.Start, exciseSpan.End)\n\t}\n\tfor _, m := range meta {\n\t\tb.ingestSST(m.FileNum)\n\t}\n\tb.setSeqNum(seqNum)\n\n\t// If the WAL is disabled, then the logNum used to create the flushable\n\t// entry doesn't matter. We just use the logNum assigned to the current\n\t// mutable memtable. If the WAL is enabled, then this logNum will be\n\t// overwritten by the logNum of the log which will contain the log entry\n\t// for the ingestedFlushable.\n\tlogNum := d.mu.mem.queue[len(d.mu.mem.queue)-1].logNum\n\tif !d.opts.DisableWAL {\n\t\t// We create a new WAL for the flushable instead of reusing the end of\n\t\t// the previous WAL. This simplifies the increment of the minimum\n\t\t// unflushed log number, and also simplifies WAL replay.\n\t\tvar prevLogSize uint64\n\t\tlogNum, prevLogSize = d.rotateWAL()\n\t\t// As the rotator of the WAL, we're responsible for updating the\n\t\t// previous flushable queue tail's log size.\n\t\td.mu.mem.queue[len(d.mu.mem.queue)-1].logSize = prevLogSize\n\n\t\td.mu.Unlock()\n\t\terr := d.commit.directWrite(b)\n\t\tif err != nil {\n\t\t\td.opts.Logger.Fatalf(\"%v\", err)\n\t\t}\n\t\td.mu.Lock()\n\t}\n\n\t// The excise span is going to outlive this ingestion call. Copy it.\n\texciseSpan = KeyRange{\n\t\tStart: slices.Clone(exciseSpan.Start),\n\t\tEnd:   slices.Clone(exciseSpan.End),\n\t}\n\tentry, err := d.newIngestedFlushableEntry(meta, seqNum, logNum, exciseSpan)\n\tif err != nil {\n\t\treturn err\n\t}\n\tnextSeqNum := seqNum + base.SeqNum(b.Count())\n\n\t// Set newLogNum to the logNum of the previous flushable. This value is\n\t// irrelevant if the WAL is disabled. If the WAL is enabled, then we set\n\t// the appropriate value below.\n\tnewLogNum := d.mu.mem.queue[len(d.mu.mem.queue)-1].logNum\n\tif !d.opts.DisableWAL {\n\t\t// newLogNum will be the WAL num of the next mutable memtable which\n\t\t// comes after the ingestedFlushable in the flushable queue. The mutable\n\t\t// memtable will be created below.\n\t\t//\n\t\t// The prevLogSize returned by rotateWAL is the WAL to which the\n\t\t// flushable ingest keys were appended. This intermediary WAL is only\n\t\t// used to record the flushable ingest and nothing else.\n\t\tnewLogNum, entry.logSize = d.rotateWAL()\n\t}\n\n\td.mu.versions.metrics.Ingest.Count++\n\tcurrMem := d.mu.mem.mutable\n\t// NB: Placing ingested sstables above the current memtables\n\t// requires rotating of the existing memtables/WAL. There is\n\t// some concern of churning through tiny memtables due to\n\t// ingested sstables being placed on top of them, but those\n\t// memtables would have to be flushed anyways.\n\td.mu.mem.queue = append(d.mu.mem.queue, entry)\n\td.rotateMemtable(newLogNum, nextSeqNum, currMem, 0 /* minSize */)\n\td.updateReadStateLocked(d.opts.DebugCheck)\n\t// TODO(aaditya): is this necessary? we call this already in rotateMemtable above\n\td.maybeScheduleFlush()\n\treturn nil\n}\n\n// See comment at Ingest() for details on how this works.\nfunc (d *DB) ingest(\n\tctx context.Context,\n\tpaths []string,\n\tshared []SharedSSTMeta,\n\texciseSpan KeyRange,\n\texternal []ExternalFile,\n) (IngestOperationStats, error) {\n\tif len(shared) > 0 && d.opts.Experimental.RemoteStorage == nil {\n\t\tpanic(\"cannot ingest shared sstables with nil SharedStorage\")\n\t}\n\tif (exciseSpan.Valid() || len(shared) > 0 || len(external) > 0) && d.FormatMajorVersion() < FormatVirtualSSTables {\n\t\treturn IngestOperationStats{}, errors.New(\"pebble: format major version too old for excise, shared or external sstable ingestion\")\n\t}\n\tif len(external) > 0 && d.FormatMajorVersion() < FormatSyntheticPrefixSuffix {\n\t\tfor i := range external {\n\t\t\tif len(external[i].SyntheticPrefix) > 0 {\n\t\t\t\treturn IngestOperationStats{}, errors.New(\"pebble: format major version too old for synthetic prefix ingestion\")\n\t\t\t}\n\t\t\tif len(external[i].SyntheticSuffix) > 0 {\n\t\t\t\treturn IngestOperationStats{}, errors.New(\"pebble: format major version too old for synthetic suffix ingestion\")\n\t\t\t}\n\t\t}\n\t}\n\t// Allocate file numbers for all of the files being ingested and mark them as\n\t// pending in order to prevent them from being deleted. Note that this causes\n\t// the file number ordering to be out of alignment with sequence number\n\t// ordering. The sorting of L0 tables by sequence number avoids relying on\n\t// that (busted) invariant.\n\tpendingOutputs := make([]base.FileNum, len(paths)+len(shared)+len(external))\n\tfor i := 0; i < len(paths)+len(shared)+len(external); i++ {\n\t\tpendingOutputs[i] = d.mu.versions.getNextFileNum()\n\t}\n\n\tjobID := d.newJobID()\n\n\t// Load the metadata for all the files being ingested. This step detects\n\t// and elides empty sstables.\n\tloadResult, err := ingestLoad(ctx, d.opts, d.FormatMajorVersion(), paths, shared, external, d.cacheID, pendingOutputs)\n\tif err != nil {\n\t\treturn IngestOperationStats{}, err\n\t}\n\n\tif loadResult.fileCount() == 0 {\n\t\t// All of the sstables to be ingested were empty. Nothing to do.\n\t\treturn IngestOperationStats{}, nil\n\t}\n\n\t// Verify the sstables do not overlap.\n\tif err := ingestSortAndVerify(d.cmp, loadResult, exciseSpan); err != nil {\n\t\treturn IngestOperationStats{}, err\n\t}\n\n\t// Hard link the sstables into the DB directory. Since the sstables aren't\n\t// referenced by a version, they won't be used. If the hard linking fails\n\t// (e.g. because the files reside on a different filesystem), ingestLinkLocal\n\t// will fall back to copying, and if that fails we undo our work and return an\n\t// error.\n\tif err := ingestLinkLocal(ctx, jobID, d.opts, d.objProvider, loadResult.local); err != nil {\n\t\treturn IngestOperationStats{}, err\n\t}\n\n\terr = d.ingestAttachRemote(jobID, loadResult)\n\tdefer d.ingestUnprotectExternalBackings(loadResult)\n\tif err != nil {\n\t\treturn IngestOperationStats{}, err\n\t}\n\n\t// Make the new tables durable. We need to do this at some point before we\n\t// update the MANIFEST (via logAndApply), otherwise a crash can have the\n\t// tables referenced in the MANIFEST, but not present in the provider.\n\tif err := d.objProvider.Sync(); err != nil {\n\t\treturn IngestOperationStats{}, err\n\t}\n\n\t// metaFlushableOverlaps is a map indicating which of the ingested sstables\n\t// overlap some table in the flushable queue. It's used to approximate\n\t// ingest-into-L0 stats when using flushable ingests.\n\tmetaFlushableOverlaps := make(map[FileNum]bool, loadResult.fileCount())\n\tvar mem *flushableEntry\n\tvar mut *memTable\n\t// asFlushable indicates whether the sstable was ingested as a flushable.\n\tvar asFlushable bool\n\tprepare := func(seqNum base.SeqNum) {\n\t\t// Note that d.commit.mu is held by commitPipeline when calling prepare.\n\n\t\t// Determine the set of bounds we care about for the purpose of checking\n\t\t// for overlap among the flushables. If there's an excise span, we need\n\t\t// to check for overlap with its bounds as well.\n\t\toverlapBounds := make([]bounded, 0, loadResult.fileCount()+1)\n\t\tfor _, m := range loadResult.local {\n\t\t\toverlapBounds = append(overlapBounds, m.fileMetadata)\n\t\t}\n\t\tfor _, m := range loadResult.shared {\n\t\t\toverlapBounds = append(overlapBounds, m.fileMetadata)\n\t\t}\n\t\tfor _, m := range loadResult.external {\n\t\t\toverlapBounds = append(overlapBounds, m.fileMetadata)\n\t\t}\n\t\tif exciseSpan.Valid() {\n\t\t\toverlapBounds = append(overlapBounds, &exciseSpan)\n\t\t}\n\n\t\td.mu.Lock()\n\t\tdefer d.mu.Unlock()\n\n\t\t// Check if any of the currently-open EventuallyFileOnlySnapshots overlap\n\t\t// in key ranges with the excise span. If so, we need to check for memtable\n\t\t// overlaps with all bounds of that EventuallyFileOnlySnapshot in addition\n\t\t// to the ingestion's own bounds too.\n\n\t\tif exciseSpan.Valid() {\n\t\t\tfor s := d.mu.snapshots.root.next; s != &d.mu.snapshots.root; s = s.next {\n\t\t\t\tif s.efos == nil {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif base.Visible(seqNum, s.efos.seqNum, base.SeqNumMax) {\n\t\t\t\t\t// We only worry about snapshots older than the excise. Any snapshots\n\t\t\t\t\t// created after the excise should see the excised view of the LSM\n\t\t\t\t\t// anyway.\n\t\t\t\t\t//\n\t\t\t\t\t// Since we delay publishing the excise seqnum as visible until after\n\t\t\t\t\t// the apply step, this case will never be hit in practice until we\n\t\t\t\t\t// make excises flushable ingests.\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif invariants.Enabled {\n\t\t\t\t\tif s.efos.hasTransitioned() {\n\t\t\t\t\t\tpanic(\"unexpected transitioned EFOS in snapshots list\")\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfor i := range s.efos.protectedRanges {\n\t\t\t\t\tif !s.efos.protectedRanges[i].OverlapsKeyRange(d.cmp, exciseSpan) {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\t// Our excise conflicts with this EFOS. We need to add its protected\n\t\t\t\t\t// ranges to our overlapBounds. Grow overlapBounds in one allocation\n\t\t\t\t\t// if necesary.\n\t\t\t\t\tprs := s.efos.protectedRanges\n\t\t\t\t\tif cap(overlapBounds) < len(overlapBounds)+len(prs) {\n\t\t\t\t\t\toldOverlapBounds := overlapBounds\n\t\t\t\t\t\toverlapBounds = make([]bounded, len(oldOverlapBounds), len(oldOverlapBounds)+len(prs))\n\t\t\t\t\t\tcopy(overlapBounds, oldOverlapBounds)\n\t\t\t\t\t}\n\t\t\t\t\tfor i := range prs {\n\t\t\t\t\t\toverlapBounds = append(overlapBounds, &prs[i])\n\t\t\t\t\t}\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// Check to see if any files overlap with any of the memtables. The queue\n\t\t// is ordered from oldest to newest with the mutable memtable being the\n\t\t// last element in the slice. We want to wait for the newest table that\n\t\t// overlaps.\n\n\t\tfor i := len(d.mu.mem.queue) - 1; i >= 0; i-- {\n\t\t\tm := d.mu.mem.queue[i]\n\t\t\tm.computePossibleOverlaps(func(b bounded) shouldContinue {\n\t\t\t\t// If this is the first table to overlap a flushable, save\n\t\t\t\t// the flushable. This ingest must be ingested or flushed\n\t\t\t\t// after it.\n\t\t\t\tif mem == nil {\n\t\t\t\t\tmem = m\n\t\t\t\t}\n\n\t\t\t\tswitch v := b.(type) {\n\t\t\t\tcase *fileMetadata:\n\t\t\t\t\t// NB: False positives are possible if `m` is a flushable\n\t\t\t\t\t// ingest that overlaps the file `v` in bounds but doesn't\n\t\t\t\t\t// contain overlapping data. This is considered acceptable\n\t\t\t\t\t// because it's rare (in CockroachDB a bound overlap likely\n\t\t\t\t\t// indicates a data overlap), and blocking the commit\n\t\t\t\t\t// pipeline while we perform I/O to check for overlap may be\n\t\t\t\t\t// more disruptive than enqueueing this ingestion on the\n\t\t\t\t\t// flushable queue and switching to a new memtable.\n\t\t\t\t\tmetaFlushableOverlaps[v.FileNum] = true\n\t\t\t\tcase *KeyRange:\n\t\t\t\t\t// An excise span or an EventuallyFileOnlySnapshot protected range;\n\t\t\t\t\t// not a file.\n\t\t\t\tdefault:\n\t\t\t\t\tpanic(\"unreachable\")\n\t\t\t\t}\n\t\t\t\treturn continueIteration\n\t\t\t}, overlapBounds...)\n\t\t}\n\n\t\tif mem == nil {\n\t\t\t// No overlap with any of the queued flushables, so no need to queue\n\t\t\t// after them.\n\n\t\t\t// New writes with higher sequence numbers may be concurrently\n\t\t\t// committed. We must ensure they don't flush before this ingest\n\t\t\t// completes. To do that, we ref the mutable memtable as a writer,\n\t\t\t// preventing its flushing (and the flushing of all subsequent\n\t\t\t// flushables in the queue). Once we've acquired the manifest lock\n\t\t\t// to add the ingested sstables to the LSM, we can unref as we're\n\t\t\t// guaranteed that the flush won't edit the LSM before this ingest.\n\t\t\tmut = d.mu.mem.mutable\n\t\t\tmut.writerRef()\n\t\t\treturn\n\t\t}\n\n\t\t// The ingestion overlaps with some entry in the flushable queue. If the\n\t\t// pre-conditions are met below, we can treat this ingestion as a flushable\n\t\t// ingest, otherwise we wait on the memtable flush before ingestion.\n\t\t//\n\t\t// TODO(aaditya): We should make flushableIngest compatible with remote\n\t\t// files.\n\t\thasRemoteFiles := len(shared) > 0 || len(external) > 0\n\t\tcanIngestFlushable := d.FormatMajorVersion() >= FormatFlushableIngest &&\n\t\t\t(len(d.mu.mem.queue) < d.opts.MemTableStopWritesThreshold) &&\n\t\t\t!d.opts.Experimental.DisableIngestAsFlushable() && !hasRemoteFiles &&\n\t\t\t(!exciseSpan.Valid() || d.FormatMajorVersion() >= FormatFlushableIngestExcises)\n\n\t\tif !canIngestFlushable {\n\t\t\t// We're not able to ingest as a flushable,\n\t\t\t// so we must synchronously flush.\n\t\t\t//\n\t\t\t// TODO(bilal): Currently, if any of the files being ingested are shared,\n\t\t\t// we cannot use flushable ingests and need\n\t\t\t// to wait synchronously.\n\t\t\tif mem.flushable == d.mu.mem.mutable {\n\t\t\t\terr = d.makeRoomForWrite(nil)\n\t\t\t}\n\t\t\t// New writes with higher sequence numbers may be concurrently\n\t\t\t// committed. We must ensure they don't flush before this ingest\n\t\t\t// completes. To do that, we ref the mutable memtable as a writer,\n\t\t\t// preventing its flushing (and the flushing of all subsequent\n\t\t\t// flushables in the queue). Once we've acquired the manifest lock\n\t\t\t// to add the ingested sstables to the LSM, we can unref as we're\n\t\t\t// guaranteed that the flush won't edit the LSM before this ingest.\n\t\t\tmut = d.mu.mem.mutable\n\t\t\tmut.writerRef()\n\t\t\tmem.flushForced = true\n\t\t\td.maybeScheduleFlush()\n\t\t\treturn\n\t\t}\n\t\t// Since there aren't too many memtables already queued up, we can\n\t\t// slide the ingested sstables on top of the existing memtables.\n\t\tasFlushable = true\n\t\tfileMetas := make([]*fileMetadata, len(loadResult.local))\n\t\tfor i := range fileMetas {\n\t\t\tfileMetas[i] = loadResult.local[i].fileMetadata\n\t\t}\n\t\terr = d.handleIngestAsFlushable(fileMetas, seqNum, exciseSpan)\n\t}\n\n\tvar ve *versionEdit\n\tapply := func(seqNum base.SeqNum) {\n\t\tif err != nil || asFlushable {\n\t\t\t// An error occurred during prepare.\n\t\t\tif mut != nil {\n\t\t\t\tif mut.writerUnref() {\n\t\t\t\t\td.mu.Lock()\n\t\t\t\t\td.maybeScheduleFlush()\n\t\t\t\t\td.mu.Unlock()\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn\n\t\t}\n\n\t\t// If there's an excise being done atomically with the same ingest, we\n\t\t// assign the lowest sequence number in the set of sequence numbers for this\n\t\t// ingestion to the excise. Note that we've already allocated fileCount+1\n\t\t// sequence numbers in this case.\n\t\tif exciseSpan.Valid() {\n\t\t\tseqNum++ // the first seqNum is reserved for the excise.\n\t\t}\n\t\t// Update the sequence numbers for all ingested sstables'\n\t\t// metadata. When the version edit is applied, the metadata is\n\t\t// written to the manifest, persisting the sequence number.\n\t\t// The sstables themselves are left unmodified.\n\t\tif err = ingestUpdateSeqNum(\n\t\t\td.cmp, d.opts.Comparer.FormatKey, seqNum, loadResult,\n\t\t); err != nil {\n\t\t\tif mut != nil {\n\t\t\t\tif mut.writerUnref() {\n\t\t\t\t\td.mu.Lock()\n\t\t\t\t\td.maybeScheduleFlush()\n\t\t\t\t\td.mu.Unlock()\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn\n\t\t}\n\n\t\t// If we overlapped with a memtable in prepare wait for the flush to\n\t\t// finish.\n\t\tif mem != nil {\n\t\t\t<-mem.flushed\n\t\t}\n\n\t\t// Assign the sstables to the correct level in the LSM and apply the\n\t\t// version edit.\n\t\tve, err = d.ingestApply(ctx, jobID, loadResult, mut, exciseSpan, seqNum)\n\t}\n\n\t// Only one ingest can occur at a time because if not, one would block waiting\n\t// for the other to finish applying. This blocking would happen while holding\n\t// the commit mutex which would prevent unrelated batches from writing their\n\t// changes to the WAL and memtable. This will cause a bigger commit hiccup\n\t// during ingestion.\n\tseqNumCount := loadResult.fileCount()\n\tif exciseSpan.Valid() {\n\t\tseqNumCount++\n\t}\n\td.commit.ingestSem <- struct{}{}\n\td.commit.AllocateSeqNum(seqNumCount, prepare, apply)\n\t<-d.commit.ingestSem\n\n\tif err != nil {\n\t\tif err2 := ingestCleanup(d.objProvider, loadResult.local); err2 != nil {\n\t\t\td.opts.Logger.Errorf(\"ingest cleanup failed: %v\", err2)\n\t\t}\n\t} else {\n\t\t// Since we either created a hard link to the ingesting files, or copied\n\t\t// them over, it is safe to remove the originals paths.\n\t\tfor i := range loadResult.local {\n\t\t\tpath := loadResult.local[i].path\n\t\t\tif err2 := d.opts.FS.Remove(path); err2 != nil {\n\t\t\t\td.opts.Logger.Errorf(\"ingest failed to remove original file: %s\", err2)\n\t\t\t}\n\t\t}\n\t}\n\n\tinfo := TableIngestInfo{\n\t\tJobID:     int(jobID),\n\t\tErr:       err,\n\t\tflushable: asFlushable,\n\t}\n\tif len(loadResult.local) > 0 {\n\t\tinfo.GlobalSeqNum = loadResult.local[0].SmallestSeqNum\n\t} else if len(loadResult.shared) > 0 {\n\t\tinfo.GlobalSeqNum = loadResult.shared[0].SmallestSeqNum\n\t} else {\n\t\tinfo.GlobalSeqNum = loadResult.external[0].SmallestSeqNum\n\t}\n\tvar stats IngestOperationStats\n\tif ve != nil {\n\t\tinfo.Tables = make([]struct {\n\t\t\tTableInfo\n\t\t\tLevel int\n\t\t}, len(ve.NewFiles))\n\t\tfor i := range ve.NewFiles {\n\t\t\te := &ve.NewFiles[i]\n\t\t\tinfo.Tables[i].Level = e.Level\n\t\t\tinfo.Tables[i].TableInfo = e.Meta.TableInfo()\n\t\t\tstats.Bytes += e.Meta.Size\n\t\t\tif e.Level == 0 {\n\t\t\t\tstats.ApproxIngestedIntoL0Bytes += e.Meta.Size\n\t\t\t}\n\t\t\tif metaFlushableOverlaps[e.Meta.FileNum] {\n\t\t\t\tstats.MemtableOverlappingFiles++\n\t\t\t}\n\t\t}\n\t} else if asFlushable {\n\t\t// NB: If asFlushable == true, there are no shared sstables.\n\t\tinfo.Tables = make([]struct {\n\t\t\tTableInfo\n\t\t\tLevel int\n\t\t}, len(loadResult.local))\n\t\tfor i, f := range loadResult.local {\n\t\t\tinfo.Tables[i].Level = -1\n\t\t\tinfo.Tables[i].TableInfo = f.TableInfo()\n\t\t\tstats.Bytes += f.Size\n\t\t\t// We don't have exact stats on which files will be ingested into\n\t\t\t// L0, because actual ingestion into the LSM has been deferred until\n\t\t\t// flush time. Instead, we infer based on memtable overlap.\n\t\t\t//\n\t\t\t// TODO(jackson): If we optimistically compute data overlap (#2112)\n\t\t\t// before entering the commit pipeline, we can use that overlap to\n\t\t\t// improve our approximation by incorporating overlap with L0, not\n\t\t\t// just memtables.\n\t\t\tif metaFlushableOverlaps[f.FileNum] {\n\t\t\t\tstats.ApproxIngestedIntoL0Bytes += f.Size\n\t\t\t\tstats.MemtableOverlappingFiles++\n\t\t\t}\n\t\t}\n\t}\n\td.opts.EventListener.TableIngested(info)\n\n\treturn stats, err\n}\n\n// excise updates ve to include a replacement of the file m with new virtual\n// sstables that exclude exciseSpan, returning a slice of newly-created files if\n// any. If the entirety of m is deleted by exciseSpan, no new sstables are added\n// and m is deleted. Note that ve is updated in-place.\n//\n// This method is agnostic to whether d.mu is held or not. Some cases call it with\n// the db mutex held (eg. ingest-time excises), while in the case of compactions\n// the mutex is not held.\nfunc (d *DB) excise(\n\tctx context.Context, exciseSpan base.UserKeyBounds, m *fileMetadata, ve *versionEdit, level int,\n) ([]manifest.NewFileEntry, error) {\n\tnumCreatedFiles := 0\n\t// Check if there's actually an overlap between m and exciseSpan.\n\tmBounds := base.UserKeyBoundsFromInternal(m.Smallest, m.Largest)\n\tif !exciseSpan.Overlaps(d.cmp, &mBounds) {\n\t\treturn nil, nil\n\t}\n\tve.DeletedFiles[deletedFileEntry{\n\t\tLevel:   level,\n\t\tFileNum: m.FileNum,\n\t}] = m\n\t// Fast path: m sits entirely within the exciseSpan, so just delete it.\n\tif exciseSpan.ContainsInternalKey(d.cmp, m.Smallest) && exciseSpan.ContainsInternalKey(d.cmp, m.Largest) {\n\t\treturn nil, nil\n\t}\n\n\tvar iters iterSet\n\tvar itersLoaded bool\n\tdefer iters.CloseAll()\n\tloadItersIfNecessary := func() error {\n\t\tif itersLoaded {\n\t\t\treturn nil\n\t\t}\n\t\tvar err error\n\t\titers, err = d.newIters(ctx, m, &IterOptions{\n\t\t\tCategory: categoryIngest,\n\t\t\tlayer:    manifest.Level(level),\n\t\t}, internalIterOpts{}, iterPointKeys|iterRangeDeletions|iterRangeKeys)\n\t\titersLoaded = true\n\t\treturn err\n\t}\n\n\tneedsBacking := false\n\t// Create a file to the left of the excise span, if necessary.\n\t// The bounds of this file will be [m.Smallest, lastKeyBefore(exciseSpan.Start)].\n\t//\n\t// We create bounds that are tight on user keys, and we make the effort to find\n\t// the last key in the original sstable that's smaller than exciseSpan.Start\n\t// even though it requires some sstable reads. We could choose to create\n\t// virtual sstables on loose userKey bounds, in which case we could just set\n\t// leftFile.Largest to an exclusive sentinel at exciseSpan.Start. The biggest\n\t// issue with that approach would be that it'd lead to lots of small virtual\n\t// sstables in the LSM that have no guarantee on containing even a single user\n\t// key within the file bounds. This has the potential to increase both read and\n\t// write-amp as we will be opening up these sstables only to find no relevant\n\t// keys in the read path, and compacting sstables on top of them instead of\n\t// directly into the space occupied by them. We choose to incur the cost of\n\t// calculating tight bounds at this time instead of creating more work in the\n\t// future.\n\t//\n\t// TODO(bilal): Some of this work can happen without grabbing the manifest\n\t// lock; we could grab one currentVersion, release the lock, calculate excised\n\t// files, then grab the lock again and recalculate for just the files that\n\t// have changed since our previous calculation. Do this optimiaztino as part of\n\t// https://github.com/cockroachdb/pebble/issues/2112 .\n\tif d.cmp(m.Smallest.UserKey, exciseSpan.Start) < 0 {\n\t\tleftFile := &fileMetadata{\n\t\t\tVirtual:     true,\n\t\t\tFileBacking: m.FileBacking,\n\t\t\tFileNum:     d.mu.versions.getNextFileNum(),\n\t\t\t// Note that these are loose bounds for smallest/largest seqnums, but they're\n\t\t\t// sufficient for maintaining correctness.\n\t\t\tSmallestSeqNum:           m.SmallestSeqNum,\n\t\t\tLargestSeqNum:            m.LargestSeqNum,\n\t\t\tLargestSeqNumAbsolute:    m.LargestSeqNumAbsolute,\n\t\t\tSyntheticPrefixAndSuffix: m.SyntheticPrefixAndSuffix,\n\t\t}\n\t\tif m.HasPointKeys && !exciseSpan.ContainsInternalKey(d.cmp, m.SmallestPointKey) {\n\t\t\t// This file will probably contain point keys.\n\t\t\tif err := loadItersIfNecessary(); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tsmallestPointKey := m.SmallestPointKey\n\t\t\tif kv := iters.Point().SeekLT(exciseSpan.Start, base.SeekLTFlagsNone); kv != nil {\n\t\t\t\tleftFile.ExtendPointKeyBounds(d.cmp, smallestPointKey, kv.K.Clone())\n\t\t\t}\n\t\t\t// Store the min of (exciseSpan.Start, rdel.End) in lastRangeDel. This\n\t\t\t// needs to be a copy if the key is owned by the range del iter.\n\t\t\tvar lastRangeDel []byte\n\t\t\tif rdel, err := iters.RangeDeletion().SeekLT(exciseSpan.Start); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t} else if rdel != nil {\n\t\t\t\tlastRangeDel = append(lastRangeDel[:0], rdel.End...)\n\t\t\t\tif d.cmp(lastRangeDel, exciseSpan.Start) > 0 {\n\t\t\t\t\tlastRangeDel = exciseSpan.Start\n\t\t\t\t}\n\t\t\t}\n\t\t\tif lastRangeDel != nil {\n\t\t\t\tleftFile.ExtendPointKeyBounds(d.cmp, smallestPointKey, base.MakeExclusiveSentinelKey(InternalKeyKindRangeDelete, lastRangeDel))\n\t\t\t}\n\t\t}\n\t\tif m.HasRangeKeys && !exciseSpan.ContainsInternalKey(d.cmp, m.SmallestRangeKey) {\n\t\t\t// This file will probably contain range keys.\n\t\t\tif err := loadItersIfNecessary(); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tsmallestRangeKey := m.SmallestRangeKey\n\t\t\t// Store the min of (exciseSpan.Start, rkey.End) in lastRangeKey. This\n\t\t\t// needs to be a copy if the key is owned by the range key iter.\n\t\t\tvar lastRangeKey []byte\n\t\t\tvar lastRangeKeyKind InternalKeyKind\n\t\t\tif rkey, err := iters.RangeKey().SeekLT(exciseSpan.Start); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t} else if rkey != nil {\n\t\t\t\tlastRangeKey = append(lastRangeKey[:0], rkey.End...)\n\t\t\t\tif d.cmp(lastRangeKey, exciseSpan.Start) > 0 {\n\t\t\t\t\tlastRangeKey = exciseSpan.Start\n\t\t\t\t}\n\t\t\t\tlastRangeKeyKind = rkey.Keys[0].Kind()\n\t\t\t}\n\t\t\tif lastRangeKey != nil {\n\t\t\t\tleftFile.ExtendRangeKeyBounds(d.cmp, smallestRangeKey, base.MakeExclusiveSentinelKey(lastRangeKeyKind, lastRangeKey))\n\t\t\t}\n\t\t}\n\t\tif leftFile.HasRangeKeys || leftFile.HasPointKeys {\n\t\t\tvar err error\n\t\t\tleftFile.Size, err = d.fileCache.estimateSize(m, leftFile.Smallest.UserKey, leftFile.Largest.UserKey)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tif leftFile.Size == 0 {\n\t\t\t\t// On occasion, estimateSize gives us a low estimate, i.e. a 0 file size,\n\t\t\t\t// such as if the excised file only has range keys/dels and no point\n\t\t\t\t// keys. This can cause panics in places where we divide by file sizes.\n\t\t\t\t// Correct for it here.\n\t\t\t\tleftFile.Size = 1\n\t\t\t}\n\t\t\tif err := leftFile.Validate(d.cmp, d.opts.Comparer.FormatKey); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tleftFile.ValidateVirtual(m)\n\t\t\tve.NewFiles = append(ve.NewFiles, newFileEntry{Level: level, Meta: leftFile})\n\t\t\tneedsBacking = true\n\t\t\tnumCreatedFiles++\n\t\t}\n\t}\n\t// Create a file to the right, if necessary.\n\tif exciseSpan.ContainsInternalKey(d.cmp, m.Largest) {\n\t\t// No key exists to the right of the excise span in this file.\n\t\tif needsBacking && !m.Virtual {\n\t\t\t// If m is virtual, then its file backing is already known to the manifest.\n\t\t\t// We don't need to create another file backing. Note that there must be\n\t\t\t// only one CreatedBackingTables entry per backing sstable. This is\n\t\t\t// indicated by the VersionEdit.CreatedBackingTables invariant.\n\t\t\tve.CreatedBackingTables = append(ve.CreatedBackingTables, m.FileBacking)\n\t\t}\n\t\treturn ve.NewFiles[len(ve.NewFiles)-numCreatedFiles:], nil\n\t}\n\t// Create a new file, rightFile, between [firstKeyAfter(exciseSpan.End), m.Largest].\n\t//\n\t// See comment before the definition of leftFile for the motivation behind\n\t// calculating tight user-key bounds.\n\trightFile := &fileMetadata{\n\t\tVirtual:     true,\n\t\tFileBacking: m.FileBacking,\n\t\tFileNum:     d.mu.versions.getNextFileNum(),\n\t\t// Note that these are loose bounds for smallest/largest seqnums, but they're\n\t\t// sufficient for maintaining correctness.\n\t\tSmallestSeqNum:           m.SmallestSeqNum,\n\t\tLargestSeqNum:            m.LargestSeqNum,\n\t\tLargestSeqNumAbsolute:    m.LargestSeqNumAbsolute,\n\t\tSyntheticPrefixAndSuffix: m.SyntheticPrefixAndSuffix,\n\t}\n\tif m.HasPointKeys && !exciseSpan.ContainsInternalKey(d.cmp, m.LargestPointKey) {\n\t\t// This file will probably contain point keys\n\t\tif err := loadItersIfNecessary(); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tlargestPointKey := m.LargestPointKey\n\t\tif kv := iters.Point().SeekGE(exciseSpan.End.Key, base.SeekGEFlagsNone); kv != nil {\n\t\t\tif exciseSpan.End.Kind == base.Inclusive && d.equal(exciseSpan.End.Key, kv.K.UserKey) {\n\t\t\t\treturn nil, base.AssertionFailedf(\"cannot excise with an inclusive end key and data overlap at end key\")\n\t\t\t}\n\t\t\trightFile.ExtendPointKeyBounds(d.cmp, kv.K.Clone(), largestPointKey)\n\t\t}\n\t\t// Store the max of (exciseSpan.End, rdel.Start) in firstRangeDel. This\n\t\t// needs to be a copy if the key is owned by the range del iter.\n\t\tvar firstRangeDel []byte\n\t\trdel, err := iters.RangeDeletion().SeekGE(exciseSpan.End.Key)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t} else if rdel != nil {\n\t\t\tfirstRangeDel = append(firstRangeDel[:0], rdel.Start...)\n\t\t\tif d.cmp(firstRangeDel, exciseSpan.End.Key) < 0 {\n\t\t\t\t// NB: This can only be done if the end bound is exclusive.\n\t\t\t\tif exciseSpan.End.Kind != base.Exclusive {\n\t\t\t\t\treturn nil, base.AssertionFailedf(\"cannot truncate rangedel during excise with an inclusive upper bound\")\n\t\t\t\t}\n\t\t\t\tfirstRangeDel = exciseSpan.End.Key\n\t\t\t}\n\t\t}\n\t\tif firstRangeDel != nil {\n\t\t\tsmallestPointKey := rdel.SmallestKey()\n\t\t\tsmallestPointKey.UserKey = firstRangeDel\n\t\t\trightFile.ExtendPointKeyBounds(d.cmp, smallestPointKey, largestPointKey)\n\t\t}\n\t}\n\tif m.HasRangeKeys && !exciseSpan.ContainsInternalKey(d.cmp, m.LargestRangeKey) {\n\t\t// This file will probably contain range keys.\n\t\tif err := loadItersIfNecessary(); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tlargestRangeKey := m.LargestRangeKey\n\t\t// Store the max of (exciseSpan.End, rkey.Start) in firstRangeKey. This\n\t\t// needs to be a copy if the key is owned by the range key iter.\n\t\tvar firstRangeKey []byte\n\t\trkey, err := iters.RangeKey().SeekGE(exciseSpan.End.Key)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t} else if rkey != nil {\n\t\t\tfirstRangeKey = append(firstRangeKey[:0], rkey.Start...)\n\t\t\tif d.cmp(firstRangeKey, exciseSpan.End.Key) < 0 {\n\t\t\t\tif exciseSpan.End.Kind != base.Exclusive {\n\t\t\t\t\treturn nil, base.AssertionFailedf(\"cannot truncate range key during excise with an inclusive upper bound\")\n\t\t\t\t}\n\t\t\t\tfirstRangeKey = exciseSpan.End.Key\n\t\t\t}\n\t\t}\n\t\tif firstRangeKey != nil {\n\t\t\tsmallestRangeKey := rkey.SmallestKey()\n\t\t\tsmallestRangeKey.UserKey = firstRangeKey\n\t\t\t// We call ExtendRangeKeyBounds so any internal boundType fields are\n\t\t\t// set correctly. Note that this is mildly wasteful as we'll be comparing\n\t\t\t// rightFile.{Smallest,Largest}RangeKey with themselves, which can be\n\t\t\t// avoided if we exported ExtendOverallKeyBounds or so.\n\t\t\trightFile.ExtendRangeKeyBounds(d.cmp, smallestRangeKey, largestRangeKey)\n\t\t}\n\t}\n\tif rightFile.HasRangeKeys || rightFile.HasPointKeys {\n\t\tvar err error\n\t\trightFile.Size, err = d.fileCache.estimateSize(m, rightFile.Smallest.UserKey, rightFile.Largest.UserKey)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif rightFile.Size == 0 {\n\t\t\t// On occasion, estimateSize gives us a low estimate, i.e. a 0 file size,\n\t\t\t// such as if the excised file only has range keys/dels and no point keys.\n\t\t\t// This can cause panics in places where we divide by file sizes. Correct\n\t\t\t// for it here.\n\t\t\trightFile.Size = 1\n\t\t}\n\t\tif err := rightFile.Validate(d.cmp, d.opts.Comparer.FormatKey); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\trightFile.ValidateVirtual(m)\n\t\tve.NewFiles = append(ve.NewFiles, newFileEntry{Level: level, Meta: rightFile})\n\t\tneedsBacking = true\n\t\tnumCreatedFiles++\n\t}\n\n\tif needsBacking && !m.Virtual {\n\t\t// If m is virtual, then its file backing is already known to the manifest.\n\t\t// We don't need to create another file backing. Note that there must be\n\t\t// only one CreatedBackingTables entry per backing sstable. This is\n\t\t// indicated by the VersionEdit.CreatedBackingTables invariant.\n\t\tve.CreatedBackingTables = append(ve.CreatedBackingTables, m.FileBacking)\n\t}\n\n\treturn ve.NewFiles[len(ve.NewFiles)-numCreatedFiles:], nil\n}\n\ntype ingestSplitFile struct {\n\t// ingestFile is the file being ingested.\n\tingestFile *fileMetadata\n\t// splitFile is the file that needs to be split to allow ingestFile to slot\n\t// into `level` level.\n\tsplitFile *fileMetadata\n\t// The level where ingestFile will go (and where splitFile already is).\n\tlevel int\n}\n\n// ingestSplit splits files specified in `files` and updates ve in-place to\n// account for existing files getting split into two virtual sstables. The map\n// `replacedFiles` contains an in-progress map of all files that have been\n// replaced with new virtual sstables in this version edit so far, which is also\n// updated in-place.\n//\n// d.mu as well as the manifest lock must be held when calling this method.\nfunc (d *DB) ingestSplit(\n\tctx context.Context,\n\tve *versionEdit,\n\tupdateMetrics func(*fileMetadata, int, []newFileEntry),\n\tfiles []ingestSplitFile,\n\treplacedFiles map[base.FileNum][]newFileEntry,\n) error {\n\tfor _, s := range files {\n\t\tingestFileBounds := s.ingestFile.UserKeyBounds()\n\t\t// replacedFiles can be thought of as a tree, where we start iterating with\n\t\t// s.splitFile and run its fileNum through replacedFiles, then find which of\n\t\t// the replaced files overlaps with s.ingestFile, which becomes the new\n\t\t// splitFile, then we check splitFile's replacements in replacedFiles again\n\t\t// for overlap with s.ingestFile, and so on until we either can't find the\n\t\t// current splitFile in replacedFiles (i.e. that's the file that now needs to\n\t\t// be split), or we don't find a file that overlaps with s.ingestFile, which\n\t\t// means a prior ingest split already produced enough room for s.ingestFile\n\t\t// to go into this level without necessitating another ingest split.\n\t\tsplitFile := s.splitFile\n\t\tfor splitFile != nil {\n\t\t\treplaced, ok := replacedFiles[splitFile.FileNum]\n\t\t\tif !ok {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tupdatedSplitFile := false\n\t\t\tfor i := range replaced {\n\t\t\t\tif replaced[i].Meta.Overlaps(d.cmp, &ingestFileBounds) {\n\t\t\t\t\tif updatedSplitFile {\n\t\t\t\t\t\t// This should never happen because the earlier ingestTargetLevel\n\t\t\t\t\t\t// function only finds split file candidates that are guaranteed to\n\t\t\t\t\t\t// have no data overlap, only boundary overlap. See the comments\n\t\t\t\t\t\t// in that method to see the definitions of data vs boundary\n\t\t\t\t\t\t// overlap. That, plus the fact that files in `replaced` are\n\t\t\t\t\t\t// guaranteed to have file bounds that are tight on user keys\n\t\t\t\t\t\t// (as that's what `d.excise` produces), means that the only case\n\t\t\t\t\t\t// where we overlap with two or more files in `replaced` is if we\n\t\t\t\t\t\t// actually had data overlap all along, or if the ingestion files\n\t\t\t\t\t\t// were overlapping, either of which is an invariant violation.\n\t\t\t\t\t\tpanic(\"updated with two files in ingestSplit\")\n\t\t\t\t\t}\n\t\t\t\t\tsplitFile = replaced[i].Meta\n\t\t\t\t\tupdatedSplitFile = true\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !updatedSplitFile {\n\t\t\t\t// None of the replaced files overlapped with the file being ingested.\n\t\t\t\t// This can happen if we've already excised a span overlapping with\n\t\t\t\t// this file, or if we have consecutive ingested files that can slide\n\t\t\t\t// within the same gap between keys in an existing file. For instance,\n\t\t\t\t// if an existing file has keys a and g and we're ingesting b-c, d-e,\n\t\t\t\t// the first loop iteration will split the existing file into one that\n\t\t\t\t// ends in a and another that starts at g, and the second iteration will\n\t\t\t\t// fall into this case and require no splitting.\n\t\t\t\t//\n\t\t\t\t// No splitting necessary.\n\t\t\t\tsplitFile = nil\n\t\t\t}\n\t\t}\n\t\tif splitFile == nil {\n\t\t\tcontinue\n\t\t}\n\t\t// NB: excise operates on [start, end). We're splitting at [start, end]\n\t\t// (assuming !s.ingestFile.Largest.IsExclusiveSentinel()). The conflation\n\t\t// of exclusive vs inclusive end bounds should not make a difference here\n\t\t// as we're guaranteed to not have any data overlap between splitFile and\n\t\t// s.ingestFile. d.excise will return an error if we pass an inclusive user\n\t\t// key bound _and_ we end up seeing data overlap at the end key.\n\t\tadded, err := d.excise(ctx, base.UserKeyBoundsFromInternal(s.ingestFile.Smallest, s.ingestFile.Largest), splitFile, ve, s.level)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif _, ok := ve.DeletedFiles[deletedFileEntry{\n\t\t\tLevel:   s.level,\n\t\t\tFileNum: splitFile.FileNum,\n\t\t}]; !ok {\n\t\t\tpanic(\"did not split file that was expected to be split\")\n\t\t}\n\t\treplacedFiles[splitFile.FileNum] = added\n\t\tfor i := range added {\n\t\t\taddedBounds := added[i].Meta.UserKeyBounds()\n\t\t\tif s.ingestFile.Overlaps(d.cmp, &addedBounds) {\n\t\t\t\tpanic(\"ingest-time split produced a file that overlaps with ingested file\")\n\t\t\t}\n\t\t}\n\t\tupdateMetrics(splitFile, s.level, added)\n\t}\n\t// Flatten the version edit by removing any entries from ve.NewFiles that\n\t// are also in ve.DeletedFiles.\n\tnewNewFiles := ve.NewFiles[:0]\n\tfor i := range ve.NewFiles {\n\t\tfn := ve.NewFiles[i].Meta.FileNum\n\t\tdeEntry := deletedFileEntry{Level: ve.NewFiles[i].Level, FileNum: fn}\n\t\tif _, ok := ve.DeletedFiles[deEntry]; ok {\n\t\t\tdelete(ve.DeletedFiles, deEntry)\n\t\t} else {\n\t\t\tnewNewFiles = append(newNewFiles, ve.NewFiles[i])\n\t\t}\n\t}\n\tve.NewFiles = newNewFiles\n\treturn nil\n}\n\nfunc (d *DB) ingestApply(\n\tctx context.Context,\n\tjobID JobID,\n\tlr ingestLoadResult,\n\tmut *memTable,\n\texciseSpan KeyRange,\n\texciseSeqNum base.SeqNum,\n) (*versionEdit, error) {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\n\tve := &versionEdit{\n\t\tNewFiles: make([]newFileEntry, lr.fileCount()),\n\t}\n\tif exciseSpan.Valid() || (d.opts.Experimental.IngestSplit != nil && d.opts.Experimental.IngestSplit()) {\n\t\tve.DeletedFiles = map[manifest.DeletedFileEntry]*manifest.FileMetadata{}\n\t}\n\tmetrics := make(map[int]*LevelMetrics)\n\n\t// Lock the manifest for writing before we use the current version to\n\t// determine the target level. This prevents two concurrent ingestion jobs\n\t// from using the same version to determine the target level, and also\n\t// provides serialization with concurrent compaction and flush jobs.\n\t// logAndApply unconditionally releases the manifest lock, but any earlier\n\t// returns must unlock the manifest.\n\td.mu.versions.logLock()\n\n\tif mut != nil {\n\t\t// Unref the mutable memtable to allows its flush to proceed. Now that we've\n\t\t// acquired the manifest lock, we can be certain that if the mutable\n\t\t// memtable has received more recent conflicting writes, the flush won't\n\t\t// beat us to applying to the manifest resulting in sequence number\n\t\t// inversion. Even though we call maybeScheduleFlush right now, this flush\n\t\t// will apply after our ingestion.\n\t\tif mut.writerUnref() {\n\t\t\td.maybeScheduleFlush()\n\t\t}\n\t}\n\n\tcurrent := d.mu.versions.currentVersion()\n\toverlapChecker := &overlapChecker{\n\t\tcomparer: d.opts.Comparer,\n\t\tnewIters: d.newIters,\n\t\topts: IterOptions{\n\t\t\tlogger:   d.opts.Logger,\n\t\t\tCategory: categoryIngest,\n\t\t},\n\t\tv: current,\n\t}\n\tshouldIngestSplit := d.opts.Experimental.IngestSplit != nil &&\n\t\td.opts.Experimental.IngestSplit() && d.FormatMajorVersion() >= FormatVirtualSSTables\n\tbaseLevel := d.mu.versions.picker.getBaseLevel()\n\t// filesToSplit is a list where each element is a pair consisting of a file\n\t// being ingested and a file being split to make room for an ingestion into\n\t// that level. Each ingested file will appear at most once in this list. It\n\t// is possible for split files to appear twice in this list.\n\tfilesToSplit := make([]ingestSplitFile, 0)\n\tcheckCompactions := false\n\tfor i := 0; i < lr.fileCount(); i++ {\n\t\t// Determine the lowest level in the LSM for which the sstable doesn't\n\t\t// overlap any existing files in the level.\n\t\tvar m *fileMetadata\n\t\tspecifiedLevel := -1\n\t\tisShared := false\n\t\tisExternal := false\n\t\tif i < len(lr.local) {\n\t\t\t// local file.\n\t\t\tm = lr.local[i].fileMetadata\n\t\t} else if (i - len(lr.local)) < len(lr.shared) {\n\t\t\t// shared file.\n\t\t\tisShared = true\n\t\t\tsharedIdx := i - len(lr.local)\n\t\t\tm = lr.shared[sharedIdx].fileMetadata\n\t\t\tspecifiedLevel = int(lr.shared[sharedIdx].shared.Level)\n\t\t} else {\n\t\t\t// external file.\n\t\t\tisExternal = true\n\t\t\texternalIdx := i - (len(lr.local) + len(lr.shared))\n\t\t\tm = lr.external[externalIdx].fileMetadata\n\t\t\tif lr.externalFilesHaveLevel {\n\t\t\t\tspecifiedLevel = int(lr.external[externalIdx].external.Level)\n\t\t\t}\n\t\t}\n\n\t\t// Add to CreatedBackingTables if this is a new backing.\n\t\t//\n\t\t// Shared files always have a new backing. External files have new backings\n\t\t// iff the backing disk file num and the file num match (see ingestAttachRemote).\n\t\tif isShared || (isExternal && m.FileBacking.DiskFileNum == base.DiskFileNum(m.FileNum)) {\n\t\t\tve.CreatedBackingTables = append(ve.CreatedBackingTables, m.FileBacking)\n\t\t}\n\n\t\tf := &ve.NewFiles[i]\n\t\tvar err error\n\t\tif specifiedLevel != -1 {\n\t\t\tf.Level = specifiedLevel\n\t\t} else {\n\t\t\tvar splitFile *fileMetadata\n\t\t\tif exciseSpan.Valid() && exciseSpan.Contains(d.cmp, m.Smallest) && exciseSpan.Contains(d.cmp, m.Largest) {\n\t\t\t\t// This file fits perfectly within the excise span. We can slot it at\n\t\t\t\t// L6, or sharedLevelsStart - 1 if we have shared files.\n\t\t\t\tif len(lr.shared) > 0 || lr.externalFilesHaveLevel {\n\t\t\t\t\tf.Level = sharedLevelsStart - 1\n\t\t\t\t\tif baseLevel > f.Level {\n\t\t\t\t\t\tf.Level = 0\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tf.Level = 6\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// We check overlap against the LSM without holding DB.mu. Note that we\n\t\t\t\t// are still holding the log lock, so the version cannot change.\n\t\t\t\t// TODO(radu): perform this check optimistically outside of the log lock.\n\t\t\t\tvar lsmOverlap overlap.WithLSM\n\t\t\t\tlsmOverlap, err = func() (overlap.WithLSM, error) {\n\t\t\t\t\td.mu.Unlock()\n\t\t\t\t\tdefer d.mu.Lock()\n\t\t\t\t\treturn overlapChecker.DetermineLSMOverlap(ctx, m.UserKeyBounds())\n\t\t\t\t}()\n\t\t\t\tif err == nil {\n\t\t\t\t\tf.Level, splitFile, err = ingestTargetLevel(\n\t\t\t\t\t\tctx, d.cmp, lsmOverlap, baseLevel, d.mu.compact.inProgress, m, shouldIngestSplit,\n\t\t\t\t\t)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif splitFile != nil {\n\t\t\t\tif invariants.Enabled {\n\t\t\t\t\tif lf := current.Levels[f.Level].Find(d.cmp, splitFile); lf.Empty() {\n\t\t\t\t\t\tpanic(\"splitFile returned is not in level it should be\")\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t// We take advantage of the fact that we won't drop the db mutex\n\t\t\t\t// between now and the call to logAndApply. So, no files should\n\t\t\t\t// get added to a new in-progress compaction at this point. We can\n\t\t\t\t// avoid having to iterate on in-progress compactions to cancel them\n\t\t\t\t// if none of the files being split have a compacting state.\n\t\t\t\tif splitFile.IsCompacting() {\n\t\t\t\t\tcheckCompactions = true\n\t\t\t\t}\n\t\t\t\tfilesToSplit = append(filesToSplit, ingestSplitFile{ingestFile: m, splitFile: splitFile, level: f.Level})\n\t\t\t}\n\t\t}\n\t\tif err != nil {\n\t\t\td.mu.versions.logUnlock()\n\t\t\treturn nil, err\n\t\t}\n\t\tif isShared && f.Level < sharedLevelsStart {\n\t\t\tpanic(fmt.Sprintf(\"cannot slot a shared file higher than the highest shared level: %d < %d\",\n\t\t\t\tf.Level, sharedLevelsStart))\n\t\t}\n\t\tf.Meta = m\n\t\tlevelMetrics := metrics[f.Level]\n\t\tif levelMetrics == nil {\n\t\t\tlevelMetrics = &LevelMetrics{}\n\t\t\tmetrics[f.Level] = levelMetrics\n\t\t}\n\t\tlevelMetrics.NumFiles++\n\t\tlevelMetrics.Size += int64(m.Size)\n\t\tlevelMetrics.BytesIngested += m.Size\n\t\tlevelMetrics.TablesIngested++\n\t}\n\t// replacedFiles maps files excised due to exciseSpan (or splitFiles returned\n\t// by ingestTargetLevel), to files that were created to replace it. This map\n\t// is used to resolve references to split files in filesToSplit, as it is\n\t// possible for a file that we want to split to no longer exist or have a\n\t// newer fileMetadata due to a split induced by another ingestion file, or an\n\t// excise.\n\treplacedFiles := make(map[base.FileNum][]newFileEntry)\n\tupdateLevelMetricsOnExcise := func(m *fileMetadata, level int, added []newFileEntry) {\n\t\tlevelMetrics := metrics[level]\n\t\tif levelMetrics == nil {\n\t\t\tlevelMetrics = &LevelMetrics{}\n\t\t\tmetrics[level] = levelMetrics\n\t\t}\n\t\tlevelMetrics.NumFiles--\n\t\tlevelMetrics.Size -= int64(m.Size)\n\t\tfor i := range added {\n\t\t\tlevelMetrics.NumFiles++\n\t\t\tlevelMetrics.Size += int64(added[i].Meta.Size)\n\t\t}\n\t}\n\tif exciseSpan.Valid() {\n\t\t// Iterate through all levels and find files that intersect with exciseSpan.\n\t\t//\n\t\t// TODO(bilal): We could drop the DB mutex here as we don't need it for\n\t\t// excises; we only need to hold the version lock which we already are\n\t\t// holding. However releasing the DB mutex could mess with the\n\t\t// ingestTargetLevel calculation that happened above, as it assumed that it\n\t\t// had a complete view of in-progress compactions that wouldn't change\n\t\t// until logAndApply is called. If we were to drop the mutex now, we could\n\t\t// schedule another in-progress compaction that would go into the chosen target\n\t\t// level and lead to file overlap within level (which would panic in\n\t\t// logAndApply). We should drop the db mutex here, do the excise, then\n\t\t// re-grab the DB mutex and rerun just the in-progress compaction check to\n\t\t// see if any new compactions are conflicting with our chosen target levels\n\t\t// for files, and if they are, we should signal those compactions to error\n\t\t// out.\n\t\tfor level := range current.Levels {\n\t\t\toverlaps := current.Overlaps(level, exciseSpan.UserKeyBounds())\n\t\t\titer := overlaps.Iter()\n\n\t\t\tfor m := iter.First(); m != nil; m = iter.Next() {\n\t\t\t\tnewFiles, err := d.excise(ctx, exciseSpan.UserKeyBounds(), m, ve, level)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\n\t\t\t\tif _, ok := ve.DeletedFiles[deletedFileEntry{\n\t\t\t\t\tLevel:   level,\n\t\t\t\t\tFileNum: m.FileNum,\n\t\t\t\t}]; !ok {\n\t\t\t\t\t// We did not excise this file.\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\treplacedFiles[m.FileNum] = newFiles\n\t\t\t\tupdateLevelMetricsOnExcise(m, level, newFiles)\n\t\t\t}\n\t\t}\n\t}\n\tif len(filesToSplit) > 0 {\n\t\t// For the same reasons as the above call to excise, we hold the db mutex\n\t\t// while calling this method.\n\t\tif err := d.ingestSplit(ctx, ve, updateLevelMetricsOnExcise, filesToSplit, replacedFiles); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\tif len(filesToSplit) > 0 || exciseSpan.Valid() {\n\t\tfor c := range d.mu.compact.inProgress {\n\t\t\tif c.versionEditApplied {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// Check if this compaction overlaps with the excise span. Note that just\n\t\t\t// checking if the inputs individually overlap with the excise span\n\t\t\t// isn't sufficient; for instance, a compaction could have [a,b] and [e,f]\n\t\t\t// as inputs and write it all out as [a,b,e,f] in one sstable. If we're\n\t\t\t// doing a [c,d) excise at the same time as this compaction, we will have\n\t\t\t// to error out the whole compaction as we can't guarantee it hasn't/won't\n\t\t\t// write a file overlapping with the excise span.\n\t\t\tif exciseSpan.OverlapsInternalKeyRange(d.cmp, c.smallest, c.largest) {\n\t\t\t\tc.cancel.Store(true)\n\t\t\t}\n\t\t\t// Check if this compaction's inputs have been replaced due to an\n\t\t\t// ingest-time split. In that case, cancel the compaction as a newly picked\n\t\t\t// compaction would need to include any new files that slid in between\n\t\t\t// previously-existing files. Note that we cancel any compaction that has a\n\t\t\t// file that was ingest-split as an input, even if it started before this\n\t\t\t// ingestion.\n\t\t\tif checkCompactions {\n\t\t\t\tfor i := range c.inputs {\n\t\t\t\t\titer := c.inputs[i].files.Iter()\n\t\t\t\t\tfor f := iter.First(); f != nil; f = iter.Next() {\n\t\t\t\t\t\tif _, ok := replacedFiles[f.FileNum]; ok {\n\t\t\t\t\t\t\tc.cancel.Store(true)\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif err := d.mu.versions.logAndApply(jobID, ve, metrics, false /* forceRotation */, func() []compactionInfo {\n\t\treturn d.getInProgressCompactionInfoLocked(nil)\n\t}); err != nil {\n\t\t// Note: any error during logAndApply is fatal; this won't be reachable in production.\n\t\treturn nil, err\n\t}\n\n\t// Check for any EventuallyFileOnlySnapshots that could be watching for\n\t// an excise on this span. There should be none as the\n\t// computePossibleOverlaps steps should have forced these EFOS to transition\n\t// to file-only snapshots by now. If we see any that conflict with this\n\t// excise, panic.\n\tif exciseSpan.Valid() {\n\t\tfor s := d.mu.snapshots.root.next; s != &d.mu.snapshots.root; s = s.next {\n\t\t\t// Skip non-EFOS snapshots, and also skip any EFOS that were created\n\t\t\t// *after* the excise.\n\t\t\tif s.efos == nil || base.Visible(exciseSeqNum, s.efos.seqNum, base.SeqNumMax) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tefos := s.efos\n\t\t\t// TODO(bilal): We can make this faster by taking advantage of the sorted\n\t\t\t// nature of protectedRanges to do a sort.Search, or even maintaining a\n\t\t\t// global list of all protected ranges instead of having to peer into every\n\t\t\t// snapshot.\n\t\t\tfor i := range efos.protectedRanges {\n\t\t\t\tif efos.protectedRanges[i].OverlapsKeyRange(d.cmp, exciseSpan) {\n\t\t\t\t\tpanic(\"unexpected excise of an EventuallyFileOnlySnapshot's bounds\")\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\td.mu.versions.metrics.Ingest.Count++\n\n\td.updateReadStateLocked(d.opts.DebugCheck)\n\t// updateReadStateLocked could have generated obsolete tables, schedule a\n\t// cleanup job if necessary.\n\td.deleteObsoleteFiles(jobID)\n\td.updateTableStatsLocked(ve.NewFiles)\n\t// The ingestion may have pushed a level over the threshold for compaction,\n\t// so check to see if one is necessary and schedule it.\n\td.maybeScheduleCompaction()\n\tvar toValidate []manifest.NewFileEntry\n\tdedup := make(map[base.DiskFileNum]struct{})\n\tfor _, entry := range ve.NewFiles {\n\t\tif _, ok := dedup[entry.Meta.FileBacking.DiskFileNum]; !ok {\n\t\t\ttoValidate = append(toValidate, entry)\n\t\t\tdedup[entry.Meta.FileBacking.DiskFileNum] = struct{}{}\n\t\t}\n\t}\n\td.maybeValidateSSTablesLocked(toValidate)\n\treturn ve, nil\n}\n\n// maybeValidateSSTablesLocked adds the slice of newFileEntrys to the pending\n// queue of files to be validated, when the feature is enabled.\n//\n// Note that if two entries with the same backing file are added twice, then the\n// block checksums for the backing file will be validated twice.\n//\n// DB.mu must be locked when calling.\nfunc (d *DB) maybeValidateSSTablesLocked(newFiles []newFileEntry) {\n\t// Only add to the validation queue when the feature is enabled.\n\tif !d.opts.Experimental.ValidateOnIngest {\n\t\treturn\n\t}\n\n\td.mu.tableValidation.pending = append(d.mu.tableValidation.pending, newFiles...)\n\tif d.shouldValidateSSTablesLocked() {\n\t\tgo d.validateSSTables()\n\t}\n}\n\n// shouldValidateSSTablesLocked returns true if SSTable validation should run.\n// DB.mu must be locked when calling.\nfunc (d *DB) shouldValidateSSTablesLocked() bool {\n\treturn !d.mu.tableValidation.validating &&\n\t\td.closed.Load() == nil &&\n\t\td.opts.Experimental.ValidateOnIngest &&\n\t\tlen(d.mu.tableValidation.pending) > 0\n}\n\n// validateSSTables runs a round of validation on the tables in the pending\n// queue.\nfunc (d *DB) validateSSTables() {\n\td.mu.Lock()\n\tif !d.shouldValidateSSTablesLocked() {\n\t\td.mu.Unlock()\n\t\treturn\n\t}\n\n\tpending := d.mu.tableValidation.pending\n\td.mu.tableValidation.pending = nil\n\td.mu.tableValidation.validating = true\n\tjobID := d.newJobIDLocked()\n\trs := d.loadReadState()\n\n\t// Drop DB.mu before performing IO.\n\td.mu.Unlock()\n\n\t// Validate all tables in the pending queue. This could lead to a situation\n\t// where we are starving IO from other tasks due to having to page through\n\t// all the blocks in all the sstables in the queue.\n\t// TODO(travers): Add some form of pacing to avoid IO starvation.\n\n\t// If we fail to validate any files due to reasons other than uncovered\n\t// corruption, accumulate them and re-queue them for another attempt.\n\tvar retry []manifest.NewFileEntry\n\n\tfor _, f := range pending {\n\t\t// The file may have been moved or deleted since it was ingested, in\n\t\t// which case we skip.\n\t\tif !rs.current.Contains(f.Level, f.Meta) {\n\t\t\t// Assume the file was moved to a lower level. It is rare enough\n\t\t\t// that a table is moved or deleted between the time it was ingested\n\t\t\t// and the time the validation routine runs that the overall cost of\n\t\t\t// this inner loop is tolerably low, when amortized over all\n\t\t\t// ingested tables.\n\t\t\tfound := false\n\t\t\tfor i := f.Level + 1; i < numLevels; i++ {\n\t\t\t\tif rs.current.Contains(i, f.Meta) {\n\t\t\t\t\tfound = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !found {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\tvar err error\n\t\tif f.Meta.Virtual {\n\t\t\terr = d.fileCache.withVirtualReader(\n\t\t\t\tf.Meta.VirtualMeta(), func(v sstable.VirtualReader) error {\n\t\t\t\t\treturn v.ValidateBlockChecksumsOnBacking()\n\t\t\t\t})\n\t\t} else {\n\t\t\terr = d.fileCache.withReader(\n\t\t\t\tf.Meta.PhysicalMeta(), func(r *sstable.Reader) error {\n\t\t\t\t\treturn r.ValidateBlockChecksums()\n\t\t\t\t})\n\t\t}\n\n\t\tif err != nil {\n\t\t\tif IsCorruptionError(err) {\n\t\t\t\t// TODO(travers): Hook into the corruption reporting pipeline, once\n\t\t\t\t// available. See pebble#1192.\n\t\t\t\td.opts.Logger.Fatalf(\"pebble: encountered corruption during ingestion: %s\", err)\n\t\t\t} else {\n\t\t\t\t// If there was some other, possibly transient, error that\n\t\t\t\t// caused table validation to fail inform the EventListener and\n\t\t\t\t// move on. We remember the table so that we can retry it in a\n\t\t\t\t// subsequent table validation job.\n\t\t\t\t//\n\t\t\t\t// TODO(jackson): If the error is not transient, this will retry\n\t\t\t\t// validation indefinitely. While not great, it's the same\n\t\t\t\t// behavior as erroring flushes and compactions. We should\n\t\t\t\t// address this as a part of #270.\n\t\t\t\td.opts.EventListener.BackgroundError(err)\n\t\t\t\tretry = append(retry, f)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\td.opts.EventListener.TableValidated(TableValidatedInfo{\n\t\t\tJobID: int(jobID),\n\t\t\tMeta:  f.Meta,\n\t\t})\n\t}\n\trs.unref()\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\td.mu.tableValidation.pending = append(d.mu.tableValidation.pending, retry...)\n\td.mu.tableValidation.validating = false\n\td.mu.tableValidation.cond.Broadcast()\n\tif d.shouldValidateSSTablesLocked() {\n\t\tgo d.validateSSTables()\n\t}\n}\n"
        },
        {
          "name": "ingest_test.go",
          "type": "blob",
          "size": 105.2041015625,
          "content": "// Copyright 2018 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\t\"math/rand/v2\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"runtime\"\n\t\"slices\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/datadriven\"\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/errors/oserror\"\n\t\"github.com/cockroachdb/pebble/bloom\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n\t\"github.com/cockroachdb/pebble/internal/rangekey\"\n\t\"github.com/cockroachdb/pebble/internal/testkeys\"\n\t\"github.com/cockroachdb/pebble/objstorage\"\n\t\"github.com/cockroachdb/pebble/objstorage/objstorageprovider\"\n\t\"github.com/cockroachdb/pebble/objstorage/remote\"\n\t\"github.com/cockroachdb/pebble/record\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n\t\"github.com/cockroachdb/pebble/sstable/block\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/cockroachdb/pebble/vfs/errorfs\"\n\t\"github.com/kr/pretty\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestSSTableKeyCompare(t *testing.T) {\n\tvar buf bytes.Buffer\n\tdatadriven.RunTest(t, \"testdata/sstable_key_compare\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"cmp\":\n\t\t\tbuf.Reset()\n\t\t\tfor _, line := range strings.Split(td.Input, \"\\n\") {\n\t\t\t\tfields := strings.Fields(line)\n\t\t\t\ta := base.ParseInternalKey(fields[0])\n\t\t\t\tb := base.ParseInternalKey(fields[1])\n\t\t\t\tgot := sstableKeyCompare(testkeys.Comparer.Compare, a, b)\n\t\t\t\tfmt.Fprintf(&buf, \"%38s\", fmt.Sprint(a.Pretty(base.DefaultFormatter)))\n\t\t\t\tswitch got {\n\t\t\t\tcase -1:\n\t\t\t\t\tfmt.Fprint(&buf, \" < \")\n\t\t\t\tcase +1:\n\t\t\t\t\tfmt.Fprint(&buf, \" > \")\n\t\t\t\tcase 0:\n\t\t\t\t\tfmt.Fprint(&buf, \" = \")\n\t\t\t\t}\n\t\t\t\tfmt.Fprintf(&buf, \"%s\\n\", fmt.Sprint(b.Pretty(base.DefaultFormatter)))\n\t\t\t}\n\t\t\treturn buf.String()\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unrecognized command %q\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc TestIngestLoad(t *testing.T) {\n\tmem := vfs.NewMem()\n\n\tdatadriven.RunTest(t, \"testdata/ingest_load\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"load\":\n\t\t\twriterOpts := sstable.WriterOptions{}\n\t\t\tvar dbVersion FormatMajorVersion\n\t\t\tfor _, cmdArgs := range td.CmdArgs {\n\t\t\t\tv, err := strconv.Atoi(cmdArgs.Vals[0])\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\tswitch k := cmdArgs.Key; k {\n\t\t\t\tcase \"writer-version\":\n\t\t\t\t\tfmv := FormatMajorVersion(v)\n\t\t\t\t\twriterOpts.TableFormat = fmv.MaxTableFormat()\n\t\t\t\tcase \"db-version\":\n\t\t\t\t\tdbVersion = FormatMajorVersion(v)\n\t\t\t\tdefault:\n\t\t\t\t\treturn fmt.Sprintf(\"unknown cmd %s\\n\", k)\n\t\t\t\t}\n\t\t\t}\n\t\t\tf, err := mem.Create(\"ext\", vfs.WriteCategoryUnspecified)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tw := sstable.NewRawWriter(objstorageprovider.NewFileWritable(f), writerOpts)\n\t\t\tfor _, data := range strings.Split(td.Input, \"\\n\") {\n\t\t\t\tif strings.HasPrefix(data, \"EncodeSpan: \") {\n\t\t\t\t\tdata = strings.TrimPrefix(data, \"EncodeSpan: \")\n\t\t\t\t\terr := w.EncodeSpan(keyspan.ParseSpan(data))\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tj := strings.Index(data, \":\")\n\t\t\t\tif j < 0 {\n\t\t\t\t\treturn fmt.Sprintf(\"malformed input: %s\\n\", data)\n\t\t\t\t}\n\t\t\t\tkey := base.ParseInternalKey(data[:j])\n\t\t\t\tvalue := []byte(data[j+1:])\n\t\t\t\tif err := w.AddWithForceObsolete(key, value, false /* forceObsolete */); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t}\n\t\t\tif err := w.Close(); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\n\t\t\topts := (&Options{\n\t\t\t\tComparer: DefaultComparer,\n\t\t\t\tFS:       mem,\n\t\t\t}).WithFSDefaults()\n\t\t\tlr, err := ingestLoad(context.Background(), opts, dbVersion, []string{\"ext\"}, nil, nil, 0, []base.FileNum{1})\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tvar buf bytes.Buffer\n\t\t\tfor _, m := range lr.local {\n\t\t\t\tfmt.Fprintf(&buf, \"%d: %s-%s\\n\", m.FileNum, m.Smallest, m.Largest)\n\t\t\t\tfmt.Fprintf(&buf, \"  points: %s-%s\\n\", m.SmallestPointKey, m.LargestPointKey)\n\t\t\t\tfmt.Fprintf(&buf, \"  ranges: %s-%s\\n\", m.SmallestRangeKey, m.LargestRangeKey)\n\t\t\t}\n\t\t\treturn buf.String()\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc TestIngestLoadRand(t *testing.T) {\n\tmem := vfs.NewMem()\n\trng := rand.New(rand.NewPCG(0, uint64(time.Now().UnixNano())))\n\tcmp := DefaultComparer.Compare\n\tversion := internalFormatNewest\n\n\trandBytes := func(size int) []byte {\n\t\tdata := make([]byte, size)\n\t\tfor i := range data {\n\t\t\tdata[i] = byte(rng.Int() & 0xff)\n\t\t}\n\t\treturn data\n\t}\n\n\tpaths := make([]string, 1+rng.IntN(10))\n\tpending := make([]base.FileNum, len(paths))\n\texpected := make([]ingestLocalMeta, len(paths))\n\tfor i := range paths {\n\t\tpaths[i] = fmt.Sprint(i)\n\t\tpending[i] = base.FileNum(rng.Uint64())\n\t\texpected[i] = ingestLocalMeta{\n\t\t\tfileMetadata: &fileMetadata{\n\t\t\t\tFileNum: pending[i],\n\t\t\t},\n\t\t\tpath: paths[i],\n\t\t}\n\t\texpected[i].fileMetadata.Stats.CompressionType = block.SnappyCompression\n\t\texpected[i].StatsMarkValid()\n\n\t\tfunc() {\n\t\t\tf, err := mem.Create(paths[i], vfs.WriteCategoryUnspecified)\n\t\t\trequire.NoError(t, err)\n\n\t\t\tkeys := make([]InternalKey, 1+rng.IntN(100))\n\t\t\tfor i := range keys {\n\t\t\t\tkeys[i] = base.MakeInternalKey(\n\t\t\t\t\trandBytes(1+rng.IntN(10)),\n\t\t\t\t\t0,\n\t\t\t\t\tInternalKeyKindSet)\n\t\t\t}\n\t\t\tslices.SortFunc(keys, func(a, b base.InternalKey) int {\n\t\t\t\treturn base.InternalCompare(cmp, a, b)\n\t\t\t})\n\n\t\t\texpected[i].ExtendPointKeyBounds(cmp, keys[0], keys[len(keys)-1])\n\n\t\t\tw := sstable.NewRawWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{\n\t\t\t\tTableFormat: version.MaxTableFormat(),\n\t\t\t})\n\t\t\tvar count uint64\n\t\t\tfor i := range keys {\n\t\t\t\tif i > 0 && base.InternalCompare(cmp, keys[i-1], keys[i]) == 0 {\n\t\t\t\t\t// Duplicate key, ignore.\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\trequire.NoError(t, w.AddWithForceObsolete(keys[i], nil, false /* forceObsolete */))\n\t\t\t\tcount++\n\t\t\t}\n\t\t\texpected[i].Stats.NumEntries = count\n\t\t\trequire.NoError(t, w.Close())\n\n\t\t\tmeta, err := w.Metadata()\n\t\t\trequire.NoError(t, err)\n\n\t\t\texpected[i].Size = meta.Size\n\t\t\texpected[i].InitPhysicalBacking()\n\t\t}()\n\t}\n\n\topts := (&Options{\n\t\tComparer: base.DefaultComparer,\n\t\tFS:       mem,\n\t}).WithFSDefaults().EnsureDefaults()\n\tlr, err := ingestLoad(context.Background(), opts, version, paths, nil, nil, 0, pending)\n\trequire.NoError(t, err)\n\n\tfor _, m := range lr.local {\n\t\tm.CreationTime = 0\n\t}\n\tt.Log(strings.Join(pretty.Diff(expected, lr.local), \"\\n\"))\n\trequire.Equal(t, expected, lr.local)\n}\n\nfunc TestIngestLoadInvalid(t *testing.T) {\n\tmem := vfs.NewMem()\n\tf, err := mem.Create(\"invalid\", vfs.WriteCategoryUnspecified)\n\trequire.NoError(t, err)\n\trequire.NoError(t, f.Close())\n\n\topts := (&Options{\n\t\tComparer: DefaultComparer,\n\t\tFS:       mem,\n\t}).WithFSDefaults()\n\tif _, err := ingestLoad(context.Background(), opts, internalFormatNewest, []string{\"invalid\"}, nil, nil, 0, []base.FileNum{1}); err == nil {\n\t\tt.Fatalf(\"expected error, but found success\")\n\t}\n}\n\nfunc TestIngestSortAndVerify(t *testing.T) {\n\tcomparers := map[string]Compare{\n\t\t\"default\": DefaultComparer.Compare,\n\t\t\"reverse\": func(a, b []byte) int {\n\t\t\treturn DefaultComparer.Compare(b, a)\n\t\t},\n\t}\n\n\tt.Run(\"\", func(t *testing.T) {\n\t\tdatadriven.RunTest(t, \"testdata/ingest_sort_and_verify\", func(t *testing.T, d *datadriven.TestData) string {\n\t\t\tswitch d.Cmd {\n\t\t\tcase \"ingest\":\n\t\t\t\tvar buf bytes.Buffer\n\t\t\t\tvar meta []ingestLocalMeta\n\t\t\t\tvar cmpName string\n\t\t\t\td.ScanArgs(t, \"cmp\", &cmpName)\n\t\t\t\tcmp := comparers[cmpName]\n\t\t\t\tif cmp == nil {\n\t\t\t\t\treturn fmt.Sprintf(\"%s unknown comparer: %s\", d.Cmd, cmpName)\n\t\t\t\t}\n\t\t\t\tfor i, data := range strings.Split(d.Input, \"\\n\") {\n\t\t\t\t\tparts := strings.Split(data, \"-\")\n\t\t\t\t\tif len(parts) != 2 {\n\t\t\t\t\t\treturn fmt.Sprintf(\"malformed test case: %s\", d.Input)\n\t\t\t\t\t}\n\t\t\t\t\tsmallest := base.ParseInternalKey(parts[0])\n\t\t\t\t\tlargest := base.ParseInternalKey(parts[1])\n\t\t\t\t\tif cmp(smallest.UserKey, largest.UserKey) > 0 {\n\t\t\t\t\t\treturn fmt.Sprintf(\"range %v-%v is not valid\", smallest, largest)\n\t\t\t\t\t}\n\t\t\t\t\tm := (&fileMetadata{}).ExtendPointKeyBounds(cmp, smallest, largest)\n\t\t\t\t\tm.InitPhysicalBacking()\n\t\t\t\t\tmeta = append(meta, ingestLocalMeta{\n\t\t\t\t\t\tfileMetadata: m,\n\t\t\t\t\t\tpath:         strconv.Itoa(i),\n\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t\tlr := ingestLoadResult{local: meta}\n\t\t\t\terr := ingestSortAndVerify(cmp, lr, KeyRange{})\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn fmt.Sprintf(\"%v\\n\", err)\n\t\t\t\t}\n\t\t\t\tfor i := range meta {\n\t\t\t\t\tfmt.Fprintf(&buf, \"%s: %v-%v\\n\", meta[i].path, meta[i].Smallest, meta[i].Largest)\n\t\t\t\t}\n\t\t\t\treturn buf.String()\n\n\t\t\tdefault:\n\t\t\t\treturn fmt.Sprintf(\"unknown command: %s\", d.Cmd)\n\t\t\t}\n\t\t})\n\t})\n}\n\nfunc TestIngestLink(t *testing.T) {\n\t// Test linking of tables into the DB directory. Test cleanup when one of the\n\t// tables cannot be linked.\n\n\tconst dir = \"db\"\n\tconst count = 10\n\tfor i := 0; i <= count; i++ {\n\t\tt.Run(\"\", func(t *testing.T) {\n\t\t\topts := &Options{FS: vfs.NewMem()}\n\t\t\topts.EnsureDefaults().WithFSDefaults()\n\t\t\trequire.NoError(t, opts.FS.MkdirAll(dir, 0755))\n\t\t\tobjProvider, err := objstorageprovider.Open(objstorageprovider.DefaultSettings(opts.FS, dir))\n\t\t\trequire.NoError(t, err)\n\t\t\tdefer objProvider.Close()\n\n\t\t\tmeta := make([]ingestLocalMeta, 10)\n\t\t\tcontents := make([][]byte, len(meta))\n\t\t\tfor j := range meta {\n\t\t\t\tmeta[j].path = fmt.Sprintf(\"external%d\", j)\n\t\t\t\tmeta[j].fileMetadata = &fileMetadata{}\n\t\t\t\tmeta[j].FileNum = FileNum(j)\n\t\t\t\tmeta[j].InitPhysicalBacking()\n\t\t\t\tf, err := opts.FS.Create(meta[j].path, vfs.WriteCategoryUnspecified)\n\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\tcontents[j] = []byte(fmt.Sprintf(\"data%d\", j))\n\t\t\t\t// memFile.Write will modify the supplied buffer when invariants are\n\t\t\t\t// enabled, so provide a throw-away copy.\n\t\t\t\t_, err = f.Write(append([]byte(nil), contents[j]...))\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\trequire.NoError(t, f.Close())\n\t\t\t}\n\n\t\t\tif i < count {\n\t\t\t\topts.FS.Remove(meta[i].path)\n\t\t\t}\n\n\t\t\terr = ingestLinkLocal(context.Background(), 0 /* jobID */, opts, objProvider, meta)\n\t\t\tif i < count {\n\t\t\t\tif err == nil {\n\t\t\t\t\tt.Fatalf(\"expected error, but found success\")\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\trequire.NoError(t, err)\n\t\t\t}\n\n\t\t\tfiles, err := opts.FS.List(dir)\n\t\t\trequire.NoError(t, err)\n\n\t\t\tsort.Strings(files)\n\n\t\t\tif i < count {\n\t\t\t\tif len(files) > 0 {\n\t\t\t\t\tt.Fatalf(\"expected all of the files to be cleaned up, but found:\\n%s\",\n\t\t\t\t\t\tstrings.Join(files, \"\\n\"))\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif len(files) != count {\n\t\t\t\t\tt.Fatalf(\"expected %d files, but found:\\n%s\", count, strings.Join(files, \"\\n\"))\n\t\t\t\t}\n\t\t\t\tfor j := range files {\n\t\t\t\t\tftype, fileNum, ok := base.ParseFilename(opts.FS, files[j])\n\t\t\t\t\tif !ok {\n\t\t\t\t\t\tt.Fatalf(\"unable to parse filename: %s\", files[j])\n\t\t\t\t\t}\n\t\t\t\t\tif fileTypeTable != ftype {\n\t\t\t\t\t\tt.Fatalf(\"expected table, but found %d\", ftype)\n\t\t\t\t\t}\n\t\t\t\t\tif j != int(fileNum) {\n\t\t\t\t\t\tt.Fatalf(\"expected table %d, but found %d\", j, fileNum)\n\t\t\t\t\t}\n\t\t\t\t\tf, err := opts.FS.Open(opts.FS.PathJoin(dir, files[j]))\n\t\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\t\tdata, err := io.ReadAll(f)\n\t\t\t\t\trequire.NoError(t, err)\n\t\t\t\t\trequire.NoError(t, f.Close())\n\t\t\t\t\tif !bytes.Equal(contents[j], data) {\n\t\t\t\t\t\tt.Fatalf(\"expected %s, but found %s\", contents[j], data)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestIngestLinkFallback(t *testing.T) {\n\t// Verify that ingestLink succeeds if linking fails by falling back to\n\t// copying.\n\tmem := vfs.NewMem()\n\tsrc, err := mem.Create(\"source\", vfs.WriteCategoryUnspecified)\n\trequire.NoError(t, err)\n\n\topts := &Options{FS: errorfs.Wrap(mem, errorfs.ErrInjected.If(errorfs.OnIndex(1)))}\n\topts.EnsureDefaults().WithFSDefaults()\n\tobjSettings := objstorageprovider.DefaultSettings(opts.FS, \"\")\n\t// Prevent the provider from listing the dir (where we may get an injected error).\n\tobjSettings.FSDirInitialListing = []string{}\n\tobjProvider, err := objstorageprovider.Open(objSettings)\n\trequire.NoError(t, err)\n\tdefer objProvider.Close()\n\n\tmeta := &fileMetadata{FileNum: 1}\n\tmeta.InitPhysicalBacking()\n\terr = ingestLinkLocal(context.Background(), 0, opts, objProvider, []ingestLocalMeta{{fileMetadata: meta, path: \"source\"}})\n\trequire.NoError(t, err)\n\n\tdest, err := mem.Open(\"000001.sst\")\n\trequire.NoError(t, err)\n\n\t// We should be able to write bytes to src, and not have them show up in\n\t// dest.\n\t_, _ = src.Write([]byte(\"test\"))\n\tdata, err := io.ReadAll(dest)\n\trequire.NoError(t, err)\n\tif len(data) != 0 {\n\t\tt.Fatalf(\"expected copy, but files appear to be hard linked: [%s] unexpectedly found\", data)\n\t}\n}\n\nfunc TestOverlappingIngestedSSTs(t *testing.T) {\n\tdir := \"\"\n\tvar (\n\t\tmem        *vfs.MemFS\n\t\tcrashClone *vfs.MemFS\n\t\td          *DB\n\t\topts       *Options\n\t\tclosed     = false\n\t\tblockFlush = false\n\t)\n\tcache := NewCache(0)\n\tdefer func() {\n\t\tif d != nil && !closed {\n\t\t\trequire.NoError(t, d.Close())\n\t\t}\n\t\tcache.Unref()\n\t}()\n\tvar fsLog struct {\n\t\tsync.Mutex\n\t\tbuf bytes.Buffer\n\t}\n\treset := func(strictMem bool) {\n\t\tif d != nil && !closed {\n\t\t\trequire.NoError(t, d.Close())\n\t\t\td = nil\n\t\t}\n\t\tblockFlush = false\n\n\t\tif strictMem {\n\t\t\tmem = vfs.NewCrashableMem()\n\t\t} else {\n\t\t\tmem = vfs.NewMem()\n\t\t}\n\n\t\trequire.NoError(t, mem.MkdirAll(\"ext\", 0755))\n\t\topts = (&Options{\n\t\t\tFS: vfs.WithLogging(mem, func(format string, args ...interface{}) {\n\t\t\t\tfsLog.Lock()\n\t\t\t\tdefer fsLog.Unlock()\n\t\t\t\tfmt.Fprintf(&fsLog.buf, format+\"\\n\", args...)\n\t\t\t}),\n\t\t\tCache:                       cache,\n\t\t\tMemTableStopWritesThreshold: 4,\n\t\t\tL0CompactionThreshold:       100,\n\t\t\tL0StopWritesThreshold:       100,\n\t\t\tDebugCheck:                  DebugCheckLevels,\n\t\t\tFormatMajorVersion:          internalFormatNewest,\n\t\t\tLogger:                      testLogger{t},\n\t\t}).WithFSDefaults()\n\t\topts.Experimental.EnableColumnarBlocks = func() bool { return true }\n\t\tif testing.Verbose() {\n\t\t\tlel := MakeLoggingEventListener(DefaultLogger)\n\t\t\topts.EventListener = &lel\n\t\t}\n\t\topts.EnsureDefaults()\n\t\t// Some of the tests require bloom filters.\n\t\topts.Levels[0].FilterPolicy = bloom.FilterPolicy(10)\n\n\t\t// Disable automatic compactions because otherwise we'll race with\n\t\t// delete-only compactions triggered by ingesting range tombstones.\n\t\topts.DisableAutomaticCompactions = true\n\n\t\tvar err error\n\t\td, err = Open(dir, opts)\n\t\trequire.NoError(t, err)\n\t\tclosed = false\n\t\td.TestOnlyWaitForCleaning()\n\t}\n\twaitForFlush := func() {\n\t\tif d == nil {\n\t\t\treturn\n\t\t}\n\t\td.mu.Lock()\n\t\tfor d.mu.compact.flushing {\n\t\t\td.mu.compact.cond.Wait()\n\t\t}\n\t\td.mu.Unlock()\n\t}\n\treset(false)\n\n\tdatadriven.RunTest(t, \"testdata/flushable_ingest\", func(t *testing.T, td *datadriven.TestData) (result string) {\n\t\tif td.HasArg(\"with-fs-logging\") {\n\t\t\tfsLog.Lock()\n\t\t\tfsLog.buf.Reset()\n\t\t\tfsLog.Unlock()\n\t\t\tdefer func() {\n\t\t\t\tfsLog.Lock()\n\t\t\t\tdefer fsLog.Unlock()\n\t\t\t\tresult = fsLog.buf.String() + result\n\t\t\t}()\n\t\t}\n\n\t\tswitch td.Cmd {\n\t\tcase \"reset\":\n\t\t\treset(td.HasArg(\"strictMem\"))\n\t\t\treturn \"\"\n\n\t\tcase \"crash-clone\":\n\t\t\tcrashClone = mem.CrashClone(vfs.CrashCloneCfg{UnsyncedDataPercent: 0})\n\t\t\treturn \"\"\n\n\t\tcase \"reset-to-crash-clone\":\n\t\t\tmem = crashClone\n\t\t\tcrashClone = nil\n\t\t\tfiles, err := mem.List(dir)\n\t\t\tsort.Strings(files)\n\t\t\trequire.NoError(t, err)\n\t\t\treturn strings.Join(files, \"\\n\")\n\n\t\tcase \"batch\":\n\t\t\tb := d.NewIndexedBatch()\n\t\t\tif err := runBatchDefineCmd(td, b); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tif err := b.Commit(nil); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"build\":\n\t\t\tif err := runBuildCmd(td, d, mem); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"ingest\":\n\t\t\tif err := runIngestCmd(td, d, mem); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tif !blockFlush {\n\t\t\t\twaitForFlush()\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"iter\":\n\t\t\titer, _ := d.NewIter(nil)\n\t\t\treturn runIterCmd(td, iter, true)\n\n\t\tcase \"lsm\":\n\t\t\treturn runLSMCmd(td, d)\n\n\t\tcase \"close\":\n\t\t\tif closed {\n\t\t\t\treturn \"already closed\"\n\t\t\t}\n\t\t\trequire.NoError(t, d.Close())\n\t\t\tclosed = true\n\t\t\treturn \"\"\n\n\t\tcase \"ls\":\n\t\t\tfiles, err := mem.List(dir)\n\t\t\tsort.Strings(files)\n\t\t\trequire.NoError(t, err)\n\t\t\treturn strings.Join(files, \"\\n\")\n\n\t\tcase \"open\":\n\t\t\topts.ReadOnly = td.HasArg(\"readOnly\")\n\t\t\tvar err error\n\t\t\td, err = Open(dir, opts)\n\t\t\tclosed = false\n\t\t\trequire.NoError(t, err)\n\t\t\twaitForFlush()\n\t\t\td.TestOnlyWaitForCleaning()\n\t\t\treturn \"\"\n\n\t\tcase \"blockFlush\":\n\t\t\tblockFlush = true\n\t\t\td.mu.Lock()\n\t\t\td.mu.compact.flushing = true\n\t\t\td.mu.Unlock()\n\t\t\treturn \"\"\n\n\t\tcase \"allowFlush\":\n\t\t\tblockFlush = false\n\t\t\td.mu.Lock()\n\t\t\td.mu.compact.flushing = false\n\t\t\td.mu.Unlock()\n\t\t\treturn \"\"\n\n\t\tcase \"flush\":\n\t\t\td.maybeScheduleFlush()\n\t\t\twaitForFlush()\n\t\t\td.TestOnlyWaitForCleaning()\n\t\t\treturn \"\"\n\n\t\tcase \"get\":\n\t\t\treturn runGetCmd(t, td, d)\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc TestExcise(t *testing.T) {\n\tvar mem vfs.FS\n\tvar d *DB\n\tvar flushed bool\n\tvar efos map[string]*EventuallyFileOnlySnapshot\n\tdefer func() {\n\t\tfor _, e := range efos {\n\t\t\trequire.NoError(t, e.Close())\n\t\t}\n\t\trequire.NoError(t, d.Close())\n\t}()\n\tclearFlushed := func() {\n\t\td.mu.Lock()\n\t\tdefer d.mu.Unlock()\n\t\t// Wait for any ongoing flushes to stop first, otherwise the\n\t\t// EventListener may set flushed=true due to a flush that was already in\n\t\t// progress.\n\t\tfor d.mu.compact.flushing {\n\t\t\td.mu.compact.cond.Wait()\n\t\t}\n\t\tflushed = false\n\t}\n\n\tvar opts *Options\n\treset := func(blockSize int) {\n\t\tfor _, e := range efos {\n\t\t\trequire.NoError(t, e.Close())\n\t\t}\n\t\tif d != nil {\n\t\t\trequire.NoError(t, d.Close())\n\t\t}\n\t\tefos = make(map[string]*EventuallyFileOnlySnapshot)\n\n\t\tmem = vfs.NewMem()\n\t\trequire.NoError(t, mem.MkdirAll(\"ext\", 0755))\n\t\topts = &Options{\n\t\t\tBlockPropertyCollectors: []func() BlockPropertyCollector{\n\t\t\t\tsstable.NewTestKeysBlockPropertyCollector,\n\t\t\t},\n\t\t\tComparer:              testkeys.Comparer,\n\t\t\tFS:                    mem,\n\t\t\tL0CompactionThreshold: 100,\n\t\t\tL0StopWritesThreshold: 100,\n\t\t\tDebugCheck:            DebugCheckLevels,\n\t\t\tEventListener: &EventListener{FlushEnd: func(info FlushInfo) {\n\t\t\t\tflushed = true\n\t\t\t}},\n\t\t\tFormatMajorVersion: FormatFlushableIngestExcises,\n\t\t\tLogger:             testLogger{t},\n\t\t}\n\t\tif blockSize != 0 {\n\t\t\topts.Levels = append(opts.Levels, LevelOptions{BlockSize: blockSize, IndexBlockSize: 32 << 10})\n\t\t}\n\t\t// Disable automatic compactions because otherwise we'll race with\n\t\t// delete-only compactions triggered by ingesting range tombstones.\n\t\topts.DisableAutomaticCompactions = true\n\t\t// Set this to true to add some testing for the virtual sstable validation\n\t\t// code paths.\n\t\topts.Experimental.ValidateOnIngest = true\n\n\t\tvar err error\n\t\td, err = Open(\"\", opts)\n\t\trequire.NoError(t, err)\n\t}\n\treset(0 /* blockSize */)\n\n\tdatadriven.RunTest(t, \"testdata/excise\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"reset\":\n\t\t\tvar blockSize int\n\t\t\tfor i := range td.CmdArgs {\n\t\t\t\tswitch td.CmdArgs[i].Key {\n\t\t\t\tcase \"tiny-blocks\":\n\t\t\t\t\tblockSize = 1\n\t\t\t\tdefault:\n\t\t\t\t\treturn fmt.Sprintf(\"unexpected arg: %s\", td.CmdArgs[i].Key)\n\t\t\t\t}\n\t\t\t}\n\t\t\treset(blockSize)\n\t\t\treturn \"\"\n\t\tcase \"reopen\":\n\t\t\trequire.NoError(t, d.Close())\n\t\t\tvar err error\n\t\t\td, err = Open(\"\", opts)\n\t\t\trequire.NoError(t, err)\n\n\t\t\treturn \"\"\n\t\tcase \"batch\":\n\t\t\tb := d.NewIndexedBatch()\n\t\t\tif err := runBatchDefineCmd(td, b); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tif err := b.Commit(nil); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\t\tcase \"build\":\n\t\t\tif err := runBuildCmd(td, d, mem); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"flush\":\n\t\t\tif err := d.Flush(); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"block-flush\":\n\t\t\td.mu.Lock()\n\t\t\td.mu.compact.flushing = true\n\t\t\td.mu.Unlock()\n\t\t\treturn \"\"\n\n\t\tcase \"allow-flush\":\n\t\t\td.mu.Lock()\n\t\t\td.mu.compact.flushing = false\n\t\t\td.mu.Unlock()\n\t\t\treturn \"\"\n\n\t\tcase \"ingest\":\n\t\t\tnoWait := td.HasArg(\"no-wait\")\n\t\t\tif !noWait {\n\t\t\t\tclearFlushed()\n\t\t\t}\n\t\t\tif err := runIngestCmd(td, d, mem); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tif noWait {\n\t\t\t\treturn \"\"\n\t\t\t}\n\t\t\t// Wait for a possible flush.\n\t\t\td.mu.Lock()\n\t\t\tfor d.mu.compact.flushing {\n\t\t\t\td.mu.compact.cond.Wait()\n\t\t\t}\n\t\t\td.mu.Unlock()\n\t\t\tif flushed {\n\t\t\t\treturn \"memtable flushed\"\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"ingest-and-excise\":\n\t\t\tvar prevFlushableIngests uint64\n\t\t\tnoWait := td.HasArg(\"no-wait\")\n\t\t\tif !noWait {\n\t\t\t\tclearFlushed()\n\t\t\t\td.mu.Lock()\n\t\t\t\tprevFlushableIngests = d.mu.versions.metrics.Flush.AsIngestCount\n\t\t\t\td.mu.Unlock()\n\t\t\t}\n\n\t\t\tif err := runIngestAndExciseCmd(td, d, mem); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tif noWait {\n\t\t\t\treturn \"\"\n\t\t\t}\n\t\t\t// Wait for a possible flush.\n\t\t\td.mu.Lock()\n\t\t\tfor d.mu.compact.flushing {\n\t\t\t\td.mu.compact.cond.Wait()\n\t\t\t}\n\t\t\tflushableIngests := d.mu.versions.metrics.Flush.AsIngestCount\n\t\t\td.mu.Unlock()\n\t\t\tif flushed {\n\t\t\t\tif prevFlushableIngests < flushableIngests {\n\t\t\t\t\treturn \"flushable ingest\"\n\t\t\t\t}\n\t\t\t\treturn \"memtable flushed\"\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"file-only-snapshot\":\n\t\t\tif len(td.CmdArgs) != 1 {\n\t\t\t\tpanic(\"insufficient args for file-only-snapshot command\")\n\t\t\t}\n\t\t\tname := td.CmdArgs[0].Key\n\t\t\tvar keyRanges []KeyRange\n\t\t\tfor _, line := range strings.Split(td.Input, \"\\n\") {\n\t\t\t\tfields := strings.Fields(line)\n\t\t\t\tif len(fields) != 2 {\n\t\t\t\t\treturn \"expected two fields for file-only snapshot KeyRanges\"\n\t\t\t\t}\n\t\t\t\tkr := KeyRange{Start: []byte(fields[0]), End: []byte(fields[1])}\n\t\t\t\tkeyRanges = append(keyRanges, kr)\n\t\t\t}\n\n\t\t\ts := d.NewEventuallyFileOnlySnapshot(keyRanges)\n\t\t\tefos[name] = s\n\t\t\treturn \"ok\"\n\n\t\tcase \"get\":\n\t\t\treturn runGetCmd(t, td, d)\n\n\t\tcase \"iter\":\n\t\t\topts := &IterOptions{\n\t\t\t\tKeyTypes: IterKeyTypePointsAndRanges,\n\t\t\t}\n\t\t\tvar reader Reader = d\n\t\t\tfor i := range td.CmdArgs {\n\t\t\t\tswitch td.CmdArgs[i].Key {\n\t\t\t\tcase \"range-key-masking\":\n\t\t\t\t\topts.RangeKeyMasking = RangeKeyMasking{\n\t\t\t\t\t\tSuffix: []byte(td.CmdArgs[i].Vals[0]),\n\t\t\t\t\t\tFilter: func() BlockPropertyFilterMask {\n\t\t\t\t\t\t\treturn sstable.NewTestKeysMaskingFilter()\n\t\t\t\t\t\t},\n\t\t\t\t\t}\n\t\t\t\tcase \"lower\":\n\t\t\t\t\tif len(td.CmdArgs[i].Vals) != 1 {\n\t\t\t\t\t\treturn fmt.Sprintf(\n\t\t\t\t\t\t\t\"%s expects at most 1 value for lower\", td.Cmd)\n\t\t\t\t\t}\n\t\t\t\t\topts.LowerBound = []byte(td.CmdArgs[i].Vals[0])\n\t\t\t\tcase \"upper\":\n\t\t\t\t\tif len(td.CmdArgs[i].Vals) != 1 {\n\t\t\t\t\t\treturn fmt.Sprintf(\n\t\t\t\t\t\t\t\"%s expects at most 1 value for upper\", td.Cmd)\n\t\t\t\t\t}\n\t\t\t\t\topts.UpperBound = []byte(td.CmdArgs[i].Vals[0])\n\t\t\t\tcase \"snapshot\":\n\t\t\t\t\treader = efos[td.CmdArgs[i].Vals[0]]\n\t\t\t\tdefault:\n\t\t\t\t\treturn fmt.Sprintf(\"unexpected argument: %s\", td.CmdArgs[i].Key)\n\t\t\t\t}\n\t\t\t}\n\t\t\titer, _ := reader.NewIter(opts)\n\t\t\treturn runIterCmd(td, iter, true)\n\n\t\tcase \"lsm\":\n\t\t\treturn runLSMCmd(td, d)\n\n\t\tcase \"metrics\":\n\t\t\t// The asynchronous loading of table stats can change metrics, so\n\t\t\t// wait for all the tables' stats to be loaded.\n\t\t\td.mu.Lock()\n\t\t\td.waitTableStats()\n\t\t\td.mu.Unlock()\n\n\t\t\treturn d.Metrics().StringForTests()\n\n\t\tcase \"wait-pending-table-stats\":\n\t\t\treturn runTableStatsCmd(td, d)\n\n\t\tcase \"excise\":\n\t\t\tve := &versionEdit{\n\t\t\t\tDeletedFiles: map[deletedFileEntry]*fileMetadata{},\n\t\t\t}\n\t\t\tvar exciseSpan KeyRange\n\t\t\tif len(td.CmdArgs) != 2 {\n\t\t\t\tpanic(\"insufficient args for compact command\")\n\t\t\t}\n\t\t\texciseSpan.Start = []byte(td.CmdArgs[0].Key)\n\t\t\texciseSpan.End = []byte(td.CmdArgs[1].Key)\n\n\t\t\td.mu.Lock()\n\t\t\td.mu.versions.logLock()\n\t\t\td.mu.Unlock()\n\t\t\tcurrent := d.mu.versions.currentVersion()\n\n\t\t\tcurrent.IterAllLevelsAndSublevels(func(iter manifest.LevelIterator, l manifest.Layer) {\n\t\t\t\tfor m := iter.SeekGE(d.cmp, exciseSpan.Start); m != nil && d.cmp(m.Smallest.UserKey, exciseSpan.End) < 0; m = iter.Next() {\n\t\t\t\t\t_, err := d.excise(context.Background(), exciseSpan.UserKeyBounds(), m, ve, l.Level())\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\ttd.Fatalf(t, \"error when excising %s: %s\", m.FileNum, err.Error())\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t})\n\n\t\t\td.mu.Lock()\n\t\t\td.mu.versions.logUnlock()\n\t\t\td.mu.Unlock()\n\t\t\treturn fmt.Sprintf(\"would excise %d files, use ingest-and-excise to excise.\\n%s\", len(ve.DeletedFiles), ve.DebugString(base.DefaultFormatter))\n\n\t\tcase \"confirm-backing\":\n\t\t\t// Confirms that the files have the same FileBacking.\n\t\t\tfileNums := make(map[base.FileNum]struct{})\n\t\t\tfor i := range td.CmdArgs {\n\t\t\t\tfNum, err := strconv.Atoi(td.CmdArgs[i].Key)\n\t\t\t\tif err != nil {\n\t\t\t\t\tpanic(\"invalid file number\")\n\t\t\t\t}\n\t\t\t\tfileNums[base.FileNum(fNum)] = struct{}{}\n\t\t\t}\n\t\t\td.mu.Lock()\n\t\t\tcurrVersion := d.mu.versions.currentVersion()\n\t\t\tvar ptr *manifest.FileBacking\n\t\t\tfor _, level := range currVersion.Levels {\n\t\t\t\tlIter := level.Iter()\n\t\t\t\tfor f := lIter.First(); f != nil; f = lIter.Next() {\n\t\t\t\t\tif _, ok := fileNums[f.FileNum]; ok {\n\t\t\t\t\t\tif ptr == nil {\n\t\t\t\t\t\t\tptr = f.FileBacking\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif f.FileBacking != ptr {\n\t\t\t\t\t\t\td.mu.Unlock()\n\t\t\t\t\t\t\treturn \"file backings are not the same\"\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\td.mu.Unlock()\n\t\t\treturn \"file backings are the same\"\n\t\tcase \"compact\":\n\t\t\tif len(td.CmdArgs) != 2 {\n\t\t\t\tpanic(\"insufficient args for compact command\")\n\t\t\t}\n\t\t\tl := td.CmdArgs[0].Key\n\t\t\tr := td.CmdArgs[1].Key\n\t\t\terr := d.Compact([]byte(l), []byte(r), false)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc testIngestSharedImpl(\n\tt *testing.T, createOnShared remote.CreateOnSharedStrategy, fileName string,\n) {\n\tvar d, d1, d2 *DB\n\tvar efos map[string]*EventuallyFileOnlySnapshot\n\tdefer func() {\n\t\tfor _, e := range efos {\n\t\t\trequire.NoError(t, e.Close())\n\t\t}\n\t\tif d1 != nil {\n\t\t\trequire.NoError(t, d1.Close())\n\t\t}\n\t\tif d2 != nil {\n\t\t\trequire.NoError(t, d2.Close())\n\t\t}\n\t}()\n\tcreatorIDCounter := uint64(1)\n\treplicateCounter := 1\n\tvar opts1, opts2 *Options\n\n\treset := func(t *testing.T) {\n\t\tfor _, e := range efos {\n\t\t\trequire.NoError(t, e.Close())\n\t\t}\n\t\tif d1 != nil {\n\t\t\trequire.NoError(t, d1.Close())\n\t\t}\n\t\tif d2 != nil {\n\t\t\trequire.NoError(t, d2.Close())\n\t\t}\n\t\tefos = make(map[string]*EventuallyFileOnlySnapshot)\n\n\t\tsstorage := remote.NewInMem()\n\t\tmem1 := vfs.NewMem()\n\t\tmem2 := vfs.NewMem()\n\t\trequire.NoError(t, mem1.MkdirAll(\"ext\", 0755))\n\t\trequire.NoError(t, mem2.MkdirAll(\"ext\", 0755))\n\t\topts1 = &Options{\n\t\t\tComparer:              testkeys.Comparer,\n\t\t\tFS:                    mem1,\n\t\t\tLBaseMaxBytes:         1,\n\t\t\tL0CompactionThreshold: 100,\n\t\t\tL0StopWritesThreshold: 100,\n\t\t\tDebugCheck:            DebugCheckLevels,\n\t\t\tFormatMajorVersion:    FormatVirtualSSTables,\n\t\t\tLogger:                testLogger{t},\n\t\t}\n\t\t// lel.\n\t\tlel := MakeLoggingEventListener(testLogger{t})\n\t\topts1.EventListener = &lel\n\t\topts1.Experimental.RemoteStorage = remote.MakeSimpleFactory(map[remote.Locator]remote.Storage{\n\t\t\t\"\": sstorage,\n\t\t})\n\t\topts1.Experimental.CreateOnShared = createOnShared\n\t\topts1.Experimental.CreateOnSharedLocator = \"\"\n\t\t// Disable automatic compactions because otherwise we'll race with\n\t\t// delete-only compactions triggered by ingesting range tombstones.\n\t\topts1.DisableAutomaticCompactions = true\n\n\t\topts2 = &Options{}\n\t\t*opts2 = *opts1\n\t\topts2.Experimental.RemoteStorage = remote.MakeSimpleFactory(map[remote.Locator]remote.Storage{\n\t\t\t\"\": sstorage,\n\t\t})\n\t\topts2.Experimental.CreateOnShared = createOnShared\n\t\topts2.Experimental.CreateOnSharedLocator = \"\"\n\t\topts2.FS = mem2\n\n\t\tvar err error\n\t\td1, err = Open(\"\", opts1)\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, d1.SetCreatorID(creatorIDCounter))\n\t\tcreatorIDCounter++\n\t\td2, err = Open(\"\", opts2)\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, d2.SetCreatorID(creatorIDCounter))\n\t\tcreatorIDCounter++\n\t\td = d1\n\t}\n\treset(t)\n\n\tdatadriven.RunTest(t, fmt.Sprintf(\"testdata/%s\", fileName), func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"restart\":\n\t\t\tfor _, e := range efos {\n\t\t\t\trequire.NoError(t, e.Close())\n\t\t\t}\n\t\t\tif d1 != nil {\n\t\t\t\trequire.NoError(t, d1.Close())\n\t\t\t}\n\t\t\tif d2 != nil {\n\t\t\t\trequire.NoError(t, d2.Close())\n\t\t\t}\n\n\t\t\tvar err error\n\t\t\td1, err = Open(\"\", opts1)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\td2, err = Open(\"\", opts2)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\td = d1\n\t\t\treturn \"ok, note that the active db has been set to 1 (use 'switch' to change)\"\n\t\tcase \"reset\":\n\t\t\treset(t)\n\t\t\treturn \"\"\n\t\tcase \"switch\":\n\t\t\tif len(td.CmdArgs) != 1 {\n\t\t\t\treturn \"usage: switch <1 or 2>\"\n\t\t\t}\n\t\t\tswitch td.CmdArgs[0].Key {\n\t\t\tcase \"1\":\n\t\t\t\td = d1\n\t\t\tcase \"2\":\n\t\t\t\td = d2\n\t\t\tdefault:\n\t\t\t\treturn \"usage: switch <1 or 2>\"\n\t\t\t}\n\t\t\treturn \"ok\"\n\t\tcase \"batch\":\n\t\t\tb := d.NewIndexedBatch()\n\t\t\tif err := runBatchDefineCmd(td, b); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tif err := b.Commit(nil); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\t\tcase \"build\":\n\t\t\tif err := runBuildCmd(td, d, d.opts.FS); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"flush\":\n\t\t\tif err := d.Flush(); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"ingest\":\n\t\t\tif err := runIngestCmd(td, d, d.opts.FS); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\t// Wait for a possible flush.\n\t\t\td.mu.Lock()\n\t\t\tfor d.mu.compact.flushing {\n\t\t\t\td.mu.compact.cond.Wait()\n\t\t\t}\n\t\t\td.mu.Unlock()\n\t\t\treturn \"\"\n\n\t\tcase \"ingest-and-excise\":\n\t\t\tif err := runIngestAndExciseCmd(td, d, d.opts.FS); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\t// Wait for a possible flush.\n\t\t\td.mu.Lock()\n\t\t\tfor d.mu.compact.flushing {\n\t\t\t\td.mu.compact.cond.Wait()\n\t\t\t}\n\t\t\td.mu.Unlock()\n\t\t\treturn \"\"\n\n\t\tcase \"replicate\":\n\t\t\tif len(td.CmdArgs) != 4 {\n\t\t\t\treturn \"usage: replicate <from-db-num> <to-db-num> <start-key> <end-key>\"\n\t\t\t}\n\t\t\tvar from, to *DB\n\t\t\tswitch td.CmdArgs[0].Key {\n\t\t\tcase \"1\":\n\t\t\t\tfrom = d1\n\t\t\tcase \"2\":\n\t\t\t\tfrom = d2\n\t\t\tdefault:\n\t\t\t\treturn \"usage: replicate <from-db-num> <to-db-num> <start-key> <end-key>\"\n\t\t\t}\n\t\t\tswitch td.CmdArgs[1].Key {\n\t\t\tcase \"1\":\n\t\t\t\tto = d1\n\t\t\tcase \"2\":\n\t\t\t\tto = d2\n\t\t\tdefault:\n\t\t\t\treturn \"usage: replicate <from-db-num> <to-db-num> <start-key> <end-key>\"\n\t\t\t}\n\t\t\tstartKey := []byte(td.CmdArgs[2].Key)\n\t\t\tendKey := []byte(td.CmdArgs[3].Key)\n\n\t\t\twriteOpts := d.opts.MakeWriterOptions(0 /* level */, to.TableFormat())\n\t\t\tsstPath := fmt.Sprintf(\"ext/replicate%d.sst\", replicateCounter)\n\t\t\tf, err := to.opts.FS.Create(sstPath, vfs.WriteCategoryUnspecified)\n\t\t\trequire.NoError(t, err)\n\t\t\treplicateCounter++\n\t\t\tw := sstable.NewRawWriter(objstorageprovider.NewFileWritable(f), writeOpts)\n\n\t\t\tvar sharedSSTs []SharedSSTMeta\n\t\t\terr = from.ScanInternal(context.TODO(), sstable.CategoryUnknown, startKey, endKey,\n\t\t\t\tfunc(key *InternalKey, value LazyValue, _ IteratorLevel) error {\n\t\t\t\t\tval, _, err := value.Value(nil)\n\t\t\t\t\trequire.NoError(t, err)\n\t\t\t\t\trequire.NoError(t, w.AddWithForceObsolete(base.MakeInternalKey(key.UserKey, 0, key.Kind()), val, false /* forceObsolete */))\n\t\t\t\t\treturn nil\n\t\t\t\t},\n\t\t\t\tfunc(start, end []byte, seqNum base.SeqNum) error {\n\t\t\t\t\trequire.NoError(t, w.EncodeSpan(keyspan.Span{\n\t\t\t\t\t\tStart: start,\n\t\t\t\t\t\tEnd:   end,\n\t\t\t\t\t\tKeys:  []keyspan.Key{{Trailer: base.MakeTrailer(0, base.InternalKeyKindRangeDelete)}},\n\t\t\t\t\t}))\n\t\t\t\t\treturn nil\n\t\t\t\t},\n\t\t\t\tfunc(start, end []byte, keys []keyspan.Key) error {\n\t\t\t\t\trequire.NoError(t, w.EncodeSpan(keyspan.Span{\n\t\t\t\t\t\tStart:     start,\n\t\t\t\t\t\tEnd:       end,\n\t\t\t\t\t\tKeys:      keys,\n\t\t\t\t\t\tKeysOrder: 0,\n\t\t\t\t\t}))\n\t\t\t\t\treturn nil\n\t\t\t\t},\n\t\t\t\tfunc(sst *SharedSSTMeta) error {\n\t\t\t\t\tsharedSSTs = append(sharedSSTs, *sst)\n\t\t\t\t\treturn nil\n\t\t\t\t},\n\t\t\t\tnil,\n\t\t\t)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.NoError(t, w.Close())\n\n\t\t\t_, err = to.IngestAndExcise(context.Background(), []string{sstPath}, sharedSSTs, nil /* external */, KeyRange{Start: startKey, End: endKey})\n\t\t\trequire.NoError(t, err)\n\t\t\treturn fmt.Sprintf(\"replicated %d shared SSTs\", len(sharedSSTs))\n\n\t\tcase \"get\":\n\t\t\treturn runGetCmd(t, td, d)\n\n\t\tcase \"iter\":\n\t\t\to := &IterOptions{KeyTypes: IterKeyTypePointsAndRanges}\n\t\t\tvar reader Reader\n\t\t\treader = d\n\t\t\tfor _, arg := range td.CmdArgs {\n\t\t\t\tswitch arg.Key {\n\t\t\t\tcase \"mask-suffix\":\n\t\t\t\t\to.RangeKeyMasking.Suffix = []byte(arg.Vals[0])\n\t\t\t\tcase \"mask-filter\":\n\t\t\t\t\to.RangeKeyMasking.Filter = func() BlockPropertyFilterMask {\n\t\t\t\t\t\treturn sstable.NewTestKeysMaskingFilter()\n\t\t\t\t\t}\n\t\t\t\tcase \"snapshot\":\n\t\t\t\t\treader = efos[arg.Vals[0]]\n\t\t\t\t}\n\t\t\t}\n\t\t\titer, err := reader.NewIter(o)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn runIterCmd(td, iter, true)\n\n\t\tcase \"lsm\":\n\t\t\treturn runLSMCmd(td, d)\n\n\t\tcase \"metrics\":\n\t\t\t// The asynchronous loading of table stats can change metrics, so\n\t\t\t// wait for all the tables' stats to be loaded.\n\t\t\td.mu.Lock()\n\t\t\td.waitTableStats()\n\t\t\td.mu.Unlock()\n\n\t\t\treturn d.Metrics().StringForTests()\n\n\t\tcase \"wait-pending-table-stats\":\n\t\t\treturn runTableStatsCmd(td, d)\n\n\t\tcase \"excise\":\n\t\t\tve := &versionEdit{\n\t\t\t\tDeletedFiles: map[deletedFileEntry]*fileMetadata{},\n\t\t\t}\n\t\t\tvar exciseSpan KeyRange\n\t\t\tif len(td.CmdArgs) != 2 {\n\t\t\t\tpanic(\"insufficient args for excise command\")\n\t\t\t}\n\t\t\texciseSpan.Start = []byte(td.CmdArgs[0].Key)\n\t\t\texciseSpan.End = []byte(td.CmdArgs[1].Key)\n\n\t\t\td.mu.Lock()\n\t\t\td.mu.versions.logLock()\n\t\t\td.mu.Unlock()\n\t\t\tcurrent := d.mu.versions.currentVersion()\n\t\t\tfor level := range current.Levels {\n\t\t\t\titer := current.Levels[level].Iter()\n\t\t\t\tfor m := iter.SeekGE(d.cmp, exciseSpan.Start); m != nil && d.cmp(m.Smallest.UserKey, exciseSpan.End) < 0; m = iter.Next() {\n\t\t\t\t\t_, err := d.excise(context.Background(), exciseSpan.UserKeyBounds(), m, ve, level)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\td.mu.Lock()\n\t\t\t\t\t\td.mu.versions.logUnlock()\n\t\t\t\t\t\td.mu.Unlock()\n\t\t\t\t\t\treturn fmt.Sprintf(\"error when excising %s: %s\", m.FileNum, err.Error())\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\td.mu.Lock()\n\t\t\td.mu.versions.logUnlock()\n\t\t\td.mu.Unlock()\n\t\t\treturn fmt.Sprintf(\"would excise %d files, use ingest-and-excise to excise.\\n%s\", len(ve.DeletedFiles), ve.String())\n\n\t\tcase \"file-only-snapshot\":\n\t\t\tif len(td.CmdArgs) != 1 {\n\t\t\t\tpanic(\"insufficient args for file-only-snapshot command\")\n\t\t\t}\n\t\t\tname := td.CmdArgs[0].Key\n\t\t\tvar keyRanges []KeyRange\n\t\t\tfor _, line := range strings.Split(td.Input, \"\\n\") {\n\t\t\t\tfields := strings.Fields(line)\n\t\t\t\tif len(fields) != 2 {\n\t\t\t\t\treturn \"expected two fields for file-only snapshot KeyRanges\"\n\t\t\t\t}\n\t\t\t\tkr := KeyRange{Start: []byte(fields[0]), End: []byte(fields[1])}\n\t\t\t\tkeyRanges = append(keyRanges, kr)\n\t\t\t}\n\n\t\t\ts := d.NewEventuallyFileOnlySnapshot(keyRanges)\n\t\t\tefos[name] = s\n\t\t\treturn \"ok\"\n\n\t\tcase \"wait-for-file-only-snapshot\":\n\t\t\tif len(td.CmdArgs) != 1 {\n\t\t\t\tpanic(\"insufficient args for file-only-snapshot command\")\n\t\t\t}\n\t\t\tname := td.CmdArgs[0].Key\n\t\t\terr := efos[name].WaitForFileOnlySnapshot(context.TODO(), 1*time.Millisecond)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"ok\"\n\n\t\tcase \"compact\":\n\t\t\terr := runCompactCmd(td, d)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"ok\"\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc TestIngestShared(t *testing.T) {\n\tfor _, strategy := range []remote.CreateOnSharedStrategy{remote.CreateOnSharedAll, remote.CreateOnSharedLower} {\n\t\tstrategyStr := \"all\"\n\t\tif strategy == remote.CreateOnSharedLower {\n\t\t\tstrategyStr = \"lower\"\n\t\t}\n\t\tt.Run(fmt.Sprintf(\"createOnShared=%s\", strategyStr), func(t *testing.T) {\n\t\t\tfileName := \"ingest_shared\"\n\t\t\tif strategy == remote.CreateOnSharedLower {\n\t\t\t\tfileName = \"ingest_shared_lower\"\n\t\t\t}\n\t\t\ttestIngestSharedImpl(t, strategy, fileName)\n\t\t})\n\t}\n}\n\nfunc TestSimpleIngestShared(t *testing.T) {\n\tmem := vfs.NewMem()\n\tvar d *DB\n\tvar provider2 objstorage.Provider\n\topts2 := Options{FS: vfs.NewMem(), FormatMajorVersion: FormatVirtualSSTables, Logger: testLogger{t}}\n\topts2.EnsureDefaults()\n\n\t// Create an objProvider where we will fake-create some sstables that can\n\t// then be shared back to the db instance.\n\tproviderSettings := objstorageprovider.Settings{\n\t\tLogger:              opts2.Logger,\n\t\tFS:                  opts2.FS,\n\t\tFSDirName:           \"\",\n\t\tFSDirInitialListing: nil,\n\t\tFSCleaner:           opts2.Cleaner,\n\t\tNoSyncOnClose:       opts2.NoSyncOnClose,\n\t\tBytesPerSync:        opts2.BytesPerSync,\n\t}\n\tproviderSettings.Remote.StorageFactory = remote.MakeSimpleFactory(map[remote.Locator]remote.Storage{\n\t\t\"\": remote.NewInMem(),\n\t})\n\tproviderSettings.Remote.CreateOnShared = remote.CreateOnSharedAll\n\tproviderSettings.Remote.CreateOnSharedLocator = \"\"\n\n\tprovider2, err := objstorageprovider.Open(providerSettings)\n\trequire.NoError(t, err)\n\tcreatorIDCounter := uint64(1)\n\tprovider2.SetCreatorID(objstorage.CreatorID(creatorIDCounter))\n\tcreatorIDCounter++\n\n\tdefer func() {\n\t\trequire.NoError(t, d.Close())\n\t}()\n\n\treset := func() {\n\t\tif d != nil {\n\t\t\trequire.NoError(t, d.Close())\n\t\t}\n\n\t\tmem = vfs.NewMem()\n\t\trequire.NoError(t, mem.MkdirAll(\"ext\", 0755))\n\t\topts := &Options{\n\t\t\tFormatMajorVersion:    FormatVirtualSSTables,\n\t\t\tFS:                    mem,\n\t\t\tL0CompactionThreshold: 100,\n\t\t\tL0StopWritesThreshold: 100,\n\t\t\tLogger:                testLogger{t},\n\t\t}\n\t\topts.Experimental.RemoteStorage = providerSettings.Remote.StorageFactory\n\t\topts.Experimental.CreateOnShared = providerSettings.Remote.CreateOnShared\n\t\topts.Experimental.CreateOnSharedLocator = providerSettings.Remote.CreateOnSharedLocator\n\n\t\tvar err error\n\t\td, err = Open(\"\", opts)\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, d.SetCreatorID(creatorIDCounter))\n\t\tcreatorIDCounter++\n\t}\n\treset()\n\n\tmetaMap := map[base.DiskFileNum]objstorage.ObjectMetadata{}\n\n\trequire.NoError(t, d.Set([]byte(\"d\"), []byte(\"unexpected\"), nil))\n\trequire.NoError(t, d.Set([]byte(\"e\"), []byte(\"unexpected\"), nil))\n\trequire.NoError(t, d.Set([]byte(\"a\"), []byte(\"unexpected\"), nil))\n\trequire.NoError(t, d.Set([]byte(\"f\"), []byte(\"unexpected\"), nil))\n\td.Flush()\n\n\t{\n\t\t// Create a shared file.\n\t\tfn := base.FileNum(2)\n\t\tf, meta, err := provider2.Create(context.TODO(), fileTypeTable, base.PhysicalTableDiskFileNum(fn), objstorage.CreateOptions{PreferSharedStorage: true})\n\t\trequire.NoError(t, err)\n\t\tw := sstable.NewWriter(f, d.opts.MakeWriterOptions(0, d.TableFormat()))\n\t\tw.Set([]byte(\"d\"), []byte(\"shared\"))\n\t\tw.Set([]byte(\"e\"), []byte(\"shared\"))\n\t\tw.Close()\n\t\tmetaMap[base.PhysicalTableDiskFileNum(fn)] = meta\n\t}\n\n\tm := metaMap[base.DiskFileNum(2)]\n\thandle, err := provider2.RemoteObjectBacking(&m)\n\trequire.NoError(t, err)\n\tsize, err := provider2.Size(m)\n\trequire.NoError(t, err)\n\n\tsharedSSTMeta := SharedSSTMeta{\n\t\tBacking:          handle,\n\t\tSmallest:         base.MakeInternalKey([]byte(\"d\"), 0, InternalKeyKindSet),\n\t\tLargest:          base.MakeInternalKey([]byte(\"e\"), 0, InternalKeyKindSet),\n\t\tSmallestPointKey: base.MakeInternalKey([]byte(\"d\"), 0, InternalKeyKindSet),\n\t\tLargestPointKey:  base.MakeInternalKey([]byte(\"e\"), 0, InternalKeyKindSet),\n\t\tLevel:            6,\n\t\tSize:             uint64(size + 5),\n\t}\n\t_, err = d.IngestAndExcise(\n\t\tcontext.Background(), []string{}, []SharedSSTMeta{sharedSSTMeta}, nil, /* external */\n\t\tKeyRange{Start: []byte(\"d\"), End: []byte(\"ee\")})\n\trequire.NoError(t, err)\n\n\t// TODO(bilal): Once reading of shared sstables is in, verify that the values\n\t// of d and e have been updated.\n}\n\ntype blockedCompaction struct {\n\tstartBlock, unblock chan struct{}\n}\n\nfunc TestConcurrentExcise(t *testing.T) {\n\tvar d, d1, d2 *DB\n\tvar efos map[string]*EventuallyFileOnlySnapshot\n\tbackgroundErrs := make(chan error, 5)\n\tvar compactions map[string]*blockedCompaction\n\tdefer func() {\n\t\tfor _, e := range efos {\n\t\t\trequire.NoError(t, e.Close())\n\t\t}\n\t\tif d1 != nil {\n\t\t\trequire.NoError(t, d1.Close())\n\t\t}\n\t\tif d2 != nil {\n\t\t\trequire.NoError(t, d2.Close())\n\t\t}\n\t}()\n\tcreatorIDCounter := uint64(1)\n\treplicateCounter := 1\n\n\tvar wg sync.WaitGroup\n\tdefer wg.Wait()\n\tvar blockNextCompaction bool\n\tvar blockedJobID int\n\tvar blockedCompactionName string\n\tvar blockedCompactionsMu sync.Mutex // protects the above three variables.\n\n\treset := func() {\n\t\twg.Wait()\n\t\tfor _, e := range efos {\n\t\t\trequire.NoError(t, e.Close())\n\t\t}\n\t\tif d1 != nil {\n\t\t\trequire.NoError(t, d1.Close())\n\t\t}\n\t\tif d2 != nil {\n\t\t\trequire.NoError(t, d2.Close())\n\t\t}\n\t\tefos = make(map[string]*EventuallyFileOnlySnapshot)\n\t\tcompactions = make(map[string]*blockedCompaction)\n\t\tbackgroundErrs = make(chan error, 5)\n\n\t\tvar el EventListener\n\t\tel.EnsureDefaults(testLogger{t: t})\n\t\tel.FlushBegin = func(info FlushInfo) {\n\t\t\t// Don't block flushes\n\t\t}\n\t\tel.BackgroundError = func(err error) {\n\t\t\tbackgroundErrs <- err\n\t\t}\n\t\tel.CompactionBegin = func(info CompactionInfo) {\n\t\t\tif info.Reason == \"move\" {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tblockedCompactionsMu.Lock()\n\t\t\tdefer blockedCompactionsMu.Unlock()\n\t\t\tif blockNextCompaction {\n\t\t\t\tblockNextCompaction = false\n\t\t\t\tblockedJobID = info.JobID\n\t\t\t}\n\t\t}\n\t\tel.TableCreated = func(info TableCreateInfo) {\n\t\t\tblockedCompactionsMu.Lock()\n\t\t\tif info.JobID != blockedJobID {\n\t\t\t\tblockedCompactionsMu.Unlock()\n\t\t\t\treturn\n\t\t\t}\n\t\t\tblockedJobID = 0\n\t\t\tc := compactions[blockedCompactionName]\n\t\t\tblockedCompactionName = \"\"\n\t\t\tblockedCompactionsMu.Unlock()\n\t\t\tc.startBlock <- struct{}{}\n\t\t\t<-c.unblock\n\t\t}\n\n\t\tsstorage := remote.NewInMem()\n\t\tmem1 := vfs.NewMem()\n\t\tmem2 := vfs.NewMem()\n\t\trequire.NoError(t, mem1.MkdirAll(\"ext\", 0755))\n\t\trequire.NoError(t, mem2.MkdirAll(\"ext\", 0755))\n\t\topts1 := &Options{\n\t\t\tComparer:              testkeys.Comparer,\n\t\t\tFS:                    mem1,\n\t\t\tL0CompactionThreshold: 100,\n\t\t\tL0StopWritesThreshold: 100,\n\t\t\tDebugCheck:            DebugCheckLevels,\n\t\t\tFormatMajorVersion:    FormatVirtualSSTables,\n\t\t\tLogger:                testLogger{t},\n\t\t}\n\t\t// lel.\n\t\tlel := MakeLoggingEventListener(testLogger{t})\n\t\ttel := TeeEventListener(lel, el)\n\t\topts1.EventListener = &tel\n\t\topts1.Experimental.RemoteStorage = remote.MakeSimpleFactory(map[remote.Locator]remote.Storage{\n\t\t\t\"\": sstorage,\n\t\t})\n\t\topts1.Experimental.CreateOnShared = remote.CreateOnSharedAll\n\t\topts1.Experimental.CreateOnSharedLocator = \"\"\n\t\t// Disable automatic compactions because otherwise we'll race with\n\t\t// delete-only compactions triggered by ingesting range tombstones.\n\t\topts1.DisableAutomaticCompactions = true\n\t\topts1.Experimental.MultiLevelCompactionHeuristic = NoMultiLevel{}\n\n\t\topts2 := &Options{}\n\t\t*opts2 = *opts1\n\t\topts2.Experimental.RemoteStorage = remote.MakeSimpleFactory(map[remote.Locator]remote.Storage{\n\t\t\t\"\": sstorage,\n\t\t})\n\t\topts2.Experimental.CreateOnShared = remote.CreateOnSharedAll\n\t\topts2.Experimental.CreateOnSharedLocator = \"\"\n\t\topts2.FS = mem2\n\n\t\tvar err error\n\t\td1, err = Open(\"\", opts1)\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, d1.SetCreatorID(creatorIDCounter))\n\t\tcreatorIDCounter++\n\t\td2, err = Open(\"\", opts2)\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, d2.SetCreatorID(creatorIDCounter))\n\t\tcreatorIDCounter++\n\t\td = d1\n\t}\n\treset()\n\n\tdatadriven.RunTest(t, \"testdata/concurrent_excise\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"reset\":\n\t\t\treset()\n\t\t\treturn \"\"\n\t\tcase \"switch\":\n\t\t\tif len(td.CmdArgs) != 1 {\n\t\t\t\treturn \"usage: switch <1 or 2>\"\n\t\t\t}\n\t\t\tswitch td.CmdArgs[0].Key {\n\t\t\tcase \"1\":\n\t\t\t\td = d1\n\t\t\tcase \"2\":\n\t\t\t\td = d2\n\t\t\tdefault:\n\t\t\t\treturn \"usage: switch <1 or 2>\"\n\t\t\t}\n\t\t\treturn \"ok\"\n\t\tcase \"batch\":\n\t\t\tb := d.NewIndexedBatch()\n\t\t\tif err := runBatchDefineCmd(td, b); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tif err := b.Commit(nil); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\t\tcase \"build\":\n\t\t\tif err := runBuildCmd(td, d, d.opts.FS); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"flush\":\n\t\t\tif err := d.Flush(); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"ingest\":\n\t\t\tif err := runIngestCmd(td, d, d.opts.FS); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\t// Wait for a possible flush.\n\t\t\td.mu.Lock()\n\t\t\tfor d.mu.compact.flushing {\n\t\t\t\td.mu.compact.cond.Wait()\n\t\t\t}\n\t\t\td.mu.Unlock()\n\t\t\treturn \"\"\n\n\t\tcase \"ingest-and-excise\":\n\t\t\td.mu.Lock()\n\t\t\tprevFlushableIngests := d.mu.versions.metrics.Flush.AsIngestCount\n\t\t\td.mu.Unlock()\n\t\t\tif err := runIngestAndExciseCmd(td, d, d.opts.FS); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\t// Wait for a possible flush.\n\t\t\td.mu.Lock()\n\t\t\tfor d.mu.compact.flushing {\n\t\t\t\td.mu.compact.cond.Wait()\n\t\t\t}\n\t\t\tflushableIngests := d.mu.versions.metrics.Flush.AsIngestCount\n\t\t\td.mu.Unlock()\n\t\t\tif prevFlushableIngests < flushableIngests {\n\t\t\t\treturn \"flushable ingest\"\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"replicate\":\n\t\t\tif len(td.CmdArgs) != 4 {\n\t\t\t\treturn \"usage: replicate <from-db-num> <to-db-num> <start-key> <end-key>\"\n\t\t\t}\n\t\t\tvar from, to *DB\n\t\t\tswitch td.CmdArgs[0].Key {\n\t\t\tcase \"1\":\n\t\t\t\tfrom = d1\n\t\t\tcase \"2\":\n\t\t\t\tfrom = d2\n\t\t\tdefault:\n\t\t\t\treturn \"usage: replicate <from-db-num> <to-db-num> <start-key> <end-key>\"\n\t\t\t}\n\t\t\tswitch td.CmdArgs[1].Key {\n\t\t\tcase \"1\":\n\t\t\t\tto = d1\n\t\t\tcase \"2\":\n\t\t\t\tto = d2\n\t\t\tdefault:\n\t\t\t\treturn \"usage: replicate <from-db-num> <to-db-num> <start-key> <end-key>\"\n\t\t\t}\n\t\t\tstartKey := []byte(td.CmdArgs[2].Key)\n\t\t\tendKey := []byte(td.CmdArgs[3].Key)\n\n\t\t\twriteOpts := d.opts.MakeWriterOptions(0 /* level */, to.TableFormat())\n\t\t\tsstPath := fmt.Sprintf(\"ext/replicate%d.sst\", replicateCounter)\n\t\t\tf, err := to.opts.FS.Create(sstPath, vfs.WriteCategoryUnspecified)\n\t\t\trequire.NoError(t, err)\n\t\t\treplicateCounter++\n\t\t\tw := sstable.NewRawWriter(objstorageprovider.NewFileWritable(f), writeOpts)\n\n\t\t\tvar sharedSSTs []SharedSSTMeta\n\t\t\terr = from.ScanInternal(context.TODO(), sstable.CategoryUnknown, startKey, endKey,\n\t\t\t\tfunc(key *InternalKey, value LazyValue, _ IteratorLevel) error {\n\t\t\t\t\tval, _, err := value.Value(nil)\n\t\t\t\t\trequire.NoError(t, err)\n\t\t\t\t\trequire.NoError(t, w.AddWithForceObsolete(base.MakeInternalKey(key.UserKey, 0, key.Kind()), val, false /* forceObsolete */))\n\t\t\t\t\treturn nil\n\t\t\t\t},\n\t\t\t\tfunc(start, end []byte, seqNum base.SeqNum) error {\n\t\t\t\t\trequire.NoError(t, w.EncodeSpan(keyspan.Span{\n\t\t\t\t\t\tStart: start,\n\t\t\t\t\t\tEnd:   end,\n\t\t\t\t\t\tKeys:  []keyspan.Key{{Trailer: base.MakeTrailer(0, base.InternalKeyKindRangeDelete)}},\n\t\t\t\t\t}))\n\t\t\t\t\treturn nil\n\t\t\t\t},\n\t\t\t\tfunc(start, end []byte, keys []keyspan.Key) error {\n\t\t\t\t\trequire.NoError(t, w.EncodeSpan(keyspan.Span{\n\t\t\t\t\t\tStart:     start,\n\t\t\t\t\t\tEnd:       end,\n\t\t\t\t\t\tKeys:      keys,\n\t\t\t\t\t\tKeysOrder: 0,\n\t\t\t\t\t}))\n\t\t\t\t\treturn nil\n\t\t\t\t},\n\t\t\t\tfunc(sst *SharedSSTMeta) error {\n\t\t\t\t\tsharedSSTs = append(sharedSSTs, *sst)\n\t\t\t\t\treturn nil\n\t\t\t\t},\n\t\t\t\tnil,\n\t\t\t)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.NoError(t, w.Close())\n\n\t\t\t_, err = to.IngestAndExcise(context.Background(), []string{sstPath}, sharedSSTs, nil, KeyRange{Start: startKey, End: endKey})\n\t\t\trequire.NoError(t, err)\n\t\t\treturn fmt.Sprintf(\"replicated %d shared SSTs\", len(sharedSSTs))\n\n\t\tcase \"get\":\n\t\t\treturn runGetCmd(t, td, d)\n\n\t\tcase \"iter\":\n\t\t\to := &IterOptions{KeyTypes: IterKeyTypePointsAndRanges}\n\t\t\tvar reader Reader\n\t\t\treader = d\n\t\t\tfor _, arg := range td.CmdArgs {\n\t\t\t\tswitch arg.Key {\n\t\t\t\tcase \"mask-suffix\":\n\t\t\t\t\to.RangeKeyMasking.Suffix = []byte(arg.Vals[0])\n\t\t\t\tcase \"mask-filter\":\n\t\t\t\t\to.RangeKeyMasking.Filter = func() BlockPropertyFilterMask {\n\t\t\t\t\t\treturn sstable.NewTestKeysMaskingFilter()\n\t\t\t\t\t}\n\t\t\t\tcase \"snapshot\":\n\t\t\t\t\treader = efos[arg.Vals[0]]\n\t\t\t\t}\n\t\t\t}\n\t\t\titer, err := reader.NewIter(o)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn runIterCmd(td, iter, true)\n\n\t\tcase \"lsm\":\n\t\t\treturn runLSMCmd(td, d)\n\n\t\tcase \"metrics\":\n\t\t\t// The asynchronous loading of table stats can change metrics, so\n\t\t\t// wait for all the tables' stats to be loaded.\n\t\t\td.mu.Lock()\n\t\t\td.waitTableStats()\n\t\t\td.mu.Unlock()\n\n\t\t\treturn d.Metrics().StringForTests()\n\n\t\tcase \"wait-pending-table-stats\":\n\t\t\treturn runTableStatsCmd(td, d)\n\n\t\tcase \"excise\":\n\t\t\tve := &versionEdit{\n\t\t\t\tDeletedFiles: map[deletedFileEntry]*fileMetadata{},\n\t\t\t}\n\t\t\tvar exciseSpan KeyRange\n\t\t\tif len(td.CmdArgs) != 2 {\n\t\t\t\tpanic(\"insufficient args for excise command\")\n\t\t\t}\n\t\t\texciseSpan.Start = []byte(td.CmdArgs[0].Key)\n\t\t\texciseSpan.End = []byte(td.CmdArgs[1].Key)\n\n\t\t\td.mu.Lock()\n\t\t\td.mu.versions.logLock()\n\t\t\td.mu.Unlock()\n\t\t\tcurrent := d.mu.versions.currentVersion()\n\t\t\tfor level := range current.Levels {\n\t\t\t\titer := current.Levels[level].Iter()\n\t\t\t\tfor m := iter.SeekGE(d.cmp, exciseSpan.Start); m != nil && d.cmp(m.Smallest.UserKey, exciseSpan.End) < 0; m = iter.Next() {\n\t\t\t\t\t_, err := d.excise(context.Background(), exciseSpan.UserKeyBounds(), m, ve, level)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\td.mu.Lock()\n\t\t\t\t\t\td.mu.versions.logUnlock()\n\t\t\t\t\t\td.mu.Unlock()\n\t\t\t\t\t\treturn fmt.Sprintf(\"error when excising %s: %s\", m.FileNum, err.Error())\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\td.mu.Lock()\n\t\t\td.mu.versions.logUnlock()\n\t\t\td.mu.Unlock()\n\t\t\treturn fmt.Sprintf(\"would excise %d files, use ingest-and-excise to excise.\\n%s\", len(ve.DeletedFiles), ve.String())\n\n\t\tcase \"file-only-snapshot\":\n\t\t\tif len(td.CmdArgs) != 1 {\n\t\t\t\tpanic(\"insufficient args for file-only-snapshot command\")\n\t\t\t}\n\t\t\tname := td.CmdArgs[0].Key\n\t\t\tvar keyRanges []KeyRange\n\t\t\tfor _, line := range strings.Split(td.Input, \"\\n\") {\n\t\t\t\tfields := strings.Fields(line)\n\t\t\t\tif len(fields) != 2 {\n\t\t\t\t\treturn \"expected two fields for file-only snapshot KeyRanges\"\n\t\t\t\t}\n\t\t\t\tkr := KeyRange{Start: []byte(fields[0]), End: []byte(fields[1])}\n\t\t\t\tkeyRanges = append(keyRanges, kr)\n\t\t\t}\n\n\t\t\ts := d.NewEventuallyFileOnlySnapshot(keyRanges)\n\t\t\tefos[name] = s\n\t\t\treturn \"ok\"\n\n\t\tcase \"wait-for-file-only-snapshot\":\n\t\t\tif len(td.CmdArgs) != 1 {\n\t\t\t\tpanic(\"insufficient args for file-only-snapshot command\")\n\t\t\t}\n\t\t\tname := td.CmdArgs[0].Key\n\t\t\terr := efos[name].WaitForFileOnlySnapshot(context.TODO(), 1*time.Millisecond)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"ok\"\n\n\t\tcase \"unblock\":\n\t\t\tname := td.CmdArgs[0].Key\n\t\t\tblockedCompactionsMu.Lock()\n\t\t\tc := compactions[name]\n\t\t\tdelete(compactions, name)\n\t\t\tblockedCompactionsMu.Unlock()\n\t\t\tc.unblock <- struct{}{}\n\t\t\treturn \"ok\"\n\n\t\tcase \"compact\":\n\t\t\tasync := false\n\t\t\tvar otherArgs []datadriven.CmdArg\n\t\t\tvar bc *blockedCompaction\n\t\t\tfor i := range td.CmdArgs {\n\t\t\t\tswitch td.CmdArgs[i].Key {\n\t\t\t\tcase \"block\":\n\t\t\t\t\tname := td.CmdArgs[i].Vals[0]\n\t\t\t\t\tbc = &blockedCompaction{startBlock: make(chan struct{}), unblock: make(chan struct{})}\n\t\t\t\t\tblockedCompactionsMu.Lock()\n\t\t\t\t\tcompactions[name] = bc\n\t\t\t\t\tblockNextCompaction = true\n\t\t\t\t\tblockedCompactionName = name\n\t\t\t\t\tblockedCompactionsMu.Unlock()\n\t\t\t\t\tasync = true\n\t\t\t\tdefault:\n\t\t\t\t\totherArgs = append(otherArgs, td.CmdArgs[i])\n\t\t\t\t}\n\t\t\t}\n\t\t\tvar tdClone datadriven.TestData\n\t\t\ttdClone = *td\n\t\t\ttdClone.CmdArgs = otherArgs\n\t\t\tif !async {\n\t\t\t\terr := runCompactCmd(td, d)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\twg.Add(1)\n\t\t\t\tgo func() {\n\t\t\t\t\tdefer wg.Done()\n\t\t\t\t\t_ = runCompactCmd(&tdClone, d)\n\t\t\t\t}()\n\t\t\t\t<-bc.startBlock\n\t\t\t\treturn \"spun off in separate goroutine\"\n\t\t\t}\n\t\t\treturn \"ok\"\n\t\tcase \"wait-for-background-error\":\n\t\t\terr := <-backgroundErrs\n\t\t\treturn err.Error()\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc TestIngestExternal(t *testing.T) {\n\tvar mem vfs.FS\n\tvar d, d1, d2 *DB\n\tvar opts *Options\n\tvar flushed bool\n\tdefer func() {\n\t\tif d1 != nil {\n\t\t\trequire.NoError(t, d1.Close())\n\t\t}\n\t\tif d2 != nil {\n\t\t\trequire.NoError(t, d2.Close())\n\t\t}\n\t}()\n\n\tvar remoteStorage remote.Storage\n\treplicateCounter := 1\n\treopen := func(t *testing.T) {\n\t\tif d != nil {\n\t\t\tif d1 != nil {\n\t\t\t\trequire.NoError(t, d1.Close())\n\t\t\t}\n\t\t\tif d2 != nil {\n\t\t\t\trequire.NoError(t, d2.Close())\n\t\t\t}\n\t\t}\n\n\t\tvar err error\n\t\td1, err = Open(\"d1\", opts)\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, d1.SetCreatorID(1))\n\t\td2, err = Open(\"d2\", opts)\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, d2.SetCreatorID(2))\n\t\td = d1\n\t}\n\treset := func(t *testing.T, majorVersion FormatMajorVersion) {\n\n\t\tmem = vfs.NewMem()\n\t\trequire.NoError(t, mem.MkdirAll(\"ext\", 0755))\n\t\tremoteStorage = remote.NewInMem()\n\t\topts = &Options{\n\t\t\tComparer:              testkeys.Comparer,\n\t\t\tFS:                    mem,\n\t\t\tL0CompactionThreshold: 100,\n\t\t\tL0StopWritesThreshold: 100,\n\t\t\tDebugCheck:            DebugCheckLevels,\n\t\t\tEventListener: &EventListener{FlushEnd: func(info FlushInfo) {\n\t\t\t\tflushed = true\n\t\t\t}},\n\t\t\tFormatMajorVersion: majorVersion,\n\t\t\tLogger:             testLogger{t},\n\t\t}\n\t\topts.Experimental.EnableColumnarBlocks = func() bool { return true }\n\t\topts.Experimental.RemoteStorage = remote.MakeSimpleFactory(map[remote.Locator]remote.Storage{\n\t\t\t\"external-locator\": remoteStorage,\n\t\t})\n\t\topts.Experimental.CreateOnShared = remote.CreateOnSharedNone\n\t\topts.Experimental.IngestSplit = func() bool {\n\t\t\treturn true\n\t\t}\n\t\t// Disable automatic compactions because otherwise we'll race with\n\t\t// delete-only compactions triggered by ingesting range tombstones.\n\t\topts.DisableAutomaticCompactions = true\n\t\tlel := MakeLoggingEventListener(testLogger{t})\n\t\topts.EventListener = &lel\n\t\treopen(t)\n\t}\n\treset(t, FormatNewest)\n\n\tdatadriven.RunTest(t, \"testdata/ingest_external\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"reopen\":\n\t\t\treopen(t)\n\t\t\treturn \"\"\n\t\tcase \"reset\":\n\t\t\tmajorVersion := int64(FormatNewest)\n\t\t\tif td.HasArg(\"format-major-version\") {\n\t\t\t\ttd.ScanArgs(t, \"format-major-version\", &majorVersion)\n\t\t\t}\n\t\t\treset(t, FormatMajorVersion(majorVersion))\n\t\t\treturn \"\"\n\t\tcase \"batch\":\n\t\t\tb := d.NewIndexedBatch()\n\t\t\tif err := runBatchDefineCmd(td, b); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tif err := b.Commit(nil); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\t\tcase \"build-remote\":\n\t\t\tif err := runBuildRemoteCmd(td, d, remoteStorage); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\t\tcase \"switch\":\n\t\t\tif len(td.CmdArgs) != 1 {\n\t\t\t\treturn \"usage: switch <1 or 2>\"\n\t\t\t}\n\t\t\tswitch td.CmdArgs[0].Key {\n\t\t\tcase \"1\":\n\t\t\t\td = d1\n\t\t\tcase \"2\":\n\t\t\t\td = d2\n\t\t\tdefault:\n\t\t\t\treturn \"usage: switch <1 or 2>\"\n\t\t\t}\n\t\t\treturn \"ok\"\n\t\tcase \"flush\":\n\t\t\tif err := d.Flush(); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"build\":\n\t\t\tif err := runBuildCmd(td, d, mem); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"ingest\":\n\t\t\tflushed = false\n\t\t\tif err := runIngestCmd(td, d, mem); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\t// Wait for a possible flush.\n\t\t\td.mu.Lock()\n\t\t\tfor d.mu.compact.flushing {\n\t\t\t\td.mu.compact.cond.Wait()\n\t\t\t}\n\t\t\td.mu.Unlock()\n\t\t\tif flushed {\n\t\t\t\treturn \"memtable flushed\"\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"ingest-external\":\n\t\t\tflushed = false\n\t\t\tif err := runIngestExternalCmd(t, td, d, remoteStorage, \"external-locator\"); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\t// Wait for a possible flush.\n\t\t\td.mu.Lock()\n\t\t\tfor d.mu.compact.flushing {\n\t\t\t\td.mu.compact.cond.Wait()\n\t\t\t}\n\t\t\td.mu.Unlock()\n\t\t\tif flushed {\n\t\t\t\treturn \"memtable flushed\"\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"download\":\n\t\t\tif len(td.CmdArgs) < 2 {\n\t\t\t\tpanic(\"insufficient args for download command\")\n\t\t\t}\n\t\t\tviaBackingFileDownload := false\n\t\t\tfor _, arg := range td.CmdArgs[2:] {\n\t\t\t\tswitch arg.Key {\n\t\t\t\tcase \"via-backing-file-download\":\n\t\t\t\t\tviaBackingFileDownload = true\n\t\t\t\tdefault:\n\t\t\t\t\treturn fmt.Sprintf(\"unexpected key: %s\", arg.Key)\n\t\t\t\t}\n\t\t\t}\n\t\t\tl := []byte(td.CmdArgs[0].Key)\n\t\t\tr := []byte(td.CmdArgs[1].Key)\n\t\t\tspans := []DownloadSpan{{StartKey: l, EndKey: r, ViaBackingFileDownload: viaBackingFileDownload}}\n\t\t\tctx, cancel := context.WithTimeout(context.TODO(), 1*time.Minute)\n\t\t\tdefer cancel()\n\t\t\terr := d.Download(ctx, spans)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"ok\"\n\n\t\tcase \"get\":\n\t\t\treturn runGetCmd(t, td, d)\n\n\t\tcase \"iter\":\n\t\t\titer, _ := d.NewIter(&IterOptions{\n\t\t\t\tKeyTypes: IterKeyTypePointsAndRanges,\n\t\t\t})\n\t\t\treturn runIterCmd(td, iter, true)\n\n\t\tcase \"lsm\":\n\t\t\treturn runLSMCmd(td, d)\n\n\t\tcase \"metrics\":\n\t\t\t// The asynchronous loading of table stats can change metrics, so\n\t\t\t// wait for all the tables' stats to be loaded.\n\t\t\td.mu.Lock()\n\t\t\td.waitTableStats()\n\t\t\td.mu.Unlock()\n\n\t\t\treturn d.Metrics().StringForTests()\n\n\t\tcase \"wait-pending-table-stats\":\n\t\t\treturn runTableStatsCmd(td, d)\n\n\t\tcase \"compact\":\n\t\t\tif len(td.CmdArgs) != 2 {\n\t\t\t\tpanic(\"insufficient args for compact command\")\n\t\t\t}\n\t\t\tl := td.CmdArgs[0].Key\n\t\t\tr := td.CmdArgs[1].Key\n\t\t\terr := d.Compact([]byte(l), []byte(r), false)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\t\tcase \"replicate\":\n\t\t\tif len(td.CmdArgs) != 4 {\n\t\t\t\treturn \"usage: replicate <from-db-num> <to-db-num> <start-key> <end-key>\"\n\t\t\t}\n\t\t\tvar from, to *DB\n\t\t\tswitch td.CmdArgs[0].Key {\n\t\t\tcase \"1\":\n\t\t\t\tfrom = d1\n\t\t\tcase \"2\":\n\t\t\t\tfrom = d2\n\t\t\tdefault:\n\t\t\t\treturn \"usage: replicate <from-db-num> <to-db-num> <start-key> <end-key>\"\n\t\t\t}\n\t\t\tswitch td.CmdArgs[1].Key {\n\t\t\tcase \"1\":\n\t\t\t\tto = d1\n\t\t\tcase \"2\":\n\t\t\t\tto = d2\n\t\t\tdefault:\n\t\t\t\treturn \"usage: replicate <from-db-num> <to-db-num> <start-key> <end-key>\"\n\t\t\t}\n\t\t\tstartKey := []byte(td.CmdArgs[2].Key)\n\t\t\tendKey := []byte(td.CmdArgs[3].Key)\n\n\t\t\twriteOpts := d.opts.MakeWriterOptions(0 /* level */, to.TableFormat())\n\t\t\tsstPath := fmt.Sprintf(\"ext/replicate%d.sst\", replicateCounter)\n\t\t\tf, err := to.opts.FS.Create(sstPath, vfs.WriteCategoryUnspecified)\n\t\t\trequire.NoError(t, err)\n\t\t\treplicateCounter++\n\t\t\tw := sstable.NewRawWriter(objstorageprovider.NewFileWritable(f), writeOpts)\n\n\t\t\tvar externalFiles []ExternalFile\n\t\t\terr = from.ScanInternal(context.TODO(), sstable.CategoryUnknown, startKey, endKey,\n\t\t\t\tfunc(key *InternalKey, value LazyValue, _ IteratorLevel) error {\n\t\t\t\t\tval, _, err := value.Value(nil)\n\t\t\t\t\trequire.NoError(t, err)\n\t\t\t\t\trequire.NoError(t, w.AddWithForceObsolete(base.MakeInternalKey(key.UserKey, 0, key.Kind()), val, false /* forceObsolete */))\n\t\t\t\t\treturn nil\n\t\t\t\t},\n\t\t\t\tfunc(start, end []byte, seqNum base.SeqNum) error {\n\t\t\t\t\trequire.NoError(t, w.EncodeSpan(keyspan.Span{\n\t\t\t\t\t\tStart: start,\n\t\t\t\t\t\tEnd:   end,\n\t\t\t\t\t\tKeys:  []keyspan.Key{{Trailer: base.MakeTrailer(0, base.InternalKeyKindRangeDelete)}},\n\t\t\t\t\t}))\n\t\t\t\t\treturn nil\n\t\t\t\t},\n\t\t\t\tfunc(start, end []byte, keys []keyspan.Key) error {\n\t\t\t\t\trequire.NoError(t, w.EncodeSpan(keyspan.Span{\n\t\t\t\t\t\tStart:     start,\n\t\t\t\t\t\tEnd:       end,\n\t\t\t\t\t\tKeys:      keys,\n\t\t\t\t\t\tKeysOrder: 0,\n\t\t\t\t\t}))\n\t\t\t\t\treturn nil\n\t\t\t\t},\n\t\t\t\tnil,\n\t\t\t\tfunc(sst *ExternalFile) error {\n\t\t\t\t\texternalFiles = append(externalFiles, *sst)\n\t\t\t\t\treturn nil\n\t\t\t\t},\n\t\t\t)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.NoError(t, w.Close())\n\t\t\t_, err = to.IngestAndExcise(context.Background(), []string{sstPath}, nil /* shared */, externalFiles, KeyRange{Start: startKey, End: endKey})\n\t\t\trequire.NoError(t, err)\n\t\t\treturn fmt.Sprintf(\"replicated %d external SSTs\", len(externalFiles))\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc TestIngestMemtableOverlaps(t *testing.T) {\n\tcomparers := []Comparer{\n\t\t{\n\t\t\tName:    \"default\",\n\t\t\tCompare: DefaultComparer.Compare,\n\t\t},\n\t\t{\n\t\t\tName:    \"reverse\",\n\t\t\tCompare: func(a, b []byte) int { return DefaultComparer.Compare(b, a) },\n\t\t},\n\t}\n\tm := make(map[string]*Comparer)\n\tfor i := range comparers {\n\t\tc := &comparers[i]\n\t\tc.AbbreviatedKey = func(key []byte) uint64 { panic(\"unimplemented\") }\n\t\tc.Successor = func(dst, a []byte) []byte { panic(\"unimplemented\") }\n\t\tc.Separator = func(dst, a, b []byte) []byte { panic(\"unimplemented\") }\n\t\tm[c.Name] = c\n\t}\n\n\tfor _, comparer := range comparers {\n\t\tt.Run(comparer.Name, func(t *testing.T) {\n\t\t\tvar mem *memTable\n\n\t\t\tparseMeta := func(s string) *fileMetadata {\n\t\t\t\tparts := strings.Split(s, \"-\")\n\t\t\t\tmeta := &fileMetadata{}\n\t\t\t\tif len(parts) != 2 {\n\t\t\t\t\tt.Fatalf(\"malformed table spec: %s\", s)\n\t\t\t\t}\n\t\t\t\tvar smallest, largest base.InternalKey\n\t\t\t\tif strings.Contains(parts[0], \".\") {\n\t\t\t\t\tif !strings.Contains(parts[1], \".\") {\n\t\t\t\t\t\tt.Fatalf(\"malformed table spec: %s\", s)\n\t\t\t\t\t}\n\t\t\t\t\tsmallest = base.ParseInternalKey(parts[0])\n\t\t\t\t\tlargest = base.ParseInternalKey(parts[1])\n\t\t\t\t} else {\n\t\t\t\t\tsmallest = InternalKey{UserKey: []byte(parts[0])}\n\t\t\t\t\tlargest = InternalKey{UserKey: []byte(parts[1])}\n\t\t\t\t}\n\t\t\t\t// If we're using a reverse comparer, flip the file bounds.\n\t\t\t\tif mem.cmp(smallest.UserKey, largest.UserKey) > 0 {\n\t\t\t\t\tsmallest, largest = largest, smallest\n\t\t\t\t}\n\t\t\t\tmeta.ExtendPointKeyBounds(comparer.Compare, smallest, largest)\n\t\t\t\tmeta.InitPhysicalBacking()\n\t\t\t\treturn meta\n\t\t\t}\n\n\t\t\tdatadriven.RunTest(t, \"testdata/ingest_memtable_overlaps\", func(t *testing.T, d *datadriven.TestData) string {\n\t\t\t\tswitch d.Cmd {\n\t\t\t\tcase \"define\":\n\t\t\t\t\tb := newBatch(nil)\n\t\t\t\t\tif err := runBatchDefineCmd(d, b); err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\n\t\t\t\t\topts := &Options{\n\t\t\t\t\t\tComparer: &comparer,\n\t\t\t\t\t}\n\t\t\t\t\topts.EnsureDefaults().WithFSDefaults()\n\t\t\t\t\tif len(d.CmdArgs) > 1 {\n\t\t\t\t\t\treturn fmt.Sprintf(\"%s expects at most 1 argument\", d.Cmd)\n\t\t\t\t\t}\n\t\t\t\t\tif len(d.CmdArgs) == 1 {\n\t\t\t\t\t\topts.Comparer = m[d.CmdArgs[0].String()]\n\t\t\t\t\t\tif opts.Comparer == nil {\n\t\t\t\t\t\t\treturn fmt.Sprintf(\"%s unknown comparer: %s\", d.Cmd, d.CmdArgs[0].String())\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tmem = newMemTable(memTableOptions{Options: opts})\n\t\t\t\t\tif err := mem.apply(b, 0); err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t\treturn \"\"\n\n\t\t\t\tcase \"overlaps\":\n\t\t\t\t\tvar buf bytes.Buffer\n\t\t\t\t\tfor _, data := range strings.Split(d.Input, \"\\n\") {\n\t\t\t\t\t\tvar keyRanges []bounded\n\t\t\t\t\t\tfor _, part := range strings.Fields(data) {\n\t\t\t\t\t\t\tmeta := parseMeta(part)\n\t\t\t\t\t\t\tkeyRanges = append(keyRanges, meta)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tvar overlaps bool\n\t\t\t\t\t\tmem.computePossibleOverlaps(func(bounded) shouldContinue {\n\t\t\t\t\t\t\toverlaps = true\n\t\t\t\t\t\t\treturn stopIteration\n\t\t\t\t\t\t}, keyRanges...)\n\t\t\t\t\t\tfmt.Fprintf(&buf, \"%t\\n\", overlaps)\n\t\t\t\t\t}\n\t\t\t\t\treturn buf.String()\n\n\t\t\t\tdefault:\n\t\t\t\t\treturn fmt.Sprintf(\"unknown command: %s\", d.Cmd)\n\t\t\t\t}\n\t\t\t})\n\t\t})\n\t}\n}\n\nfunc TestKeyRangeBasic(t *testing.T) {\n\tcmp := base.DefaultComparer.Compare\n\tk1 := KeyRange{Start: []byte(\"b\"), End: []byte(\"c\")}\n\n\t// Tests for Contains()\n\trequire.True(t, k1.Contains(cmp, base.MakeInternalKey([]byte(\"b\"), 1, InternalKeyKindSet)))\n\trequire.False(t, k1.Contains(cmp, base.MakeInternalKey([]byte(\"c\"), 1, InternalKeyKindSet)))\n\trequire.True(t, k1.Contains(cmp, base.MakeInternalKey([]byte(\"bb\"), 1, InternalKeyKindSet)))\n\trequire.True(t, k1.Contains(cmp, base.MakeExclusiveSentinelKey(InternalKeyKindRangeDelete, []byte(\"c\"))))\n\n\tm1 := &fileMetadata{\n\t\tSmallest: base.MakeInternalKey([]byte(\"b\"), 1, InternalKeyKindSet),\n\t\tLargest:  base.MakeInternalKey([]byte(\"c\"), 1, InternalKeyKindSet),\n\t}\n\trequire.True(t, k1.Overlaps(cmp, m1))\n\tm2 := &fileMetadata{\n\t\tSmallest: base.MakeInternalKey([]byte(\"c\"), 1, InternalKeyKindSet),\n\t\tLargest:  base.MakeInternalKey([]byte(\"d\"), 1, InternalKeyKindSet),\n\t}\n\trequire.False(t, k1.Overlaps(cmp, m2))\n\tm3 := &fileMetadata{\n\t\tSmallest: base.MakeInternalKey([]byte(\"a\"), 1, InternalKeyKindSet),\n\t\tLargest:  base.MakeExclusiveSentinelKey(InternalKeyKindRangeDelete, []byte(\"b\")),\n\t}\n\trequire.False(t, k1.Overlaps(cmp, m3))\n\tm4 := &fileMetadata{\n\t\tSmallest: base.MakeInternalKey([]byte(\"a\"), 1, InternalKeyKindSet),\n\t\tLargest:  base.MakeInternalKey([]byte(\"b\"), 1, InternalKeyKindSet),\n\t}\n\trequire.True(t, k1.Overlaps(cmp, m4))\n}\n\nfunc BenchmarkIngestOverlappingMemtable(b *testing.B) {\n\tassertNoError := func(err error) {\n\t\tb.Helper()\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n\n\tfor count := 1; count < 6; count++ {\n\t\tb.Run(fmt.Sprintf(\"memtables=%d\", count), func(b *testing.B) {\n\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\tb.StopTimer()\n\t\t\t\tmem := vfs.NewMem()\n\t\t\t\td, err := Open(\"\", &Options{\n\t\t\t\t\tFS: mem,\n\t\t\t\t})\n\t\t\t\tassertNoError(err)\n\n\t\t\t\t// Create memtables.\n\t\t\t\tfor {\n\t\t\t\t\tassertNoError(d.Set([]byte(\"a\"), nil, nil))\n\t\t\t\t\td.mu.Lock()\n\t\t\t\t\tdone := len(d.mu.mem.queue) == count\n\t\t\t\t\td.mu.Unlock()\n\t\t\t\t\tif done {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\t// Create the overlapping sstable that will force a flush when ingested.\n\t\t\t\tf, err := mem.Create(\"ext\", vfs.WriteCategoryUnspecified)\n\t\t\t\tassertNoError(err)\n\t\t\t\tw := sstable.NewWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{})\n\t\t\t\tassertNoError(w.Set([]byte(\"a\"), nil))\n\t\t\t\tassertNoError(w.Close())\n\n\t\t\t\tb.StartTimer()\n\t\t\t\tassertNoError(d.Ingest(context.Background(), []string{\"ext\"}))\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestIngestTargetLevel(t *testing.T) {\n\tvar d *DB\n\tdefer func() {\n\t\tif d != nil {\n\t\t\t// Ignore errors because this test defines fake in-progress transactions\n\t\t\t// that prohibit clean shutdown.\n\t\t\t_ = d.Close()\n\t\t}\n\t}()\n\n\tdatadriven.RunTest(t, \"testdata/ingest_target_level\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"define\":\n\t\t\tif d != nil {\n\t\t\t\t// Ignore errors because this test defines fake in-progress\n\t\t\t\t// transactions that prohibit clean shutdown.\n\t\t\t\t_ = d.Close()\n\t\t\t}\n\n\t\t\tvar err error\n\t\t\topts := Options{\n\t\t\t\tFormatMajorVersion: internalFormatNewest,\n\t\t\t}\n\t\t\topts.WithFSDefaults()\n\t\t\tif d, err = runDBDefineCmd(td, &opts); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\n\t\t\treadState := d.loadReadState()\n\t\t\tc := &checkConfig{\n\t\t\t\tlogger:    d.opts.Logger,\n\t\t\t\tcomparer:  d.opts.Comparer,\n\t\t\t\treadState: readState,\n\t\t\t\tnewIters:  d.newIters,\n\t\t\t\t// TODO: runDBDefineCmd doesn't properly update the visible\n\t\t\t\t// sequence number. So we have to explicitly configure level checker with a very large\n\t\t\t\t// sequence number, otherwise the DB appears empty.\n\t\t\t\tseqNum: base.SeqNumMax,\n\t\t\t}\n\t\t\tif err := checkLevelsInternal(c); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treadState.unref()\n\n\t\t\td.mu.Lock()\n\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\td.mu.Unlock()\n\t\t\treturn s\n\n\t\tcase \"target\":\n\t\t\tvar buf bytes.Buffer\n\t\t\tsuggestSplit := false\n\t\t\tfor _, cmd := range td.CmdArgs {\n\t\t\t\tswitch cmd.Key {\n\t\t\t\tcase \"suggest-split\":\n\t\t\t\t\tsuggestSplit = true\n\t\t\t\t}\n\t\t\t}\n\t\t\tfor _, target := range strings.Split(td.Input, \"\\n\") {\n\t\t\t\tmeta, err := manifest.ParseFileMetadataDebug(target)\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\toverlapChecker := &overlapChecker{\n\t\t\t\t\tcomparer: d.opts.Comparer,\n\t\t\t\t\tnewIters: d.newIters,\n\t\t\t\t\topts: IterOptions{\n\t\t\t\t\t\tlogger:   d.opts.Logger,\n\t\t\t\t\t\tCategory: categoryIngest,\n\t\t\t\t\t},\n\t\t\t\t\tv: d.mu.versions.currentVersion(),\n\t\t\t\t}\n\t\t\t\tlsmOverlap, err := overlapChecker.DetermineLSMOverlap(context.Background(), meta.UserKeyBounds())\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\tlevel, overlapFile, err := ingestTargetLevel(\n\t\t\t\t\tcontext.Background(), d.cmp, lsmOverlap, 1, d.mu.compact.inProgress, meta, suggestSplit)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\tif overlapFile != nil {\n\t\t\t\t\tfmt.Fprintf(&buf, \"%d (split file: %s)\\n\", level, overlapFile.FileNum)\n\t\t\t\t} else {\n\t\t\t\t\tfmt.Fprintf(&buf, \"%d\\n\", level)\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn buf.String()\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc TestIngest(t *testing.T) {\n\tvar mem vfs.FS\n\tvar d *DB\n\tvar flushed bool\n\tif runtime.GOARCH == \"386\" {\n\t\tt.Skip(\"skipped on 32-bit due to slightly varied output\")\n\t}\n\tdefer func() {\n\t\trequire.NoError(t, d.Close())\n\t}()\n\n\treset := func(split bool) {\n\t\tif d != nil {\n\t\t\trequire.NoError(t, d.Close())\n\t\t}\n\n\t\tmem = vfs.NewMem()\n\t\trequire.NoError(t, mem.MkdirAll(\"ext\", 0755))\n\t\topts := &Options{\n\t\t\tFS:                    mem,\n\t\t\tL0CompactionThreshold: 100,\n\t\t\tL0StopWritesThreshold: 100,\n\t\t\tDebugCheck:            DebugCheckLevels,\n\t\t\tEventListener: &EventListener{FlushEnd: func(info FlushInfo) {\n\t\t\t\tflushed = true\n\t\t\t}},\n\t\t\tFormatMajorVersion: internalFormatNewest,\n\t\t}\n\t\topts.Experimental.EnableColumnarBlocks = func() bool { return true }\n\t\topts.Experimental.IngestSplit = func() bool {\n\t\t\treturn split\n\t\t}\n\t\t// Disable automatic compactions because otherwise we'll race with\n\t\t// delete-only compactions triggered by ingesting range tombstones.\n\t\topts.DisableAutomaticCompactions = true\n\n\t\tvar err error\n\t\td, err = Open(\"\", opts)\n\t\trequire.NoError(t, err)\n\t}\n\treset(false /* split */)\n\n\tdatadriven.RunTest(t, \"testdata/ingest\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"reset\":\n\t\t\tsplit := false\n\t\t\tfor _, cmd := range td.CmdArgs {\n\t\t\t\tswitch cmd.Key {\n\t\t\t\tcase \"enable-split\":\n\t\t\t\t\tsplit = true\n\t\t\t\tdefault:\n\t\t\t\t\treturn fmt.Sprintf(\"unexpected key: %s\", cmd.Key)\n\t\t\t\t}\n\t\t\t}\n\t\t\treset(split)\n\t\t\treturn \"\"\n\t\tcase \"batch\":\n\t\t\tb := d.NewIndexedBatch()\n\t\t\tif err := runBatchDefineCmd(td, b); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tif err := b.Commit(nil); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"build\":\n\t\t\tif err := runBuildCmd(td, d, mem); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"ingest\":\n\t\t\tflushed = false\n\t\t\tif err := runIngestCmd(td, d, mem); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\t// Wait for a possible flush.\n\t\t\td.mu.Lock()\n\t\t\tfor d.mu.compact.flushing {\n\t\t\t\td.mu.compact.cond.Wait()\n\t\t\t}\n\t\t\td.mu.Unlock()\n\t\t\tif flushed {\n\t\t\t\treturn \"memtable flushed\"\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"get\":\n\t\t\treturn runGetCmd(t, td, d)\n\n\t\tcase \"iter\":\n\t\t\titer, _ := d.NewIter(&IterOptions{\n\t\t\t\tKeyTypes: IterKeyTypePointsAndRanges,\n\t\t\t})\n\t\t\treturn runIterCmd(td, iter, true)\n\n\t\tcase \"lsm\":\n\t\t\treturn runLSMCmd(td, d)\n\n\t\tcase \"metrics\":\n\t\t\t// The asynchronous loading of table stats can change metrics, so\n\t\t\t// wait for all the tables' stats to be loaded.\n\t\t\td.mu.Lock()\n\t\t\td.waitTableStats()\n\t\t\td.mu.Unlock()\n\n\t\t\treturn d.Metrics().StringForTests()\n\n\t\tcase \"wait-pending-table-stats\":\n\t\t\treturn runTableStatsCmd(td, d)\n\n\t\tcase \"compact\":\n\t\t\tif len(td.CmdArgs) != 2 {\n\t\t\t\tpanic(\"insufficient args for compact command\")\n\t\t\t}\n\t\t\tl := td.CmdArgs[0].Key\n\t\t\tr := td.CmdArgs[1].Key\n\t\t\terr := d.Compact([]byte(l), []byte(r), false)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc TestIngestError(t *testing.T) {\n\tfor i := int32(0); ; i++ {\n\t\tmem := vfs.NewMem()\n\n\t\tf0, err := mem.Create(\"ext0\", vfs.WriteCategoryUnspecified)\n\t\trequire.NoError(t, err)\n\t\tw := sstable.NewWriter(objstorageprovider.NewFileWritable(f0), sstable.WriterOptions{})\n\t\trequire.NoError(t, w.Set([]byte(\"d\"), nil))\n\t\trequire.NoError(t, w.Close())\n\t\tf1, err := mem.Create(\"ext1\", vfs.WriteCategoryUnspecified)\n\t\trequire.NoError(t, err)\n\t\tw = sstable.NewWriter(objstorageprovider.NewFileWritable(f1), sstable.WriterOptions{})\n\t\trequire.NoError(t, w.Set([]byte(\"d\"), nil))\n\t\trequire.NoError(t, w.Close())\n\n\t\tii := errorfs.OnIndex(-1)\n\t\td, err := Open(\"\", &Options{\n\t\t\tFS:                    errorfs.Wrap(mem, errorfs.ErrInjected.If(ii)),\n\t\t\tLogger:                panicLogger{},\n\t\t\tL0CompactionThreshold: 8,\n\t\t})\n\t\trequire.NoError(t, err)\n\t\t// Force the creation of an L0 sstable that overlaps with the tables\n\t\t// we'll attempt to ingest. This ensures that we exercise filesystem\n\t\t// codepaths when determining the ingest target level.\n\t\trequire.NoError(t, d.Set([]byte(\"a\"), nil, nil))\n\t\trequire.NoError(t, d.Set([]byte(\"d\"), nil, nil))\n\t\trequire.NoError(t, d.Flush())\n\n\t\tt.Run(fmt.Sprintf(\"index-%d\", i), func(t *testing.T) {\n\t\t\tdefer func() {\n\t\t\t\tif r := recover(); r != nil {\n\t\t\t\t\tif e, ok := r.(error); ok && errors.Is(e, errorfs.ErrInjected) {\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\t// d.opts.Logger.Fatalf won't propagate ErrInjected\n\t\t\t\t\t// itself, but should contain the error message.\n\t\t\t\t\tif strings.HasSuffix(fmt.Sprint(r), errorfs.ErrInjected.Error()) {\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\tt.Fatal(r)\n\t\t\t\t}\n\t\t\t}()\n\n\t\t\tii.Store(i)\n\t\t\terr1 := d.Ingest(context.Background(), []string{\"ext0\"})\n\t\t\terr2 := d.Ingest(context.Background(), []string{\"ext1\"})\n\t\t\terr := firstError(err1, err2)\n\t\t\tif err != nil && !errors.Is(err, errorfs.ErrInjected) {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t})\n\n\t\t// d.Close may error if we failed to flush the manifest.\n\t\t_ = d.Close()\n\n\t\t// If the injector's index is non-negative, the i-th filesystem\n\t\t// operation was never executed.\n\t\tif ii.Load() >= 0 {\n\t\t\tbreak\n\t\t}\n\t}\n}\n\nfunc TestIngestIdempotence(t *testing.T) {\n\t// Use an on-disk filesystem, because Ingest with a MemFS will copy, not\n\t// link the ingested file.\n\tdir, err := os.MkdirTemp(\"\", \"ingest-idempotence\")\n\trequire.NoError(t, err)\n\tdefer os.RemoveAll(dir)\n\tfs := vfs.Default\n\n\tpath := fs.PathJoin(dir, \"ext\")\n\tf, err := fs.Create(fs.PathJoin(dir, \"ext\"), vfs.WriteCategoryUnspecified)\n\trequire.NoError(t, err)\n\tw := sstable.NewWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{})\n\trequire.NoError(t, w.Set([]byte(\"d\"), nil))\n\trequire.NoError(t, w.Close())\n\n\td, err := Open(dir, &Options{\n\t\tFS: fs,\n\t})\n\trequire.NoError(t, err)\n\tconst count = 4\n\tfor i := 0; i < count; i++ {\n\t\tingestPath := fs.PathJoin(dir, fmt.Sprintf(\"ext%d\", i))\n\t\trequire.NoError(t, fs.Link(path, ingestPath))\n\t\trequire.NoError(t, d.Ingest(context.Background(), []string{ingestPath}))\n\t}\n\trequire.NoError(t, d.Close())\n}\n\nfunc TestIngestCompact(t *testing.T) {\n\tmem := vfs.NewMem()\n\tlel := MakeLoggingEventListener(&base.InMemLogger{})\n\td, err := Open(\"\", &Options{\n\t\tEventListener:         &lel,\n\t\tFS:                    mem,\n\t\tL0CompactionThreshold: 1,\n\t\tL0StopWritesThreshold: 1,\n\t})\n\trequire.NoError(t, err)\n\n\tsrc := func(i int) string {\n\t\treturn fmt.Sprintf(\"ext%d\", i)\n\t}\n\tf, err := mem.Create(src(0), vfs.WriteCategoryUnspecified)\n\trequire.NoError(t, err)\n\n\tw := sstable.NewRawWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{})\n\tkey := []byte(\"a\")\n\trequire.NoError(t, w.AddWithForceObsolete(base.MakeInternalKey(key, 0, InternalKeyKindSet), nil, false /* forceObsolete */))\n\trequire.NoError(t, w.Close())\n\n\t// Make N copies of the sstable.\n\tconst count = 20\n\tfor i := 1; i < count; i++ {\n\t\trequire.NoError(t, vfs.Copy(d.opts.FS, src(0), src(i)))\n\t}\n\n\t// Ingest the same sstable multiple times. Compaction should take place as\n\t// ingestion happens, preventing an indefinite write stall from occurring.\n\tfor i := 0; i < count; i++ {\n\t\tif i == 10 {\n\t\t\t// Half-way through the ingestions, set a key in the memtable to force\n\t\t\t// overlap with the memtable which will require the memtable to be\n\t\t\t// flushed.\n\t\t\trequire.NoError(t, d.Set(key, nil, nil))\n\t\t}\n\t\trequire.NoError(t, d.Ingest(context.Background(), []string{src(i)}))\n\t}\n\n\trequire.NoError(t, d.Close())\n}\n\nfunc TestConcurrentIngest(t *testing.T) {\n\tmem := vfs.NewMem()\n\td, err := Open(\"\", &Options{\n\t\tFS: mem,\n\t})\n\trequire.NoError(t, err)\n\n\t// Create an sstable with 2 keys. This is necessary to trigger the overlap\n\t// bug because an sstable with a single key will not have overlap in internal\n\t// key space and the sequence number assignment had already guaranteed\n\t// correct ordering.\n\tsrc := func(i int) string {\n\t\treturn fmt.Sprintf(\"ext%d\", i)\n\t}\n\tf, err := mem.Create(src(0), vfs.WriteCategoryUnspecified)\n\trequire.NoError(t, err)\n\n\tw := sstable.NewWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{})\n\trequire.NoError(t, w.Set([]byte(\"a\"), nil))\n\trequire.NoError(t, w.Set([]byte(\"b\"), nil))\n\trequire.NoError(t, w.Close())\n\n\t// Make N copies of the sstable.\n\terrCh := make(chan error, 5)\n\tfor i := 1; i < cap(errCh); i++ {\n\t\trequire.NoError(t, vfs.Copy(d.opts.FS, src(0), src(i)))\n\t}\n\n\t// Perform N ingestions concurrently.\n\tfor i := 0; i < cap(errCh); i++ {\n\t\tgo func(i int) {\n\t\t\terr := d.Ingest(context.Background(), []string{src(i)})\n\t\t\tif err == nil {\n\t\t\t\tif _, err = d.opts.FS.Stat(src(i)); oserror.IsNotExist(err) {\n\t\t\t\t\terr = nil\n\t\t\t\t}\n\t\t\t}\n\t\t\terrCh <- err\n\t\t}(i)\n\t}\n\tfor i := 0; i < cap(errCh); i++ {\n\t\trequire.NoError(t, <-errCh)\n\t}\n\n\trequire.NoError(t, d.Close())\n}\n\nfunc TestConcurrentIngestCompact(t *testing.T) {\n\tfor i := 0; i < 2; i++ {\n\t\tt.Run(\"\", func(t *testing.T) {\n\t\t\tmem := vfs.NewMem()\n\t\t\tcompactionReady := make(chan struct{})\n\t\t\tcompactionBegin := make(chan struct{})\n\t\t\td, err := Open(\"\", &Options{\n\t\t\t\tFS: mem,\n\t\t\t\tEventListener: &EventListener{\n\t\t\t\t\tTableCreated: func(info TableCreateInfo) {\n\t\t\t\t\t\tif info.Reason == \"compacting\" {\n\t\t\t\t\t\t\tclose(compactionReady)\n\t\t\t\t\t\t\t<-compactionBegin\n\t\t\t\t\t\t}\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t})\n\t\t\trequire.NoError(t, err)\n\n\t\t\tingest := func(keys ...string) {\n\t\t\t\tt.Helper()\n\t\t\t\tf, err := mem.Create(\"ext\", vfs.WriteCategoryUnspecified)\n\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\tw := sstable.NewWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{})\n\t\t\t\tfor _, k := range keys {\n\t\t\t\t\trequire.NoError(t, w.Set([]byte(k), nil))\n\t\t\t\t}\n\t\t\t\trequire.NoError(t, w.Close())\n\t\t\t\trequire.NoError(t, d.Ingest(context.Background(), []string{\"ext\"}))\n\t\t\t}\n\n\t\t\tcompact := func(start, end string) {\n\t\t\t\tt.Helper()\n\t\t\t\trequire.NoError(t, d.Compact([]byte(start), []byte(end), false))\n\t\t\t}\n\n\t\t\tlsm := func() string {\n\t\t\t\td.mu.Lock()\n\t\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\t\td.mu.Unlock()\n\t\t\t\treturn s\n\t\t\t}\n\n\t\t\texpectLSM := func(expected string) {\n\t\t\t\tt.Helper()\n\t\t\t\texpected = strings.TrimSpace(expected)\n\t\t\t\tactual := strings.TrimSpace(lsm())\n\t\t\t\tif expected != actual {\n\t\t\t\t\tt.Fatalf(\"expected\\n%s\\nbut found\\n%s\", expected, actual)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tingest(\"a\")\n\t\t\tingest(\"a\")\n\t\t\tingest(\"c\")\n\t\t\tingest(\"c\")\n\n\t\t\texpectLSM(`\nL0.0:\n  000005:[a#11,SET-a#11,SET]\n  000007:[c#13,SET-c#13,SET]\nL6:\n  000004:[a#10,SET-a#10,SET]\n  000006:[c#12,SET-c#12,SET]\n`)\n\n\t\t\t// At this point ingestion of an sstable containing only key \"b\" will be\n\t\t\t// targeted at L6. Yet a concurrent compaction of sstables 5 and 7 will\n\t\t\t// create a new sstable in L6 spanning [\"a\"-\"c\"]. So the ingestion must\n\t\t\t// actually target L5.\n\n\t\t\tswitch i {\n\t\t\tcase 0:\n\t\t\t\t// Compact, then ingest.\n\t\t\t\tgo func() {\n\t\t\t\t\t<-compactionReady\n\n\t\t\t\t\tingest(\"b\")\n\n\t\t\t\t\tclose(compactionBegin)\n\t\t\t\t}()\n\n\t\t\t\tcompact(\"a\", \"z\")\n\n\t\t\t\texpectLSM(`\nL0.0:\n  000009:[b#14,SET-b#14,SET]\nL6:\n  000008:[a#0,SET-c#0,SET]\n`)\n\n\t\t\tcase 1:\n\t\t\t\t// Ingest, then compact\n\t\t\t\tvar wg sync.WaitGroup\n\t\t\t\twg.Add(1)\n\t\t\t\tgo func() {\n\t\t\t\t\tdefer wg.Done()\n\t\t\t\t\tclose(compactionBegin)\n\t\t\t\t\tcompact(\"a\", \"z\")\n\t\t\t\t}()\n\n\t\t\t\tingest(\"b\")\n\t\t\t\twg.Wait()\n\n\t\t\t\t// Because we're performing the ingestion and compaction concurrently,\n\t\t\t\t// we can't guarantee any particular LSM structure at this point. The\n\t\t\t\t// test will fail with an assertion error due to overlapping sstables\n\t\t\t\t// if there is insufficient synchronization between ingestion and\n\t\t\t\t// compaction.\n\t\t\t}\n\n\t\t\trequire.NoError(t, d.Close())\n\t\t})\n\t}\n}\n\nfunc TestIngestFlushQueuedMemTable(t *testing.T) {\n\t// Verify that ingestion forces a flush of a queued memtable.\n\n\tmem := vfs.NewMem()\n\to := &Options{FS: mem}\n\to.testingRandomized(t)\n\td, err := Open(\"\", o)\n\trequire.NoError(t, err)\n\n\t// Add the key \"a\" to the memtable, then fill up the memtable with the key\n\t// \"b\". The ingested sstable will only overlap with the queued memtable.\n\trequire.NoError(t, d.Set([]byte(\"a\"), nil, nil))\n\tfor {\n\t\trequire.NoError(t, d.Set([]byte(\"b\"), nil, nil))\n\t\td.mu.Lock()\n\t\tdone := len(d.mu.mem.queue) == 2\n\t\td.mu.Unlock()\n\t\tif done {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tingest := func(keys ...string) {\n\t\tf, err := mem.Create(\"ext\", vfs.WriteCategoryUnspecified)\n\t\trequire.NoError(t, err)\n\n\t\tw := sstable.NewWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{\n\t\t\tTableFormat: o.FormatMajorVersion.MinTableFormat(),\n\t\t})\n\t\tfor _, k := range keys {\n\t\t\trequire.NoError(t, w.Set([]byte(k), nil))\n\t\t}\n\t\trequire.NoError(t, w.Close())\n\t\tstats, err := d.IngestWithStats(context.Background(), []string{\"ext\"})\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, stats.ApproxIngestedIntoL0Bytes, stats.Bytes)\n\t\trequire.Equal(t, 1, stats.MemtableOverlappingFiles)\n\t\trequire.Less(t, uint64(0), stats.Bytes)\n\t}\n\n\tingest(\"a\")\n\n\trequire.NoError(t, d.Close())\n}\n\nfunc TestIngestStats(t *testing.T) {\n\tmem := vfs.NewMem()\n\td, err := Open(\"\", &Options{\n\t\tFS: mem,\n\t})\n\trequire.NoError(t, err)\n\n\tingest := func(expectedLevel int, keys ...string) {\n\t\tt.Helper()\n\t\tf, err := mem.Create(\"ext\", vfs.WriteCategoryUnspecified)\n\t\trequire.NoError(t, err)\n\n\t\tw := sstable.NewWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{})\n\t\tfor _, k := range keys {\n\t\t\trequire.NoError(t, w.Set([]byte(k), nil))\n\t\t}\n\t\trequire.NoError(t, w.Close())\n\t\tstats, err := d.IngestWithStats(context.Background(), []string{\"ext\"})\n\t\trequire.NoError(t, err)\n\t\tif expectedLevel == 0 {\n\t\t\trequire.Equal(t, stats.ApproxIngestedIntoL0Bytes, stats.Bytes)\n\t\t} else {\n\t\t\trequire.EqualValues(t, 0, stats.ApproxIngestedIntoL0Bytes)\n\t\t}\n\t\trequire.Less(t, uint64(0), stats.Bytes)\n\t}\n\tingest(6, \"a\")\n\tingest(0, \"a\")\n\tingest(6, \"b\", \"g\")\n\tingest(0, \"c\")\n\trequire.NoError(t, d.Close())\n}\n\nfunc TestIngestFlushQueuedLargeBatch(t *testing.T) {\n\t// Verify that ingestion forces a flush of a queued large batch.\n\n\tmem := vfs.NewMem()\n\td, err := Open(\"\", &Options{\n\t\tFS: mem,\n\t})\n\trequire.NoError(t, err)\n\n\t// The default large batch threshold is slightly less than 1/2 of the\n\t// memtable size which makes triggering a problem with flushing queued large\n\t// batches irritating. Manually adjust the threshold to 1/8 of the memtable\n\t// size in order to more easily create a situation where a large batch is\n\t// queued but not automatically flushed.\n\td.mu.Lock()\n\td.largeBatchThreshold = d.opts.MemTableSize / 8\n\td.mu.Unlock()\n\n\t// Set a record with a large value. This will be transformed into a large\n\t// batch and placed in the flushable queue.\n\trequire.NoError(t, d.Set([]byte(\"a\"), bytes.Repeat([]byte(\"v\"), int(d.largeBatchThreshold)), nil))\n\n\tingest := func(keys ...string) {\n\t\tt.Helper()\n\t\tf, err := mem.Create(\"ext\", vfs.WriteCategoryUnspecified)\n\t\trequire.NoError(t, err)\n\n\t\tw := sstable.NewWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{})\n\t\tfor _, k := range keys {\n\t\t\trequire.NoError(t, w.Set([]byte(k), nil))\n\t\t}\n\t\trequire.NoError(t, w.Close())\n\t\trequire.NoError(t, d.Ingest(context.Background(), []string{\"ext\"}))\n\t}\n\n\tingest(\"a\")\n\n\trequire.NoError(t, d.Close())\n}\n\nfunc TestIngestMemtablePendingOverlap(t *testing.T) {\n\tmem := vfs.NewMem()\n\td, err := Open(\"\", &Options{\n\t\tFS: mem,\n\t})\n\trequire.NoError(t, err)\n\n\td.mu.Lock()\n\t// Use a custom commit pipeline apply function to give us control over\n\t// timing of events.\n\tassignedBatch := make(chan struct{})\n\tapplyBatch := make(chan struct{})\n\toriginalApply := d.commit.env.apply\n\td.commit.env.apply = func(b *Batch, mem *memTable) error {\n\t\tassignedBatch <- struct{}{}\n\t\tapplyBatch <- struct{}{}\n\t\treturn originalApply(b, mem)\n\t}\n\td.mu.Unlock()\n\n\tingest := func(keys ...string) {\n\t\tt.Helper()\n\t\tf, err := mem.Create(\"ext\", vfs.WriteCategoryUnspecified)\n\t\trequire.NoError(t, err)\n\n\t\tw := sstable.NewWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{})\n\t\tfor _, k := range keys {\n\t\t\trequire.NoError(t, w.Set([]byte(k), nil))\n\t\t}\n\t\trequire.NoError(t, w.Close())\n\t\trequire.NoError(t, d.Ingest(context.Background(), []string{\"ext\"}))\n\t}\n\n\tvar wg sync.WaitGroup\n\twg.Add(2)\n\n\t// First, Set('c') begins. This call will:\n\t//\n\t// * enqueue the batch to the pending queue.\n\t// * allocate a sequence number `x`.\n\t// * write the batch to the WAL.\n\t//\n\t// and then block until we read from the `applyBatch` channel down below.\n\tgo func() {\n\t\terr := d.Set([]byte(\"c\"), nil, nil)\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t}\n\t\twg.Done()\n\t}()\n\n\t// When the above Set('c') is ready to apply, it sends on the\n\t// `assignedBatch` channel. Once that happens, we start Ingest('a', 'c').\n\t// The Ingest('a', 'c') allocates sequence number `x + 1`.\n\tgo func() {\n\t\t// Wait until the Set has grabbed a sequence number before ingesting.\n\t\t<-assignedBatch\n\t\tingest(\"a\", \"c\")\n\t\twg.Done()\n\t}()\n\n\t// The Set('c')#1 and Ingest('a', 'c')#2 are both pending. To maintain\n\t// sequence number invariants, the Set needs to be applied and flushed\n\t// before the Ingest determines its target level.\n\t//\n\t// Sleep a bit to ensure that the Ingest has time to call into\n\t// AllocateSeqNum. Once it allocates its sequence number, it should see\n\t// that there are unpublished sequence numbers below it and spin until the\n\t// Set's sequence number is published. After sleeping, read from\n\t// `applyBatch` to actually allow the Set to apply and publish its\n\t// sequence number.\n\ttime.Sleep(100 * time.Millisecond)\n\t<-applyBatch\n\n\t// Wait for both calls to complete.\n\twg.Wait()\n\trequire.NoError(t, d.Flush())\n\trequire.NoError(t, d.CheckLevels(nil))\n\trequire.NoError(t, d.Close())\n}\n\ntype testLogger struct {\n\tt testing.TB\n}\n\nfunc (l testLogger) Infof(format string, args ...interface{}) {\n\tl.t.Logf(format, args...)\n}\n\nfunc (l testLogger) Errorf(format string, args ...interface{}) {\n\tl.t.Logf(format, args...)\n}\n\nfunc (l testLogger) Fatalf(format string, args ...interface{}) {\n\tl.t.Fatalf(format, args...)\n}\n\n// TestIngestMemtableOverlapRace is a regression test for the race described in\n// #2196. If an ingest that checks for overlap with the mutable memtable and\n// finds no overlap, it must not allow overlapping keys with later sequence\n// numbers to be applied to the memtable and the memtable to be flushed before\n// the ingest completes.\n//\n// This test operates by committing the same key concurrently:\n//   - 1 goroutine repeatedly ingests the same sstable writing the key `foo`\n//   - n goroutines repeatedly apply batches writing the key `foo` and trigger\n//     flushes.\n//\n// After a while, the database is closed and the manifest is verified. Version\n// edits should contain new files with monotonically increasing sequence\n// numbers, since every flush and every ingest conflicts with one another.\nfunc TestIngestMemtableOverlapRace(t *testing.T) {\n\tmem := vfs.NewMem()\n\tel := MakeLoggingEventListener(testLogger{t: t})\n\td, err := Open(\"\", &Options{\n\t\tFS: mem,\n\t\t// Disable automatic compactions to keep the manifest clean; only\n\t\t// flushes and ingests.\n\t\tDisableAutomaticCompactions: true,\n\t\t// Disable the WAL to speed up batch commits.\n\t\tDisableWAL:    true,\n\t\tEventListener: &el,\n\t\t// We're endlessly appending to L0 without clearing it, so set a maximal\n\t\t// stop writes threshold.\n\t\tL0StopWritesThreshold: math.MaxInt,\n\t\t// Accumulating more than 1 immutable memtable doesn't help us exercise\n\t\t// the bug, since the committed keys need to be flushed promptly.\n\t\tMemTableStopWritesThreshold: 2,\n\t})\n\trequire.NoError(t, err)\n\n\t// Prepare a sstable `ext` deleting foo.\n\tf, err := mem.Create(\"ext\", vfs.WriteCategoryUnspecified)\n\trequire.NoError(t, err)\n\tw := sstable.NewWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{})\n\trequire.NoError(t, w.Delete([]byte(\"foo\")))\n\trequire.NoError(t, w.Close())\n\n\tvar done atomic.Bool\n\tconst numSetters = 2\n\tvar wg sync.WaitGroup\n\twg.Add(numSetters + 1)\n\n\tuntilDone := func(fn func()) {\n\t\tdefer wg.Done()\n\t\tfor !done.Load() {\n\t\t\tfn()\n\t\t}\n\t}\n\n\t// Ingest in the background.\n\ttotalIngests := 0\n\tgo untilDone(func() {\n\t\tfilename := fmt.Sprintf(\"ext%d\", totalIngests)\n\t\trequire.NoError(t, mem.Link(\"ext\", filename))\n\t\trequire.NoError(t, d.Ingest(context.Background(), []string{filename}))\n\t\ttotalIngests++\n\t})\n\n\t// Apply batches and trigger flushes in the background.\n\two := &WriteOptions{Sync: false}\n\tvar localCommits [numSetters]int\n\tfor i := 0; i < numSetters; i++ {\n\t\ti := i\n\t\tv := []byte(fmt.Sprintf(\"v%d\", i+1))\n\t\tgo untilDone(func() {\n\t\t\t// Commit a batch setting foo=vN.\n\t\t\tb := d.NewBatch()\n\t\t\trequire.NoError(t, b.Set([]byte(\"foo\"), v, nil))\n\t\t\trequire.NoError(t, b.Commit(wo))\n\t\t\tlocalCommits[i]++\n\t\t\td.AsyncFlush()\n\t\t})\n\t}\n\ttime.Sleep(100 * time.Millisecond)\n\tdone.Store(true)\n\twg.Wait()\n\n\tvar totalCommits int\n\tfor i := 0; i < numSetters; i++ {\n\t\ttotalCommits += localCommits[i]\n\t}\n\tm := d.Metrics()\n\ttot := m.Total()\n\tt.Logf(\"Committed %d batches.\", totalCommits)\n\tt.Logf(\"Flushed %d times.\", m.Flush.Count)\n\tt.Logf(\"Ingested %d sstables.\", tot.TablesIngested)\n\trequire.NoError(t, d.CheckLevels(nil))\n\trequire.NoError(t, d.Close())\n\n\t// Replay the manifest. Every flush and ingest is a separate version edit.\n\t// Since they all write the same key and compactions are disabled, sequence\n\t// numbers of new files should be monotonically increasing.\n\t//\n\t// This check is necessary because most of these sstables are ingested into\n\t// L0. The L0 sublevels construction will order them by LargestSeqNum, even\n\t// if they're added to L0 out-of-order. The CheckLevels call at the end of\n\t// the test may find that the sublevels are all appropriately ordered, but\n\t// the manifest may reveal they were added to the LSM out-of-order.\n\tdbDesc, err := Peek(\"\", mem)\n\trequire.NoError(t, err)\n\trequire.True(t, dbDesc.Exists)\n\trequire.Greater(t, len(dbDesc.OptionsFilename), 0)\n\tf, err = mem.Open(dbDesc.ManifestFilename)\n\trequire.NoError(t, err)\n\tdefer f.Close()\n\trr := record.NewReader(f, 0 /* logNum */)\n\tvar largest *fileMetadata\n\tfor {\n\t\tr, err := rr.Next()\n\t\tif err == io.EOF || err == record.ErrInvalidChunk {\n\t\t\tbreak\n\t\t}\n\t\trequire.NoError(t, err)\n\t\tvar ve manifest.VersionEdit\n\t\trequire.NoError(t, ve.Decode(r))\n\t\tt.Log(ve.String())\n\t\tfor _, f := range ve.NewFiles {\n\t\t\tif largest != nil {\n\t\t\t\trequire.Equal(t, 0, f.Level)\n\t\t\t\tif largest.LargestSeqNum > f.Meta.LargestSeqNum {\n\t\t\t\t\tt.Fatalf(\"previous largest file %s has sequence number > next file %s\", largest, f.Meta)\n\t\t\t\t}\n\t\t\t}\n\t\t\tlargest = f.Meta\n\t\t}\n\t}\n}\n\ntype ingestCrashFS struct {\n\tvfs.FS\n}\n\nfunc (fs ingestCrashFS) Link(oldname, newname string) error {\n\tif err := fs.FS.Link(oldname, newname); err != nil {\n\t\treturn err\n\t}\n\tpanic(errorfs.ErrInjected)\n}\n\ntype noRemoveFS struct {\n\tvfs.FS\n}\n\nfunc (fs noRemoveFS) Remove(string) error {\n\treturn errorfs.ErrInjected\n}\n\nfunc TestIngestFileNumReuseCrash(t *testing.T) {\n\tconst count = 10\n\t// Use an on-disk filesystem, because Ingest with a MemFS will copy, not\n\t// link the ingested file.\n\tdir, err := os.MkdirTemp(\"\", \"ingest-filenum-reuse\")\n\trequire.NoError(t, err)\n\tdefer os.RemoveAll(dir)\n\tfs := vfs.Default\n\n\treadFile := func(s string) []byte {\n\t\tf, err := fs.Open(fs.PathJoin(dir, s))\n\t\trequire.NoError(t, err)\n\t\tb, err := io.ReadAll(f)\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, f.Close())\n\t\treturn b\n\t}\n\n\t// Create sstables to ingest.\n\tvar files []string\n\tvar fileBytes [][]byte\n\tfor i := 0; i < count; i++ {\n\t\tname := fmt.Sprintf(\"ext%d\", i)\n\t\tf, err := fs.Create(fs.PathJoin(dir, name), vfs.WriteCategoryUnspecified)\n\t\trequire.NoError(t, err)\n\t\tw := sstable.NewWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{})\n\t\trequire.NoError(t, w.Set([]byte(fmt.Sprintf(\"foo%d\", i)), nil))\n\t\trequire.NoError(t, w.Close())\n\t\tfiles = append(files, name)\n\t\tfileBytes = append(fileBytes, readFile(name))\n\t}\n\n\t// Open a database with a filesystem that will successfully link the\n\t// ingested files but then panic. This is an approximation of what a crash\n\t// after linking but before updating the manifest would look like.\n\td, err := Open(dir, &Options{\n\t\tFS: ingestCrashFS{FS: fs},\n\t})\n\t// A flush here ensures the file num bumps from creating OPTIONS files,\n\t// etc get recorded in the manifest. We want the nextFileNum after the\n\t// restart to be the same as one of our ingested sstables.\n\trequire.NoError(t, err)\n\trequire.NoError(t, d.Set([]byte(\"boop\"), nil, nil))\n\trequire.NoError(t, d.Flush())\n\tfor _, f := range files {\n\t\tfunc() {\n\t\t\tdefer func() { err = recover().(error) }()\n\t\t\terr = d.Ingest(context.Background(), []string{fs.PathJoin(dir, f)})\n\t\t}()\n\t\tif err == nil || !errors.Is(err, errorfs.ErrInjected) {\n\t\t\tt.Fatalf(\"expected injected error, got %v\", err)\n\t\t}\n\t}\n\t// Leave something in the WAL so that Open will flush while replaying the\n\t// WAL.\n\trequire.NoError(t, d.Set([]byte(\"wal\"), nil, nil))\n\trequire.NoError(t, d.Close())\n\n\t// There are now two links to each external file: the original extX link\n\t// and a numbered sstable link. The sstable files are still not a part of\n\t// the manifest and so they may be overwritten. Open will detect the\n\t// obsolete number sstables and try to remove them. The FS here is wrapped\n\t// to induce errors on Remove calls. Even if we're unsuccessful in\n\t// removing the obsolete files, the external files should not be\n\t// overwritten.\n\td, err = Open(dir, &Options{FS: noRemoveFS{FS: fs}, Logger: testLogger{t}})\n\trequire.NoError(t, err)\n\trequire.NoError(t, d.Set([]byte(\"bar\"), nil, nil))\n\trequire.NoError(t, d.Flush())\n\trequire.NoError(t, d.Close())\n\n\t// None of the external files should change despite modifying the linked\n\t// versions.\n\tfor i, f := range files {\n\t\tafterBytes := readFile(f)\n\t\trequire.Equal(t, fileBytes[i], afterBytes)\n\t}\n}\n\nfunc TestIngest_UpdateSequenceNumber(t *testing.T) {\n\tmem := vfs.NewMem()\n\tcmp := base.DefaultComparer.Compare\n\tparse := func(input string) (sstable.RawWriter, error) {\n\t\tf, err := mem.Create(\"ext\", vfs.WriteCategoryUnspecified)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tw := sstable.NewRawWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{\n\t\t\tTableFormat: sstable.TableFormatMax,\n\t\t})\n\t\tfor _, data := range strings.Split(input, \"\\n\") {\n\t\t\tif strings.HasPrefix(data, \"EncodeSpan: \") {\n\t\t\t\tdata = strings.TrimPrefix(data, \"EncodeSpan: \")\n\t\t\t\terr := w.EncodeSpan(keyspan.ParseSpan(data))\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tj := strings.Index(data, \":\")\n\t\t\tif j < 0 {\n\t\t\t\treturn nil, errors.Newf(\"malformed input: %s\\n\", data)\n\t\t\t}\n\t\t\tkey := base.ParseInternalKey(data[:j])\n\t\t\tvalue := []byte(data[j+1:])\n\t\t\tif err := w.AddWithForceObsolete(key, value, false /* forceObsolete */); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t\treturn w, nil\n\t}\n\n\tvar (\n\t\tseqNum base.SeqNum\n\t\tmetas  []*fileMetadata\n\t)\n\tdatadriven.RunTest(t, \"testdata/ingest_update_seqnums\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"starting-seqnum\":\n\t\t\tseqNum = base.ParseSeqNum(td.Input)\n\t\t\treturn \"\"\n\n\t\tcase \"reset\":\n\t\t\tmetas = metas[:0]\n\t\t\treturn \"\"\n\n\t\tcase \"load\":\n\t\t\tw, err := parse(td.Input)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tif err = w.Close(); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tdefer w.Close()\n\n\t\t\t// Format the bounds of the table.\n\t\t\twm, err := w.Metadata()\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\n\t\t\t// Upper bounds for range dels and range keys are expected to be sentinel\n\t\t\t// keys.\n\t\t\tmaybeUpdateUpperBound := func(key base.InternalKey) base.InternalKey {\n\t\t\t\tswitch k := key.Kind(); {\n\t\t\t\tcase k == base.InternalKeyKindRangeDelete:\n\t\t\t\t\tkey.Trailer = base.InternalKeyRangeDeleteSentinel\n\t\t\t\tcase rangekey.IsRangeKey(k):\n\t\t\t\t\treturn base.MakeExclusiveSentinelKey(k, key.UserKey)\n\t\t\t\t}\n\t\t\t\treturn key\n\t\t\t}\n\n\t\t\t// Construct the file metadata from the writer metadata.\n\t\t\tm := &fileMetadata{\n\t\t\t\tSmallestSeqNum: 0, // Simulate an ingestion.\n\t\t\t\tLargestSeqNum:  0,\n\t\t\t}\n\t\t\tif wm.HasPointKeys {\n\t\t\t\tm.ExtendPointKeyBounds(cmp, wm.SmallestPoint, wm.LargestPoint)\n\t\t\t}\n\t\t\tif wm.HasRangeDelKeys {\n\t\t\t\tm.ExtendPointKeyBounds(\n\t\t\t\t\tcmp,\n\t\t\t\t\twm.SmallestRangeDel,\n\t\t\t\t\tmaybeUpdateUpperBound(wm.LargestRangeDel),\n\t\t\t\t)\n\t\t\t}\n\t\t\tif wm.HasRangeKeys {\n\t\t\t\tm.ExtendRangeKeyBounds(\n\t\t\t\t\tcmp,\n\t\t\t\t\twm.SmallestRangeKey,\n\t\t\t\t\tmaybeUpdateUpperBound(wm.LargestRangeKey),\n\t\t\t\t)\n\t\t\t}\n\t\t\tm.InitPhysicalBacking()\n\t\t\tif err := m.Validate(cmp, base.DefaultFormatter); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\n\t\t\t// Collect this file.\n\t\t\tmetas = append(metas, m)\n\n\t\t\t// Return an index number for the file.\n\t\t\treturn fmt.Sprintf(\"file %d\\n\", len(metas)-1)\n\n\t\tcase \"update-files\":\n\t\t\t// Update the bounds across all files.\n\t\t\tfor i, m := range metas {\n\t\t\t\tif err := setSeqNumInMetadata(m, seqNum+base.SeqNum(i), cmp, base.DefaultFormatter); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tvar buf bytes.Buffer\n\t\t\tfor i, m := range metas {\n\t\t\t\tfmt.Fprintf(&buf, \"file %d:\\n\", i)\n\t\t\t\tfmt.Fprintf(&buf, \"  combined: %s-%s\\n\", m.Smallest, m.Largest)\n\t\t\t\tfmt.Fprintf(&buf, \"    points: %s-%s\\n\", m.SmallestPointKey, m.LargestPointKey)\n\t\t\t\tfmt.Fprintf(&buf, \"    ranges: %s-%s\\n\", m.SmallestRangeKey, m.LargestRangeKey)\n\t\t\t}\n\n\t\t\treturn buf.String()\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command %s\\n\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc TestIngestCleanup(t *testing.T) {\n\tfns := []base.FileNum{0, 1, 2}\n\n\ttestCases := []struct {\n\t\tcloseFiles   []base.FileNum\n\t\tcleanupFiles []base.FileNum\n\t\twantErr      string\n\t}{\n\t\t// Close and remove all files.\n\t\t{\n\t\t\tcloseFiles:   fns,\n\t\t\tcleanupFiles: fns,\n\t\t},\n\t\t// Remove a non-existent file.\n\t\t{\n\t\t\tcloseFiles:   fns,\n\t\t\tcleanupFiles: []base.FileNum{3},\n\t\t\twantErr:      \"unknown to the objstorage provider\",\n\t\t},\n\t\t// Remove a file that has not been closed.\n\t\t{\n\t\t\tcloseFiles:   []base.FileNum{0, 2},\n\t\t\tcleanupFiles: fns,\n\t\t\twantErr:      oserror.ErrInvalid.Error(),\n\t\t},\n\t\t// Remove all files, one of which is still open, plus a file that does not exist.\n\t\t{\n\t\t\tcloseFiles:   []base.FileNum{0, 2},\n\t\t\tcleanupFiles: []base.FileNum{0, 1, 2, 3},\n\t\t\twantErr:      oserror.ErrInvalid.Error(), // The first error encountered is due to the open file.\n\t\t},\n\t}\n\n\tfor _, tc := range testCases {\n\t\tt.Run(\"\", func(t *testing.T) {\n\t\t\tmem := vfs.NewMem()\n\t\t\tmem.UseWindowsSemantics(true)\n\t\t\tobjProvider, err := objstorageprovider.Open(objstorageprovider.DefaultSettings(mem, \"\"))\n\t\t\trequire.NoError(t, err)\n\t\t\tdefer objProvider.Close()\n\n\t\t\t// Create the files in the VFS.\n\t\t\tmetaMap := make(map[base.FileNum]objstorage.Writable)\n\t\t\tfor _, fn := range fns {\n\t\t\t\tw, _, err := objProvider.Create(context.Background(), base.FileTypeTable, base.PhysicalTableDiskFileNum(fn), objstorage.CreateOptions{})\n\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\tmetaMap[fn] = w\n\t\t\t}\n\n\t\t\t// Close a select number of files.\n\t\t\tfor _, m := range tc.closeFiles {\n\t\t\t\tw, ok := metaMap[m]\n\t\t\t\tif !ok {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\trequire.NoError(t, w.Finish())\n\t\t\t}\n\n\t\t\t// Cleanup the set of files in the FS.\n\t\t\tvar toRemove []ingestLocalMeta\n\t\t\tfor _, fn := range tc.cleanupFiles {\n\t\t\t\tm := &fileMetadata{FileNum: fn}\n\t\t\t\tm.InitPhysicalBacking()\n\t\t\t\ttoRemove = append(toRemove, ingestLocalMeta{fileMetadata: m})\n\t\t\t}\n\n\t\t\terr = ingestCleanup(objProvider, toRemove)\n\t\t\tif tc.wantErr != \"\" {\n\t\t\t\trequire.Error(t, err, \"got no error, expected %s\", tc.wantErr)\n\t\t\t\trequire.Contains(t, err.Error(), tc.wantErr)\n\t\t\t} else {\n\t\t\t\trequire.NoError(t, err)\n\t\t\t}\n\t\t})\n\t}\n}\n\n// fatalCapturingLogger captures a fatal error instead of panicking.\ntype fatalCapturingLogger struct {\n\tt   testing.TB\n\terr error\n}\n\n// Infof implements the Logger interface.\nfunc (l *fatalCapturingLogger) Infof(fmt string, args ...interface{}) {\n\tl.t.Logf(fmt, args...)\n}\n\n// Errorf implements the Logger interface.\nfunc (l *fatalCapturingLogger) Errorf(fmt string, args ...interface{}) {\n\tl.t.Logf(fmt, args...)\n}\n\n// Fatalf implements the Logger interface.\nfunc (l *fatalCapturingLogger) Fatalf(_ string, args ...interface{}) {\n\tl.err = args[0].(error)\n}\n\nfunc TestIngestValidation(t *testing.T) {\n\ttype keyVal struct {\n\t\tkey, val []byte\n\t}\n\t// The corruptionLocation enum defines where to corrupt an sstable if\n\t// anywhere. corruptionLocation{Start,End} describe the start and end\n\t// data blocks. corruptionLocationInternal describes a random data block\n\t// that's neither the start or end blocks. The Ingest operation does not\n\t// read the entire sstable, only the start and end blocks, so corruption\n\t// introduced using corruptionLocationInternal will not be discovered until\n\t// the asynchronous validation job runs.\n\ttype corruptionLocation int\n\tconst (\n\t\tcorruptionLocationNone corruptionLocation = iota\n\t\tcorruptionLocationStart\n\t\tcorruptionLocationEnd\n\t\tcorruptionLocationInternal\n\t)\n\t// The errReportLocation type defines an enum to allow tests to enforce\n\t// expectations about how an error surfaced during ingestion or validation\n\t// is reported. Asynchronous validation that uncovers corruption should call\n\t// Fatalf on the Logger. Asychronous validation that encounters\n\t// non-corruption errors should surface it through the\n\t// EventListener.BackgroundError func.\n\ttype errReportLocation int\n\tconst (\n\t\terrReportLocationNone errReportLocation = iota\n\t\terrReportLocationIngest\n\t\terrReportLocationFatal\n\t\terrReportLocationBackgroundError\n\t)\n\tconst (\n\t\tnKeys     = 1_000\n\t\tkeySize   = 16\n\t\tvalSize   = 100\n\t\tblockSize = 100\n\n\t\tingestTableName = \"ext\"\n\t)\n\n\tseed := uint64(time.Now().UnixNano())\n\trng := rand.New(rand.NewPCG(0, seed))\n\tt.Logf(\"rng seed = %d\", seed)\n\n\t// errfsCounter is used by test cases that make use of an errorfs.Injector\n\t// to inject errors into the ingest validation code path.\n\tvar errfsCounter atomic.Int32\n\ttestCases := []struct {\n\t\tdescription     string\n\t\tcLoc            corruptionLocation\n\t\twantErrType     errReportLocation\n\t\twantErr         error\n\t\terrorfsInjector errorfs.Injector\n\t}{\n\t\t{\n\t\t\tdescription: \"no corruption\",\n\t\t\tcLoc:        corruptionLocationNone,\n\t\t\twantErrType: errReportLocationNone,\n\t\t},\n\t\t{\n\t\t\tdescription: \"start block\",\n\t\t\tcLoc:        corruptionLocationStart,\n\t\t\twantErr:     ErrCorruption,\n\t\t\twantErrType: errReportLocationIngest,\n\t\t},\n\t\t{\n\t\t\tdescription: \"end block\",\n\t\t\tcLoc:        corruptionLocationEnd,\n\t\t\twantErr:     ErrCorruption,\n\t\t\twantErrType: errReportLocationIngest,\n\t\t},\n\t\t{\n\t\t\tdescription: \"non-end block\",\n\t\t\tcLoc:        corruptionLocationInternal,\n\t\t\twantErr:     ErrCorruption,\n\t\t\twantErrType: errReportLocationFatal,\n\t\t},\n\t\t{\n\t\t\tdescription: \"non-corruption error\",\n\t\t\tcLoc:        corruptionLocationNone,\n\t\t\twantErr:     errorfs.ErrInjected,\n\t\t\twantErrType: errReportLocationBackgroundError,\n\t\t\terrorfsInjector: errorfs.InjectorFunc(func(op errorfs.Op) error {\n\t\t\t\t// Inject an error on the first read-at operation on an sstable\n\t\t\t\t// (excluding the read on the sstable before ingestion has\n\t\t\t\t// linked it in).\n\t\t\t\tif op.Path != \"ext\" && op.Kind != errorfs.OpFileReadAt || filepath.Ext(op.Path) != \".sst\" {\n\t\t\t\t\treturn nil\n\t\t\t\t}\n\t\t\t\tif errfsCounter.Add(1) == 1 {\n\t\t\t\t\treturn errorfs.ErrInjected\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t}),\n\t\t},\n\t}\n\n\tfor _, tc := range testCases {\n\t\tt.Run(tc.description, func(t *testing.T) {\n\t\t\terrfsCounter.Store(0)\n\t\t\tvar wg sync.WaitGroup\n\t\t\twg.Add(1)\n\n\t\t\tfs := vfs.NewMem()\n\t\t\tvar testFS vfs.FS = fs\n\t\t\tif tc.errorfsInjector != nil {\n\t\t\t\ttestFS = errorfs.Wrap(fs, tc.errorfsInjector)\n\t\t\t}\n\n\t\t\t// backgroundErr is populated by EventListener.BackgroundError.\n\t\t\tvar backgroundErr error\n\t\t\tlogger := &fatalCapturingLogger{t: t}\n\t\t\topts := &Options{\n\t\t\t\t// Disable table stats so that injected errors can't be accidentally\n\t\t\t\t// injected into the table stats collector read, and so the table\n\t\t\t\t// stats collector won't prime the table+block cache such that the\n\t\t\t\t// error injection won't trigger at all during ingest validation.\n\t\t\t\tDisableTableStats: true,\n\t\t\t\tFS:                testFS,\n\t\t\t\tLogger:            logger,\n\t\t\t\tEventListener: &EventListener{\n\t\t\t\t\tTableValidated: func(i TableValidatedInfo) {\n\t\t\t\t\t\twg.Done()\n\t\t\t\t\t},\n\t\t\t\t\tBackgroundError: func(err error) {\n\t\t\t\t\t\tbackgroundErr = err\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t}\n\t\t\topts.Experimental.ValidateOnIngest = true\n\t\t\td, err := Open(\"\", opts)\n\t\t\trequire.NoError(t, err)\n\t\t\tdefer func() { require.NoError(t, d.Close()) }()\n\n\t\t\tcorrupt := func(f vfs.File) {\n\t\t\t\treadable, err := sstable.NewSimpleReadable(f)\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\t// Compute the layout of the sstable in order to find the\n\t\t\t\t// appropriate block locations to corrupt.\n\t\t\t\tr, err := sstable.NewReader(context.Background(), readable, opts.MakeReaderOptions())\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\tl, err := r.Layout()\n\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\t// Select an appropriate data block to corrupt.\n\t\t\t\tvar blockIdx int\n\t\t\t\tswitch tc.cLoc {\n\t\t\t\tcase corruptionLocationStart:\n\t\t\t\t\tblockIdx = 0\n\t\t\t\tcase corruptionLocationEnd:\n\t\t\t\t\tblockIdx = len(l.Data) - 1\n\t\t\t\tcase corruptionLocationInternal:\n\t\t\t\t\tblockIdx = 1 + rng.IntN(len(l.Data)-2)\n\t\t\t\tdefault:\n\t\t\t\t\tt.Fatalf(\"unknown corruptionLocation: %T\", tc.cLoc)\n\t\t\t\t}\n\t\t\t\tbh := l.Data[blockIdx]\n\n\t\t\t\t// Corrupting a key will cause the ingestion to fail due to a\n\t\t\t\t// malformed key, rather than a block checksum mismatch.\n\t\t\t\t// Instead, we corrupt the last byte in the selected block,\n\t\t\t\t// before the trailer, which corresponds to a value.\n\t\t\t\toffset := bh.Offset + bh.Length - 1\n\t\t\t\t_, err = f.WriteAt([]byte(\"\\xff\"), int64(offset))\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\trequire.NoError(t, r.Close())\n\t\t\t}\n\n\t\t\ttype errT struct {\n\t\t\t\terrLoc errReportLocation\n\t\t\t\terr    error\n\t\t\t}\n\t\t\trunIngest := func(keyVals []keyVal) (et errT) {\n\t\t\t\tf, err := fs.Create(ingestTableName, vfs.WriteCategoryUnspecified)\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\tdefer func() { _ = fs.Remove(ingestTableName) }()\n\n\t\t\t\tw := sstable.NewWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{\n\t\t\t\t\tBlockSize:   blockSize, // Create many smaller blocks.\n\t\t\t\t\tComparer:    opts.Comparer,\n\t\t\t\t\tCompression: NoCompression, // For simpler debugging.\n\t\t\t\t\tKeySchema:   opts.KeySchemas[opts.KeySchema],\n\t\t\t\t})\n\t\t\t\tfor _, kv := range keyVals {\n\t\t\t\t\trequire.NoError(t, w.Set(kv.key, kv.val))\n\t\t\t\t}\n\t\t\t\trequire.NoError(t, w.Close())\n\n\t\t\t\t// Possibly corrupt the file.\n\t\t\t\tif tc.cLoc != corruptionLocationNone {\n\t\t\t\t\tf, err = fs.OpenReadWrite(ingestTableName, vfs.WriteCategoryUnspecified)\n\t\t\t\t\trequire.NoError(t, err)\n\t\t\t\t\tcorrupt(f)\n\t\t\t\t}\n\n\t\t\t\t// Ingest the external table.\n\t\t\t\terr = d.Ingest(context.Background(), []string{ingestTableName})\n\t\t\t\tif err != nil {\n\t\t\t\t\tet.errLoc = errReportLocationIngest\n\t\t\t\t\tet.err = err\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\t// Wait for the validation on the sstable to complete.\n\t\t\t\twg.Wait()\n\n\t\t\t\t// Return any error encountered during validation.\n\t\t\t\tif logger.err != nil {\n\t\t\t\t\tet.errLoc = errReportLocationFatal\n\t\t\t\t\tet.err = logger.err\n\t\t\t\t} else if backgroundErr != nil {\n\t\t\t\t\tet.errLoc = errReportLocationBackgroundError\n\t\t\t\t\tet.err = backgroundErr\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// Construct a set of keys to ingest.\n\t\t\tvar keyVals []keyVal\n\t\t\tfor i := 0; i < nKeys; i++ {\n\t\t\t\tkey := make([]byte, keySize)\n\t\t\t\tfor j := range key {\n\t\t\t\t\tkey[j] = byte(rng.Uint32())\n\t\t\t\t}\n\n\t\t\t\tval := make([]byte, valSize)\n\t\t\t\tfor j := range val {\n\t\t\t\t\tval[j] = byte(rng.Uint32())\n\t\t\t\t}\n\n\t\t\t\tkeyVals = append(keyVals, keyVal{key, val})\n\t\t\t}\n\n\t\t\t// Keys must be sorted.\n\t\t\tslices.SortFunc(keyVals, func(a, b keyVal) int { return d.cmp(a.key, b.key) })\n\n\t\t\t// Run the ingestion.\n\t\t\tet := runIngest(keyVals)\n\n\t\t\t// Assert we saw the errors we expect.\n\t\t\tswitch tc.wantErrType {\n\t\t\tcase errReportLocationNone:\n\t\t\t\trequire.Equal(t, errReportLocationNone, et.errLoc)\n\t\t\t\trequire.NoError(t, et.err)\n\t\t\tcase errReportLocationIngest:\n\t\t\t\trequire.Equal(t, errReportLocationIngest, et.errLoc)\n\t\t\t\trequire.Error(t, et.err)\n\t\t\t\trequire.True(t, errors.Is(et.err, tc.wantErr))\n\t\t\tcase errReportLocationFatal:\n\t\t\t\trequire.Equal(t, errReportLocationFatal, et.errLoc)\n\t\t\t\trequire.Error(t, et.err)\n\t\t\t\trequire.True(t, errors.Is(et.err, tc.wantErr))\n\t\t\tcase errReportLocationBackgroundError:\n\t\t\t\trequire.Equal(t, errReportLocationBackgroundError, et.errLoc)\n\t\t\t\trequire.Error(t, et.err)\n\t\t\t\trequire.True(t, errors.Is(et.err, tc.wantErr))\n\t\t\tdefault:\n\t\t\t\tt.Fatalf(\"unknown wantErrType %T\", tc.wantErrType)\n\t\t\t}\n\t\t})\n\t}\n}\n\n// BenchmarkManySSTables measures the cost of various operations with various\n// counts of SSTables within the database.\nfunc BenchmarkManySSTables(b *testing.B) {\n\tcounts := []int{10, 1_000, 10_000, 100_000, 1_000_000}\n\tops := []string{\"ingest\", \"calculateInuseKeyRanges\"}\n\tfor _, op := range ops {\n\t\tb.Run(op, func(b *testing.B) {\n\t\t\tfor _, count := range counts {\n\t\t\t\tb.Run(fmt.Sprintf(\"sstables=%d\", count), func(b *testing.B) {\n\t\t\t\t\tmem := vfs.NewMem()\n\t\t\t\t\td, err := Open(\"\", &Options{\n\t\t\t\t\t\tFS: mem,\n\t\t\t\t\t})\n\t\t\t\t\trequire.NoError(b, err)\n\n\t\t\t\t\tvar paths []string\n\t\t\t\t\tfor i := 0; i < count; i++ {\n\t\t\t\t\t\tn := fmt.Sprintf(\"%07d\", i)\n\t\t\t\t\t\tf, err := mem.Create(n, vfs.WriteCategoryUnspecified)\n\t\t\t\t\t\trequire.NoError(b, err)\n\t\t\t\t\t\tw := sstable.NewWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{})\n\t\t\t\t\t\trequire.NoError(b, w.Set([]byte(n), nil))\n\t\t\t\t\t\trequire.NoError(b, w.Close())\n\t\t\t\t\t\tpaths = append(paths, n)\n\t\t\t\t\t}\n\t\t\t\t\trequire.NoError(b, d.Ingest(context.Background(), paths))\n\n\t\t\t\t\t{\n\t\t\t\t\t\tconst broadIngest = \"broad.sst\"\n\t\t\t\t\t\tf, err := mem.Create(broadIngest, vfs.WriteCategoryUnspecified)\n\t\t\t\t\t\trequire.NoError(b, err)\n\t\t\t\t\t\tw := sstable.NewWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{})\n\t\t\t\t\t\trequire.NoError(b, w.Set([]byte(\"0\"), nil))\n\t\t\t\t\t\trequire.NoError(b, w.Set([]byte(\"Z\"), nil))\n\t\t\t\t\t\trequire.NoError(b, w.Close())\n\t\t\t\t\t\trequire.NoError(b, d.Ingest(context.Background(), []string{broadIngest}))\n\t\t\t\t\t}\n\n\t\t\t\t\tswitch op {\n\t\t\t\t\tcase \"ingest\":\n\t\t\t\t\t\trunBenchmarkManySSTablesIngest(b, d, mem, count)\n\t\t\t\t\tcase \"calculateInuseKeyRanges\":\n\t\t\t\t\t\trunBenchmarkManySSTablesInUseKeyRanges(b, d, count)\n\t\t\t\t\t}\n\t\t\t\t\trequire.NoError(b, d.Close())\n\t\t\t\t})\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc runBenchmarkManySSTablesIngest(b *testing.B, d *DB, fs vfs.FS, count int) {\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\tn := fmt.Sprintf(\"%07d\", count+i)\n\t\tf, err := fs.Create(n, vfs.WriteCategoryUnspecified)\n\t\trequire.NoError(b, err)\n\t\tw := sstable.NewWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{})\n\t\trequire.NoError(b, w.Set([]byte(n), nil))\n\t\trequire.NoError(b, w.Close())\n\t\trequire.NoError(b, d.Ingest(context.Background(), []string{n}))\n\t}\n}\n\nfunc runBenchmarkManySSTablesInUseKeyRanges(b *testing.B, d *DB, count int) {\n\t// This benchmark is pretty contrived, but it's not easy to write a\n\t// microbenchmark for this in a more natural way. L6 has many files, and\n\t// L5 has 1 file spanning the entire breadth of L5.\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\tv := d.mu.versions.currentVersion()\n\tb.ResetTimer()\n\n\tsmallest := []byte(\"0\")\n\tlargest := []byte(\"z\")\n\tfor i := 0; i < b.N; i++ {\n\t\t_ = v.CalculateInuseKeyRanges(0, numLevels-1, smallest, largest)\n\t}\n}\n"
        },
        {
          "name": "internal.go",
          "type": "blob",
          "size": 2.8876953125,
          "content": "// Copyright 2018 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport \"github.com/cockroachdb/pebble/internal/base\"\n\n// SeqNum exports the base.SeqNum type.\ntype SeqNum = base.SeqNum\n\n// InternalKeyKind exports the base.InternalKeyKind type.\ntype InternalKeyKind = base.InternalKeyKind\n\n// These constants are part of the file format, and should not be changed.\nconst (\n\tInternalKeyKindDelete         = base.InternalKeyKindDelete\n\tInternalKeyKindSet            = base.InternalKeyKindSet\n\tInternalKeyKindMerge          = base.InternalKeyKindMerge\n\tInternalKeyKindLogData        = base.InternalKeyKindLogData\n\tInternalKeyKindSingleDelete   = base.InternalKeyKindSingleDelete\n\tInternalKeyKindRangeDelete    = base.InternalKeyKindRangeDelete\n\tInternalKeyKindMax            = base.InternalKeyKindMax\n\tInternalKeyKindSetWithDelete  = base.InternalKeyKindSetWithDelete\n\tInternalKeyKindRangeKeySet    = base.InternalKeyKindRangeKeySet\n\tInternalKeyKindRangeKeyUnset  = base.InternalKeyKindRangeKeyUnset\n\tInternalKeyKindRangeKeyDelete = base.InternalKeyKindRangeKeyDelete\n\tInternalKeyKindRangeKeyMin    = base.InternalKeyKindRangeKeyMin\n\tInternalKeyKindRangeKeyMax    = base.InternalKeyKindRangeKeyMax\n\tInternalKeyKindIngestSST      = base.InternalKeyKindIngestSST\n\tInternalKeyKindDeleteSized    = base.InternalKeyKindDeleteSized\n\tInternalKeyKindExcise         = base.InternalKeyKindExcise\n\tInternalKeyKindInvalid        = base.InternalKeyKindInvalid\n)\n\n// InternalKeyTrailer exports the base.InternalKeyTrailer type.\ntype InternalKeyTrailer = base.InternalKeyTrailer\n\n// InternalKey exports the base.InternalKey type.\ntype InternalKey = base.InternalKey\n\n// MakeInternalKey constructs an internal key from a specified user key,\n// sequence number and kind.\nfunc MakeInternalKey(userKey []byte, seqNum SeqNum, kind InternalKeyKind) InternalKey {\n\treturn base.MakeInternalKey(userKey, seqNum, kind)\n}\n\n// MakeInternalKeyTrailer constructs a trailer from a specified sequence number\n// and kind.\nfunc MakeInternalKeyTrailer(seqNum SeqNum, kind InternalKeyKind) InternalKeyTrailer {\n\treturn base.MakeTrailer(seqNum, kind)\n}\n\ntype internalIterator = base.InternalIterator\n\ntype topLevelIterator = base.TopLevelIterator\n\n// ErrCorruption is a marker to indicate that data in a file (WAL, MANIFEST,\n// sstable) isn't in the expected format.\nvar ErrCorruption = base.ErrCorruption\n\n// AttributeAndLen exports the base.AttributeAndLen type.\ntype AttributeAndLen = base.AttributeAndLen\n\n// ShortAttribute exports the base.ShortAttribute type.\ntype ShortAttribute = base.ShortAttribute\n\n// LazyFetcher exports the base.LazyFetcher type. This export is needed since\n// LazyValue.Clone requires a pointer to a LazyFetcher struct to avoid\n// allocations. No code outside Pebble needs to peer into a LazyFetcher.\ntype LazyFetcher = base.LazyFetcher\n"
        },
        {
          "name": "internal",
          "type": "tree",
          "content": null
        },
        {
          "name": "iterator.go",
          "type": "blob",
          "size": 121.3017578125,
          "content": "// Copyright 2011 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"io\"\n\t\"math/rand/v2\"\n\t\"sync\"\n\t\"unsafe\"\n\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/bytealloc\"\n\t\"github.com/cockroachdb/pebble/internal/humanize\"\n\t\"github.com/cockroachdb/pebble/internal/invariants\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan/keyspanimpl\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n\t\"github.com/cockroachdb/pebble/internal/rangekeystack\"\n\t\"github.com/cockroachdb/pebble/internal/treeprinter\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n\t\"github.com/cockroachdb/redact\"\n)\n\n// iterPos describes the state of the internal iterator, in terms of whether it\n// is at the position returned to the user (cur), one ahead of the position\n// returned (next for forward iteration and prev for reverse iteration). The cur\n// position is split into two states, for forward and reverse iteration, since\n// we need to differentiate for switching directions.\n//\n// There is subtlety in what is considered the current position of the Iterator.\n// The internal iterator exposes a sequence of internal keys. There is not\n// always a single internalIterator position corresponding to the position\n// returned to the user. Consider the example:\n//\n//\ta.MERGE.9 a.MERGE.8 a.MERGE.7 a.SET.6 b.DELETE.9 b.DELETE.5 b.SET.4\n//\t\\                                   /\n//\t  \\       Iterator.Key() = 'a'    /\n//\n// The Iterator exposes one valid position at user key 'a' and the two exhausted\n// positions at the beginning and end of iteration. The underlying\n// internalIterator contains 7 valid positions and 2 exhausted positions.\n//\n// Iterator positioning methods must set iterPos to iterPosCur{Foward,Backward}\n// iff the user key at the current internalIterator position equals the\n// Iterator.Key returned to the user. This guarantees that a call to nextUserKey\n// or prevUserKey will advance to the next or previous iterator position.\n// iterPosCur{Forward,Backward} does not make any guarantee about the internal\n// iterator position among internal keys with matching user keys, and it will\n// vary subtly depending on the particular key kinds encountered. In the above\n// example, the iterator returning 'a' may set iterPosCurForward if the internal\n// iterator is positioned at any of a.MERGE.9, a.MERGE.8, a.MERGE.7 or a.SET.6.\n//\n// When setting iterPos to iterPosNext or iterPosPrev, the internal iterator\n// must be advanced to the first internalIterator position at a user key greater\n// (iterPosNext) or less (iterPosPrev) than the key returned to the user. An\n// internalIterator position that's !Valid() must also be considered greater or\n// less—depending on the direction of iteration—than the last valid Iterator\n// position.\ntype iterPos int8\n\nconst (\n\titerPosCurForward iterPos = 0\n\titerPosNext       iterPos = 1\n\titerPosPrev       iterPos = -1\n\titerPosCurReverse iterPos = -2\n\n\t// For limited iteration. When the iterator is at iterPosCurForwardPaused\n\t// - Next*() call should behave as if the internal iterator is already\n\t//   at next (akin to iterPosNext).\n\t// - Prev*() call should behave as if the internal iterator is at the\n\t//   current key (akin to iterPosCurForward).\n\t//\n\t// Similar semantics apply to CurReversePaused.\n\titerPosCurForwardPaused iterPos = 2\n\titerPosCurReversePaused iterPos = -3\n)\n\n// Approximate gap in bytes between samples of data read during iteration.\n// This is multiplied with a default ReadSamplingMultiplier of 1 << 4 to yield\n// 1 << 20 (1MB). The 1MB factor comes from:\n// https://github.com/cockroachdb/pebble/issues/29#issuecomment-494477985\nconst readBytesPeriod uint64 = 1 << 16\n\nvar errReversePrefixIteration = errors.New(\"pebble: unsupported reverse prefix iteration\")\n\n// IteratorMetrics holds per-iterator metrics. These do not change over the\n// lifetime of the iterator.\ntype IteratorMetrics struct {\n\t// The read amplification experienced by this iterator. This is the sum of\n\t// the memtables, the L0 sublevels and the non-empty Ln levels. Higher read\n\t// amplification generally results in slower reads, though allowing higher\n\t// read amplification can also result in faster writes.\n\tReadAmp int\n}\n\n// IteratorStatsKind describes the two kind of iterator stats.\ntype IteratorStatsKind int8\n\nconst (\n\t// InterfaceCall represents calls to Iterator.\n\tInterfaceCall IteratorStatsKind = iota\n\t// InternalIterCall represents calls by Iterator to its internalIterator.\n\tInternalIterCall\n\t// NumStatsKind is the number of kinds, and is used for array sizing.\n\tNumStatsKind\n)\n\n// IteratorStats contains iteration stats.\ntype IteratorStats struct {\n\t// ForwardSeekCount includes SeekGE, SeekPrefixGE, First.\n\tForwardSeekCount [NumStatsKind]int\n\t// ReverseSeek includes SeekLT, Last.\n\tReverseSeekCount [NumStatsKind]int\n\t// ForwardStepCount includes Next.\n\tForwardStepCount [NumStatsKind]int\n\t// ReverseStepCount includes Prev.\n\tReverseStepCount [NumStatsKind]int\n\tInternalStats    InternalIteratorStats\n\tRangeKeyStats    RangeKeyIteratorStats\n}\n\nvar _ redact.SafeFormatter = &IteratorStats{}\n\n// InternalIteratorStats contains miscellaneous stats produced by internal\n// iterators.\ntype InternalIteratorStats = base.InternalIteratorStats\n\n// RangeKeyIteratorStats contains miscellaneous stats about range keys\n// encountered by the iterator.\ntype RangeKeyIteratorStats struct {\n\t// Count records the number of range keys encountered during\n\t// iteration. Range keys may be counted multiple times if the iterator\n\t// leaves a range key's bounds and then returns.\n\tCount int\n\t// ContainedPoints records the number of point keys encountered within the\n\t// bounds of a range key. Note that this includes point keys with suffixes\n\t// that sort both above and below the covering range key's suffix.\n\tContainedPoints int\n\t// SkippedPoints records the count of the subset of ContainedPoints point\n\t// keys that were skipped during iteration due to range-key masking. It does\n\t// not include point keys that were never loaded because a\n\t// RangeKeyMasking.Filter excluded the entire containing block.\n\tSkippedPoints int\n}\n\n// Merge adds all of the argument's statistics to the receiver. It may be used\n// to accumulate stats across multiple iterators.\nfunc (s *RangeKeyIteratorStats) Merge(o RangeKeyIteratorStats) {\n\ts.Count += o.Count\n\ts.ContainedPoints += o.ContainedPoints\n\ts.SkippedPoints += o.SkippedPoints\n}\n\nfunc (s *RangeKeyIteratorStats) String() string {\n\treturn redact.StringWithoutMarkers(s)\n}\n\n// SafeFormat implements the redact.SafeFormatter interface.\nfunc (s *RangeKeyIteratorStats) SafeFormat(p redact.SafePrinter, verb rune) {\n\tp.Printf(\"range keys: %s, contained points: %s (%s skipped)\",\n\t\thumanize.Count.Uint64(uint64(s.Count)),\n\t\thumanize.Count.Uint64(uint64(s.ContainedPoints)),\n\t\thumanize.Count.Uint64(uint64(s.SkippedPoints)))\n}\n\n// LazyValue is a lazy value. See the long comment in base.LazyValue.\ntype LazyValue = base.LazyValue\n\n// Iterator iterates over a DB's key/value pairs in key order.\n//\n// An iterator must be closed after use, but it is not necessary to read an\n// iterator until exhaustion.\n//\n// An iterator is not goroutine-safe, but it is safe to use multiple iterators\n// concurrently, with each in a dedicated goroutine.\n//\n// It is also safe to use an iterator concurrently with modifying its\n// underlying DB, if that DB permits modification. However, the resultant\n// key/value pairs are not guaranteed to be a consistent snapshot of that DB\n// at a particular point in time.\n//\n// If an iterator encounters an error during any operation, it is stored by\n// the Iterator and surfaced through the Error method. All absolute\n// positioning methods (eg, SeekLT, SeekGT, First, Last, etc) reset any\n// accumulated error before positioning. All relative positioning methods (eg,\n// Next, Prev) return without advancing if the iterator has an accumulated\n// error.\ntype Iterator struct {\n\t// The context is stored here since (a) Iterators are expected to be\n\t// short-lived (since they pin memtables and sstables), (b) plumbing a\n\t// context into every method is very painful, (c) they do not (yet) respect\n\t// context cancellation and are only used for tracing.\n\tctx       context.Context\n\topts      IterOptions\n\tmerge     Merge\n\tcomparer  base.Comparer\n\titer      internalIterator\n\tpointIter topLevelIterator\n\t// Either readState or version is set, but not both.\n\treadState *readState\n\tversion   *version\n\t// rangeKey holds iteration state specific to iteration over range keys.\n\t// The range key field may be nil if the Iterator has never been configured\n\t// to iterate over range keys. Its non-nilness cannot be used to determine\n\t// if the Iterator is currently iterating over range keys: For that, consult\n\t// the IterOptions using opts.rangeKeys(). If non-nil, its rangeKeyIter\n\t// field is guaranteed to be non-nil too.\n\trangeKey *iteratorRangeKeyState\n\t// rangeKeyMasking holds state for range-key masking of point keys.\n\trangeKeyMasking rangeKeyMasking\n\terr             error\n\t// When iterValidityState=IterValid, key represents the current key, which\n\t// is backed by keyBuf.\n\tkey    []byte\n\tkeyBuf []byte\n\tvalue  LazyValue\n\t// For use in LazyValue.Clone.\n\tvalueBuf []byte\n\tfetcher  base.LazyFetcher\n\t// For use in LazyValue.Value.\n\tlazyValueBuf []byte\n\tvalueCloser  io.Closer\n\t// boundsBuf holds two buffers used to store the lower and upper bounds.\n\t// Whenever the Iterator's bounds change, the new bounds are copied into\n\t// boundsBuf[boundsBufIdx]. The two bounds share a slice to reduce\n\t// allocations. opts.LowerBound and opts.UpperBound point into this slice.\n\tboundsBuf    [2][]byte\n\tboundsBufIdx int\n\t// iterKV reflects the latest position of iter, except when SetBounds is\n\t// called. In that case, it is explicitly set to nil.\n\titerKV              *base.InternalKV\n\talloc               *iterAlloc\n\tgetIterAlloc        *getIterAlloc\n\tprefixOrFullSeekKey []byte\n\treadSampling        readSampling\n\tstats               IteratorStats\n\texternalReaders     [][]*sstable.Reader\n\n\t// Following fields used when constructing an iterator stack, eg, in Clone\n\t// and SetOptions or when re-fragmenting a batch's range keys/range dels.\n\t// Non-nil if this Iterator includes a Batch.\n\tbatch            *Batch\n\tfc               *fileCacheContainer\n\tnewIters         tableNewIters\n\tnewIterRangeKey  keyspanimpl.TableNewSpanIter\n\tlazyCombinedIter lazyCombinedIter\n\tseqNum           base.SeqNum\n\t// batchSeqNum is used by Iterators over indexed batches to detect when the\n\t// underlying batch has been mutated. The batch beneath an indexed batch may\n\t// be mutated while the Iterator is open, but new keys are not surfaced\n\t// until the next call to SetOptions.\n\tbatchSeqNum base.SeqNum\n\t// batch{PointIter,RangeDelIter,RangeKeyIter} are used when the Iterator is\n\t// configured to read through an indexed batch. If a batch is set, these\n\t// iterators will be included within the iterator stack regardless of\n\t// whether the batch currently contains any keys of their kind. These\n\t// pointers are used during a call to SetOptions to refresh the Iterator's\n\t// view of its indexed batch.\n\tbatchPointIter    batchIter\n\tbatchRangeDelIter keyspan.Iter\n\tbatchRangeKeyIter keyspan.Iter\n\t// merging is a pointer to this iterator's point merging iterator. It\n\t// appears here because key visibility is handled by the merging iterator.\n\t// During SetOptions on an iterator over an indexed batch, this field is\n\t// used to update the merging iterator's batch snapshot.\n\tmerging *mergingIter\n\n\t// Keeping the bools here after all the 8 byte aligned fields shrinks the\n\t// sizeof this struct by 24 bytes.\n\n\t// INVARIANT:\n\t// iterValidityState==IterAtLimit <=>\n\t//  pos==iterPosCurForwardPaused || pos==iterPosCurReversePaused\n\titerValidityState IterValidityState\n\t// Set to true by SetBounds, SetOptions. Causes the Iterator to appear\n\t// exhausted externally, while preserving the correct iterValidityState for\n\t// the iterator's internal state. Preserving the correct internal validity\n\t// is used for SeekPrefixGE(..., trySeekUsingNext), and SeekGE/SeekLT\n\t// optimizations after \"no-op\" calls to SetBounds and SetOptions.\n\trequiresReposition bool\n\t// The position of iter. When this is iterPos{Prev,Next} the iter has been\n\t// moved past the current key-value, which can only happen if\n\t// iterValidityState=IterValid, i.e., there is something to return to the\n\t// client for the current position.\n\tpos iterPos\n\t// Relates to the prefixOrFullSeekKey field above.\n\thasPrefix bool\n\t// Used for deriving the value of SeekPrefixGE(..., trySeekUsingNext),\n\t// and SeekGE/SeekLT optimizations\n\tlastPositioningOp lastPositioningOpKind\n\t// Used for determining when it's safe to perform SeekGE optimizations that\n\t// reuse the iterator state to avoid the cost of a full seek if the iterator\n\t// is already positioned in the correct place. If the iterator's view of its\n\t// indexed batch was just refreshed, some optimizations cannot be applied on\n\t// the first seek after the refresh:\n\t// - SeekGE has a no-op optimization that does not seek on the internal\n\t//   iterator at all if the iterator is already in the correct place.\n\t//   This optimization cannot be performed if the internal iterator was\n\t//   last positioned when the iterator had a different view of an\n\t//   underlying batch.\n\t// - Seek[Prefix]GE set flags.TrySeekUsingNext()=true when the seek key is\n\t//   greater than the previous operation's seek key, under the expectation\n\t//   that the various internal iterators can use their current position to\n\t//   avoid a full expensive re-seek. This applies to the batchIter as well.\n\t//   However, if the view of the batch was just refreshed, the batchIter's\n\t//   position is not useful because it may already be beyond new keys less\n\t//   than the seek key. To prevent the use of this optimization in\n\t//   batchIter, Seek[Prefix]GE set flags.BatchJustRefreshed()=true if this\n\t//   bit is enabled.\n\tbatchJustRefreshed bool\n\t// batchOnlyIter is set to true for Batch.NewBatchOnlyIter.\n\tbatchOnlyIter bool\n\t// Used in some tests to disable the random disabling of seek optimizations.\n\tforceEnableSeekOpt bool\n\t// Set to true if NextPrefix is not currently permitted. Defaults to false\n\t// in case an iterator never had any bounds.\n\tnextPrefixNotPermittedByUpperBound bool\n}\n\n// cmp is a convenience shorthand for the i.comparer.Compare function.\nfunc (i *Iterator) cmp(a, b []byte) int {\n\treturn i.comparer.Compare(a, b)\n}\n\n// equal is a convenience shorthand for the i.comparer.Equal function.\nfunc (i *Iterator) equal(a, b []byte) bool {\n\treturn i.comparer.Equal(a, b)\n}\n\n// iteratorRangeKeyState holds an iterator's range key iteration state.\ntype iteratorRangeKeyState struct {\n\topts  *IterOptions\n\tcmp   base.Compare\n\tsplit base.Split\n\t// rangeKeyIter holds the range key iterator stack that iterates over the\n\t// merged spans across the entirety of the LSM.\n\trangeKeyIter keyspan.FragmentIterator\n\tiiter        keyspan.InterleavingIter\n\t// stale is set to true when the range key state recorded here (in start,\n\t// end and keys) may not be in sync with the current range key at the\n\t// interleaving iterator's current position.\n\t//\n\t// When the interelaving iterator passes over a new span, it invokes the\n\t// SpanChanged hook defined on the `rangeKeyMasking` type,  which sets stale\n\t// to true if the span is non-nil.\n\t//\n\t// The parent iterator may not be positioned over the interleaving\n\t// iterator's current position (eg, i.iterPos = iterPos{Next,Prev}), so\n\t// {keys,start,end} are only updated to the new range key during a call to\n\t// Iterator.saveRangeKey.\n\tstale bool\n\t// updated is used to signal to the Iterator client whether the state of\n\t// range keys has changed since the previous iterator position through the\n\t// `RangeKeyChanged` method. It's set to true during an Iterator positioning\n\t// operation that changes the state of the current range key. Each Iterator\n\t// positioning operation sets it back to false before executing.\n\t//\n\t// TODO(jackson): The lifecycle of {stale,updated,prevPosHadRangeKey} is\n\t// intricate and confusing. Try to refactor to reduce complexity.\n\tupdated bool\n\t// prevPosHadRangeKey records whether the previous Iterator position had a\n\t// range key (HasPointAndRage() = (_, true)). It's updated at the beginning\n\t// of each new Iterator positioning operation. It's required by saveRangeKey to\n\t// to set `updated` appropriately: Without this record of the previous iterator\n\t// state, it's ambiguous whether an iterator only temporarily stepped onto a\n\t// position without a range key.\n\tprevPosHadRangeKey bool\n\t// rangeKeyOnly is set to true if at the current iterator position there is\n\t// no point key, only a range key start boundary.\n\trangeKeyOnly bool\n\t// hasRangeKey is true when the current iterator position has a covering\n\t// range key (eg, a range key with bounds [<lower>,<upper>) such that\n\t// <lower> ≤ Key() < <upper>).\n\thasRangeKey bool\n\t// start and end are the [start, end) boundaries of the current range keys.\n\tstart []byte\n\tend   []byte\n\n\trangeKeyBuffers\n\n\t// iterConfig holds fields that are used for the construction of the\n\t// iterator stack, but do not need to be directly accessed during iteration.\n\t// This struct is bundled within the iteratorRangeKeyState struct to reduce\n\t// allocations.\n\titerConfig rangekeystack.UserIteratorConfig\n}\n\ntype rangeKeyBuffers struct {\n\t// keys is sorted by Suffix ascending.\n\tkeys []RangeKeyData\n\t// buf is used to save range-key data before moving the range-key iterator.\n\t// Start and end boundaries, suffixes and values are all copied into buf.\n\tbuf bytealloc.A\n\t// internal holds buffers used by the range key internal iterators.\n\tinternal rangekeystack.Buffers\n}\n\nfunc (b *rangeKeyBuffers) PrepareForReuse() {\n\tconst maxKeysReuse = 100\n\tif len(b.keys) > maxKeysReuse {\n\t\tb.keys = nil\n\t}\n\t// Avoid caching the key buf if it is overly large. The constant is\n\t// fairly arbitrary.\n\tif cap(b.buf) >= maxKeyBufCacheSize {\n\t\tb.buf = nil\n\t} else {\n\t\tb.buf = b.buf[:0]\n\t}\n\tb.internal.PrepareForReuse()\n}\n\nfunc (i *iteratorRangeKeyState) init(cmp base.Compare, split base.Split, opts *IterOptions) {\n\ti.cmp = cmp\n\ti.split = split\n\ti.opts = opts\n}\n\nvar iterRangeKeyStateAllocPool = sync.Pool{\n\tNew: func() interface{} {\n\t\treturn &iteratorRangeKeyState{}\n\t},\n}\n\n// isEphemeralPosition returns true iff the current iterator position is\n// ephemeral, and won't be visited during subsequent relative positioning\n// operations.\n//\n// The iterator position resulting from a SeekGE or SeekPrefixGE that lands on a\n// straddling range key without a coincident point key is such a position.\nfunc (i *Iterator) isEphemeralPosition() bool {\n\treturn i.opts.rangeKeys() && i.rangeKey != nil && i.rangeKey.rangeKeyOnly &&\n\t\t!i.equal(i.rangeKey.start, i.key)\n}\n\ntype lastPositioningOpKind int8\n\nconst (\n\tunknownLastPositionOp lastPositioningOpKind = iota\n\tseekPrefixGELastPositioningOp\n\tseekGELastPositioningOp\n\tseekLTLastPositioningOp\n\t// internalNextOp is a special internal iterator positioning operation used\n\t// by CanDeterministicallySingleDelete. It exists for enforcing requirements\n\t// around calling CanDeterministicallySingleDelete at most once per external\n\t// iterator position.\n\tinternalNextOp\n)\n\n// Limited iteration mode. Not for use with prefix iteration.\n//\n// SeekGE, SeekLT, Prev, Next have WithLimit variants, that pause the iterator\n// at the limit in a best-effort manner. The client should behave correctly\n// even if the limits are ignored. These limits are not \"deep\", in that they\n// are not passed down to the underlying collection of internalIterators. This\n// is because the limits are transient, and apply only until the next\n// iteration call. They serve mainly as a way to bound the amount of work when\n// two (or more) Iterators are being coordinated at a higher level.\n//\n// In limited iteration mode:\n// - Avoid using Iterator.Valid if the last call was to a *WithLimit() method.\n//   The return value from the *WithLimit() method provides a more precise\n//   disposition.\n// - The limit is exclusive for forward and inclusive for reverse.\n//\n//\n// Limited iteration mode & range keys\n//\n// Limited iteration interacts with range-key iteration. When range key\n// iteration is enabled, range keys are interleaved at their start boundaries.\n// Limited iteration must ensure that if a range key exists within the limit,\n// the iterator visits the range key.\n//\n// During forward limited iteration, this is trivial: An overlapping range key\n// must have a start boundary less than the limit, and the range key's start\n// boundary will be interleaved and found to be within the limit.\n//\n// During reverse limited iteration, the tail of the range key may fall within\n// the limit. The range key must be surfaced even if the range key's start\n// boundary is less than the limit, and if there are no point keys between the\n// current iterator position and the limit. To provide this guarantee, reverse\n// limited iteration ignores the limit as long as there is a range key\n// overlapping the iteration position.\n\n// IterValidityState captures the state of the Iterator.\ntype IterValidityState int8\n\nconst (\n\t// IterExhausted represents an Iterator that is exhausted.\n\tIterExhausted IterValidityState = iota\n\t// IterValid represents an Iterator that is valid.\n\tIterValid\n\t// IterAtLimit represents an Iterator that has a non-exhausted\n\t// internalIterator, but has reached a limit without any key for the\n\t// caller.\n\tIterAtLimit\n)\n\n// readSampling stores variables used to sample a read to trigger a read\n// compaction\ntype readSampling struct {\n\tbytesUntilReadSampling uint64\n\tinitialSamplePassed    bool\n\tpendingCompactions     readCompactionQueue\n\t// forceReadSampling is used for testing purposes to force a read sample on every\n\t// call to Iterator.maybeSampleRead()\n\tforceReadSampling bool\n}\n\nfunc (i *Iterator) findNextEntry(limit []byte) {\n\ti.iterValidityState = IterExhausted\n\ti.pos = iterPosCurForward\n\tif i.opts.rangeKeys() && i.rangeKey != nil {\n\t\ti.rangeKey.rangeKeyOnly = false\n\t}\n\n\t// Close the closer for the current value if one was open.\n\tif i.closeValueCloser() != nil {\n\t\treturn\n\t}\n\n\tfor i.iterKV != nil {\n\t\tkey := i.iterKV.K\n\n\t\t// The topLevelIterator.StrictSeekPrefixGE contract requires that in\n\t\t// prefix mode [i.hasPrefix=t], every point key returned by the internal\n\t\t// iterator must have the current iteration prefix.\n\t\tif invariants.Enabled && i.hasPrefix {\n\t\t\t// Range keys are an exception to the contract and may return a different\n\t\t\t// prefix. This case is explicitly handled in the switch statement below.\n\t\t\tif key.Kind() != base.InternalKeyKindRangeKeySet {\n\t\t\t\tif p := i.comparer.Split.Prefix(key.UserKey); !i.equal(i.prefixOrFullSeekKey, p) {\n\t\t\t\t\ti.opts.logger.Fatalf(\"pebble: prefix violation: key %q does not have prefix %q\\n\", key.UserKey, i.prefixOrFullSeekKey)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// Compare with limit every time we start at a different user key.\n\t\t// Note that given the best-effort contract of limit, we could avoid a\n\t\t// comparison in the common case by doing this only after\n\t\t// i.nextUserKey is called for the deletes below. However that makes\n\t\t// the behavior non-deterministic (since the behavior will vary based\n\t\t// on what has been compacted), which makes it hard to test with the\n\t\t// metamorphic test. So we forego that performance optimization.\n\t\tif limit != nil && i.cmp(limit, i.iterKV.K.UserKey) <= 0 {\n\t\t\ti.iterValidityState = IterAtLimit\n\t\t\ti.pos = iterPosCurForwardPaused\n\t\t\treturn\n\t\t}\n\n\t\t// If the user has configured a SkipPoint function, invoke it to see\n\t\t// whether we should skip over the current user key.\n\t\tif i.opts.SkipPoint != nil && key.Kind() != InternalKeyKindRangeKeySet && i.opts.SkipPoint(i.iterKV.K.UserKey) {\n\t\t\t// NB: We could call nextUserKey, but in some cases the SkipPoint\n\t\t\t// predicate function might be cheaper than nextUserKey's key copy\n\t\t\t// and key comparison. This should be the case for MVCC suffix\n\t\t\t// comparisons, for example. In the future, we could expand the\n\t\t\t// SkipPoint interface to give the implementor more control over\n\t\t\t// whether we skip over just the internal key, the user key, or even\n\t\t\t// the key prefix.\n\t\t\ti.stats.ForwardStepCount[InternalIterCall]++\n\t\t\ti.iterKV = i.iter.Next()\n\t\t\tcontinue\n\t\t}\n\n\t\tswitch key.Kind() {\n\t\tcase InternalKeyKindRangeKeySet:\n\t\t\tif i.hasPrefix {\n\t\t\t\tif p := i.comparer.Split.Prefix(key.UserKey); !i.equal(i.prefixOrFullSeekKey, p) {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t\t// Save the current key.\n\t\t\ti.keyBuf = append(i.keyBuf[:0], key.UserKey...)\n\t\t\ti.key = i.keyBuf\n\t\t\ti.value = LazyValue{}\n\t\t\t// There may also be a live point key at this userkey that we have\n\t\t\t// not yet read. We need to find the next entry with this user key\n\t\t\t// to find it. Save the range key so we don't lose it when we Next\n\t\t\t// the underlying iterator.\n\t\t\ti.saveRangeKey()\n\t\t\tpointKeyExists := i.nextPointCurrentUserKey()\n\t\t\tif i.err != nil {\n\t\t\t\ti.iterValidityState = IterExhausted\n\t\t\t\treturn\n\t\t\t}\n\t\t\ti.rangeKey.rangeKeyOnly = !pointKeyExists\n\t\t\ti.iterValidityState = IterValid\n\t\t\treturn\n\n\t\tcase InternalKeyKindDelete, InternalKeyKindSingleDelete, InternalKeyKindDeleteSized:\n\t\t\t// NB: treating InternalKeyKindSingleDelete as equivalent to DEL is not\n\t\t\t// only simpler, but is also necessary for correctness due to\n\t\t\t// InternalKeyKindSSTableInternalObsoleteBit.\n\t\t\ti.nextUserKey()\n\t\t\tcontinue\n\n\t\tcase InternalKeyKindSet, InternalKeyKindSetWithDelete:\n\t\t\ti.keyBuf = append(i.keyBuf[:0], key.UserKey...)\n\t\t\ti.key = i.keyBuf\n\t\t\ti.value = i.iterKV.V\n\t\t\ti.iterValidityState = IterValid\n\t\t\ti.saveRangeKey()\n\t\t\treturn\n\n\t\tcase InternalKeyKindMerge:\n\t\t\t// Resolving the merge may advance us to the next point key, which\n\t\t\t// may be covered by a different set of range keys. Save the range\n\t\t\t// key state so we don't lose it.\n\t\t\ti.saveRangeKey()\n\t\t\tif i.mergeForward(key) {\n\t\t\t\ti.iterValidityState = IterValid\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// The merge didn't yield a valid key, either because the value\n\t\t\t// merger indicated it should be deleted, or because an error was\n\t\t\t// encountered.\n\t\t\ti.iterValidityState = IterExhausted\n\t\t\tif i.err != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif i.pos != iterPosNext {\n\t\t\t\ti.nextUserKey()\n\t\t\t}\n\t\t\tif i.closeValueCloser() != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\ti.pos = iterPosCurForward\n\n\t\tdefault:\n\t\t\ti.err = base.CorruptionErrorf(\"pebble: invalid internal key kind: %d\", errors.Safe(key.Kind()))\n\t\t\ti.iterValidityState = IterExhausted\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Is iterKey nil due to an error?\n\tif err := i.iter.Error(); err != nil {\n\t\ti.err = err\n\t\ti.iterValidityState = IterExhausted\n\t}\n}\n\nfunc (i *Iterator) nextPointCurrentUserKey() bool {\n\t// If the user has configured a SkipPoint function and the current user key\n\t// would be skipped by it, there's no need to step forward looking for a\n\t// point key. If we were to find one, it should be skipped anyways.\n\tif i.opts.SkipPoint != nil && i.opts.SkipPoint(i.key) {\n\t\treturn false\n\t}\n\n\ti.pos = iterPosCurForward\n\n\ti.iterKV = i.iter.Next()\n\ti.stats.ForwardStepCount[InternalIterCall]++\n\tif i.iterKV == nil {\n\t\tif err := i.iter.Error(); err != nil {\n\t\t\ti.err = err\n\t\t} else {\n\t\t\ti.pos = iterPosNext\n\t\t}\n\t\treturn false\n\t}\n\tif !i.equal(i.key, i.iterKV.K.UserKey) {\n\t\ti.pos = iterPosNext\n\t\treturn false\n\t}\n\n\tkey := i.iterKV.K\n\tswitch key.Kind() {\n\tcase InternalKeyKindRangeKeySet:\n\t\t// RangeKeySets must always be interleaved as the first internal key\n\t\t// for a user key.\n\t\ti.err = base.CorruptionErrorf(\"pebble: unexpected range key set mid-user key\")\n\t\treturn false\n\n\tcase InternalKeyKindDelete, InternalKeyKindSingleDelete, InternalKeyKindDeleteSized:\n\t\t// NB: treating InternalKeyKindSingleDelete as equivalent to DEL is not\n\t\t// only simpler, but is also necessary for correctness due to\n\t\t// InternalKeyKindSSTableInternalObsoleteBit.\n\t\treturn false\n\n\tcase InternalKeyKindSet, InternalKeyKindSetWithDelete:\n\t\ti.value = i.iterKV.V\n\t\treturn true\n\n\tcase InternalKeyKindMerge:\n\t\treturn i.mergeForward(key)\n\n\tdefault:\n\t\ti.err = base.CorruptionErrorf(\"pebble: invalid internal key kind: %d\", errors.Safe(key.Kind()))\n\t\treturn false\n\t}\n}\n\n// mergeForward resolves a MERGE key, advancing the underlying iterator forward\n// to merge with subsequent keys with the same userkey. mergeForward returns a\n// boolean indicating whether or not the merge yielded a valid key. A merge may\n// not yield a valid key if an error occurred, in which case i.err is non-nil,\n// or the user's value merger specified the key to be deleted.\n//\n// mergeForward does not update iterValidityState.\nfunc (i *Iterator) mergeForward(key base.InternalKey) (valid bool) {\n\tvar iterValue []byte\n\titerValue, _, i.err = i.iterKV.Value(nil)\n\tif i.err != nil {\n\t\treturn false\n\t}\n\tvar valueMerger ValueMerger\n\tvalueMerger, i.err = i.merge(key.UserKey, iterValue)\n\tif i.err != nil {\n\t\treturn false\n\t}\n\n\ti.mergeNext(key, valueMerger)\n\tif i.err != nil {\n\t\treturn false\n\t}\n\n\tvar needDelete bool\n\tvar value []byte\n\tvalue, needDelete, i.valueCloser, i.err = finishValueMerger(\n\t\tvalueMerger, true /* includesBase */)\n\ti.value = base.MakeInPlaceValue(value)\n\tif i.err != nil {\n\t\treturn false\n\t}\n\tif needDelete {\n\t\t_ = i.closeValueCloser()\n\t\treturn false\n\t}\n\treturn true\n}\n\nfunc (i *Iterator) closeValueCloser() error {\n\tif i.valueCloser != nil {\n\t\ti.err = i.valueCloser.Close()\n\t\ti.valueCloser = nil\n\t}\n\treturn i.err\n}\n\nfunc (i *Iterator) nextUserKey() {\n\tif i.iterKV == nil {\n\t\treturn\n\t}\n\ttrailer := i.iterKV.K.Trailer\n\tdone := i.iterKV.K.Trailer <= base.InternalKeyZeroSeqnumMaxTrailer\n\tif i.iterValidityState != IterValid {\n\t\ti.keyBuf = append(i.keyBuf[:0], i.iterKV.K.UserKey...)\n\t\ti.key = i.keyBuf\n\t}\n\tfor {\n\t\ti.stats.ForwardStepCount[InternalIterCall]++\n\t\ti.iterKV = i.iter.Next()\n\t\tif i.iterKV == nil {\n\t\t\tif err := i.iter.Error(); err != nil {\n\t\t\t\ti.err = err\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\t// NB: We're guaranteed to be on the next user key if the previous key\n\t\t// had a zero sequence number (`done`), or the new key has a trailer\n\t\t// greater or equal to the previous key's trailer. This is true because\n\t\t// internal keys with the same user key are sorted by InternalKeyTrailer in\n\t\t// strictly monotonically descending order. We expect the trailer\n\t\t// optimization to trigger around 50% of the time with randomly\n\t\t// distributed writes. We expect it to trigger very frequently when\n\t\t// iterating through ingested sstables, which contain keys that all have\n\t\t// the same sequence number.\n\t\tif done || i.iterKV == nil || i.iterKV.K.Trailer >= trailer {\n\t\t\tbreak\n\t\t}\n\t\tif !i.equal(i.key, i.iterKV.K.UserKey) {\n\t\t\tbreak\n\t\t}\n\t\tdone = i.iterKV.K.Trailer <= base.InternalKeyZeroSeqnumMaxTrailer\n\t\ttrailer = i.iterKV.K.Trailer\n\t}\n}\n\nfunc (i *Iterator) maybeSampleRead() {\n\t// This method is only called when a public method of Iterator is\n\t// returning, and below we exclude the case were the iterator is paused at\n\t// a limit. The effect of these choices is that keys that are deleted, but\n\t// are encountered during iteration, are not accounted for in the read\n\t// sampling and will not cause read driven compactions, even though we are\n\t// incurring cost in iterating over them. And this issue is not limited to\n\t// Iterator, which does not see the effect of range deletes, which may be\n\t// causing iteration work in mergingIter. It is not clear at this time\n\t// whether this is a deficiency worth addressing.\n\tif i.iterValidityState != IterValid {\n\t\treturn\n\t}\n\tif i.readState == nil {\n\t\treturn\n\t}\n\tif i.readSampling.forceReadSampling {\n\t\ti.sampleRead()\n\t\treturn\n\t}\n\tsamplingPeriod := int32(int64(readBytesPeriod) * i.readState.db.opts.Experimental.ReadSamplingMultiplier)\n\tif samplingPeriod <= 0 {\n\t\treturn\n\t}\n\tbytesRead := uint64(len(i.key) + i.value.Len())\n\tfor i.readSampling.bytesUntilReadSampling < bytesRead {\n\t\ti.readSampling.bytesUntilReadSampling += uint64(rand.Uint32N(2 * uint32(samplingPeriod)))\n\t\t// The block below tries to adjust for the case where this is the\n\t\t// first read in a newly-opened iterator. As bytesUntilReadSampling\n\t\t// starts off at zero, we don't want to sample the first read of\n\t\t// every newly-opened iterator, but we do want to sample some of them.\n\t\tif !i.readSampling.initialSamplePassed {\n\t\t\ti.readSampling.initialSamplePassed = true\n\t\t\tif i.readSampling.bytesUntilReadSampling > bytesRead {\n\t\t\t\tif rand.Uint64N(i.readSampling.bytesUntilReadSampling) > bytesRead {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\ti.sampleRead()\n\t}\n\ti.readSampling.bytesUntilReadSampling -= bytesRead\n}\n\nfunc (i *Iterator) sampleRead() {\n\tvar topFile *manifest.FileMetadata\n\ttopLevel, numOverlappingLevels := numLevels, 0\n\tmi := i.merging\n\tif mi == nil {\n\t\treturn\n\t}\n\tif len(mi.levels) > 1 {\n\t\tmi.ForEachLevelIter(func(li *levelIter) (done bool) {\n\t\t\tif li.layer.IsFlushableIngests() {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tl := li.layer.Level()\n\t\t\tif f := li.iterFile; f != nil {\n\t\t\t\tvar containsKey bool\n\t\t\t\tif i.pos == iterPosNext || i.pos == iterPosCurForward ||\n\t\t\t\t\ti.pos == iterPosCurForwardPaused {\n\t\t\t\t\tcontainsKey = i.cmp(f.SmallestPointKey.UserKey, i.key) <= 0\n\t\t\t\t} else if i.pos == iterPosPrev || i.pos == iterPosCurReverse ||\n\t\t\t\t\ti.pos == iterPosCurReversePaused {\n\t\t\t\t\tcontainsKey = i.cmp(f.LargestPointKey.UserKey, i.key) >= 0\n\t\t\t\t}\n\t\t\t\t// Do nothing if the current key is not contained in f's\n\t\t\t\t// bounds. We could seek the LevelIterator at this level\n\t\t\t\t// to find the right file, but the performance impacts of\n\t\t\t\t// doing that are significant enough to negate the benefits\n\t\t\t\t// of read sampling in the first place. See the discussion\n\t\t\t\t// at:\n\t\t\t\t// https://github.com/cockroachdb/pebble/pull/1041#issuecomment-763226492\n\t\t\t\tif containsKey {\n\t\t\t\t\tnumOverlappingLevels++\n\t\t\t\t\tif numOverlappingLevels >= 2 {\n\t\t\t\t\t\t// Terminate the loop early if at least 2 overlapping levels are found.\n\t\t\t\t\t\treturn true\n\t\t\t\t\t}\n\t\t\t\t\ttopLevel = l\n\t\t\t\t\ttopFile = f\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn false\n\t\t})\n\t}\n\tif topFile == nil || topLevel >= numLevels {\n\t\treturn\n\t}\n\tif numOverlappingLevels >= 2 {\n\t\tallowedSeeks := topFile.AllowedSeeks.Add(-1)\n\t\tif allowedSeeks == 0 {\n\n\t\t\t// Since the compaction queue can handle duplicates, we can keep\n\t\t\t// adding to the queue even once allowedSeeks hits 0.\n\t\t\t// In fact, we NEED to keep adding to the queue, because the queue\n\t\t\t// is small and evicts older and possibly useful compactions.\n\t\t\ttopFile.AllowedSeeks.Add(topFile.InitAllowedSeeks)\n\n\t\t\tread := readCompaction{\n\t\t\t\tstart:   topFile.SmallestPointKey.UserKey,\n\t\t\t\tend:     topFile.LargestPointKey.UserKey,\n\t\t\t\tlevel:   topLevel,\n\t\t\t\tfileNum: topFile.FileNum,\n\t\t\t}\n\t\t\ti.readSampling.pendingCompactions.add(&read, i.cmp)\n\t\t}\n\t}\n}\n\nfunc (i *Iterator) findPrevEntry(limit []byte) {\n\ti.iterValidityState = IterExhausted\n\ti.pos = iterPosCurReverse\n\tif i.opts.rangeKeys() && i.rangeKey != nil {\n\t\ti.rangeKey.rangeKeyOnly = false\n\t}\n\n\t// Close the closer for the current value if one was open.\n\tif i.valueCloser != nil {\n\t\ti.err = i.valueCloser.Close()\n\t\ti.valueCloser = nil\n\t\tif i.err != nil {\n\t\t\ti.iterValidityState = IterExhausted\n\t\t\treturn\n\t\t}\n\t}\n\n\tvar valueMerger ValueMerger\n\tfirstLoopIter := true\n\trangeKeyBoundary := false\n\t// The code below compares with limit in multiple places. As documented in\n\t// findNextEntry, this is being done to make the behavior of limit\n\t// deterministic to allow for metamorphic testing. It is not required by\n\t// the best-effort contract of limit.\n\tfor i.iterKV != nil {\n\t\tkey := i.iterKV.K\n\n\t\t// NB: We cannot pause if the current key is covered by a range key.\n\t\t// Otherwise, the user might not ever learn of a range key that covers\n\t\t// the key space being iterated over in which there are no point keys.\n\t\t// Since limits are best effort, ignoring the limit in this case is\n\t\t// allowed by the contract of limit.\n\t\tif firstLoopIter && limit != nil && i.cmp(limit, i.iterKV.K.UserKey) > 0 && !i.rangeKeyWithinLimit(limit) {\n\t\t\ti.iterValidityState = IterAtLimit\n\t\t\ti.pos = iterPosCurReversePaused\n\t\t\treturn\n\t\t}\n\t\tfirstLoopIter = false\n\n\t\tif i.iterValidityState == IterValid {\n\t\t\tif !i.equal(key.UserKey, i.key) {\n\t\t\t\t// We've iterated to the previous user key.\n\t\t\t\ti.pos = iterPosPrev\n\t\t\t\tif valueMerger != nil {\n\t\t\t\t\tvar needDelete bool\n\t\t\t\t\tvar value []byte\n\t\t\t\t\tvalue, needDelete, i.valueCloser, i.err = finishValueMerger(valueMerger, true /* includesBase */)\n\t\t\t\t\ti.value = base.MakeInPlaceValue(value)\n\t\t\t\t\tif i.err == nil && needDelete {\n\t\t\t\t\t\t// The point key at this key is deleted. If we also have\n\t\t\t\t\t\t// a range key boundary at this key, we still want to\n\t\t\t\t\t\t// return. Otherwise, we need to continue looking for\n\t\t\t\t\t\t// a live key.\n\t\t\t\t\t\ti.value = LazyValue{}\n\t\t\t\t\t\tif rangeKeyBoundary {\n\t\t\t\t\t\t\ti.rangeKey.rangeKeyOnly = true\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\ti.iterValidityState = IterExhausted\n\t\t\t\t\t\t\tif i.closeValueCloser() == nil {\n\t\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif i.err != nil {\n\t\t\t\t\ti.iterValidityState = IterExhausted\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\t// If the user has configured a SkipPoint function, invoke it to see\n\t\t// whether we should skip over the current user key.\n\t\tif i.opts.SkipPoint != nil && key.Kind() != InternalKeyKindRangeKeySet && i.opts.SkipPoint(key.UserKey) {\n\t\t\t// NB: We could call prevUserKey, but in some cases the SkipPoint\n\t\t\t// predicate function might be cheaper than prevUserKey's key copy\n\t\t\t// and key comparison. This should be the case for MVCC suffix\n\t\t\t// comparisons, for example. In the future, we could expand the\n\t\t\t// SkipPoint interface to give the implementor more control over\n\t\t\t// whether we skip over just the internal key, the user key, or even\n\t\t\t// the key prefix.\n\t\t\ti.stats.ReverseStepCount[InternalIterCall]++\n\t\t\ti.iterKV = i.iter.Prev()\n\t\t\tif i.iterKV == nil {\n\t\t\t\tif err := i.iter.Error(); err != nil {\n\t\t\t\t\ti.err = err\n\t\t\t\t\ti.iterValidityState = IterExhausted\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t\tif limit != nil && i.iterKV != nil && i.cmp(limit, i.iterKV.K.UserKey) > 0 && !i.rangeKeyWithinLimit(limit) {\n\t\t\t\ti.iterValidityState = IterAtLimit\n\t\t\t\ti.pos = iterPosCurReversePaused\n\t\t\t\treturn\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\tswitch key.Kind() {\n\t\tcase InternalKeyKindRangeKeySet:\n\t\t\t// Range key start boundary markers are interleaved with the maximum\n\t\t\t// sequence number, so if there's a point key also at this key, we\n\t\t\t// must've already iterated over it.\n\t\t\t// This is the final entry at this user key, so we may return\n\t\t\ti.rangeKey.rangeKeyOnly = i.iterValidityState != IterValid\n\t\t\ti.keyBuf = append(i.keyBuf[:0], key.UserKey...)\n\t\t\ti.key = i.keyBuf\n\t\t\ti.iterValidityState = IterValid\n\t\t\ti.saveRangeKey()\n\t\t\t// In all other cases, previous iteration requires advancing to\n\t\t\t// iterPosPrev in order to determine if the key is live and\n\t\t\t// unshadowed by another key at the same user key. In this case,\n\t\t\t// because range key start boundary markers are always interleaved\n\t\t\t// at the maximum sequence number, we know that there aren't any\n\t\t\t// additional keys with the same user key in the backward direction.\n\t\t\t//\n\t\t\t// We Prev the underlying iterator once anyways for consistency, so\n\t\t\t// that we can maintain the invariant during backward iteration that\n\t\t\t// i.iterPos = iterPosPrev.\n\t\t\ti.stats.ReverseStepCount[InternalIterCall]++\n\t\t\ti.iterKV = i.iter.Prev()\n\n\t\t\t// Set rangeKeyBoundary so that on the next iteration, we know to\n\t\t\t// return the key even if the MERGE point key is deleted.\n\t\t\trangeKeyBoundary = true\n\n\t\tcase InternalKeyKindDelete, InternalKeyKindSingleDelete, InternalKeyKindDeleteSized:\n\t\t\ti.value = LazyValue{}\n\t\t\ti.iterValidityState = IterExhausted\n\t\t\tvalueMerger = nil\n\t\t\ti.stats.ReverseStepCount[InternalIterCall]++\n\t\t\ti.iterKV = i.iter.Prev()\n\t\t\t// Compare with the limit. We could optimize by only checking when\n\t\t\t// we step to the previous user key, but detecting that requires a\n\t\t\t// comparison too. Note that this position may already passed a\n\t\t\t// number of versions of this user key, but they are all deleted, so\n\t\t\t// the fact that a subsequent Prev*() call will not see them is\n\t\t\t// harmless. Also note that this is the only place in the loop,\n\t\t\t// other than the firstLoopIter and SkipPoint cases above, where we\n\t\t\t// could step to a different user key and start processing it for\n\t\t\t// returning to the caller.\n\t\t\tif limit != nil && i.iterKV != nil && i.cmp(limit, i.iterKV.K.UserKey) > 0 && !i.rangeKeyWithinLimit(limit) {\n\t\t\t\ti.iterValidityState = IterAtLimit\n\t\t\t\ti.pos = iterPosCurReversePaused\n\t\t\t\treturn\n\t\t\t}\n\t\t\tcontinue\n\n\t\tcase InternalKeyKindSet, InternalKeyKindSetWithDelete:\n\t\t\ti.keyBuf = append(i.keyBuf[:0], key.UserKey...)\n\t\t\ti.key = i.keyBuf\n\t\t\t// iterValue is owned by i.iter and could change after the Prev()\n\t\t\t// call, so use valueBuf instead. Note that valueBuf is only used\n\t\t\t// in this one instance; everywhere else (eg. in findNextEntry),\n\t\t\t// we just point i.value to the unsafe i.iter-owned value buffer.\n\t\t\ti.value, i.valueBuf = i.iterKV.V.Clone(i.valueBuf[:0], &i.fetcher)\n\t\t\ti.saveRangeKey()\n\t\t\ti.iterValidityState = IterValid\n\t\t\ti.iterKV = i.iter.Prev()\n\t\t\ti.stats.ReverseStepCount[InternalIterCall]++\n\t\t\tvalueMerger = nil\n\t\t\tcontinue\n\n\t\tcase InternalKeyKindMerge:\n\t\t\tif i.iterValidityState == IterExhausted {\n\t\t\t\ti.keyBuf = append(i.keyBuf[:0], key.UserKey...)\n\t\t\t\ti.key = i.keyBuf\n\t\t\t\ti.saveRangeKey()\n\t\t\t\tvar iterValue []byte\n\t\t\t\titerValue, _, i.err = i.iterKV.Value(nil)\n\t\t\t\tif i.err != nil {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tvalueMerger, i.err = i.merge(i.key, iterValue)\n\t\t\t\tif i.err != nil {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\ti.iterValidityState = IterValid\n\t\t\t} else if valueMerger == nil {\n\t\t\t\t// Extract value before iterValue since we use value before iterValue\n\t\t\t\t// and the underlying iterator is not required to provide backing\n\t\t\t\t// memory for both simultaneously.\n\t\t\t\tvar value []byte\n\t\t\t\tvar callerOwned bool\n\t\t\t\tvalue, callerOwned, i.err = i.value.Value(i.lazyValueBuf)\n\t\t\t\tif callerOwned {\n\t\t\t\t\ti.lazyValueBuf = value[:0]\n\t\t\t\t}\n\t\t\t\tif i.err != nil {\n\t\t\t\t\ti.iterValidityState = IterExhausted\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tvalueMerger, i.err = i.merge(i.key, value)\n\t\t\t\tvar iterValue []byte\n\t\t\t\titerValue, _, i.err = i.iterKV.Value(nil)\n\t\t\t\tif i.err != nil {\n\t\t\t\t\ti.iterValidityState = IterExhausted\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tif i.err == nil {\n\t\t\t\t\ti.err = valueMerger.MergeNewer(iterValue)\n\t\t\t\t}\n\t\t\t\tif i.err != nil {\n\t\t\t\t\ti.iterValidityState = IterExhausted\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tvar iterValue []byte\n\t\t\t\titerValue, _, i.err = i.iterKV.Value(nil)\n\t\t\t\tif i.err != nil {\n\t\t\t\t\ti.iterValidityState = IterExhausted\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\ti.err = valueMerger.MergeNewer(iterValue)\n\t\t\t\tif i.err != nil {\n\t\t\t\t\ti.iterValidityState = IterExhausted\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t\ti.iterKV = i.iter.Prev()\n\t\t\ti.stats.ReverseStepCount[InternalIterCall]++\n\t\t\tcontinue\n\n\t\tdefault:\n\t\t\ti.err = base.CorruptionErrorf(\"pebble: invalid internal key kind: %d\", errors.Safe(key.Kind()))\n\t\t\ti.iterValidityState = IterExhausted\n\t\t\treturn\n\t\t}\n\t}\n\t// i.iterKV == nil, so broke out of the preceding loop.\n\n\t// Is iterKey nil due to an error?\n\tif i.err = i.iter.Error(); i.err != nil {\n\t\ti.iterValidityState = IterExhausted\n\t\treturn\n\t}\n\n\tif i.iterValidityState == IterValid {\n\t\ti.pos = iterPosPrev\n\t\tif valueMerger != nil {\n\t\t\tvar needDelete bool\n\t\t\tvar value []byte\n\t\t\tvalue, needDelete, i.valueCloser, i.err = finishValueMerger(valueMerger, true /* includesBase */)\n\t\t\ti.value = base.MakeInPlaceValue(value)\n\t\t\tif i.err == nil && needDelete {\n\t\t\t\ti.key = nil\n\t\t\t\ti.value = LazyValue{}\n\t\t\t\ti.iterValidityState = IterExhausted\n\t\t\t}\n\t\t}\n\t\tif i.err != nil {\n\t\t\ti.iterValidityState = IterExhausted\n\t\t}\n\t}\n}\n\nfunc (i *Iterator) prevUserKey() {\n\tif i.iterKV == nil {\n\t\treturn\n\t}\n\tif i.iterValidityState != IterValid {\n\t\t// If we're going to compare against the prev key, we need to save the\n\t\t// current key.\n\t\ti.keyBuf = append(i.keyBuf[:0], i.iterKV.K.UserKey...)\n\t\ti.key = i.keyBuf\n\t}\n\tfor {\n\t\ti.iterKV = i.iter.Prev()\n\t\ti.stats.ReverseStepCount[InternalIterCall]++\n\t\tif i.iterKV == nil {\n\t\t\tif err := i.iter.Error(); err != nil {\n\t\t\t\ti.err = err\n\t\t\t\ti.iterValidityState = IterExhausted\n\t\t\t}\n\t\t\tbreak\n\t\t}\n\t\tif !i.equal(i.key, i.iterKV.K.UserKey) {\n\t\t\tbreak\n\t\t}\n\t}\n}\n\nfunc (i *Iterator) mergeNext(key InternalKey, valueMerger ValueMerger) {\n\t// Save the current key.\n\ti.keyBuf = append(i.keyBuf[:0], key.UserKey...)\n\ti.key = i.keyBuf\n\n\t// Loop looking for older values for this key and merging them.\n\tfor {\n\t\ti.iterKV = i.iter.Next()\n\t\ti.stats.ForwardStepCount[InternalIterCall]++\n\t\tif i.iterKV == nil {\n\t\t\tif i.err = i.iter.Error(); i.err != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\ti.pos = iterPosNext\n\t\t\treturn\n\t\t}\n\t\tkey = i.iterKV.K\n\t\tif !i.equal(i.key, key.UserKey) {\n\t\t\t// We've advanced to the next key.\n\t\t\ti.pos = iterPosNext\n\t\t\treturn\n\t\t}\n\t\tswitch key.Kind() {\n\t\tcase InternalKeyKindDelete, InternalKeyKindSingleDelete, InternalKeyKindDeleteSized:\n\t\t\t// We've hit a deletion tombstone. Return everything up to this\n\t\t\t// point.\n\t\t\t//\n\t\t\t// NB: treating InternalKeyKindSingleDelete as equivalent to DEL is not\n\t\t\t// only simpler, but is also necessary for correctness due to\n\t\t\t// InternalKeyKindSSTableInternalObsoleteBit.\n\t\t\treturn\n\n\t\tcase InternalKeyKindSet, InternalKeyKindSetWithDelete:\n\t\t\t// We've hit a Set value. Merge with the existing value and return.\n\t\t\tvar iterValue []byte\n\t\t\titerValue, _, i.err = i.iterKV.Value(nil)\n\t\t\tif i.err != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\ti.err = valueMerger.MergeOlder(iterValue)\n\t\t\treturn\n\n\t\tcase InternalKeyKindMerge:\n\t\t\t// We've hit another Merge value. Merge with the existing value and\n\t\t\t// continue looping.\n\t\t\tvar iterValue []byte\n\t\t\titerValue, _, i.err = i.iterKV.Value(nil)\n\t\t\tif i.err != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\ti.err = valueMerger.MergeOlder(iterValue)\n\t\t\tif i.err != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tcontinue\n\n\t\tcase InternalKeyKindRangeKeySet:\n\t\t\t// The RANGEKEYSET marker must sort before a MERGE at the same user key.\n\t\t\ti.err = base.CorruptionErrorf(\"pebble: out of order range key marker\")\n\t\t\treturn\n\n\t\tdefault:\n\t\t\ti.err = base.CorruptionErrorf(\"pebble: invalid internal key kind: %d\", errors.Safe(key.Kind()))\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// SeekGE moves the iterator to the first key/value pair whose key is greater\n// than or equal to the given key. Returns true if the iterator is pointing at\n// a valid entry and false otherwise.\nfunc (i *Iterator) SeekGE(key []byte) bool {\n\treturn i.SeekGEWithLimit(key, nil) == IterValid\n}\n\n// SeekGEWithLimit moves the iterator to the first key/value pair whose key is\n// greater than or equal to the given key.\n//\n// If limit is provided, it serves as a best-effort exclusive limit. If the\n// first key greater than or equal to the given search key is also greater than\n// or equal to limit, the Iterator may pause and return IterAtLimit. Because\n// limits are best-effort, SeekGEWithLimit may return a key beyond limit.\n//\n// If the Iterator is configured to iterate over range keys, SeekGEWithLimit\n// guarantees it will surface any range keys with bounds overlapping the\n// keyspace [key, limit).\nfunc (i *Iterator) SeekGEWithLimit(key []byte, limit []byte) IterValidityState {\n\tif i.rangeKey != nil {\n\t\t// NB: Check Valid() before clearing requiresReposition.\n\t\ti.rangeKey.prevPosHadRangeKey = i.rangeKey.hasRangeKey && i.Valid()\n\t\t// If we have a range key but did not expose it at the previous iterator\n\t\t// position (because the iterator was not at a valid position), updated\n\t\t// must be true. This ensures that after an iterator op sequence like:\n\t\t//   - Next()             → (IterValid, RangeBounds() = [a,b))\n\t\t//   - NextWithLimit(...) → (IterAtLimit, RangeBounds() = -)\n\t\t//   - SeekGE(...)        → (IterValid, RangeBounds() = [a,b))\n\t\t// the iterator returns RangeKeyChanged()=true.\n\t\t//\n\t\t// The remainder of this function will only update i.rangeKey.updated if\n\t\t// the iterator moves into a new range key, or out of the current range\n\t\t// key.\n\t\ti.rangeKey.updated = i.rangeKey.hasRangeKey && !i.Valid() && i.opts.rangeKeys()\n\t}\n\tlastPositioningOp := i.lastPositioningOp\n\t// Set it to unknown, since this operation may not succeed, in which case\n\t// the SeekGE following this should not make any assumption about iterator\n\t// position.\n\ti.lastPositioningOp = unknownLastPositionOp\n\ti.requiresReposition = false\n\ti.err = nil // clear cached iteration error\n\ti.hasPrefix = false\n\ti.stats.ForwardSeekCount[InterfaceCall]++\n\tif lowerBound := i.opts.GetLowerBound(); lowerBound != nil && i.cmp(key, lowerBound) < 0 {\n\t\tkey = lowerBound\n\t} else if upperBound := i.opts.GetUpperBound(); upperBound != nil && i.cmp(key, upperBound) > 0 {\n\t\tkey = upperBound\n\t}\n\tseekInternalIter := true\n\n\tvar flags base.SeekGEFlags\n\tif i.batchJustRefreshed {\n\t\ti.batchJustRefreshed = false\n\t\tflags = flags.EnableBatchJustRefreshed()\n\t}\n\tif lastPositioningOp == seekGELastPositioningOp {\n\t\tcmp := i.cmp(i.prefixOrFullSeekKey, key)\n\t\t// If this seek is to the same or later key, and the iterator is\n\t\t// already positioned there, this is a noop. This can be helpful for\n\t\t// sparse key spaces that have many deleted keys, where one can avoid\n\t\t// the overhead of iterating past them again and again.\n\t\tif cmp <= 0 {\n\t\t\tif !flags.BatchJustRefreshed() &&\n\t\t\t\t(i.iterValidityState == IterExhausted ||\n\t\t\t\t\t(i.iterValidityState == IterValid && i.cmp(key, i.key) <= 0 &&\n\t\t\t\t\t\t(limit == nil || i.cmp(i.key, limit) < 0))) {\n\t\t\t\t// Noop\n\t\t\t\tif i.forceEnableSeekOpt || !testingDisableSeekOpt(key, uintptr(unsafe.Pointer(i))) {\n\t\t\t\t\ti.lastPositioningOp = seekGELastPositioningOp\n\t\t\t\t\treturn i.iterValidityState\n\t\t\t\t}\n\t\t\t}\n\t\t\t// cmp == 0 is not safe to optimize since\n\t\t\t// - i.pos could be at iterPosNext, due to a merge.\n\t\t\t// - Even if i.pos were at iterPosCurForward, we could have a DELETE,\n\t\t\t//   SET pair for a key, and the iterator would have moved past DELETE\n\t\t\t//   but stayed at iterPosCurForward. A similar situation occurs for a\n\t\t\t//   MERGE, SET pair where the MERGE is consumed and the iterator is\n\t\t\t//   at the SET.\n\t\t\t// We also leverage the IterAtLimit <=> i.pos invariant defined in the\n\t\t\t// comment on iterValidityState, to exclude any cases where i.pos\n\t\t\t// is iterPosCur{Forward,Reverse}Paused. This avoids the need to\n\t\t\t// special-case those iterator positions and their interactions with\n\t\t\t// TrySeekUsingNext, as the main uses for TrySeekUsingNext in CockroachDB\n\t\t\t// do not use limited Seeks in the first place.\n\t\t\tif cmp < 0 && i.iterValidityState != IterAtLimit && limit == nil {\n\t\t\t\tflags = flags.EnableTrySeekUsingNext()\n\t\t\t}\n\t\t\tif testingDisableSeekOpt(key, uintptr(unsafe.Pointer(i))) && !i.forceEnableSeekOpt {\n\t\t\t\tflags = flags.DisableTrySeekUsingNext()\n\t\t\t}\n\t\t\tif !flags.BatchJustRefreshed() && i.pos == iterPosCurForwardPaused && i.cmp(key, i.iterKV.K.UserKey) <= 0 {\n\t\t\t\t// Have some work to do, but don't need to seek, and we can\n\t\t\t\t// start doing findNextEntry from i.iterKey.\n\t\t\t\tseekInternalIter = false\n\t\t\t}\n\t\t}\n\t}\n\tif seekInternalIter {\n\t\ti.iterKV = i.iter.SeekGE(key, flags)\n\t\ti.stats.ForwardSeekCount[InternalIterCall]++\n\t\tif err := i.iter.Error(); err != nil {\n\t\t\ti.err = err\n\t\t\ti.iterValidityState = IterExhausted\n\t\t\treturn i.iterValidityState\n\t\t}\n\t}\n\ti.findNextEntry(limit)\n\ti.maybeSampleRead()\n\tif i.Error() == nil {\n\t\t// Prepare state for a future noop optimization.\n\t\ti.prefixOrFullSeekKey = append(i.prefixOrFullSeekKey[:0], key...)\n\t\ti.lastPositioningOp = seekGELastPositioningOp\n\t}\n\treturn i.iterValidityState\n}\n\n// SeekPrefixGE moves the iterator to the first key/value pair whose key is\n// greater than or equal to the given key and which has the same \"prefix\" as\n// the given key. The prefix for a key is determined by the user-defined\n// Comparer.Split function. The iterator will not observe keys not matching the\n// \"prefix\" of the search key. Calling SeekPrefixGE puts the iterator in prefix\n// iteration mode. The iterator remains in prefix iteration until a subsequent\n// call to another absolute positioning method (SeekGE, SeekLT, First,\n// Last). Reverse iteration (Prev) is not supported when an iterator is in\n// prefix iteration mode. Returns true if the iterator is pointing at a valid\n// entry and false otherwise.\n//\n// The semantics of SeekPrefixGE are slightly unusual and designed for\n// iteration to be able to take advantage of bloom filters that have been\n// created on the \"prefix\". If you're not using bloom filters, there is no\n// reason to use SeekPrefixGE.\n//\n// An example Split function may separate a timestamp suffix from the prefix of\n// the key.\n//\n//\tSplit(<key>@<timestamp>) -> <key>\n//\n// Consider the keys \"a@1\", \"a@2\", \"aa@3\", \"aa@4\". The prefixes for these keys\n// are \"a\", and \"aa\". Note that despite \"a\" and \"aa\" sharing a prefix by the\n// usual definition, those prefixes differ by the definition of the Split\n// function. To see how this works, consider the following set of calls on this\n// data set:\n//\n//\tSeekPrefixGE(\"a@0\") -> \"a@1\"\n//\tNext()              -> \"a@2\"\n//\tNext()              -> EOF\n//\n// If you're just looking to iterate over keys with a shared prefix, as\n// defined by the configured comparer, set iterator bounds instead:\n//\n//\titer := db.NewIter(&pebble.IterOptions{\n//\t  LowerBound: []byte(\"prefix\"),\n//\t  UpperBound: []byte(\"prefiy\"),\n//\t})\n//\tfor iter.First(); iter.Valid(); iter.Next() {\n//\t  // Only keys beginning with \"prefix\" will be visited.\n//\t}\n//\n// See ExampleIterator_SeekPrefixGE for a working example.\n//\n// When iterating with range keys enabled, all range keys encountered are\n// truncated to the seek key's prefix's bounds. The truncation of the upper\n// bound requires that the database's Comparer is configured with a\n// ImmediateSuccessor method. For example, a SeekPrefixGE(\"a@9\") call with the\n// prefix \"a\" will truncate range key bounds to [a,ImmediateSuccessor(a)].\nfunc (i *Iterator) SeekPrefixGE(key []byte) bool {\n\tif i.rangeKey != nil {\n\t\t// NB: Check Valid() before clearing requiresReposition.\n\t\ti.rangeKey.prevPosHadRangeKey = i.rangeKey.hasRangeKey && i.Valid()\n\t\t// If we have a range key but did not expose it at the previous iterator\n\t\t// position (because the iterator was not at a valid position), updated\n\t\t// must be true. This ensures that after an iterator op sequence like:\n\t\t//   - Next()             → (IterValid, RangeBounds() = [a,b))\n\t\t//   - NextWithLimit(...) → (IterAtLimit, RangeBounds() = -)\n\t\t//   - SeekPrefixGE(...)  → (IterValid, RangeBounds() = [a,b))\n\t\t// the iterator returns RangeKeyChanged()=true.\n\t\t//\n\t\t// The remainder of this function will only update i.rangeKey.updated if\n\t\t// the iterator moves into a new range key, or out of the current range\n\t\t// key.\n\t\ti.rangeKey.updated = i.rangeKey.hasRangeKey && !i.Valid() && i.opts.rangeKeys()\n\t}\n\tlastPositioningOp := i.lastPositioningOp\n\t// Set it to unknown, since this operation may not succeed, in which case\n\t// the SeekPrefixGE following this should not make any assumption about\n\t// iterator position.\n\ti.lastPositioningOp = unknownLastPositionOp\n\ti.requiresReposition = false\n\ti.err = nil // clear cached iteration error\n\ti.stats.ForwardSeekCount[InterfaceCall]++\n\tif i.comparer.ImmediateSuccessor == nil && i.opts.KeyTypes != IterKeyTypePointsOnly {\n\t\tpanic(\"pebble: ImmediateSuccessor must be provided for SeekPrefixGE with range keys\")\n\t}\n\tprefixLen := i.comparer.Split(key)\n\tkeyPrefix := key[:prefixLen]\n\tvar flags base.SeekGEFlags\n\tif i.batchJustRefreshed {\n\t\tflags = flags.EnableBatchJustRefreshed()\n\t\ti.batchJustRefreshed = false\n\t}\n\tif lastPositioningOp == seekPrefixGELastPositioningOp {\n\t\tif !i.hasPrefix {\n\t\t\tpanic(\"lastPositioningOpsIsSeekPrefixGE is true, but hasPrefix is false\")\n\t\t}\n\t\t// The iterator has not been repositioned after the last SeekPrefixGE.\n\t\t// See if we are seeking to a larger key, since then we can optimize\n\t\t// the seek by using next. Note that we could also optimize if Next\n\t\t// has been called, if the iterator is not exhausted and the current\n\t\t// position is <= the seek key. We are keeping this limited for now\n\t\t// since such optimizations require care for correctness, and to not\n\t\t// become de-optimizations (if one usually has to do all the next\n\t\t// calls and then the seek). This SeekPrefixGE optimization\n\t\t// specifically benefits CockroachDB.\n\t\tcmp := i.cmp(i.prefixOrFullSeekKey, keyPrefix)\n\t\t// cmp == 0 is not safe to optimize since\n\t\t// - i.pos could be at iterPosNext, due to a merge.\n\t\t// - Even if i.pos were at iterPosCurForward, we could have a DELETE,\n\t\t//   SET pair for a key, and the iterator would have moved past DELETE\n\t\t//   but stayed at iterPosCurForward. A similar situation occurs for a\n\t\t//   MERGE, SET pair where the MERGE is consumed and the iterator is\n\t\t//   at the SET.\n\t\t// In general some versions of i.prefix could have been consumed by\n\t\t// the iterator, so we only optimize for cmp < 0.\n\t\tif cmp < 0 {\n\t\t\tflags = flags.EnableTrySeekUsingNext()\n\t\t}\n\t\tif testingDisableSeekOpt(key, uintptr(unsafe.Pointer(i))) && !i.forceEnableSeekOpt {\n\t\t\tflags = flags.DisableTrySeekUsingNext()\n\t\t}\n\t}\n\t// Make a copy of the prefix so that modifications to the key after\n\t// SeekPrefixGE returns does not affect the stored prefix.\n\tif cap(i.prefixOrFullSeekKey) < prefixLen {\n\t\ti.prefixOrFullSeekKey = make([]byte, prefixLen)\n\t} else {\n\t\ti.prefixOrFullSeekKey = i.prefixOrFullSeekKey[:prefixLen]\n\t}\n\ti.hasPrefix = true\n\tcopy(i.prefixOrFullSeekKey, keyPrefix)\n\n\tif lowerBound := i.opts.GetLowerBound(); lowerBound != nil && i.cmp(key, lowerBound) < 0 {\n\t\tif p := i.comparer.Split.Prefix(lowerBound); !bytes.Equal(i.prefixOrFullSeekKey, p) {\n\t\t\ti.err = errors.New(\"pebble: SeekPrefixGE supplied with key outside of lower bound\")\n\t\t\ti.iterValidityState = IterExhausted\n\t\t\treturn false\n\t\t}\n\t\tkey = lowerBound\n\t} else if upperBound := i.opts.GetUpperBound(); upperBound != nil && i.cmp(key, upperBound) > 0 {\n\t\tif p := i.comparer.Split.Prefix(upperBound); !bytes.Equal(i.prefixOrFullSeekKey, p) {\n\t\t\ti.err = errors.New(\"pebble: SeekPrefixGE supplied with key outside of upper bound\")\n\t\t\ti.iterValidityState = IterExhausted\n\t\t\treturn false\n\t\t}\n\t\tkey = upperBound\n\t}\n\ti.iterKV = i.iter.SeekPrefixGE(i.prefixOrFullSeekKey, key, flags)\n\ti.stats.ForwardSeekCount[InternalIterCall]++\n\ti.findNextEntry(nil)\n\ti.maybeSampleRead()\n\tif i.Error() == nil {\n\t\ti.lastPositioningOp = seekPrefixGELastPositioningOp\n\t}\n\treturn i.iterValidityState == IterValid\n}\n\n// Deterministic disabling (in testing mode) of the seek optimizations. It uses\n// the iterator pointer, since we want diversity in iterator behavior for the\n// same key.  Used for tests.\nfunc testingDisableSeekOpt(key []byte, ptr uintptr) bool {\n\tif !invariants.Enabled {\n\t\treturn false\n\t}\n\t// Fibonacci hash https://probablydance.com/2018/06/16/fibonacci-hashing-the-optimization-that-the-world-forgot-or-a-better-alternative-to-integer-modulo/\n\tsimpleHash := (11400714819323198485 * uint64(ptr)) >> 63\n\treturn key != nil && key[0]&byte(1) == 0 && simpleHash == 0\n}\n\n// SeekLT moves the iterator to the last key/value pair whose key is less than\n// the given key. Returns true if the iterator is pointing at a valid entry and\n// false otherwise.\nfunc (i *Iterator) SeekLT(key []byte) bool {\n\treturn i.SeekLTWithLimit(key, nil) == IterValid\n}\n\n// SeekLTWithLimit moves the iterator to the last key/value pair whose key is\n// less than the given key.\n//\n// If limit is provided, it serves as a best-effort inclusive limit. If the last\n// key less than the given search key is also less than limit, the Iterator may\n// pause and return IterAtLimit. Because limits are best-effort, SeekLTWithLimit\n// may return a key beyond limit.\n//\n// If the Iterator is configured to iterate over range keys, SeekLTWithLimit\n// guarantees it will surface any range keys with bounds overlapping the\n// keyspace up to limit.\nfunc (i *Iterator) SeekLTWithLimit(key []byte, limit []byte) IterValidityState {\n\tif i.rangeKey != nil {\n\t\t// NB: Check Valid() before clearing requiresReposition.\n\t\ti.rangeKey.prevPosHadRangeKey = i.rangeKey.hasRangeKey && i.Valid()\n\t\t// If we have a range key but did not expose it at the previous iterator\n\t\t// position (because the iterator was not at a valid position), updated\n\t\t// must be true. This ensures that after an iterator op sequence like:\n\t\t//   - Next()               → (IterValid, RangeBounds() = [a,b))\n\t\t//   - NextWithLimit(...)   → (IterAtLimit, RangeBounds() = -)\n\t\t//   - SeekLTWithLimit(...) → (IterValid, RangeBounds() = [a,b))\n\t\t// the iterator returns RangeKeyChanged()=true.\n\t\t//\n\t\t// The remainder of this function will only update i.rangeKey.updated if\n\t\t// the iterator moves into a new range key, or out of the current range\n\t\t// key.\n\t\ti.rangeKey.updated = i.rangeKey.hasRangeKey && !i.Valid() && i.opts.rangeKeys()\n\t}\n\tlastPositioningOp := i.lastPositioningOp\n\t// Set it to unknown, since this operation may not succeed, in which case\n\t// the SeekLT following this should not make any assumption about iterator\n\t// position.\n\ti.lastPositioningOp = unknownLastPositionOp\n\ti.batchJustRefreshed = false\n\ti.requiresReposition = false\n\ti.err = nil // clear cached iteration error\n\ti.stats.ReverseSeekCount[InterfaceCall]++\n\tif upperBound := i.opts.GetUpperBound(); upperBound != nil && i.cmp(key, upperBound) > 0 {\n\t\tkey = upperBound\n\t} else if lowerBound := i.opts.GetLowerBound(); lowerBound != nil && i.cmp(key, lowerBound) < 0 {\n\t\tkey = lowerBound\n\t}\n\ti.hasPrefix = false\n\tseekInternalIter := true\n\t// The following noop optimization only applies when i.batch == nil, since\n\t// an iterator over a batch is iterating over mutable data, that may have\n\t// changed since the last seek.\n\tif lastPositioningOp == seekLTLastPositioningOp && i.batch == nil {\n\t\tcmp := i.cmp(key, i.prefixOrFullSeekKey)\n\t\t// If this seek is to the same or earlier key, and the iterator is\n\t\t// already positioned there, this is a noop. This can be helpful for\n\t\t// sparse key spaces that have many deleted keys, where one can avoid\n\t\t// the overhead of iterating past them again and again.\n\t\tif cmp <= 0 {\n\t\t\t// NB: when pos != iterPosCurReversePaused, the invariant\n\t\t\t// documented earlier implies that iterValidityState !=\n\t\t\t// IterAtLimit.\n\t\t\tif i.iterValidityState == IterExhausted ||\n\t\t\t\t(i.iterValidityState == IterValid && i.cmp(i.key, key) < 0 &&\n\t\t\t\t\t(limit == nil || i.cmp(limit, i.key) <= 0)) {\n\t\t\t\tif !testingDisableSeekOpt(key, uintptr(unsafe.Pointer(i))) {\n\t\t\t\t\ti.lastPositioningOp = seekLTLastPositioningOp\n\t\t\t\t\treturn i.iterValidityState\n\t\t\t\t}\n\t\t\t}\n\t\t\tif i.pos == iterPosCurReversePaused && i.cmp(i.iterKV.K.UserKey, key) < 0 {\n\t\t\t\t// Have some work to do, but don't need to seek, and we can\n\t\t\t\t// start doing findPrevEntry from i.iterKey.\n\t\t\t\tseekInternalIter = false\n\t\t\t}\n\t\t}\n\t}\n\tif seekInternalIter {\n\t\ti.iterKV = i.iter.SeekLT(key, base.SeekLTFlagsNone)\n\t\ti.stats.ReverseSeekCount[InternalIterCall]++\n\t\tif err := i.iter.Error(); err != nil {\n\t\t\ti.err = err\n\t\t\ti.iterValidityState = IterExhausted\n\t\t\treturn i.iterValidityState\n\t\t}\n\t}\n\ti.findPrevEntry(limit)\n\ti.maybeSampleRead()\n\tif i.Error() == nil && i.batch == nil {\n\t\t// Prepare state for a future noop optimization.\n\t\ti.prefixOrFullSeekKey = append(i.prefixOrFullSeekKey[:0], key...)\n\t\ti.lastPositioningOp = seekLTLastPositioningOp\n\t}\n\treturn i.iterValidityState\n}\n\n// First moves the iterator the first key/value pair. Returns true if the\n// iterator is pointing at a valid entry and false otherwise.\nfunc (i *Iterator) First() bool {\n\tif i.rangeKey != nil {\n\t\t// NB: Check Valid() before clearing requiresReposition.\n\t\ti.rangeKey.prevPosHadRangeKey = i.rangeKey.hasRangeKey && i.Valid()\n\t\t// If we have a range key but did not expose it at the previous iterator\n\t\t// position (because the iterator was not at a valid position), updated\n\t\t// must be true. This ensures that after an iterator op sequence like:\n\t\t//   - Next()             → (IterValid, RangeBounds() = [a,b))\n\t\t//   - NextWithLimit(...) → (IterAtLimit, RangeBounds() = -)\n\t\t//   - First(...)         → (IterValid, RangeBounds() = [a,b))\n\t\t// the iterator returns RangeKeyChanged()=true.\n\t\t//\n\t\t// The remainder of this function will only update i.rangeKey.updated if\n\t\t// the iterator moves into a new range key, or out of the current range\n\t\t// key.\n\t\ti.rangeKey.updated = i.rangeKey.hasRangeKey && !i.Valid() && i.opts.rangeKeys()\n\t}\n\ti.err = nil // clear cached iteration error\n\ti.hasPrefix = false\n\ti.batchJustRefreshed = false\n\ti.lastPositioningOp = unknownLastPositionOp\n\ti.requiresReposition = false\n\ti.stats.ForwardSeekCount[InterfaceCall]++\n\n\ti.err = i.iterFirstWithinBounds()\n\tif i.err != nil {\n\t\ti.iterValidityState = IterExhausted\n\t\treturn false\n\t}\n\ti.findNextEntry(nil)\n\ti.maybeSampleRead()\n\treturn i.iterValidityState == IterValid\n}\n\n// Last moves the iterator the last key/value pair. Returns true if the\n// iterator is pointing at a valid entry and false otherwise.\nfunc (i *Iterator) Last() bool {\n\tif i.rangeKey != nil {\n\t\t// NB: Check Valid() before clearing requiresReposition.\n\t\ti.rangeKey.prevPosHadRangeKey = i.rangeKey.hasRangeKey && i.Valid()\n\t\t// If we have a range key but did not expose it at the previous iterator\n\t\t// position (because the iterator was not at a valid position), updated\n\t\t// must be true. This ensures that after an iterator op sequence like:\n\t\t//   - Next()             → (IterValid, RangeBounds() = [a,b))\n\t\t//   - NextWithLimit(...) → (IterAtLimit, RangeBounds() = -)\n\t\t//   - Last(...)          → (IterValid, RangeBounds() = [a,b))\n\t\t// the iterator returns RangeKeyChanged()=true.\n\t\t//\n\t\t// The remainder of this function will only update i.rangeKey.updated if\n\t\t// the iterator moves into a new range key, or out of the current range\n\t\t// key.\n\t\ti.rangeKey.updated = i.rangeKey.hasRangeKey && !i.Valid() && i.opts.rangeKeys()\n\t}\n\ti.err = nil // clear cached iteration error\n\ti.hasPrefix = false\n\ti.batchJustRefreshed = false\n\ti.lastPositioningOp = unknownLastPositionOp\n\ti.requiresReposition = false\n\ti.stats.ReverseSeekCount[InterfaceCall]++\n\n\tif i.err = i.iterLastWithinBounds(); i.err != nil {\n\t\ti.iterValidityState = IterExhausted\n\t\treturn false\n\t}\n\ti.findPrevEntry(nil)\n\ti.maybeSampleRead()\n\treturn i.iterValidityState == IterValid\n}\n\n// Next moves the iterator to the next key/value pair. Returns true if the\n// iterator is pointing at a valid entry and false otherwise.\nfunc (i *Iterator) Next() bool {\n\treturn i.nextWithLimit(nil) == IterValid\n}\n\n// NextWithLimit moves the iterator to the next key/value pair.\n//\n// If limit is provided, it serves as a best-effort exclusive limit. If the next\n// key  is greater than or equal to limit, the Iterator may pause and return\n// IterAtLimit. Because limits are best-effort, NextWithLimit may return a key\n// beyond limit.\n//\n// If the Iterator is configured to iterate over range keys, NextWithLimit\n// guarantees it will surface any range keys with bounds overlapping the\n// keyspace up to limit.\nfunc (i *Iterator) NextWithLimit(limit []byte) IterValidityState {\n\treturn i.nextWithLimit(limit)\n}\n\n// NextPrefix moves the iterator to the next key/value pair with a key\n// containing a different prefix than the current key. Prefixes are determined\n// by Comparer.Split. Exhausts the iterator if invoked while in prefix-iteration\n// mode.\n//\n// It is not permitted to invoke NextPrefix while at a IterAtLimit position.\n// When called in this condition, NextPrefix has non-deterministic behavior.\n//\n// It is not permitted to invoke NextPrefix when the Iterator has an\n// upper-bound that is a versioned MVCC key (see the comment for\n// Comparer.Split). It returns an error in this case.\nfunc (i *Iterator) NextPrefix() bool {\n\tif i.nextPrefixNotPermittedByUpperBound {\n\t\ti.lastPositioningOp = unknownLastPositionOp\n\t\ti.requiresReposition = false\n\t\ti.err = errors.Errorf(\"NextPrefix not permitted with upper bound %s\",\n\t\t\ti.comparer.FormatKey(i.opts.UpperBound))\n\t\ti.iterValidityState = IterExhausted\n\t\treturn false\n\t}\n\tif i.hasPrefix {\n\t\ti.iterValidityState = IterExhausted\n\t\treturn false\n\t}\n\tif i.Error() != nil {\n\t\treturn false\n\t}\n\treturn i.nextPrefix() == IterValid\n}\n\nfunc (i *Iterator) nextPrefix() IterValidityState {\n\tif i.rangeKey != nil {\n\t\t// NB: Check Valid() before clearing requiresReposition.\n\t\ti.rangeKey.prevPosHadRangeKey = i.rangeKey.hasRangeKey && i.Valid()\n\t\t// If we have a range key but did not expose it at the previous iterator\n\t\t// position (because the iterator was not at a valid position), updated\n\t\t// must be true. This ensures that after an iterator op sequence like:\n\t\t//   - Next()             → (IterValid, RangeBounds() = [a,b))\n\t\t//   - NextWithLimit(...) → (IterAtLimit, RangeBounds() = -)\n\t\t//   - NextWithLimit(...) → (IterValid, RangeBounds() = [a,b))\n\t\t// the iterator returns RangeKeyChanged()=true.\n\t\t//\n\t\t// The remainder of this function will only update i.rangeKey.updated if\n\t\t// the iterator moves into a new range key, or out of the current range\n\t\t// key.\n\t\ti.rangeKey.updated = i.rangeKey.hasRangeKey && !i.Valid() && i.opts.rangeKeys()\n\t}\n\n\t// Although NextPrefix documents that behavior at IterAtLimit is undefined,\n\t// this function handles these cases as a simple prefix-agnostic Next. This\n\t// is done for deterministic behavior in the metamorphic tests.\n\t//\n\t// TODO(jackson): If the metamorphic test operation generator is adjusted to\n\t// make generation of some operations conditional on the previous\n\t// operations, then we can remove this behavior and explicitly error.\n\n\ti.lastPositioningOp = unknownLastPositionOp\n\ti.requiresReposition = false\n\tswitch i.pos {\n\tcase iterPosCurForward:\n\t\t// Positioned on the current key. Advance to the next prefix.\n\t\ti.internalNextPrefix(i.comparer.Split(i.key))\n\tcase iterPosCurForwardPaused:\n\t\t// Positioned at a limit. Implement as a prefix-agnostic Next. See TODO\n\t\t// up above. The iterator is already positioned at the next key.\n\tcase iterPosCurReverse:\n\t\t// Switching directions.\n\t\t// Unless the iterator was exhausted, reverse iteration needs to\n\t\t// position the iterator at iterPosPrev.\n\t\tif i.iterKV != nil {\n\t\t\ti.err = errors.New(\"switching from reverse to forward but iter is not at prev\")\n\t\t\ti.iterValidityState = IterExhausted\n\t\t\treturn i.iterValidityState\n\t\t}\n\t\t// The Iterator is exhausted and i.iter is positioned before the first\n\t\t// key. Reposition to point to the first internal key.\n\t\tif i.err = i.iterFirstWithinBounds(); i.err != nil {\n\t\t\ti.iterValidityState = IterExhausted\n\t\t\treturn i.iterValidityState\n\t\t}\n\tcase iterPosCurReversePaused:\n\t\t// Positioned at a limit. Implement as a prefix-agnostic Next. See TODO\n\t\t// up above.\n\t\t//\n\t\t// Switching directions; The iterator must not be exhausted since it\n\t\t// paused.\n\t\tif i.iterKV == nil {\n\t\t\ti.err = errors.New(\"switching paused from reverse to forward but iter is exhausted\")\n\t\t\ti.iterValidityState = IterExhausted\n\t\t\treturn i.iterValidityState\n\t\t}\n\t\ti.nextUserKey()\n\tcase iterPosPrev:\n\t\t// The underlying iterator is pointed to the previous key (this can\n\t\t// only happen when switching iteration directions).\n\t\tif i.iterKV == nil {\n\t\t\t// We're positioned before the first key. Need to reposition to point to\n\t\t\t// the first key.\n\t\t\ti.err = i.iterFirstWithinBounds()\n\t\t\tif i.iterKV == nil {\n\t\t\t\ti.iterValidityState = IterExhausted\n\t\t\t\treturn i.iterValidityState\n\t\t\t}\n\t\t\tif invariants.Enabled && !i.equal(i.iterKV.K.UserKey, i.key) {\n\t\t\t\ti.opts.getLogger().Fatalf(\"pebble: invariant violation: First internal iterator from iterPosPrev landed on %q, not %q\",\n\t\t\t\t\ti.iterKV.K.UserKey, i.key)\n\t\t\t}\n\t\t} else {\n\t\t\t// Move the internal iterator back onto the user key stored in\n\t\t\t// i.key. iterPosPrev guarantees that it's positioned at the last\n\t\t\t// key with the user key less than i.key, so we're guaranteed to\n\t\t\t// land on the correct key with a single Next.\n\t\t\ti.iterKV = i.iter.Next()\n\t\t\tif i.iterKV == nil {\n\t\t\t\t// This should only be possible if i.iter.Next() encountered an\n\t\t\t\t// error.\n\t\t\t\tif i.iter.Error() == nil {\n\t\t\t\t\ti.opts.getLogger().Fatalf(\"pebble: invariant violation: Nexting internal iterator from iterPosPrev found nothing\")\n\t\t\t\t}\n\t\t\t\t// NB: Iterator.Error() will return i.iter.Error().\n\t\t\t\ti.iterValidityState = IterExhausted\n\t\t\t\treturn i.iterValidityState\n\t\t\t}\n\t\t\tif invariants.Enabled && !i.equal(i.iterKV.K.UserKey, i.key) {\n\t\t\t\ti.opts.getLogger().Fatalf(\"pebble: invariant violation: Nexting internal iterator from iterPosPrev landed on %q, not %q\",\n\t\t\t\t\ti.iterKV.K.UserKey, i.key)\n\t\t\t}\n\t\t}\n\t\t// The internal iterator is now positioned at i.key. Advance to the next\n\t\t// prefix.\n\t\ti.internalNextPrefix(i.comparer.Split(i.key))\n\tcase iterPosNext:\n\t\t// Already positioned on the next key. Only call nextPrefixKey if the\n\t\t// next key shares the same prefix.\n\t\tif i.iterKV != nil {\n\t\t\tcurrKeyPrefixLen := i.comparer.Split(i.key)\n\t\t\tif bytes.Equal(i.comparer.Split.Prefix(i.iterKV.K.UserKey), i.key[:currKeyPrefixLen]) {\n\t\t\t\ti.internalNextPrefix(currKeyPrefixLen)\n\t\t\t}\n\t\t}\n\t}\n\n\ti.stats.ForwardStepCount[InterfaceCall]++\n\ti.findNextEntry(nil /* limit */)\n\ti.maybeSampleRead()\n\treturn i.iterValidityState\n}\n\nfunc (i *Iterator) internalNextPrefix(currKeyPrefixLen int) {\n\tif i.iterKV == nil {\n\t\treturn\n\t}\n\t// The Next \"fast-path\" is not really a fast-path when there is more than\n\t// one version. However, even with TableFormatPebblev3, there is a small\n\t// slowdown (~10%) for one version if we remove it and only call NextPrefix.\n\t// When there are two versions, only calling NextPrefix is ~30% faster.\n\ti.stats.ForwardStepCount[InternalIterCall]++\n\tif i.iterKV = i.iter.Next(); i.iterKV == nil {\n\t\treturn\n\t}\n\tif !bytes.Equal(i.comparer.Split.Prefix(i.iterKV.K.UserKey), i.key[:currKeyPrefixLen]) {\n\t\treturn\n\t}\n\ti.stats.ForwardStepCount[InternalIterCall]++\n\ti.prefixOrFullSeekKey = i.comparer.ImmediateSuccessor(i.prefixOrFullSeekKey[:0], i.key[:currKeyPrefixLen])\n\tif i.iterKV.K.IsExclusiveSentinel() {\n\t\tpanic(errors.AssertionFailedf(\"pebble: unexpected exclusive sentinel key: %q\", i.iterKV.K))\n\t}\n\n\ti.iterKV = i.iter.NextPrefix(i.prefixOrFullSeekKey)\n\tif invariants.Enabled && i.iterKV != nil {\n\t\tif p := i.comparer.Split.Prefix(i.iterKV.K.UserKey); i.cmp(p, i.prefixOrFullSeekKey) < 0 {\n\t\t\tpanic(errors.AssertionFailedf(\"pebble: iter.NextPrefix did not advance beyond the current prefix: now at %q; expected to be geq %q\",\n\t\t\t\ti.iterKV.K, i.prefixOrFullSeekKey))\n\t\t}\n\t}\n}\n\nfunc (i *Iterator) nextWithLimit(limit []byte) IterValidityState {\n\ti.stats.ForwardStepCount[InterfaceCall]++\n\tif i.hasPrefix {\n\t\tif limit != nil {\n\t\t\ti.err = errors.New(\"cannot use limit with prefix iteration\")\n\t\t\ti.iterValidityState = IterExhausted\n\t\t\treturn i.iterValidityState\n\t\t} else if i.iterValidityState == IterExhausted {\n\t\t\t// No-op, already exhausted. We avoid executing the Next because it\n\t\t\t// can break invariants: Specifically, a file that fails the bloom\n\t\t\t// filter test may result in its level being removed from the\n\t\t\t// merging iterator. The level's removal can cause a lazy combined\n\t\t\t// iterator to miss range keys and trigger a switch to combined\n\t\t\t// iteration at a larger key, breaking keyspan invariants.\n\t\t\treturn i.iterValidityState\n\t\t}\n\t}\n\tif i.err != nil {\n\t\treturn i.iterValidityState\n\t}\n\tif i.rangeKey != nil {\n\t\t// NB: Check Valid() before clearing requiresReposition.\n\t\ti.rangeKey.prevPosHadRangeKey = i.rangeKey.hasRangeKey && i.Valid()\n\t\t// If we have a range key but did not expose it at the previous iterator\n\t\t// position (because the iterator was not at a valid position), updated\n\t\t// must be true. This ensures that after an iterator op sequence like:\n\t\t//   - Next()             → (IterValid, RangeBounds() = [a,b))\n\t\t//   - NextWithLimit(...) → (IterAtLimit, RangeBounds() = -)\n\t\t//   - NextWithLimit(...) → (IterValid, RangeBounds() = [a,b))\n\t\t// the iterator returns RangeKeyChanged()=true.\n\t\t//\n\t\t// The remainder of this function will only update i.rangeKey.updated if\n\t\t// the iterator moves into a new range key, or out of the current range\n\t\t// key.\n\t\ti.rangeKey.updated = i.rangeKey.hasRangeKey && !i.Valid() && i.opts.rangeKeys()\n\t}\n\ti.lastPositioningOp = unknownLastPositionOp\n\ti.requiresReposition = false\n\tswitch i.pos {\n\tcase iterPosCurForward:\n\t\ti.nextUserKey()\n\tcase iterPosCurForwardPaused:\n\t\t// Already at the right place.\n\tcase iterPosCurReverse:\n\t\t// Switching directions.\n\t\t// Unless the iterator was exhausted, reverse iteration needs to\n\t\t// position the iterator at iterPosPrev.\n\t\tif i.iterKV != nil {\n\t\t\ti.err = errors.New(\"switching from reverse to forward but iter is not at prev\")\n\t\t\ti.iterValidityState = IterExhausted\n\t\t\treturn i.iterValidityState\n\t\t}\n\t\t// We're positioned before the first key. Need to reposition to point to\n\t\t// the first key.\n\t\tif i.err = i.iterFirstWithinBounds(); i.err != nil {\n\t\t\ti.iterValidityState = IterExhausted\n\t\t\treturn i.iterValidityState\n\t\t}\n\tcase iterPosCurReversePaused:\n\t\t// Switching directions.\n\t\t// The iterator must not be exhausted since it paused.\n\t\tif i.iterKV == nil {\n\t\t\ti.err = errors.New(\"switching paused from reverse to forward but iter is exhausted\")\n\t\t\ti.iterValidityState = IterExhausted\n\t\t\treturn i.iterValidityState\n\t\t}\n\t\ti.nextUserKey()\n\tcase iterPosPrev:\n\t\t// The underlying iterator is pointed to the previous key (this can\n\t\t// only happen when switching iteration directions). We set\n\t\t// i.iterValidityState to IterExhausted here to force the calls to\n\t\t// nextUserKey to save the current key i.iter is pointing at in order\n\t\t// to determine when the next user-key is reached.\n\t\ti.iterValidityState = IterExhausted\n\t\tif i.iterKV == nil {\n\t\t\t// We're positioned before the first key. Need to reposition to point to\n\t\t\t// the first key.\n\t\t\ti.err = i.iterFirstWithinBounds()\n\t\t} else {\n\t\t\ti.nextUserKey()\n\t\t}\n\t\tif i.err != nil {\n\t\t\ti.iterValidityState = IterExhausted\n\t\t\treturn i.iterValidityState\n\t\t}\n\t\ti.nextUserKey()\n\tcase iterPosNext:\n\t\t// Already at the right place.\n\t}\n\ti.findNextEntry(limit)\n\ti.maybeSampleRead()\n\treturn i.iterValidityState\n}\n\n// Prev moves the iterator to the previous key/value pair. Returns true if the\n// iterator is pointing at a valid entry and false otherwise.\nfunc (i *Iterator) Prev() bool {\n\treturn i.PrevWithLimit(nil) == IterValid\n}\n\n// PrevWithLimit moves the iterator to the previous key/value pair.\n//\n// If limit is provided, it serves as a best-effort inclusive limit. If the\n// previous key is less than limit, the Iterator may pause and return\n// IterAtLimit. Because limits are best-effort, PrevWithLimit may return a key\n// beyond limit.\n//\n// If the Iterator is configured to iterate over range keys, PrevWithLimit\n// guarantees it will surface any range keys with bounds overlapping the\n// keyspace up to limit.\nfunc (i *Iterator) PrevWithLimit(limit []byte) IterValidityState {\n\ti.stats.ReverseStepCount[InterfaceCall]++\n\tif i.err != nil {\n\t\treturn i.iterValidityState\n\t}\n\tif i.rangeKey != nil {\n\t\t// NB: Check Valid() before clearing requiresReposition.\n\t\ti.rangeKey.prevPosHadRangeKey = i.rangeKey.hasRangeKey && i.Valid()\n\t\t// If we have a range key but did not expose it at the previous iterator\n\t\t// position (because the iterator was not at a valid position), updated\n\t\t// must be true. This ensures that after an iterator op sequence like:\n\t\t//   - Next()             → (IterValid, RangeBounds() = [a,b))\n\t\t//   - NextWithLimit(...) → (IterAtLimit, RangeBounds() = -)\n\t\t//   - PrevWithLimit(...) → (IterValid, RangeBounds() = [a,b))\n\t\t// the iterator returns RangeKeyChanged()=true.\n\t\t//\n\t\t// The remainder of this function will only update i.rangeKey.updated if\n\t\t// the iterator moves into a new range key, or out of the current range\n\t\t// key.\n\t\ti.rangeKey.updated = i.rangeKey.hasRangeKey && !i.Valid() && i.opts.rangeKeys()\n\t}\n\ti.lastPositioningOp = unknownLastPositionOp\n\ti.requiresReposition = false\n\tif i.hasPrefix {\n\t\ti.err = errReversePrefixIteration\n\t\ti.iterValidityState = IterExhausted\n\t\treturn i.iterValidityState\n\t}\n\tswitch i.pos {\n\tcase iterPosCurForward:\n\t\t// Switching directions, and will handle this below.\n\tcase iterPosCurForwardPaused:\n\t\t// Switching directions, and will handle this below.\n\tcase iterPosCurReverse:\n\t\ti.prevUserKey()\n\tcase iterPosCurReversePaused:\n\t\t// Already at the right place.\n\tcase iterPosNext:\n\t\t// The underlying iterator is pointed to the next key (this can only happen\n\t\t// when switching iteration directions). We will handle this below.\n\tcase iterPosPrev:\n\t\t// Already at the right place.\n\t}\n\tif i.pos == iterPosCurForward || i.pos == iterPosNext || i.pos == iterPosCurForwardPaused {\n\t\t// Switching direction.\n\t\tstepAgain := i.pos == iterPosNext\n\n\t\t// Synthetic range key markers are a special case. Consider SeekGE(b)\n\t\t// which finds a range key [a, c). To ensure the user observes the range\n\t\t// key, the Iterator pauses at Key() = b. The iterator must advance the\n\t\t// internal iterator to see if there's also a coincident point key at\n\t\t// 'b', leaving the iterator at iterPosNext if there's not.\n\t\t//\n\t\t// This is a problem: Synthetic range key markers are only interleaved\n\t\t// during the original seek. A subsequent Prev() of i.iter will not move\n\t\t// back onto the synthetic range key marker. In this case where the\n\t\t// previous iterator position was a synthetic range key start boundary,\n\t\t// we must not step a second time.\n\t\tif i.isEphemeralPosition() {\n\t\t\tstepAgain = false\n\t\t}\n\n\t\t// We set i.iterValidityState to IterExhausted here to force the calls\n\t\t// to prevUserKey to save the current key i.iter is pointing at in\n\t\t// order to determine when the prev user-key is reached.\n\t\ti.iterValidityState = IterExhausted\n\t\tif i.iterKV == nil {\n\t\t\t// We're positioned after the last key. Need to reposition to point to\n\t\t\t// the last key.\n\t\t\ti.err = i.iterLastWithinBounds()\n\t\t} else {\n\t\t\ti.prevUserKey()\n\t\t}\n\t\tif i.err != nil {\n\t\t\treturn i.iterValidityState\n\t\t}\n\t\tif stepAgain {\n\t\t\ti.prevUserKey()\n\t\t\tif i.err != nil {\n\t\t\t\treturn i.iterValidityState\n\t\t\t}\n\t\t}\n\t}\n\ti.findPrevEntry(limit)\n\ti.maybeSampleRead()\n\treturn i.iterValidityState\n}\n\n// iterFirstWithinBounds moves the internal iterator to the first key,\n// respecting bounds.\nfunc (i *Iterator) iterFirstWithinBounds() error {\n\ti.stats.ForwardSeekCount[InternalIterCall]++\n\tif lowerBound := i.opts.GetLowerBound(); lowerBound != nil {\n\t\ti.iterKV = i.iter.SeekGE(lowerBound, base.SeekGEFlagsNone)\n\t} else {\n\t\ti.iterKV = i.iter.First()\n\t}\n\tif i.iterKV == nil {\n\t\treturn i.iter.Error()\n\t}\n\treturn nil\n}\n\n// iterLastWithinBounds moves the internal iterator to the last key, respecting\n// bounds.\nfunc (i *Iterator) iterLastWithinBounds() error {\n\ti.stats.ReverseSeekCount[InternalIterCall]++\n\tif upperBound := i.opts.GetUpperBound(); upperBound != nil {\n\t\ti.iterKV = i.iter.SeekLT(upperBound, base.SeekLTFlagsNone)\n\t} else {\n\t\ti.iterKV = i.iter.Last()\n\t}\n\tif i.iterKV == nil {\n\t\treturn i.iter.Error()\n\t}\n\treturn nil\n}\n\n// RangeKeyData describes a range key's data, set through RangeKeySet. The key\n// boundaries of the range key is provided by Iterator.RangeBounds.\ntype RangeKeyData struct {\n\tSuffix []byte\n\tValue  []byte\n}\n\n// rangeKeyWithinLimit is called during limited reverse iteration when\n// positioned over a key beyond the limit. If there exists a range key that lies\n// within the limit, the iterator must not pause in order to ensure the user has\n// an opportunity to observe the range key within limit.\n//\n// It would be valid to ignore the limit whenever there's a range key covering\n// the key, but that would introduce nondeterminism. To preserve determinism for\n// testing, the iterator ignores the limit only if the covering range key does\n// cover the keyspace within the limit.\n//\n// This awkwardness exists because range keys are interleaved at their inclusive\n// start positions. Note that limit is inclusive.\nfunc (i *Iterator) rangeKeyWithinLimit(limit []byte) bool {\n\tif i.rangeKey == nil || !i.opts.rangeKeys() {\n\t\treturn false\n\t}\n\ts := i.rangeKey.iiter.Span()\n\t// If the range key ends beyond the limit, then the range key does not cover\n\t// any portion of the keyspace within the limit and it is safe to pause.\n\treturn s != nil && i.cmp(s.End, limit) > 0\n}\n\n// saveRangeKey saves the current range key to the underlying iterator's current\n// range key state. If the range key has not changed, saveRangeKey is a no-op.\n// If there is a new range key, saveRangeKey copies all of the key, value and\n// suffixes into Iterator-managed buffers.\nfunc (i *Iterator) saveRangeKey() {\n\tif i.rangeKey == nil || i.opts.KeyTypes == IterKeyTypePointsOnly {\n\t\treturn\n\t}\n\n\ts := i.rangeKey.iiter.Span()\n\tif s == nil {\n\t\ti.rangeKey.hasRangeKey = false\n\t\ti.rangeKey.updated = i.rangeKey.prevPosHadRangeKey\n\t\treturn\n\t} else if !i.rangeKey.stale {\n\t\t// The range key `s` is identical to the one currently saved. No-op.\n\t\treturn\n\t}\n\n\tif s.KeysOrder != keyspan.BySuffixAsc {\n\t\tpanic(\"pebble: range key span's keys unexpectedly not in ascending suffix order\")\n\t}\n\n\t// Although `i.rangeKey.stale` is true, the span s may still be identical\n\t// to the currently saved span. This is possible when seeking the iterator,\n\t// which may land back on the same range key. If we previously had a range\n\t// key and the new one has an identical start key, then it must be the same\n\t// range key and we can avoid copying and keep `i.rangeKey.updated=false`.\n\t//\n\t// TODO(jackson): These key comparisons could be avoidable during relative\n\t// positioning operations continuing in the same direction, because these\n\t// ops will never encounter the previous position's range key while\n\t// stale=true. However, threading whether the current op is a seek or step\n\t// maybe isn't worth it. This key comparison is only necessary once when we\n\t// step onto a new range key, which should be relatively rare.\n\tif i.rangeKey.prevPosHadRangeKey && i.equal(i.rangeKey.start, s.Start) &&\n\t\ti.equal(i.rangeKey.end, s.End) {\n\t\ti.rangeKey.updated = false\n\t\ti.rangeKey.stale = false\n\t\ti.rangeKey.hasRangeKey = true\n\t\treturn\n\t}\n\ti.stats.RangeKeyStats.Count += len(s.Keys)\n\ti.rangeKey.buf.Reset()\n\ti.rangeKey.hasRangeKey = true\n\ti.rangeKey.updated = true\n\ti.rangeKey.stale = false\n\ti.rangeKey.buf, i.rangeKey.start = i.rangeKey.buf.Copy(s.Start)\n\ti.rangeKey.buf, i.rangeKey.end = i.rangeKey.buf.Copy(s.End)\n\ti.rangeKey.keys = i.rangeKey.keys[:0]\n\tfor j := 0; j < len(s.Keys); j++ {\n\t\tif invariants.Enabled {\n\t\t\tif s.Keys[j].Kind() != base.InternalKeyKindRangeKeySet {\n\t\t\t\tpanic(\"pebble: user iteration encountered non-RangeKeySet key kind\")\n\t\t\t} else if j > 0 && i.comparer.CompareRangeSuffixes(s.Keys[j].Suffix, s.Keys[j-1].Suffix) < 0 {\n\t\t\t\tpanic(\"pebble: user iteration encountered range keys not in suffix order\")\n\t\t\t}\n\t\t}\n\t\tvar rkd RangeKeyData\n\t\ti.rangeKey.buf, rkd.Suffix = i.rangeKey.buf.Copy(s.Keys[j].Suffix)\n\t\ti.rangeKey.buf, rkd.Value = i.rangeKey.buf.Copy(s.Keys[j].Value)\n\t\ti.rangeKey.keys = append(i.rangeKey.keys, rkd)\n\t}\n}\n\n// RangeKeyChanged indicates whether the most recent iterator positioning\n// operation resulted in the iterator stepping into or out of a new range key.\n// If true, previously returned range key bounds and data has been invalidated.\n// If false, previously obtained range key bounds, suffix and value slices are\n// still valid and may continue to be read.\n//\n// Invalid iterator positions are considered to not hold range keys, meaning\n// that if an iterator steps from an IterExhausted or IterAtLimit position onto\n// a position with a range key, RangeKeyChanged will yield true.\nfunc (i *Iterator) RangeKeyChanged() bool {\n\treturn i.iterValidityState == IterValid && i.rangeKey != nil && i.rangeKey.updated\n}\n\n// HasPointAndRange indicates whether there exists a point key, a range key or\n// both at the current iterator position.\nfunc (i *Iterator) HasPointAndRange() (hasPoint, hasRange bool) {\n\tif i.iterValidityState != IterValid || i.requiresReposition {\n\t\treturn false, false\n\t}\n\tif i.opts.KeyTypes == IterKeyTypePointsOnly {\n\t\treturn true, false\n\t}\n\treturn i.rangeKey == nil || !i.rangeKey.rangeKeyOnly, i.rangeKey != nil && i.rangeKey.hasRangeKey\n}\n\n// RangeBounds returns the start (inclusive) and end (exclusive) bounds of the\n// range key covering the current iterator position. RangeBounds returns nil\n// bounds if there is no range key covering the current iterator position, or\n// the iterator is not configured to surface range keys.\n//\n// If valid, the returned start bound is less than or equal to Key() and the\n// returned end bound is greater than Key().\nfunc (i *Iterator) RangeBounds() (start, end []byte) {\n\tif i.rangeKey == nil || !i.opts.rangeKeys() || !i.rangeKey.hasRangeKey {\n\t\treturn nil, nil\n\t}\n\treturn i.rangeKey.start, i.rangeKey.end\n}\n\n// Key returns the key of the current key/value pair, or nil if done. The\n// caller should not modify the contents of the returned slice, and its\n// contents may change on the next call to Next.\n//\n// If positioned at an iterator position that only holds a range key, Key()\n// always returns the start bound of the range key. Otherwise, it returns the\n// point key's key.\nfunc (i *Iterator) Key() []byte {\n\treturn i.key\n}\n\n// Value returns the value of the current key/value pair, or nil if done. The\n// caller should not modify the contents of the returned slice, and its\n// contents may change on the next call to Next.\n//\n// Only valid if HasPointAndRange() returns true for hasPoint.\n// Deprecated: use ValueAndErr instead.\nfunc (i *Iterator) Value() []byte {\n\tval, _ := i.ValueAndErr()\n\treturn val\n}\n\n// ValueAndErr returns the value, and any error encountered in extracting the value.\n// REQUIRES: i.Error()==nil and HasPointAndRange() returns true for hasPoint.\n//\n// The caller should not modify the contents of the returned slice, and its\n// contents may change on the next call to Next.\nfunc (i *Iterator) ValueAndErr() ([]byte, error) {\n\tval, callerOwned, err := i.value.Value(i.lazyValueBuf)\n\tif err != nil {\n\t\ti.err = err\n\t\ti.iterValidityState = IterExhausted\n\t}\n\tif callerOwned {\n\t\ti.lazyValueBuf = val[:0]\n\t}\n\treturn val, err\n}\n\n// LazyValue returns the LazyValue. Only for advanced use cases.\n// REQUIRES: i.Error()==nil and HasPointAndRange() returns true for hasPoint.\nfunc (i *Iterator) LazyValue() LazyValue {\n\treturn i.value\n}\n\n// RangeKeys returns the range key values and their suffixes covering the\n// current iterator position. The range bounds may be retrieved separately\n// through Iterator.RangeBounds().\nfunc (i *Iterator) RangeKeys() []RangeKeyData {\n\tif i.rangeKey == nil || !i.opts.rangeKeys() || !i.rangeKey.hasRangeKey {\n\t\treturn nil\n\t}\n\treturn i.rangeKey.keys\n}\n\n// Valid returns true if the iterator is positioned at a valid key/value pair\n// and false otherwise.\nfunc (i *Iterator) Valid() bool {\n\tvalid := i.iterValidityState == IterValid && !i.requiresReposition\n\tif invariants.Enabled {\n\t\tif err := i.Error(); valid && err != nil {\n\t\t\tpanic(errors.AssertionFailedf(\"pebble: iterator is valid with non-nil Error: %+v\", err))\n\t\t}\n\t}\n\treturn valid\n}\n\n// Error returns any accumulated error.\nfunc (i *Iterator) Error() error {\n\tif i.err != nil {\n\t\treturn i.err\n\t}\n\tif i.iter != nil {\n\t\treturn i.iter.Error()\n\t}\n\treturn nil\n}\n\nconst maxKeyBufCacheSize = 4 << 10 // 4 KB\n\n// Close closes the iterator and returns any accumulated error. Exhausting\n// all the key/value pairs in a table is not considered to be an error.\n// It is not valid to call any method, including Close, after the iterator\n// has been closed.\nfunc (i *Iterator) Close() error {\n\t// Close the child iterator before releasing the readState because when the\n\t// readState is released sstables referenced by the readState may be deleted\n\t// which will fail on Windows if the sstables are still open by the child\n\t// iterator.\n\tif i.iter != nil {\n\t\ti.err = firstError(i.err, i.iter.Close())\n\n\t\t// Closing i.iter did not necessarily close the point and range key\n\t\t// iterators. Calls to SetOptions may have 'disconnected' either one\n\t\t// from i.iter if iteration key types were changed. Both point and range\n\t\t// key iterators are preserved in case the iterator needs to switch key\n\t\t// types again. We explicitly close both of these iterators here.\n\t\t//\n\t\t// NB: If the iterators were still connected to i.iter, they may be\n\t\t// closed, but calling Close on a closed internal iterator or fragment\n\t\t// iterator is allowed.\n\t\tif i.pointIter != nil {\n\t\t\ti.err = firstError(i.err, i.pointIter.Close())\n\t\t}\n\t\tif i.rangeKey != nil && i.rangeKey.rangeKeyIter != nil {\n\t\t\ti.rangeKey.rangeKeyIter.Close()\n\t\t}\n\t}\n\terr := i.err\n\n\tif i.readState != nil {\n\t\tif i.readSampling.pendingCompactions.size > 0 {\n\t\t\t// Copy pending read compactions using db.mu.Lock()\n\t\t\ti.readState.db.mu.Lock()\n\t\t\ti.readState.db.mu.compact.readCompactions.combine(&i.readSampling.pendingCompactions, i.cmp)\n\t\t\treschedule := i.readState.db.mu.compact.rescheduleReadCompaction\n\t\t\ti.readState.db.mu.compact.rescheduleReadCompaction = false\n\t\t\tconcurrentCompactions := i.readState.db.mu.compact.compactingCount\n\t\t\ti.readState.db.mu.Unlock()\n\n\t\t\tif reschedule && concurrentCompactions == 0 {\n\t\t\t\t// In a read heavy workload, flushes may not happen frequently enough to\n\t\t\t\t// schedule compactions.\n\t\t\t\ti.readState.db.compactionSchedulers.Add(1)\n\t\t\t\tgo i.readState.db.maybeScheduleCompactionAsync()\n\t\t\t}\n\t\t}\n\n\t\ti.readState.unref()\n\t\ti.readState = nil\n\t}\n\n\tif i.version != nil {\n\t\ti.version.Unref()\n\t}\n\n\tfor _, readers := range i.externalReaders {\n\t\tfor _, r := range readers {\n\t\t\terr = firstError(err, r.Close())\n\t\t}\n\t}\n\n\t// Close the closer for the current value if one was open.\n\tif i.valueCloser != nil {\n\t\terr = firstError(err, i.valueCloser.Close())\n\t\ti.valueCloser = nil\n\t}\n\n\tif i.rangeKey != nil {\n\n\t\ti.rangeKey.rangeKeyBuffers.PrepareForReuse()\n\t\t*i.rangeKey = iteratorRangeKeyState{\n\t\t\trangeKeyBuffers: i.rangeKey.rangeKeyBuffers,\n\t\t}\n\t\titerRangeKeyStateAllocPool.Put(i.rangeKey)\n\t\ti.rangeKey = nil\n\t}\n\tif alloc := i.alloc; alloc != nil {\n\t\tvar (\n\t\t\tkeyBuf               []byte\n\t\t\tboundsBuf            [2][]byte\n\t\t\tprefixOrFullSeekKey  []byte\n\t\t\tmergingIterHeapItems []*mergingIterLevel\n\t\t)\n\n\t\t// Avoid caching the key buf if it is overly large. The constant is fairly\n\t\t// arbitrary.\n\t\tif cap(i.keyBuf) < maxKeyBufCacheSize {\n\t\t\tkeyBuf = i.keyBuf\n\t\t}\n\t\tif cap(i.prefixOrFullSeekKey) < maxKeyBufCacheSize {\n\t\t\tprefixOrFullSeekKey = i.prefixOrFullSeekKey\n\t\t}\n\t\tfor j := range i.boundsBuf {\n\t\t\tif cap(i.boundsBuf[j]) < maxKeyBufCacheSize {\n\t\t\t\tboundsBuf[j] = i.boundsBuf[j]\n\t\t\t}\n\t\t}\n\t\tmergingIterHeapItems = alloc.merging.heap.items\n\n\t\t// Reset the alloc struct, re-assign the fields that are being recycled, and\n\t\t// then return it to the pool. Splitting the first two steps performs better\n\t\t// than doing them in a single step (e.g. *alloc = iterAlloc{...}) because\n\t\t// the compiler can avoid the use of a stack allocated autotmp iterAlloc\n\t\t// variable (~12KB, as of Dec 2024), which must first be zeroed out, then\n\t\t// assigned into, then copied over into the heap-allocated alloc. Instead,\n\t\t// the two-step process allows the compiler to quickly zero out the heap\n\t\t// allocated object and then assign the few fields we want to preserve.\n\t\t//\n\t\t// TODO(nvanbenschoten): even with this optimization, zeroing out the alloc\n\t\t// struct still shows up in profiles because it is such a large struct. Can\n\t\t// we do something better here? We are hanging 22 separated iterators off of\n\t\t// the alloc struct (or more, depending on how you count), many of which are\n\t\t// only used in a few cases. Can those iterators be responsible for zeroing\n\t\t// out their own memory on Close, allowing us to assume that most of the\n\t\t// alloc struct is already zeroed out by this point?\n\t\t*alloc = iterAlloc{}\n\t\talloc.keyBuf = keyBuf\n\t\talloc.boundsBuf = boundsBuf\n\t\talloc.prefixOrFullSeekKey = prefixOrFullSeekKey\n\t\talloc.merging.heap.items = mergingIterHeapItems\n\n\t\titerAllocPool.Put(alloc)\n\t} else if alloc := i.getIterAlloc; alloc != nil {\n\t\tif cap(i.keyBuf) >= maxKeyBufCacheSize {\n\t\t\talloc.keyBuf = nil\n\t\t} else {\n\t\t\talloc.keyBuf = i.keyBuf\n\t\t}\n\t\t*alloc = getIterAlloc{\n\t\t\tkeyBuf: alloc.keyBuf,\n\t\t}\n\t\tgetIterAllocPool.Put(alloc)\n\t}\n\treturn err\n}\n\n// SetBounds sets the lower and upper bounds for the iterator. Once SetBounds\n// returns, the caller is free to mutate the provided slices.\n//\n// The iterator will always be invalidated and must be repositioned with a call\n// to SeekGE, SeekPrefixGE, SeekLT, First, or Last.\nfunc (i *Iterator) SetBounds(lower, upper []byte) {\n\t// Ensure that the Iterator appears exhausted, regardless of whether we\n\t// actually have to invalidate the internal iterator. Optimizations that\n\t// avoid exhaustion are an internal implementation detail that shouldn't\n\t// leak through the interface. The caller should still call an absolute\n\t// positioning method to reposition the iterator.\n\ti.requiresReposition = true\n\n\tif ((i.opts.LowerBound == nil) == (lower == nil)) &&\n\t\t((i.opts.UpperBound == nil) == (upper == nil)) &&\n\t\ti.equal(i.opts.LowerBound, lower) &&\n\t\ti.equal(i.opts.UpperBound, upper) {\n\t\t// Unchanged, noop.\n\t\treturn\n\t}\n\n\t// Copy the user-provided bounds into an Iterator-owned buffer, and set them\n\t// on i.opts.{Lower,Upper}Bound.\n\ti.processBounds(lower, upper)\n\n\ti.iter.SetBounds(i.opts.LowerBound, i.opts.UpperBound)\n\t// If the iterator has an open point iterator that's not currently being\n\t// used, propagate the new bounds to it.\n\tif i.pointIter != nil && !i.opts.pointKeys() {\n\t\ti.pointIter.SetBounds(i.opts.LowerBound, i.opts.UpperBound)\n\t}\n\t// If the iterator has a range key iterator, propagate bounds to it. The\n\t// top-level SetBounds on the interleaving iterator (i.iter) won't propagate\n\t// bounds to the range key iterator stack, because the FragmentIterator\n\t// interface doesn't define a SetBounds method. We need to directly inform\n\t// the iterConfig stack.\n\tif i.rangeKey != nil {\n\t\ti.rangeKey.iterConfig.SetBounds(i.opts.LowerBound, i.opts.UpperBound)\n\t}\n\n\t// Even though this is not a positioning operation, the alteration of the\n\t// bounds means we cannot optimize Seeks by using Next.\n\ti.invalidate()\n}\n\n// SetContext replaces the context provided at iterator creation, or the last\n// one provided by SetContext. Even though iterators are expected to be\n// short-lived, there are some cases where either (a) iterators are used far\n// from the code that created them, (b) iterators are reused (while being\n// short-lived) for processing different requests. For such scenarios, we\n// allow the caller to replace the context.\nfunc (i *Iterator) SetContext(ctx context.Context) {\n\ti.ctx = ctx\n\ti.iter.SetContext(ctx)\n\t// If the iterator has an open point iterator that's not currently being\n\t// used, propagate the new context to it.\n\tif i.pointIter != nil && !i.opts.pointKeys() {\n\t\ti.pointIter.SetContext(i.ctx)\n\t}\n}\n\n// Initialization and changing of the bounds must call processBounds.\n// processBounds saves the bounds and computes derived state from those\n// bounds.\nfunc (i *Iterator) processBounds(lower, upper []byte) {\n\t// Copy the user-provided bounds into an Iterator-owned buffer. We can't\n\t// overwrite the current bounds, because some internal iterators compare old\n\t// and new bounds for optimizations.\n\n\tbuf := i.boundsBuf[i.boundsBufIdx][:0]\n\tif lower != nil {\n\t\tbuf = append(buf, lower...)\n\t\ti.opts.LowerBound = buf\n\t} else {\n\t\ti.opts.LowerBound = nil\n\t}\n\ti.nextPrefixNotPermittedByUpperBound = false\n\tif upper != nil {\n\t\tbuf = append(buf, upper...)\n\t\ti.opts.UpperBound = buf[len(buf)-len(upper):]\n\t\tif i.comparer.Split(i.opts.UpperBound) != len(i.opts.UpperBound) {\n\t\t\t// Setting an upper bound that is a versioned MVCC key. This means\n\t\t\t// that a key can have some MVCC versions before the upper bound and\n\t\t\t// some after. This causes significant complications for NextPrefix,\n\t\t\t// so we bar the user of NextPrefix.\n\t\t\ti.nextPrefixNotPermittedByUpperBound = true\n\t\t}\n\t} else {\n\t\ti.opts.UpperBound = nil\n\t}\n\ti.boundsBuf[i.boundsBufIdx] = buf\n\ti.boundsBufIdx = 1 - i.boundsBufIdx\n}\n\n// SetOptions sets new iterator options for the iterator. Note that the lower\n// and upper bounds applied here will supersede any bounds set by previous calls\n// to SetBounds.\n//\n// Note that the slices provided in this SetOptions must not be changed by the\n// caller until the iterator is closed, or a subsequent SetBounds or SetOptions\n// has returned. This is because comparisons between the existing and new bounds\n// are sometimes used to optimize seeking. See the extended commentary on\n// SetBounds.\n//\n// If the iterator was created over an indexed mutable batch, the iterator's\n// view of the mutable batch is refreshed.\n//\n// The iterator will always be invalidated and must be repositioned with a call\n// to SeekGE, SeekPrefixGE, SeekLT, First, or Last.\n//\n// If only lower and upper bounds need to be modified, prefer SetBounds.\nfunc (i *Iterator) SetOptions(o *IterOptions) {\n\tif i.externalReaders != nil {\n\t\tif err := validateExternalIterOpts(o); err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t}\n\n\t// Ensure that the Iterator appears exhausted, regardless of whether we\n\t// actually have to invalidate the internal iterator. Optimizations that\n\t// avoid exhaustion are an internal implementation detail that shouldn't\n\t// leak through the interface. The caller should still call an absolute\n\t// positioning method to reposition the iterator.\n\ti.requiresReposition = true\n\n\t// Check if global state requires we close all internal iterators.\n\t//\n\t// If the Iterator is in an error state, invalidate the existing iterators\n\t// so that we reconstruct an iterator state from scratch.\n\t//\n\t// If OnlyReadGuaranteedDurable changed, the iterator stacks are incorrect,\n\t// improperly including or excluding memtables. Invalidate them so that\n\t// finishInitializingIter will reconstruct them.\n\tcloseBoth := i.err != nil ||\n\t\to.OnlyReadGuaranteedDurable != i.opts.OnlyReadGuaranteedDurable\n\n\t// If either options specify block property filters for an iterator stack,\n\t// reconstruct it.\n\tif i.pointIter != nil && (closeBoth || len(o.PointKeyFilters) > 0 || len(i.opts.PointKeyFilters) > 0 ||\n\t\to.RangeKeyMasking.Filter != nil || i.opts.RangeKeyMasking.Filter != nil || o.SkipPoint != nil ||\n\t\ti.opts.SkipPoint != nil) {\n\t\ti.err = firstError(i.err, i.pointIter.Close())\n\t\ti.pointIter = nil\n\t}\n\tif i.rangeKey != nil {\n\t\tif closeBoth || len(o.RangeKeyFilters) > 0 || len(i.opts.RangeKeyFilters) > 0 {\n\t\t\ti.rangeKey.rangeKeyIter.Close()\n\t\t\ti.rangeKey = nil\n\t\t} else {\n\t\t\t// If there's still a range key iterator stack, invalidate the\n\t\t\t// iterator. This ensures RangeKeyChanged() returns true if a\n\t\t\t// subsequent positioning operation discovers a range key. It also\n\t\t\t// prevents seek no-op optimizations.\n\t\t\ti.invalidate()\n\t\t}\n\t}\n\n\t// If the iterator is backed by a batch that's been mutated, refresh its\n\t// existing point and range-key iterators, and invalidate the iterator to\n\t// prevent seek-using-next optimizations. If we don't yet have a point-key\n\t// iterator or range-key iterator but we require one, it'll be created in\n\t// the slow path that reconstructs the iterator in finishInitializingIter.\n\tif i.batch != nil {\n\t\tnextBatchSeqNum := (base.SeqNum(len(i.batch.data)) | base.SeqNumBatchBit)\n\t\tif nextBatchSeqNum != i.batchSeqNum {\n\t\t\ti.batchSeqNum = nextBatchSeqNum\n\t\t\tif i.merging != nil {\n\t\t\t\ti.merging.batchSnapshot = nextBatchSeqNum\n\t\t\t}\n\t\t\t// Prevent a no-op seek optimization on the next seek. We won't be\n\t\t\t// able to reuse the top-level Iterator state, because it may be\n\t\t\t// incorrect after the inclusion of new batch mutations.\n\t\t\ti.batchJustRefreshed = true\n\t\t\tif i.pointIter != nil && i.batch.countRangeDels > 0 {\n\t\t\t\tif i.batchRangeDelIter.Count() == 0 {\n\t\t\t\t\t// When we constructed this iterator, there were no\n\t\t\t\t\t// rangedels in the batch. Iterator construction will\n\t\t\t\t\t// have excluded the batch rangedel iterator from the\n\t\t\t\t\t// point iterator stack. We need to reconstruct the\n\t\t\t\t\t// point iterator to add i.batchRangeDelIter into the\n\t\t\t\t\t// iterator stack.\n\t\t\t\t\ti.err = firstError(i.err, i.pointIter.Close())\n\t\t\t\t\ti.pointIter = nil\n\t\t\t\t} else {\n\t\t\t\t\t// There are range deletions in the batch and we already\n\t\t\t\t\t// have a batch rangedel iterator. We can update the\n\t\t\t\t\t// batch rangedel iterator in place.\n\t\t\t\t\t//\n\t\t\t\t\t// NB: There may or may not be new range deletions. We\n\t\t\t\t\t// can't tell based on i.batchRangeDelIter.Count(),\n\t\t\t\t\t// which is the count of fragmented range deletions, NOT\n\t\t\t\t\t// the number of range deletions written to the batch\n\t\t\t\t\t// [i.batch.countRangeDels].\n\t\t\t\t\ti.batch.initRangeDelIter(&i.opts, &i.batchRangeDelIter, nextBatchSeqNum)\n\t\t\t\t}\n\t\t\t}\n\t\t\tif i.rangeKey != nil && i.batch.countRangeKeys > 0 {\n\t\t\t\tif i.batchRangeKeyIter.Count() == 0 {\n\t\t\t\t\t// When we constructed this iterator, there were no range\n\t\t\t\t\t// keys in the batch. Iterator construction will have\n\t\t\t\t\t// excluded the batch rangekey iterator from the range key\n\t\t\t\t\t// iterator stack. We need to reconstruct the range key\n\t\t\t\t\t// iterator to add i.batchRangeKeyIter into the iterator\n\t\t\t\t\t// stack.\n\t\t\t\t\ti.rangeKey.rangeKeyIter.Close()\n\t\t\t\t\ti.rangeKey = nil\n\t\t\t\t} else {\n\t\t\t\t\t// There are range keys in the batch and we already\n\t\t\t\t\t// have a batch rangekey iterator. We can update the batch\n\t\t\t\t\t// rangekey iterator in place.\n\t\t\t\t\t//\n\t\t\t\t\t// NB: There may or may not be new range keys. We can't\n\t\t\t\t\t// tell based on i.batchRangeKeyIter.Count(), which is the\n\t\t\t\t\t// count of fragmented range keys, NOT the number of\n\t\t\t\t\t// range keys written to the batch [i.batch.countRangeKeys].\n\t\t\t\t\ti.batch.initRangeKeyIter(&i.opts, &i.batchRangeKeyIter, nextBatchSeqNum)\n\t\t\t\t\ti.invalidate()\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// Reset combinedIterState.initialized in case the iterator key types\n\t// changed. If there's already a range key iterator stack, the combined\n\t// iterator is already initialized.  Additionally, if the iterator is not\n\t// configured to include range keys, mark it as initialized to signal that\n\t// lower level iterators should not trigger a switch to combined iteration.\n\ti.lazyCombinedIter.combinedIterState = combinedIterState{\n\t\tinitialized: i.rangeKey != nil || !i.opts.rangeKeys(),\n\t}\n\n\tboundsEqual := ((i.opts.LowerBound == nil) == (o.LowerBound == nil)) &&\n\t\t((i.opts.UpperBound == nil) == (o.UpperBound == nil)) &&\n\t\ti.equal(i.opts.LowerBound, o.LowerBound) &&\n\t\ti.equal(i.opts.UpperBound, o.UpperBound)\n\n\tif boundsEqual && o.KeyTypes == i.opts.KeyTypes &&\n\t\t(i.pointIter != nil || !i.opts.pointKeys()) &&\n\t\t(i.rangeKey != nil || !i.opts.rangeKeys() || i.opts.KeyTypes == IterKeyTypePointsAndRanges) &&\n\t\ti.comparer.CompareRangeSuffixes(o.RangeKeyMasking.Suffix, i.opts.RangeKeyMasking.Suffix) == 0 &&\n\t\to.UseL6Filters == i.opts.UseL6Filters {\n\t\t// The options are identical, so we can likely use the fast path. In\n\t\t// addition to all the above constraints, we cannot use the fast path if\n\t\t// configured to perform lazy combined iteration but an indexed batch\n\t\t// used by the iterator now contains range keys. Lazy combined iteration\n\t\t// is not compatible with batch range keys because we always need to\n\t\t// merge the batch's range keys into iteration.\n\t\tif i.rangeKey != nil || !i.opts.rangeKeys() || i.batch == nil || i.batch.countRangeKeys == 0 {\n\t\t\t// Fast path. This preserves the Seek-using-Next optimizations as\n\t\t\t// long as the iterator wasn't already invalidated up above.\n\t\t\treturn\n\t\t}\n\t}\n\t// Slow path.\n\n\t// The options changed. Save the new ones to i.opts.\n\tif boundsEqual {\n\t\t// Copying the options into i.opts will overwrite LowerBound and\n\t\t// UpperBound fields with the user-provided slices. We need to hold on\n\t\t// to the Pebble-owned slices, so save them and re-set them after the\n\t\t// copy.\n\t\tlower, upper := i.opts.LowerBound, i.opts.UpperBound\n\t\ti.opts = *o\n\t\ti.opts.LowerBound, i.opts.UpperBound = lower, upper\n\t} else {\n\t\ti.opts = *o\n\t\ti.processBounds(o.LowerBound, o.UpperBound)\n\t\t// Propagate the changed bounds to the existing point iterator.\n\t\t// NB: We propagate i.opts.{Lower,Upper}Bound, not o.{Lower,Upper}Bound\n\t\t// because i.opts now point to buffers owned by Pebble.\n\t\tif i.pointIter != nil {\n\t\t\ti.pointIter.SetBounds(i.opts.LowerBound, i.opts.UpperBound)\n\t\t}\n\t\tif i.rangeKey != nil {\n\t\t\ti.rangeKey.iterConfig.SetBounds(i.opts.LowerBound, i.opts.UpperBound)\n\t\t}\n\t}\n\n\t// Even though this is not a positioning operation, the invalidation of the\n\t// iterator stack means we cannot optimize Seeks by using Next.\n\ti.invalidate()\n\n\t// Iterators created through NewExternalIter have a different iterator\n\t// initialization process.\n\tif i.externalReaders != nil {\n\t\tfinishInitializingExternal(i.ctx, i)\n\t\treturn\n\t}\n\tfinishInitializingIter(i.ctx, i.alloc)\n}\n\nfunc (i *Iterator) invalidate() {\n\ti.lastPositioningOp = unknownLastPositionOp\n\ti.hasPrefix = false\n\ti.iterKV = nil\n\ti.err = nil\n\t// This switch statement isn't necessary for correctness since callers\n\t// should call a repositioning method. We could have arbitrarily set i.pos\n\t// to one of the values. But it results in more intuitive behavior in\n\t// tests, which do not always reposition.\n\tswitch i.pos {\n\tcase iterPosCurForward, iterPosNext, iterPosCurForwardPaused:\n\t\ti.pos = iterPosCurForward\n\tcase iterPosCurReverse, iterPosPrev, iterPosCurReversePaused:\n\t\ti.pos = iterPosCurReverse\n\t}\n\ti.iterValidityState = IterExhausted\n\tif i.rangeKey != nil {\n\t\ti.rangeKey.iiter.Invalidate()\n\t\ti.rangeKey.prevPosHadRangeKey = false\n\t}\n}\n\n// Metrics returns per-iterator metrics.\nfunc (i *Iterator) Metrics() IteratorMetrics {\n\tm := IteratorMetrics{\n\t\tReadAmp: 1,\n\t}\n\tif mi, ok := i.iter.(*mergingIter); ok {\n\t\tm.ReadAmp = len(mi.levels)\n\t}\n\treturn m\n}\n\n// ResetStats resets the stats to 0.\nfunc (i *Iterator) ResetStats() {\n\ti.stats = IteratorStats{}\n}\n\n// Stats returns the current stats.\nfunc (i *Iterator) Stats() IteratorStats {\n\treturn i.stats\n}\n\n// CloneOptions configures an iterator constructed through Iterator.Clone.\ntype CloneOptions struct {\n\t// IterOptions, if non-nil, define the iterator options to configure a\n\t// cloned iterator. If nil, the clone adopts the same IterOptions as the\n\t// iterator being cloned.\n\tIterOptions *IterOptions\n\t// RefreshBatchView may be set to true when cloning an Iterator over an\n\t// indexed batch. When false, the clone adopts the same (possibly stale)\n\t// view of the indexed batch as the cloned Iterator. When true, the clone is\n\t// constructed with a refreshed view of the batch, observing all of the\n\t// batch's mutations at the time of the Clone. If the cloned iterator was\n\t// not constructed to read over an indexed batch, RefreshVatchView has no\n\t// effect.\n\tRefreshBatchView bool\n}\n\n// Clone creates a new Iterator over the same underlying data, i.e., over the\n// same {batch, memtables, sstables}). The resulting iterator is not positioned.\n// It starts with the same IterOptions, unless opts.IterOptions is set.\n//\n// When called on an Iterator over an indexed batch, the clone's visibility of\n// the indexed batch is determined by CloneOptions.RefreshBatchView. If false,\n// the clone inherits the iterator's current (possibly stale) view of the batch,\n// and callers may call SetOptions to subsequently refresh the clone's view to\n// include all batch mutations. If true, the clone is constructed with a\n// complete view of the indexed batch's mutations at the time of the Clone.\n//\n// Callers can use Clone if they need multiple iterators that need to see\n// exactly the same underlying state of the DB. This should not be used to\n// extend the lifetime of the data backing the original Iterator since that\n// will cause an increase in memory and disk usage (use NewSnapshot for that\n// purpose).\nfunc (i *Iterator) Clone(opts CloneOptions) (*Iterator, error) {\n\treturn i.CloneWithContext(context.Background(), opts)\n}\n\n// CloneWithContext is like Clone, and additionally accepts a context for\n// tracing.\nfunc (i *Iterator) CloneWithContext(ctx context.Context, opts CloneOptions) (*Iterator, error) {\n\tif opts.IterOptions == nil {\n\t\topts.IterOptions = &i.opts\n\t}\n\tif i.batchOnlyIter {\n\t\treturn nil, errors.Errorf(\"cannot Clone a batch-only Iterator\")\n\t}\n\treadState := i.readState\n\tvers := i.version\n\tif readState == nil && vers == nil {\n\t\treturn nil, errors.Errorf(\"cannot Clone a closed Iterator\")\n\t}\n\t// i is already holding a ref, so there is no race with unref here.\n\t//\n\t// TODO(bilal): If the underlying iterator was created on a snapshot, we could\n\t// grab a reference to the current readState instead of reffing the original\n\t// readState. This allows us to release references to some zombie sstables\n\t// and memtables.\n\tif readState != nil {\n\t\treadState.ref()\n\t}\n\tif vers != nil {\n\t\tvers.Ref()\n\t}\n\t// Bundle various structures under a single umbrella in order to allocate\n\t// them together.\n\tbuf := iterAllocPool.Get().(*iterAlloc)\n\tdbi := &buf.dbi\n\t*dbi = Iterator{\n\t\tctx:                 ctx,\n\t\topts:                *opts.IterOptions,\n\t\talloc:               buf,\n\t\tmerge:               i.merge,\n\t\tcomparer:            i.comparer,\n\t\treadState:           readState,\n\t\tversion:             vers,\n\t\tkeyBuf:              buf.keyBuf,\n\t\tprefixOrFullSeekKey: buf.prefixOrFullSeekKey,\n\t\tboundsBuf:           buf.boundsBuf,\n\t\tbatch:               i.batch,\n\t\tbatchSeqNum:         i.batchSeqNum,\n\t\tfc:                  i.fc,\n\t\tnewIters:            i.newIters,\n\t\tnewIterRangeKey:     i.newIterRangeKey,\n\t\tseqNum:              i.seqNum,\n\t}\n\tdbi.processBounds(dbi.opts.LowerBound, dbi.opts.UpperBound)\n\n\t// If the caller requested the clone have a current view of the indexed\n\t// batch, set the clone's batch sequence number appropriately.\n\tif i.batch != nil && opts.RefreshBatchView {\n\t\tdbi.batchSeqNum = (base.SeqNum(len(i.batch.data)) | base.SeqNumBatchBit)\n\t}\n\n\treturn finishInitializingIter(ctx, buf), nil\n}\n\n// Merge adds all of the argument's statistics to the receiver. It may be used\n// to accumulate stats across multiple iterators.\nfunc (stats *IteratorStats) Merge(o IteratorStats) {\n\tfor i := InterfaceCall; i < NumStatsKind; i++ {\n\t\tstats.ForwardSeekCount[i] += o.ForwardSeekCount[i]\n\t\tstats.ReverseSeekCount[i] += o.ReverseSeekCount[i]\n\t\tstats.ForwardStepCount[i] += o.ForwardStepCount[i]\n\t\tstats.ReverseStepCount[i] += o.ReverseStepCount[i]\n\t}\n\tstats.InternalStats.Merge(o.InternalStats)\n\tstats.RangeKeyStats.Merge(o.RangeKeyStats)\n}\n\nfunc (stats *IteratorStats) String() string {\n\treturn redact.StringWithoutMarkers(stats)\n}\n\n// SafeFormat implements the redact.SafeFormatter interface.\nfunc (stats *IteratorStats) SafeFormat(s redact.SafePrinter, verb rune) {\n\tif stats.ReverseSeekCount[InterfaceCall] == 0 && stats.ReverseSeekCount[InternalIterCall] == 0 {\n\t\ts.Printf(\"seeked %s times (%s internal)\",\n\t\t\thumanize.Count.Uint64(uint64(stats.ForwardSeekCount[InterfaceCall])),\n\t\t\thumanize.Count.Uint64(uint64(stats.ForwardSeekCount[InternalIterCall])),\n\t\t)\n\t} else {\n\t\ts.Printf(\"seeked %s times (%s fwd/%s rev, internal: %s fwd/%s rev)\",\n\t\t\thumanize.Count.Uint64(uint64(stats.ForwardSeekCount[InterfaceCall]+stats.ReverseSeekCount[InterfaceCall])),\n\t\t\thumanize.Count.Uint64(uint64(stats.ForwardSeekCount[InterfaceCall])),\n\t\t\thumanize.Count.Uint64(uint64(stats.ReverseSeekCount[InterfaceCall])),\n\t\t\thumanize.Count.Uint64(uint64(stats.ForwardSeekCount[InternalIterCall])),\n\t\t\thumanize.Count.Uint64(uint64(stats.ReverseSeekCount[InternalIterCall])),\n\t\t)\n\t}\n\ts.SafeString(\"; \")\n\n\tif stats.ReverseStepCount[InterfaceCall] == 0 && stats.ReverseStepCount[InternalIterCall] == 0 {\n\t\ts.Printf(\"stepped %s times (%s internal)\",\n\t\t\thumanize.Count.Uint64(uint64(stats.ForwardStepCount[InterfaceCall])),\n\t\t\thumanize.Count.Uint64(uint64(stats.ForwardStepCount[InternalIterCall])),\n\t\t)\n\t} else {\n\t\ts.Printf(\"stepped %s times (%s fwd/%s rev, internal: %s fwd/%s rev)\",\n\t\t\thumanize.Count.Uint64(uint64(stats.ForwardStepCount[InterfaceCall]+stats.ReverseStepCount[InterfaceCall])),\n\t\t\thumanize.Count.Uint64(uint64(stats.ForwardStepCount[InterfaceCall])),\n\t\t\thumanize.Count.Uint64(uint64(stats.ReverseStepCount[InterfaceCall])),\n\t\t\thumanize.Count.Uint64(uint64(stats.ForwardStepCount[InternalIterCall])),\n\t\t\thumanize.Count.Uint64(uint64(stats.ReverseStepCount[InternalIterCall])),\n\t\t)\n\t}\n\n\tif stats.InternalStats != (InternalIteratorStats{}) {\n\t\ts.SafeString(\"; \")\n\t\tstats.InternalStats.SafeFormat(s, verb)\n\t}\n\tif stats.RangeKeyStats != (RangeKeyIteratorStats{}) {\n\t\ts.SafeString(\", \")\n\t\tstats.RangeKeyStats.SafeFormat(s, verb)\n\t}\n}\n\n// CanDeterministicallySingleDelete takes a valid iterator and examines internal\n// state to determine if a SingleDelete deleting Iterator.Key() would\n// deterministically delete the key. CanDeterministicallySingleDelete requires\n// the iterator to be oriented in the forward direction (eg, the last\n// positioning operation must've been a First, a Seek[Prefix]GE, or a\n// Next[Prefix][WithLimit]).\n//\n// This function does not change the external position of the iterator, and all\n// positioning methods should behave the same as if it was never called. This\n// function will only return a meaningful result the first time it's invoked at\n// an iterator position. This function invalidates the iterator Value's memory,\n// and the caller must not rely on the memory safety of the previous Iterator\n// position.\n//\n// If CanDeterministicallySingleDelete returns true AND the key at the iterator\n// position is not modified between the creation of the Iterator and the commit\n// of a batch containing a SingleDelete over the key, then the caller can be\n// assured that SingleDelete is equivalent to Delete on the local engine, but it\n// may not be true on another engine that received the same writes and with\n// logically equivalent state since this engine may have collapsed multiple SETs\n// into one.\nfunc CanDeterministicallySingleDelete(it *Iterator) (bool, error) {\n\t// This function may only be called once per external iterator position. We\n\t// can validate this by checking the last positioning operation.\n\tif it.lastPositioningOp == internalNextOp {\n\t\treturn false, errors.New(\"pebble: CanDeterministicallySingleDelete called twice\")\n\t}\n\tvalidity, kind := it.internalNext()\n\tvar shadowedBySingleDelete bool\n\tfor validity == internalNextValid {\n\t\tswitch kind {\n\t\tcase InternalKeyKindDelete, InternalKeyKindDeleteSized:\n\t\t\t// A DEL or DELSIZED tombstone is okay. An internal key\n\t\t\t// sequence like SINGLEDEL; SET; DEL; SET can be handled\n\t\t\t// deterministically. If there are SETs further down, we\n\t\t\t// don't care about them.\n\t\t\treturn true, nil\n\t\tcase InternalKeyKindSingleDelete:\n\t\t\t// A SingleDelete is okay as long as when that SingleDelete was\n\t\t\t// written, it was written deterministically (eg, with its own\n\t\t\t// CanDeterministicallySingleDelete check). Validate that it was\n\t\t\t// written deterministically. We'll allow one set to appear after\n\t\t\t// the SingleDelete.\n\t\t\tshadowedBySingleDelete = true\n\t\t\tvalidity, kind = it.internalNext()\n\t\t\tcontinue\n\t\tcase InternalKeyKindSet, InternalKeyKindSetWithDelete, InternalKeyKindMerge:\n\t\t\t// If we observed a single delete, it's allowed to delete 1 key.\n\t\t\t// We'll keep looping to validate that the internal keys beneath the\n\t\t\t// already-written single delete are copacetic.\n\t\t\tif shadowedBySingleDelete {\n\t\t\t\tshadowedBySingleDelete = false\n\t\t\t\tvalidity, kind = it.internalNext()\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// We encountered a shadowed SET, SETWITHDEL, MERGE. A SINGLEDEL\n\t\t\t// that deleted the KV at the original iterator position could\n\t\t\t// result in this key becoming visible.\n\t\t\treturn false, nil\n\t\tcase InternalKeyKindRangeDelete:\n\t\t\t// RangeDeletes are handled by the merging iterator and should never\n\t\t\t// be observed by the top-level Iterator.\n\t\t\tpanic(errors.AssertionFailedf(\"pebble: unexpected range delete\"))\n\t\tcase InternalKeyKindRangeKeySet, InternalKeyKindRangeKeyUnset, InternalKeyKindRangeKeyDelete:\n\t\t\t// Range keys are interleaved at the maximal sequence number and\n\t\t\t// should never be observed within a user key.\n\t\t\tpanic(errors.AssertionFailedf(\"pebble: unexpected range key\"))\n\t\tdefault:\n\t\t\tpanic(errors.AssertionFailedf(\"pebble: unexpected key kind: %s\", errors.Safe(kind)))\n\t\t}\n\t}\n\tif validity == internalNextError {\n\t\treturn false, it.Error()\n\t}\n\treturn true, nil\n}\n\n// internalNextValidity enumerates the potential outcomes of a call to\n// internalNext.\ntype internalNextValidity int8\n\nconst (\n\t// internalNextError is returned by internalNext when an error occurred and\n\t// the caller is responsible for checking iter.Error().\n\tinternalNextError internalNextValidity = iota\n\t// internalNextExhausted is returned by internalNext when the next internal\n\t// key is an internal key with a different user key than Iterator.Key().\n\tinternalNextExhausted\n\t// internalNextValid is returned by internalNext when the internal next\n\t// found a shadowed internal key with a user key equal to Iterator.Key().\n\tinternalNextValid\n)\n\n// internalNext advances internal Iterator state forward to expose the\n// InternalKeyKind of the next internal key with a user key equal to Key().\n//\n// internalNext is a highly specialized operation and is unlikely to be\n// generally useful. See Iterator.Next for how to reposition the iterator to the\n// next key. internalNext requires the Iterator to be at a valid position in the\n// forward direction (the last positioning operation must've been a First, a\n// Seek[Prefix]GE, or a Next[Prefix][WithLimit] and Valid() must return true).\n//\n// internalNext, unlike all other Iterator methods, exposes internal LSM state.\n// internalNext advances the Iterator's internal iterator to the next shadowed\n// key with a user key equal to Key(). When a key is overwritten or deleted, its\n// removal from the LSM occurs lazily as a part of compactions. internalNext\n// allows the caller to see whether an obsolete internal key exists with the\n// current Key(), and what it's key kind is. Note that the existence of an\n// internal key is nondeterministic and dependent on internal LSM state. These\n// semantics are unlikely to be applicable to almost all use cases.\n//\n// If internalNext finds a key that shares the same user key as Key(), it\n// returns internalNextValid and the internal key's kind. If internalNext\n// encounters an error, it returns internalNextError and the caller is expected\n// to call Iterator.Error() to retrieve it. In all other circumstances,\n// internalNext returns internalNextExhausted, indicating that there are no more\n// additional internal keys with the user key Key().\n//\n// internalNext does not change the external position of the iterator, and a\n// Next operation should behave the same as if internalNext was never called.\n// internalNext does invalidate the iterator Value's memory, and the caller must\n// not rely on the memory safety of the previous Iterator position.\nfunc (i *Iterator) internalNext() (internalNextValidity, base.InternalKeyKind) {\n\ti.stats.ForwardStepCount[InterfaceCall]++\n\tif i.err != nil {\n\t\treturn internalNextError, base.InternalKeyKindInvalid\n\t} else if i.iterValidityState != IterValid {\n\t\treturn internalNextExhausted, base.InternalKeyKindInvalid\n\t}\n\ti.lastPositioningOp = internalNextOp\n\n\tswitch i.pos {\n\tcase iterPosCurForward:\n\t\ti.iterKV = i.iter.Next()\n\t\tif i.iterKV == nil {\n\t\t\t// We check i.iter.Error() here and return an internalNextError enum\n\t\t\t// variant so that the caller does not need to check i.iter.Error()\n\t\t\t// in the common case that the next internal key has a new user key.\n\t\t\tif i.err = i.iter.Error(); i.err != nil {\n\t\t\t\treturn internalNextError, base.InternalKeyKindInvalid\n\t\t\t}\n\t\t\ti.pos = iterPosNext\n\t\t\treturn internalNextExhausted, base.InternalKeyKindInvalid\n\t\t} else if i.comparer.Equal(i.iterKV.K.UserKey, i.key) {\n\t\t\treturn internalNextValid, i.iterKV.Kind()\n\t\t}\n\t\ti.pos = iterPosNext\n\t\treturn internalNextExhausted, base.InternalKeyKindInvalid\n\tcase iterPosCurReverse, iterPosCurReversePaused, iterPosPrev:\n\t\ti.err = errors.New(\"switching from reverse to forward via internalNext is prohibited\")\n\t\ti.iterValidityState = IterExhausted\n\t\treturn internalNextError, base.InternalKeyKindInvalid\n\tcase iterPosNext, iterPosCurForwardPaused:\n\t\t// The previous method already moved onto the next user key. This is\n\t\t// only possible if\n\t\t//   - the last positioning method was a call to internalNext, and we\n\t\t//     advanced to a new user key.\n\t\t//   - the previous non-internalNext iterator operation encountered a\n\t\t//     range key or merge, forcing an internal Next that found a new\n\t\t//     user key that's not equal to i.Iterator.Key().\n\t\treturn internalNextExhausted, base.InternalKeyKindInvalid\n\tdefault:\n\t\tpanic(\"unreachable\")\n\t}\n}\n\nvar _ base.IteratorDebug = (*Iterator)(nil)\n\n// DebugTree implements the base.IteratorDebug interface.\nfunc (i *Iterator) DebugTree(tp treeprinter.Node) {\n\tn := tp.Childf(\"%T(%p)\", i, i)\n\tif i.iter != nil {\n\t\ti.iter.DebugTree(n)\n\t}\n\tif i.pointIter != nil {\n\t\ti.pointIter.DebugTree(n)\n\t}\n}\n"
        },
        {
          "name": "iterator_example_test.go",
          "type": "blob",
          "size": 2.5546875,
          "content": "// Copyright 2021 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble_test\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\n\t\"github.com/cockroachdb/pebble\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n)\n\nfunc ExampleIterator() {\n\tdb, err := pebble.Open(\"\", &pebble.Options{FS: vfs.NewMem()})\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\tkeys := []string{\"hello\", \"world\", \"hello world\"}\n\tfor _, key := range keys {\n\t\tif err := db.Set([]byte(key), nil, pebble.Sync); err != nil {\n\t\t\tlog.Fatal(err)\n\t\t}\n\t}\n\n\titer, _ := db.NewIter(nil)\n\tfor iter.First(); iter.Valid(); iter.Next() {\n\t\tfmt.Printf(\"%s\\n\", iter.Key())\n\t}\n\tif err := iter.Close(); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tif err := db.Close(); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\t// Output:\n\t// hello\n\t// hello world\n\t// world\n}\n\nfunc ExampleIterator_prefixIteration() {\n\tdb, err := pebble.Open(\"\", &pebble.Options{FS: vfs.NewMem()})\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\tkeyUpperBound := func(b []byte) []byte {\n\t\tend := make([]byte, len(b))\n\t\tcopy(end, b)\n\t\tfor i := len(end) - 1; i >= 0; i-- {\n\t\t\tend[i] = end[i] + 1\n\t\t\tif end[i] != 0 {\n\t\t\t\treturn end[:i+1]\n\t\t\t}\n\t\t}\n\t\treturn nil // no upper-bound\n\t}\n\n\tprefixIterOptions := func(prefix []byte) *pebble.IterOptions {\n\t\treturn &pebble.IterOptions{\n\t\t\tLowerBound: prefix,\n\t\t\tUpperBound: keyUpperBound(prefix),\n\t\t}\n\t}\n\n\tkeys := []string{\"hello\", \"world\", \"hello world\"}\n\tfor _, key := range keys {\n\t\tif err := db.Set([]byte(key), nil, pebble.Sync); err != nil {\n\t\t\tlog.Fatal(err)\n\t\t}\n\t}\n\n\titer, _ := db.NewIter(prefixIterOptions([]byte(\"hello\")))\n\tfor iter.First(); iter.Valid(); iter.Next() {\n\t\tfmt.Printf(\"%s\\n\", iter.Key())\n\t}\n\tif err := iter.Close(); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tif err := db.Close(); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\t// Output:\n\t// hello\n\t// hello world\n}\n\nfunc ExampleIterator_SeekGE() {\n\tdb, err := pebble.Open(\"\", &pebble.Options{FS: vfs.NewMem()})\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\n\tkeys := []string{\"hello\", \"world\", \"hello world\"}\n\tfor _, key := range keys {\n\t\tif err := db.Set([]byte(key), nil, pebble.Sync); err != nil {\n\t\t\tlog.Fatal(err)\n\t\t}\n\t}\n\n\titer, _ := db.NewIter(nil)\n\tif iter.SeekGE([]byte(\"a\")); iter.Valid() {\n\t\tfmt.Printf(\"%s\\n\", iter.Key())\n\t}\n\tif iter.SeekGE([]byte(\"hello w\")); iter.Valid() {\n\t\tfmt.Printf(\"%s\\n\", iter.Key())\n\t}\n\tif iter.SeekGE([]byte(\"w\")); iter.Valid() {\n\t\tfmt.Printf(\"%s\\n\", iter.Key())\n\t}\n\tif err := iter.Close(); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tif err := db.Close(); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\t// Output:\n\t// hello\n\t// hello world\n\t// world\n}\n"
        },
        {
          "name": "iterator_histories_test.go",
          "type": "blob",
          "size": 11.3427734375,
          "content": "// Copyright 2022 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"path/filepath\"\n\t\"strconv\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/cockroachdb/datadriven\"\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/invariants\"\n\t\"github.com/cockroachdb/pebble/internal/testkeys\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/stretchr/testify/require\"\n)\n\n// TODO(jackson): Add a range keys test with concurrency: the logic to cache\n// fragmented spans is susceptible to races.\n\nfunc TestIterHistories(t *testing.T) {\n\tdatadriven.Walk(t, \"testdata/iter_histories\", func(t *testing.T, path string) {\n\t\tfilename := filepath.Base(path)\n\t\tswitch {\n\t\tcase invariants.Enabled && strings.Contains(filename, \"no_invariants\"):\n\t\t\tt.Skip(\"disabled when run with -tags invariants due to nondeterminism\")\n\t\t}\n\n\t\tvar d *DB\n\t\tvar refedCache bool\n\t\tvar buf bytes.Buffer\n\t\titers := map[string]*Iterator{}\n\t\tbatches := map[string]*Batch{}\n\t\tnewIter := func(name string, reader Reader, o *IterOptions) *Iterator {\n\t\t\tit, _ := reader.NewIter(o)\n\t\t\titers[name] = it\n\t\t\treturn it\n\t\t}\n\t\tvar opts *Options\n\t\tparseOpts := func(td *datadriven.TestData) (*Options, error) {\n\t\t\topts = &Options{\n\t\t\t\tFS:                 vfs.NewMem(),\n\t\t\t\tComparer:           testkeys.Comparer,\n\t\t\t\tFormatMajorVersion: FormatMinSupported,\n\t\t\t\tBlockPropertyCollectors: []func() BlockPropertyCollector{\n\t\t\t\t\tsstable.NewTestKeysBlockPropertyCollector,\n\t\t\t\t},\n\t\t\t\tLogger: testLogger{t},\n\t\t\t}\n\n\t\t\topts.DisableAutomaticCompactions = true\n\t\t\topts.EnsureDefaults()\n\t\t\topts.WithFSDefaults()\n\t\t\toriginalCache := opts.Cache\n\t\t\tif err := parseDBOptionsArgs(opts, td.CmdArgs); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\t// If the test replaced the cache, we'll need to unref the\n\t\t\t// new cache later.\n\t\t\trefedCache = opts.Cache != originalCache\n\t\t\treturn opts, nil\n\t\t}\n\t\tcleanup := func() (err error) {\n\t\t\tfor key, batch := range batches {\n\t\t\t\terr = firstError(err, batch.Close())\n\t\t\t\tdelete(batches, key)\n\t\t\t}\n\t\t\tfor key, iter := range iters {\n\t\t\t\terr = firstError(err, iter.Close())\n\t\t\t\tdelete(iters, key)\n\t\t\t}\n\n\t\t\tif d != nil {\n\t\t\t\t// Close all open snapshots.\n\t\t\t\td.mu.Lock()\n\t\t\t\tvar ss []*Snapshot\n\t\t\t\tl := &d.mu.snapshots\n\t\t\t\tfor i := l.root.next; i != &l.root; i = i.next {\n\t\t\t\t\tss = append(ss, i)\n\t\t\t\t}\n\t\t\t\td.mu.Unlock()\n\t\t\t\tfor i := range ss {\n\t\t\t\t\terr = firstError(err, ss[i].Close())\n\t\t\t\t}\n\n\t\t\t\terr = firstError(err, d.Close())\n\t\t\t\td = nil\n\t\t\t}\n\n\t\t\tif refedCache {\n\t\t\t\topts.Cache.Unref()\n\t\t\t\topts.Cache = nil\n\t\t\t\trefedCache = false\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\t\tdefer cleanup()\n\n\t\tdatadriven.RunTest(t, path, func(t *testing.T, td *datadriven.TestData) string {\n\t\t\tbuf.Reset()\n\t\t\tswitch td.Cmd {\n\t\t\tcase \"define\":\n\t\t\t\tvar err error\n\t\t\t\tif err := cleanup(); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\topts, err = parseOpts(td)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\td, err = runDBDefineCmd(td, opts)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\treturn runLSMCmd(td, d)\n\t\t\tcase \"reopen\":\n\t\t\t\tvar err error\n\t\t\t\tif err := cleanup(); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\toriginalCache := opts.Cache\n\t\t\t\tif err := parseDBOptionsArgs(opts, td.CmdArgs); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\t// If the test replaced the cache, we'll need to unref the\n\t\t\t\t// new cache later.\n\t\t\t\trefedCache = originalCache != opts.Cache\n\t\t\t\td, err = Open(\"\", opts)\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\treturn \"\"\n\t\t\tcase \"reset\":\n\t\t\t\tvar err error\n\t\t\t\tif err := cleanup(); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\topts, err = parseOpts(td)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\n\t\t\t\td, err = Open(\"\", opts)\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\treturn \"\"\n\t\t\tcase \"populate\":\n\t\t\t\tb := d.NewBatch()\n\t\t\t\trunPopulateCmd(t, td, b)\n\t\t\t\tcount := b.Count()\n\t\t\t\trequire.NoError(t, b.Commit(nil))\n\t\t\t\treturn fmt.Sprintf(\"wrote %d keys\\n\", count)\n\t\t\tcase \"batch\":\n\t\t\t\tvar name string\n\t\t\t\ttd.MaybeScanArgs(t, \"name\", &name)\n\t\t\t\tcommit := td.HasArg(\"commit\")\n\t\t\t\tb := d.NewIndexedBatch()\n\t\t\t\trequire.NoError(t, runBatchDefineCmd(td, b))\n\t\t\t\tvar err error\n\t\t\t\tif commit {\n\t\t\t\t\tfunc() {\n\t\t\t\t\t\tdefer func() {\n\t\t\t\t\t\t\tif r := recover(); r != nil {\n\t\t\t\t\t\t\t\terr = errors.New(r.(string))\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}()\n\t\t\t\t\t\terr = b.Commit(nil)\n\t\t\t\t\t}()\n\t\t\t\t} else if name != \"\" {\n\t\t\t\t\tbatches[name] = b\n\t\t\t\t}\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\tcount := b.Count()\n\t\t\t\tif commit {\n\t\t\t\t\treturn fmt.Sprintf(\"committed %d keys\\n\", count)\n\t\t\t\t}\n\t\t\t\treturn fmt.Sprintf(\"wrote %d keys to batch %q\\n\", count, name)\n\t\t\tcase \"compact\":\n\t\t\t\tif err := runCompactCmd(td, d); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\treturn runLSMCmd(td, d)\n\t\t\tcase \"flush\":\n\t\t\t\terr := d.Flush()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\treturn \"\"\n\t\t\tcase \"disable-flushes\":\n\t\t\t\td.mu.Lock()\n\t\t\t\td.mu.compact.flushing = true\n\t\t\t\td.mu.Unlock()\n\t\t\t\treturn \"\"\n\t\t\tcase \"enable-flushes\":\n\t\t\t\td.mu.Lock()\n\t\t\t\td.mu.compact.flushing = false\n\t\t\t\td.mu.Unlock()\n\t\t\t\treturn \"\"\n\t\t\tcase \"get\":\n\t\t\t\tvar reader Reader = d\n\t\t\t\tif arg, ok := td.Arg(\"reader\"); ok {\n\t\t\t\t\tif reader, ok = batches[arg.Vals[0]]; !ok {\n\t\t\t\t\t\treturn fmt.Sprintf(\"unknown reader %q\", arg.Vals[0])\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfor _, l := range strings.Split(td.Input, \"\\n\") {\n\t\t\t\t\tv, closer, err := reader.Get([]byte(l))\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tfmt.Fprintf(&buf, \"%s: error: %s\\n\", l, err)\n\t\t\t\t\t} else {\n\t\t\t\t\t\tfmt.Fprintf(&buf, \"%s: %s\\n\", l, v)\n\t\t\t\t\t}\n\t\t\t\t\tif err := closer.Close(); err != nil {\n\t\t\t\t\t\tfmt.Fprintf(&buf, \"close err: %s\\n\", err)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn buf.String()\n\t\t\tcase \"build\":\n\t\t\t\tif err := runBuildCmd(td, d, d.opts.FS); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\treturn \"\"\n\t\t\tcase \"ingest-existing\":\n\t\t\t\tif err := runIngestCmd(td, d, d.opts.FS); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\treturn \"\"\n\t\t\tcase \"ingest\":\n\t\t\t\tif err := runBuildCmd(td, d, d.opts.FS); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\tif err := runIngestCmd(td, d, d.opts.FS); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\treturn \"\"\n\t\t\tcase \"layout\":\n\t\t\t\tvar verbose bool\n\t\t\t\tvar filename string\n\t\t\t\ttd.ScanArgs(t, \"filename\", &filename)\n\t\t\t\ttd.MaybeScanArgs(t, \"verbose\", &verbose)\n\t\t\t\tf, err := opts.FS.Open(filename)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\treadable, err := sstable.NewSimpleReadable(f)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\tr, err := sstable.NewReader(context.Background(), readable, opts.MakeReaderOptions())\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\tdefer r.Close()\n\t\t\t\tl, err := r.Layout()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\treturn l.Describe(verbose, r, nil)\n\t\t\tcase \"lsm\":\n\t\t\t\treturn runLSMCmd(td, d)\n\t\t\tcase \"metrics\":\n\t\t\t\td.mu.Lock()\n\t\t\t\td.waitTableStats()\n\t\t\t\td.mu.Unlock()\n\t\t\t\tm := d.Metrics()\n\t\t\t\treturn fmt.Sprintf(\"Metrics.Keys.RangeKeySetsCount = %d\\n\", m.Keys.RangeKeySetsCount)\n\t\t\tcase \"mutate\":\n\t\t\t\tvar batchName string\n\t\t\t\ttd.ScanArgs(t, \"batch\", &batchName)\n\t\t\t\tmut := newBatch(d)\n\t\t\t\tif err := runBatchDefineCmd(td, mut); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\tif err := batches[batchName].Apply(mut, nil); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\treturn \"\"\n\t\t\tcase \"clone\":\n\t\t\t\tvar from, to string\n\t\t\t\tvar cloneOpts CloneOptions\n\t\t\t\tvar iterOpts IterOptions\n\t\t\t\ttd.ScanArgs(t, \"from\", &from)\n\t\t\t\ttd.ScanArgs(t, \"to\", &to)\n\t\t\t\ttd.ScanArgs(t, \"refresh-batch\", &cloneOpts.RefreshBatchView)\n\t\t\t\tfromIter := iters[from]\n\t\t\t\tif foundAny, err := parseIterOptions(&iterOpts, &fromIter.opts, strings.Fields(td.Input)); err != nil {\n\t\t\t\t\treturn fmt.Sprintf(\"clone: %s\", err.Error())\n\t\t\t\t} else if foundAny {\n\t\t\t\t\tcloneOpts.IterOptions = &iterOpts\n\t\t\t\t}\n\t\t\t\tvar err error\n\t\t\t\titers[to], err = fromIter.Clone(cloneOpts)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\treturn \"\"\n\t\t\tcase \"commit\":\n\t\t\t\tname := pluckStringCmdArg(td, \"batch\")\n\t\t\t\tb := batches[name]\n\t\t\t\tdefer b.Close()\n\t\t\t\tcount := b.Count()\n\t\t\t\trequire.NoError(t, d.Apply(b, nil))\n\t\t\t\tdelete(batches, name)\n\t\t\t\treturn fmt.Sprintf(\"committed %d keys\\n\", count)\n\t\t\tcase \"combined-iter\":\n\t\t\t\to := &IterOptions{KeyTypes: IterKeyTypePointsAndRanges}\n\t\t\t\tvar reader Reader = d\n\t\t\t\tvar name string\n\t\t\t\tfor _, arg := range td.CmdArgs {\n\t\t\t\t\tswitch arg.Key {\n\t\t\t\t\tcase \"mask-suffix\":\n\t\t\t\t\t\to.RangeKeyMasking.Suffix = []byte(arg.Vals[0])\n\t\t\t\t\tcase \"mask-filter\":\n\t\t\t\t\t\to.RangeKeyMasking.Filter = func() BlockPropertyFilterMask {\n\t\t\t\t\t\t\treturn sstable.NewTestKeysMaskingFilter()\n\t\t\t\t\t\t}\n\t\t\t\t\tcase \"lower\":\n\t\t\t\t\t\to.LowerBound = []byte(arg.Vals[0])\n\t\t\t\t\tcase \"upper\":\n\t\t\t\t\t\to.UpperBound = []byte(arg.Vals[0])\n\t\t\t\t\tcase \"name\":\n\t\t\t\t\t\tname = arg.Vals[0]\n\t\t\t\t\tcase \"reader\":\n\t\t\t\t\t\treader = batches[arg.Vals[0]]\n\t\t\t\t\t\tif reader == nil {\n\t\t\t\t\t\t\treturn fmt.Sprintf(\"unknown reader %q\", arg.Vals[0])\n\t\t\t\t\t\t}\n\t\t\t\t\tcase \"point-key-filter\":\n\t\t\t\t\t\tif len(arg.Vals) != 2 {\n\t\t\t\t\t\t\treturn fmt.Sprintf(\"blockprop-filter expects 2 arguments, received %d\", len(arg.Vals))\n\t\t\t\t\t\t}\n\t\t\t\t\t\tmin, err := strconv.ParseUint(arg.Vals[0], 10, 64)\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t\t}\n\t\t\t\t\t\tmax, err := strconv.ParseUint(arg.Vals[1], 10, 64)\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t\t}\n\t\t\t\t\t\to.PointKeyFilters = []sstable.BlockPropertyFilter{\n\t\t\t\t\t\t\tsstable.NewTestKeysBlockPropertyFilter(min, max),\n\t\t\t\t\t\t}\n\t\t\t\t\t\to.SkipPoint = func(k []byte) bool {\n\t\t\t\t\t\t\ti := testkeys.Comparer.Split(k)\n\t\t\t\t\t\t\tif i == len(k) {\n\t\t\t\t\t\t\t\treturn false\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tv, err := testkeys.ParseSuffix(k[i:])\n\t\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\t\treturn false\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\treturn uint64(v) < min || uint64(v) >= max\n\t\t\t\t\t\t}\n\t\t\t\t\tcase \"snapshot\":\n\t\t\t\t\t\ts := base.ParseSeqNum(arg.Vals[0])\n\t\t\t\t\t\tfunc() {\n\t\t\t\t\t\t\td.mu.Lock()\n\t\t\t\t\t\t\tdefer d.mu.Unlock()\n\t\t\t\t\t\t\tl := &d.mu.snapshots\n\t\t\t\t\t\t\tfor i := l.root.next; i != &l.root; i = i.next {\n\t\t\t\t\t\t\t\tif i.seqNum == s {\n\t\t\t\t\t\t\t\t\treader = i\n\t\t\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}()\n\t\t\t\t\tcase \"use-l6-filter\":\n\t\t\t\t\t\to.UseL6Filters = true\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tvar iter *Iterator\n\t\t\t\tvar err error\n\t\t\t\tfunc() {\n\t\t\t\t\tdefer func() {\n\t\t\t\t\t\tif r := recover(); r != nil {\n\t\t\t\t\t\t\tswitch v := r.(type) {\n\t\t\t\t\t\t\tcase string:\n\t\t\t\t\t\t\t\terr = errors.New(v)\n\t\t\t\t\t\t\tcase error:\n\t\t\t\t\t\t\t\terr = v\n\t\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t\t\tpanic(r)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}()\n\t\t\t\t\titer = newIter(name, reader, o)\n\t\t\t\t}()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\treturn runIterCmd(td, iter, name == \"\" /* close iter */)\n\t\t\tcase \"rangekey-iter\":\n\t\t\t\tname := pluckStringCmdArg(td, \"name\")\n\t\t\t\titer := newIter(name, d, &IterOptions{KeyTypes: IterKeyTypeRangesOnly})\n\t\t\t\treturn runIterCmd(td, iter, name == \"\" /* close iter */)\n\t\t\tcase \"scan-rangekeys\":\n\t\t\t\titer := newIter(\n\t\t\t\t\tpluckStringCmdArg(td, \"name\"),\n\t\t\t\t\td,\n\t\t\t\t\t&IterOptions{KeyTypes: IterKeyTypeRangesOnly},\n\t\t\t\t)\n\t\t\t\tfunc() {\n\t\t\t\t\tdefer iter.Close()\n\t\t\t\t\tfor iter.First(); iter.Valid(); iter.Next() {\n\t\t\t\t\t\tstart, end := iter.RangeBounds()\n\t\t\t\t\t\tfmt.Fprintf(&buf, \"[%s, %s)\\n\", start, end)\n\t\t\t\t\t\twriteRangeKeys(&buf, iter)\n\t\t\t\t\t\tfmt.Fprintln(&buf)\n\t\t\t\t\t}\n\t\t\t\t}()\n\t\t\t\treturn buf.String()\n\t\t\tcase \"iter\":\n\t\t\t\tvar name string\n\t\t\t\ttd.ScanArgs(t, \"iter\", &name)\n\t\t\t\treturn runIterCmd(td, iters[name], false /* close iter */)\n\t\t\tcase \"wait-table-stats\":\n\t\t\t\td.mu.Lock()\n\t\t\t\td.waitTableStats()\n\t\t\t\td.mu.Unlock()\n\t\t\t\treturn \"\"\n\t\t\tdefault:\n\t\t\t\treturn fmt.Sprintf(\"unknown command %q\", td.Cmd)\n\t\t\t}\n\t\t})\n\t})\n}\n\nfunc pluckStringCmdArg(td *datadriven.TestData, key string) string {\n\tif arg, ok := td.Arg(key); ok {\n\t\treturn arg.Vals[0]\n\t}\n\treturn \"\"\n}\n"
        },
        {
          "name": "iterator_test.go",
          "type": "blob",
          "size": 86.64453125,
          "content": "// Copyright 2013 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/binary\"\n\t\"flag\"\n\t\"fmt\"\n\t\"io\"\n\t\"math/rand/v2\"\n\t\"runtime\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync/atomic\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/datadriven\"\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/bytealloc\"\n\t\"github.com/cockroachdb/pebble/internal/cache\"\n\t\"github.com/cockroachdb/pebble/internal/invalidating\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n\t\"github.com/cockroachdb/pebble/internal/testkeys\"\n\t\"github.com/cockroachdb/pebble/objstorage/objstorageprovider\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/stretchr/testify/require\"\n\t\"golang.org/x/sync/errgroup\"\n)\n\nvar testKeyValuePairs = []string{\n\t\"10:10\",\n\t\"11:11\",\n\t\"12:12\",\n\t\"13:13\",\n\t\"14:14\",\n\t\"15:15\",\n\t\"16:16\",\n\t\"17:17\",\n\t\"18:18\",\n\t\"19:19\",\n}\n\n// testIterator tests creating a combined iterator from a number of sub-\n// iterators. newFunc is a constructor function. splitFunc returns a random\n// split of the testKeyValuePairs slice such that walking a combined iterator\n// over those splits should recover the original key/value pairs in order.\nfunc testIterator(\n\tt *testing.T,\n\tnewFunc func(...internalIterator) internalIterator,\n\tsplitFunc func(r *rand.Rand) [][]string,\n) {\n\tfakeIterWithCloseErr := func(kvs []base.InternalKV, errorMsg string) *base.FakeIter {\n\t\tf := base.NewFakeIter(kvs)\n\t\tf.SetCloseErr(errors.New(errorMsg))\n\t\treturn f\n\t}\n\t// Test pre-determined sub-iterators. The sub-iterators are designed\n\t// so that the combined key/value pair order is the same whether the\n\t// combined iterator is concatenating or merging.\n\ttestCases := []struct {\n\t\tdesc  string\n\t\titers []internalIterator\n\t\twant  string\n\t}{\n\t\t{\n\t\t\t\"one sub-iterator\",\n\t\t\t[]internalIterator{\n\t\t\t\tbase.NewFakeIter(base.FakeKVs(\"e:1\", \"w:2\")),\n\t\t\t},\n\t\t\t\"<e:1><w:2>.\",\n\t\t},\n\t\t{\n\t\t\t\"two sub-iterators\",\n\t\t\t[]internalIterator{\n\t\t\t\tbase.NewFakeIter(base.FakeKVs(\"a0:0\")),\n\t\t\t\tbase.NewFakeIter(base.FakeKVs(\"b1:1\", \"b2:2\")),\n\t\t\t},\n\t\t\t\"<a0:0><b1:1><b2:2>.\",\n\t\t},\n\t\t{\n\t\t\t\"empty sub-iterators\",\n\t\t\t[]internalIterator{\n\t\t\t\tbase.NewFakeIter(nil),\n\t\t\t\tbase.NewFakeIter(nil),\n\t\t\t\tbase.NewFakeIter(nil),\n\t\t\t},\n\t\t\t\".\",\n\t\t},\n\t\t{\n\t\t\t\"sub-iterator errors\",\n\t\t\t[]internalIterator{\n\t\t\t\tbase.NewFakeIter(base.FakeKVs(\"a0:0\", \"a1:1\")),\n\t\t\t\tfakeIterWithCloseErr(base.FakeKVs(\"b2:2\", \"b3:3\", \"b4:4\"), \"the sky is falling\"),\n\t\t\t\tfakeIterWithCloseErr(base.FakeKVs(\"c5:5\", \"c6:6\"), \"run for your lives\"),\n\t\t\t},\n\t\t\t\"<a0:0><a1:1><b2:2><b3:3><b4:4>err=the sky is falling\",\n\t\t},\n\t}\n\tfor _, tc := range testCases {\n\t\tvar b bytes.Buffer\n\t\titer := invalidating.NewIter(newFunc(tc.iters...))\n\t\tfor kv := iter.First(); kv != nil; kv = iter.Next() {\n\t\t\tfmt.Fprintf(&b, \"<%s:%d>\", kv.K.UserKey, kv.SeqNum())\n\t\t}\n\t\tif err := iter.Close(); err != nil {\n\t\t\tfmt.Fprintf(&b, \"err=%v\", err)\n\t\t} else {\n\t\t\tb.WriteByte('.')\n\t\t}\n\t\tif got := b.String(); got != tc.want {\n\t\t\tt.Errorf(\"%s:\\ngot  %q\\nwant %q\", tc.desc, got, tc.want)\n\t\t}\n\t}\n\n\t// Test randomly generated sub-iterators.\n\tr := rand.New(rand.NewPCG(0, 0))\n\tfor i, nBad := 0, 0; i < 1000; i++ {\n\t\tbad := false\n\n\t\tsplits := splitFunc(r)\n\t\titers := make([]internalIterator, len(splits))\n\t\tfor i, split := range splits {\n\t\t\titers[i] = base.NewFakeIter(base.FakeKVs(split...))\n\t\t}\n\t\titer := invalidating.NewIter(newFunc(iters...))\n\t\tkv := iter.First()\n\t\tj := 0\n\t\tfor ; kv != nil && j < len(testKeyValuePairs); j++ {\n\t\t\tgot := fmt.Sprintf(\"%s:%d\", kv.K.UserKey, kv.SeqNum())\n\t\t\twant := testKeyValuePairs[j]\n\t\t\tif got != want {\n\t\t\t\tbad = true\n\t\t\t\tt.Errorf(\"random splits: i=%d, j=%d: got %q, want %q\", i, j, got, want)\n\t\t\t}\n\t\t\tkv = iter.Next()\n\t\t}\n\t\tif kv != nil {\n\t\t\tbad = true\n\t\t\tt.Errorf(\"random splits: i=%d, j=%d: iter was not exhausted\", i, j)\n\t\t}\n\t\tif j != len(testKeyValuePairs) {\n\t\t\tbad = true\n\t\t\tt.Errorf(\"random splits: i=%d, j=%d: want j=%d\", i, j, len(testKeyValuePairs))\n\t\t\treturn\n\t\t}\n\t\tif err := iter.Close(); err != nil {\n\t\t\tbad = true\n\t\t\tt.Errorf(\"random splits: i=%d, j=%d: %v\", i, j, err)\n\t\t}\n\n\t\tif bad {\n\t\t\tnBad++\n\t\t\tif nBad == 10 {\n\t\t\t\tt.Fatal(\"random splits: too many errors; stopping\")\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestIterator(t *testing.T) {\n\tvar merge Merge\n\tvar kvs []base.InternalKV\n\n\tnewIter := func(seqNum base.SeqNum, opts IterOptions) *Iterator {\n\t\tif merge == nil {\n\t\t\tmerge = DefaultMerger.Merge\n\t\t}\n\t\twrappedMerge := func(key, value []byte) (ValueMerger, error) {\n\t\t\tif len(key) == 0 {\n\t\t\t\tt.Fatalf(\"an empty key is passed into Merge\")\n\t\t\t}\n\t\t\treturn merge(key, value)\n\t\t}\n\t\tit := &Iterator{\n\t\t\topts:     opts,\n\t\t\tcomparer: *testkeys.Comparer,\n\t\t\tmerge:    wrappedMerge,\n\t\t}\n\t\t// NB: Use a mergingIter to filter entries newer than seqNum.\n\t\tfakeIter := base.NewFakeIter(kvs)\n\t\tfakeIter.SetBounds(opts.GetLowerBound(), opts.GetUpperBound())\n\t\titer := newMergingIter(nil /* logger */, &it.stats.InternalStats, it.cmp, it.comparer.Split, fakeIter)\n\t\titer.snapshot = seqNum\n\t\t// NB: This Iterator cannot be cloned since it is not constructed\n\t\t// with a readState. It suffices for this test.\n\t\tit.iter = invalidating.NewIter(iter)\n\t\treturn it\n\t}\n\n\tdatadriven.RunTest(t, \"testdata/iterator\", func(t *testing.T, d *datadriven.TestData) string {\n\t\tswitch d.Cmd {\n\t\tcase \"define\":\n\t\t\tmerge = nil\n\t\t\tif arg, ok := d.Arg(\"merger\"); ok && len(arg.Vals[0]) > 0 && arg.Vals[0] == \"deletable\" {\n\t\t\t\tmerge = base.NewDeletableSumValueMerger\n\t\t\t}\n\t\t\tkvs = kvs[:0]\n\t\t\tfor _, key := range strings.Split(d.Input, \"\\n\") {\n\t\t\t\tj := strings.Index(key, \":\")\n\t\t\t\tkvs = append(kvs, base.MakeInternalKV(base.ParseInternalKey(key[:j]), []byte(key[j+1:])))\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"iter\":\n\t\t\tvar seqNum uint64\n\t\t\tvar opts IterOptions\n\t\t\td.MaybeScanArgs(t, \"seq\", &seqNum)\n\t\t\tvar lower, upper string\n\t\t\tif d.MaybeScanArgs(t, \"lower\", &lower) {\n\t\t\t\topts.LowerBound = []byte(lower)\n\t\t\t}\n\t\t\tif d.MaybeScanArgs(t, \"upper\", &upper) {\n\t\t\t\topts.UpperBound = []byte(upper)\n\t\t\t}\n\n\t\t\titer := newIter(base.SeqNum(seqNum), opts)\n\t\t\titerOutput := runIterCmd(d, iter, true)\n\t\t\tstats := iter.Stats()\n\t\t\treturn fmt.Sprintf(\"%sstats: %s\\n\", iterOutput, stats.String())\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", d.Cmd)\n\t\t}\n\t})\n}\n\ntype minSeqNumPropertyCollector struct {\n\tminSeqNum base.SeqNum\n}\n\nvar _ BlockPropertyCollector = (*minSeqNumPropertyCollector)(nil)\n\nfunc (c *minSeqNumPropertyCollector) Name() string {\n\treturn \"minSeqNumPropertyCollector\"\n}\n\nfunc (c *minSeqNumPropertyCollector) AddPointKey(key InternalKey, value []byte) error {\n\tif c.minSeqNum == 0 || c.minSeqNum > key.SeqNum() {\n\t\tc.minSeqNum = key.SeqNum()\n\t}\n\treturn nil\n}\n\nfunc (c *minSeqNumPropertyCollector) AddRangeKeys(span sstable.Span) error {\n\tfor _, k := range span.Keys {\n\t\tif c.minSeqNum == 0 || c.minSeqNum > k.SeqNum() {\n\t\t\tc.minSeqNum = k.SeqNum()\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (c *minSeqNumPropertyCollector) FinishDataBlock(buf []byte) ([]byte, error) {\n\treturn nil, nil\n}\n\nfunc (c *minSeqNumPropertyCollector) AddPrevDataBlockToIndexBlock() {}\n\nfunc (c *minSeqNumPropertyCollector) FinishIndexBlock(buf []byte) ([]byte, error) {\n\treturn nil, nil\n}\n\nfunc (c *minSeqNumPropertyCollector) FinishTable(buf []byte) ([]byte, error) {\n\treturn binary.AppendUvarint(buf, uint64(c.minSeqNum)), nil\n}\n\nfunc (c *minSeqNumPropertyCollector) AddCollectedWithSuffixReplacement(\n\toldProp []byte, oldSuffix, newSuffix []byte,\n) error {\n\treturn errors.Errorf(\"not implemented\")\n}\n\nfunc (c *minSeqNumPropertyCollector) SupportsSuffixReplacement() bool {\n\treturn false\n}\n\n// minSeqNumFilter is a BlockPropertyFilter that uses the\n// minSeqNumPropertyCollector data to filter out entire tables.\ntype minSeqNumFilter struct {\n\tseqNumUpperBound uint64\n}\n\nvar _ BlockPropertyFilter = (*minSeqNumFilter)(nil)\n\nfunc (*minSeqNumFilter) Name() string {\n\treturn (&minSeqNumPropertyCollector{}).Name()\n}\n\nfunc (f *minSeqNumFilter) Intersects(prop []byte) (bool, error) {\n\t// Blocks will have no data.\n\tif len(prop) == 0 {\n\t\treturn true, nil\n\t}\n\tminSeqNum, n := binary.Uvarint(prop)\n\tif n <= 0 {\n\t\treturn false, errors.Errorf(\"invalid block property data %v\", prop)\n\t}\n\treturn minSeqNum < f.seqNumUpperBound, nil\n}\n\nfunc (f *minSeqNumFilter) SyntheticSuffixIntersects(prop []byte, suffix []byte) (bool, error) {\n\tpanic(\"unimplemented\")\n}\n\nfunc TestReadSampling(t *testing.T) {\n\tvar d *DB\n\tdefer func() {\n\t\tif d != nil {\n\t\t\trequire.NoError(t, d.Close())\n\t\t}\n\t}()\n\n\tvar iter *Iterator\n\tdefer func() {\n\t\tif iter != nil {\n\t\t\trequire.NoError(t, iter.Close())\n\t\t}\n\t}()\n\n\tdatadriven.RunTest(t, \"testdata/iterator_read_sampling\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"define\":\n\t\t\tif iter != nil {\n\t\t\t\tif err := iter.Close(); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t}\n\t\t\tif d != nil {\n\t\t\t\tif err := d.Close(); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t}\n\n\t\t\topts := &Options{}\n\t\t\topts.BlockPropertyCollectors = []func() BlockPropertyCollector{\n\t\t\t\tfunc() BlockPropertyCollector {\n\t\t\t\t\treturn &minSeqNumPropertyCollector{}\n\t\t\t\t},\n\t\t\t}\n\n\t\t\tvar err error\n\t\t\tif d, err = runDBDefineCmd(td, opts); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\n\t\t\td.mu.Lock()\n\t\t\t// Disable the \"dynamic base level\" code for this test.\n\t\t\t// d.mu.versions.picker.forceBaseLevel1()\n\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\td.mu.Unlock()\n\t\t\treturn s\n\n\t\tcase \"set\":\n\t\t\tif d == nil {\n\t\t\t\treturn fmt.Sprintf(\"%s: db is not defined\", td.Cmd)\n\t\t\t}\n\n\t\t\tvar allowedSeeks int64\n\t\t\ttd.ScanArgs(t, \"allowed-seeks\", &allowedSeeks)\n\n\t\t\td.mu.Lock()\n\t\t\tfor _, l := range d.mu.versions.currentVersion().Levels {\n\t\t\t\tl.Slice().Each(func(f *fileMetadata) {\n\t\t\t\t\tf.AllowedSeeks.Store(allowedSeeks)\n\t\t\t\t})\n\t\t\t}\n\t\t\td.mu.Unlock()\n\t\t\treturn \"\"\n\n\t\tcase \"show\":\n\t\t\tif d == nil {\n\t\t\t\treturn fmt.Sprintf(\"%s: db is not defined\", td.Cmd)\n\t\t\t}\n\n\t\t\tvar fileNum int64\n\t\t\tfor _, arg := range td.CmdArgs {\n\t\t\t\tif len(arg.Vals) != 2 {\n\t\t\t\t\treturn fmt.Sprintf(\"%s: %s=<value>\", td.Cmd, arg.Key)\n\t\t\t\t}\n\t\t\t\tswitch arg.Key {\n\t\t\t\tcase \"allowed-seeks\":\n\t\t\t\t\tvar err error\n\t\t\t\t\tfileNum, err = strconv.ParseInt(arg.Vals[0], 10, 64)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tvar foundAllowedSeeks int64 = -1\n\t\t\td.mu.Lock()\n\t\t\tfor _, l := range d.mu.versions.currentVersion().Levels {\n\t\t\t\tl.Slice().Each(func(f *fileMetadata) {\n\t\t\t\t\tif f.FileNum == base.FileNum(fileNum) {\n\t\t\t\t\t\tactualAllowedSeeks := f.AllowedSeeks.Load()\n\t\t\t\t\t\tfoundAllowedSeeks = actualAllowedSeeks\n\t\t\t\t\t}\n\t\t\t\t})\n\t\t\t}\n\t\t\td.mu.Unlock()\n\n\t\t\tif foundAllowedSeeks == -1 {\n\t\t\t\treturn fmt.Sprintf(\"invalid file num: %d\", fileNum)\n\t\t\t}\n\t\t\treturn fmt.Sprintf(\"%d\", foundAllowedSeeks)\n\n\t\tcase \"iter\":\n\t\t\tif iter == nil || iter.iter == nil {\n\t\t\t\t// TODO(peter): runDBDefineCmd doesn't properly update the visible\n\t\t\t\t// sequence number. So we have to use a snapshot with a very large\n\t\t\t\t// sequence number, otherwise the DB appears empty.\n\t\t\t\tsnap := Snapshot{\n\t\t\t\t\tdb:     d,\n\t\t\t\t\tseqNum: base.SeqNumMax,\n\t\t\t\t}\n\t\t\t\titer, _ = snap.NewIter(nil)\n\t\t\t\titer.readSampling.forceReadSampling = true\n\t\t\t}\n\t\t\treturn runIterCmd(td, iter, false)\n\n\t\tcase \"read-compactions\":\n\t\t\tif d == nil {\n\t\t\t\treturn fmt.Sprintf(\"%s: db is not defined\", td.Cmd)\n\t\t\t}\n\n\t\t\td.mu.Lock()\n\t\t\tvar sb strings.Builder\n\t\t\tif d.mu.compact.readCompactions.size == 0 {\n\t\t\t\tsb.WriteString(\"(none)\")\n\t\t\t}\n\t\t\tfor i := 0; i < d.mu.compact.readCompactions.size; i++ {\n\t\t\t\trc := d.mu.compact.readCompactions.at(i)\n\t\t\t\tsb.WriteString(fmt.Sprintf(\"(level: %d, start: %s, end: %s)\\n\", rc.level, string(rc.start), string(rc.end)))\n\t\t\t}\n\t\t\td.mu.Unlock()\n\t\t\treturn sb.String()\n\n\t\tcase \"iter-read-compactions\":\n\t\t\tif iter == nil {\n\t\t\t\treturn fmt.Sprintf(\"%s: iter is not defined\", td.Cmd)\n\t\t\t}\n\n\t\t\tvar sb strings.Builder\n\t\t\tif iter.readSampling.pendingCompactions.size == 0 {\n\t\t\t\tsb.WriteString(\"(none)\")\n\t\t\t}\n\t\t\tfor i := 0; i < iter.readSampling.pendingCompactions.size; i++ {\n\t\t\t\trc := iter.readSampling.pendingCompactions.at(i)\n\t\t\t\tsb.WriteString(fmt.Sprintf(\"(level: %d, start: %s, end: %s)\\n\", rc.level, string(rc.start), string(rc.end)))\n\t\t\t}\n\t\t\treturn sb.String()\n\n\t\tcase \"close-iter\":\n\t\t\tif iter != nil {\n\t\t\t\tif err := iter.Close(); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc TestIteratorTableFilter(t *testing.T) {\n\tvar d *DB\n\tdefer func() {\n\t\tif d != nil {\n\t\t\trequire.NoError(t, d.Close())\n\t\t}\n\t}()\n\n\tdatadriven.RunTest(t, \"testdata/iterator_table_filter\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"define\":\n\t\t\tif d != nil {\n\t\t\t\tif err := d.Close(); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t}\n\n\t\t\topts := &Options{}\n\t\t\topts.BlockPropertyCollectors = []func() BlockPropertyCollector{\n\t\t\t\tfunc() BlockPropertyCollector {\n\t\t\t\t\treturn &minSeqNumPropertyCollector{}\n\t\t\t\t},\n\t\t\t}\n\n\t\t\tvar err error\n\t\t\tif d, err = runDBDefineCmd(td, opts); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\n\t\t\td.mu.Lock()\n\t\t\t// Disable the \"dynamic base level\" code for this test.\n\t\t\td.mu.versions.picker.forceBaseLevel1()\n\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\td.mu.Unlock()\n\t\t\treturn s\n\n\t\tcase \"iter\":\n\t\t\t// We're using an iterator table filter to approximate what is done by\n\t\t\t// snapshots.\n\t\t\titerOpts := &IterOptions{}\n\t\t\tvar filterSeqNum uint64\n\t\t\tif td.MaybeScanArgs(t, \"filter\", &filterSeqNum) {\n\t\t\t\titerOpts.PointKeyFilters = []BlockPropertyFilter{\n\t\t\t\t\t&minSeqNumFilter{seqNumUpperBound: filterSeqNum},\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// TODO(peter): runDBDefineCmd doesn't properly update the visible\n\t\t\t// sequence number. So we have to use a snapshot with a very large\n\t\t\t// sequence number, otherwise the DB appears empty.\n\t\t\tsnap := Snapshot{\n\t\t\t\tdb:     d,\n\t\t\t\tseqNum: base.SeqNumMax,\n\t\t\t}\n\t\t\titer, _ := snap.NewIter(iterOpts)\n\t\t\treturn runIterCmd(td, iter, true)\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc TestIteratorNextPrev(t *testing.T) {\n\tvar mem vfs.FS\n\tvar d *DB\n\tdefer func() {\n\t\trequire.NoError(t, d.Close())\n\t}()\n\n\treset := func() {\n\t\tif d != nil {\n\t\t\trequire.NoError(t, d.Close())\n\t\t}\n\n\t\tmem = vfs.NewMem()\n\t\trequire.NoError(t, mem.MkdirAll(\"ext\", 0755))\n\t\topts := &Options{FS: mem}\n\t\t// Automatic compactions may compact away tombstones from L6, making\n\t\t// some testcases non-deterministic.\n\t\topts.DisableAutomaticCompactions = true\n\t\tvar err error\n\t\td, err = Open(\"\", opts)\n\t\trequire.NoError(t, err)\n\t}\n\treset()\n\n\tdatadriven.RunTest(t, \"testdata/iterator_next_prev\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"reset\":\n\t\t\treset()\n\t\t\treturn \"\"\n\n\t\tcase \"build\":\n\t\t\tif err := runBuildCmd(td, d, mem); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"ingest\":\n\t\t\tif err := runIngestCmd(td, d, mem); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn runLSMCmd(td, d)\n\n\t\tcase \"iter\":\n\t\t\tsnap := Snapshot{\n\t\t\t\tdb:     d,\n\t\t\t\tseqNum: base.SeqNumMax,\n\t\t\t}\n\t\t\tif td.HasArg(\"seq\") {\n\t\t\t\tvar n uint64\n\t\t\t\ttd.ScanArgs(t, \"seq\", &n)\n\t\t\t\tsnap.seqNum = base.SeqNum(n)\n\t\t\t}\n\t\t\titer, _ := snap.NewIter(nil)\n\t\t\treturn runIterCmd(td, iter, true)\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc TestIteratorStats(t *testing.T) {\n\tvar mem vfs.FS\n\tvar d *DB\n\tdefer func() {\n\t\trequire.NoError(t, d.Close())\n\t}()\n\n\treset := func() {\n\t\tif d != nil {\n\t\t\trequire.NoError(t, d.Close())\n\t\t}\n\n\t\tmem = vfs.NewMem()\n\t\trequire.NoError(t, mem.MkdirAll(\"ext\", 0755))\n\t\topts := &Options{Comparer: testkeys.Comparer, FS: mem, FormatMajorVersion: internalFormatNewest}\n\t\t// Automatic compactions may make some testcases non-deterministic.\n\t\topts.DisableAutomaticCompactions = true\n\t\topts.Experimental.EnableColumnarBlocks = func() bool { return true }\n\t\tvar err error\n\t\td, err = Open(\"\", opts)\n\t\trequire.NoError(t, err)\n\t}\n\treset()\n\n\tdatadriven.RunTest(t, \"testdata/iterator_stats\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"reset\":\n\t\t\treset()\n\t\t\treturn \"\"\n\n\t\tcase \"build\":\n\t\t\tif err := runBuildCmd(td, d, mem); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"ingest\":\n\t\t\tif err := runIngestCmd(td, d, mem); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn runLSMCmd(td, d)\n\n\t\tcase \"iter\":\n\t\t\tsnap := Snapshot{\n\t\t\t\tdb:     d,\n\t\t\t\tseqNum: base.SeqNumMax,\n\t\t\t}\n\t\t\ttd.MaybeScanArgs(t, \"seq\", &snap.seqNum)\n\t\t\titer, _ := snap.NewIter(nil)\n\t\t\treturn runIterCmd(td, iter, true)\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t}\n\t})\n}\n\ntype iterSeekOptWrapper struct {\n\tinternalIterator\n\n\tseekGEUsingNext, seekPrefixGEUsingNext *int\n}\n\nfunc (i *iterSeekOptWrapper) SeekGE(key []byte, flags base.SeekGEFlags) *base.InternalKV {\n\tif flags.TrySeekUsingNext() {\n\t\t*i.seekGEUsingNext++\n\t}\n\treturn i.internalIterator.SeekGE(key, flags)\n}\n\nfunc (i *iterSeekOptWrapper) SeekPrefixGE(\n\tprefix, key []byte, flags base.SeekGEFlags,\n) *base.InternalKV {\n\tif flags.TrySeekUsingNext() {\n\t\t*i.seekPrefixGEUsingNext++\n\t}\n\treturn i.internalIterator.SeekPrefixGE(prefix, key, flags)\n}\n\nfunc TestIteratorSeekOpt(t *testing.T) {\n\tvar d *DB\n\tdefer func() {\n\t\trequire.NoError(t, d.Close())\n\t}()\n\tvar iter *Iterator\n\tdefer func() {\n\t\tif iter != nil {\n\t\t\trequire.NoError(t, iter.Close())\n\t\t}\n\t}()\n\tvar seekGEUsingNext, seekPrefixGEUsingNext int\n\n\tdatadriven.RunTest(t, \"testdata/iterator_seek_opt\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"define\":\n\t\t\tif iter != nil {\n\t\t\t\tif err := iter.Close(); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t}\n\t\t\tif d != nil {\n\t\t\t\tif err := d.Close(); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t}\n\t\t\tseekGEUsingNext = 0\n\t\t\tseekPrefixGEUsingNext = 0\n\n\t\t\topts := &Options{}\n\t\t\topts.BlockPropertyCollectors = []func() BlockPropertyCollector{\n\t\t\t\tfunc() BlockPropertyCollector {\n\t\t\t\t\treturn &minSeqNumPropertyCollector{}\n\t\t\t\t},\n\t\t\t}\n\n\t\t\tvar err error\n\t\t\tif d, err = runDBDefineCmd(td, opts); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\n\t\t\td.mu.Lock()\n\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\td.mu.Unlock()\n\t\t\toldNewIters := d.newIters\n\t\t\td.newIters = func(\n\t\t\t\tctx context.Context, file *manifest.FileMetadata, opts *IterOptions,\n\t\t\t\tinternalOpts internalIterOpts, kinds iterKinds) (iterSet, error) {\n\t\t\t\titers, err := oldNewIters(ctx, file, opts, internalOpts, kinds)\n\t\t\t\titers.point = &iterSeekOptWrapper{\n\t\t\t\t\tinternalIterator:      iters.point,\n\t\t\t\t\tseekGEUsingNext:       &seekGEUsingNext,\n\t\t\t\t\tseekPrefixGEUsingNext: &seekPrefixGEUsingNext,\n\t\t\t\t}\n\t\t\t\treturn iters, err\n\t\t\t}\n\t\t\td.opts.Comparer.Split = func(a []byte) int { return len(a) }\n\t\t\treturn s\n\n\t\tcase \"iter\":\n\t\t\tif iter == nil || iter.iter == nil {\n\t\t\t\t// TODO(peter): runDBDefineCmd doesn't properly update the visible\n\t\t\t\t// sequence number. So we have to use a snapshot with a very large\n\t\t\t\t// sequence number, otherwise the DB appears empty.\n\t\t\t\tsnap := Snapshot{\n\t\t\t\t\tdb:     d,\n\t\t\t\t\tseqNum: base.SeqNumMax,\n\t\t\t\t}\n\t\t\t\titer, _ = snap.NewIter(nil)\n\t\t\t\titer.readSampling.forceReadSampling = true\n\t\t\t\titer.comparer.Split = d.opts.Comparer.Split\n\t\t\t\titer.forceEnableSeekOpt = true\n\t\t\t\titer.merging.forceEnableSeekOpt = true\n\t\t\t}\n\t\t\titerOutput := runIterCmd(td, iter, false)\n\t\t\tstats := iter.Stats()\n\t\t\t// InternalStats are non-deterministic since they depend on how data is\n\t\t\t// distributed across memtables and sstables in the DB.\n\t\t\tstats.InternalStats = InternalIteratorStats{}\n\t\t\tvar builder strings.Builder\n\t\t\tfmt.Fprintf(&builder, \"%sstats: %s\\n\", iterOutput, stats.String())\n\t\t\tfmt.Fprintf(&builder, \"SeekGEs with trySeekUsingNext: %d\\n\", seekGEUsingNext)\n\t\t\tfmt.Fprintf(&builder, \"SeekPrefixGEs with trySeekUsingNext: %d\\n\", seekPrefixGEUsingNext)\n\t\t\treturn builder.String()\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t}\n\t})\n}\n\ntype errorSeekIter struct {\n\tinternalIterator\n\t// Fields controlling error injection for seeks.\n\tinjectSeekErrorCounts []int\n\tseekCount             int\n\terr                   error\n}\n\nfunc (i *errorSeekIter) SeekGE(key []byte, flags base.SeekGEFlags) *base.InternalKV {\n\tif i.tryInjectError() {\n\t\treturn nil\n\t}\n\ti.err = nil\n\ti.seekCount++\n\treturn i.internalIterator.SeekGE(key, flags)\n}\n\nfunc (i *errorSeekIter) SeekPrefixGE(prefix, key []byte, flags base.SeekGEFlags) *base.InternalKV {\n\tif i.tryInjectError() {\n\t\treturn nil\n\t}\n\ti.err = nil\n\ti.seekCount++\n\treturn i.internalIterator.SeekPrefixGE(prefix, key, flags)\n}\n\nfunc (i *errorSeekIter) SeekLT(key []byte, flags base.SeekLTFlags) *base.InternalKV {\n\tif i.tryInjectError() {\n\t\treturn nil\n\t}\n\ti.err = nil\n\ti.seekCount++\n\treturn i.internalIterator.SeekLT(key, flags)\n}\n\nfunc (i *errorSeekIter) tryInjectError() bool {\n\tif len(i.injectSeekErrorCounts) > 0 && i.injectSeekErrorCounts[0] == i.seekCount {\n\t\ti.seekCount++\n\t\ti.err = errors.Errorf(\"injecting error\")\n\t\ti.injectSeekErrorCounts = i.injectSeekErrorCounts[1:]\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc (i *errorSeekIter) First() *base.InternalKV {\n\ti.err = nil\n\treturn i.internalIterator.First()\n}\n\nfunc (i *errorSeekIter) Last() *base.InternalKV {\n\ti.err = nil\n\treturn i.internalIterator.Last()\n}\n\nfunc (i *errorSeekIter) Next() *base.InternalKV {\n\tif i.err != nil {\n\t\treturn nil\n\t}\n\treturn i.internalIterator.Next()\n}\n\nfunc (i *errorSeekIter) Prev() *base.InternalKV {\n\tif i.err != nil {\n\t\treturn nil\n\t}\n\treturn i.internalIterator.Prev()\n}\n\nfunc (i *errorSeekIter) Error() error {\n\tif i.err != nil {\n\t\treturn i.err\n\t}\n\treturn i.internalIterator.Error()\n}\n\nfunc TestIteratorSeekOptErrors(t *testing.T) {\n\tvar kvs []base.InternalKV\n\n\tvar errorIter errorSeekIter\n\tnewIter := func(opts IterOptions) *Iterator {\n\t\titer := base.NewFakeIter(kvs)\n\t\titer.SetBounds(opts.GetLowerBound(), opts.GetUpperBound())\n\t\terrorIter = errorSeekIter{internalIterator: invalidating.NewIter(iter)}\n\t\t// NB: This Iterator cannot be cloned since it is not constructed\n\t\t// with a readState. It suffices for this test.\n\t\treturn &Iterator{\n\t\t\topts:     opts,\n\t\t\tcomparer: *testkeys.Comparer,\n\t\t\tmerge:    DefaultMerger.Merge,\n\t\t\titer:     &errorIter,\n\t\t}\n\t}\n\n\tdatadriven.RunTest(t, \"testdata/iterator_seek_opt_errors\", func(t *testing.T, d *datadriven.TestData) string {\n\t\tswitch d.Cmd {\n\t\tcase \"define\":\n\t\t\tkvs = kvs[:0]\n\t\t\tfor _, key := range strings.Split(d.Input, \"\\n\") {\n\t\t\t\tj := strings.Index(key, \":\")\n\t\t\t\tkvs = append(kvs, base.MakeInternalKV(base.ParseInternalKey(key[:j]), []byte(key[j+1:])))\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"iter\":\n\t\t\tvar opts IterOptions\n\t\t\tvar injectSeekGEErrorCounts []int\n\t\t\tfor _, arg := range d.CmdArgs {\n\t\t\t\tif len(arg.Vals) < 1 {\n\t\t\t\t\treturn fmt.Sprintf(\"%s: %s=<value>\", d.Cmd, arg.Key)\n\t\t\t\t}\n\t\t\t\tswitch arg.Key {\n\t\t\t\tcase \"lower\":\n\t\t\t\t\topts.LowerBound = []byte(arg.Vals[0])\n\t\t\t\tcase \"upper\":\n\t\t\t\t\topts.UpperBound = []byte(arg.Vals[0])\n\t\t\t\tcase \"seek-error\":\n\t\t\t\t\tfor i := 0; i < len(arg.Vals); i++ {\n\t\t\t\t\t\tn, err := strconv.Atoi(arg.Vals[i])\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t\t}\n\t\t\t\t\t\tinjectSeekGEErrorCounts = append(injectSeekGEErrorCounts, n)\n\t\t\t\t\t}\n\t\t\t\tdefault:\n\t\t\t\t\treturn fmt.Sprintf(\"%s: unknown arg: %s\", d.Cmd, arg.Key)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\titer := newIter(opts)\n\t\t\terrorIter.injectSeekErrorCounts = injectSeekGEErrorCounts\n\t\t\treturn runIterCmd(d, iter, true)\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", d.Cmd)\n\t\t}\n\t})\n}\n\ntype testBlockIntervalMapper struct {\n\tnumLength     int\n\toffsetFromEnd int\n}\n\nvar _ sstable.IntervalMapper = (*testBlockIntervalMapper)(nil)\n\nfunc (bi *testBlockIntervalMapper) MapPointKey(\n\tkey InternalKey, value []byte,\n) (sstable.BlockInterval, error) {\n\tk := key.UserKey\n\tif len(k) < bi.numLength+bi.offsetFromEnd {\n\t\treturn sstable.BlockInterval{}, nil\n\t}\n\tn := len(k) - bi.offsetFromEnd - bi.numLength\n\tval, err := strconv.Atoi(string(k[n : n+bi.numLength]))\n\tif err != nil {\n\t\treturn sstable.BlockInterval{}, err\n\t}\n\tif val < 0 {\n\t\tpanic(\"testBlockIntervalMapper expects values >= 0\")\n\t}\n\tuval := uint64(val)\n\treturn sstable.BlockInterval{Lower: uval, Upper: uval + 1}, nil\n}\n\nfunc (bi *testBlockIntervalMapper) MapRangeKeys(span sstable.Span) (sstable.BlockInterval, error) {\n\treturn sstable.BlockInterval{}, nil\n}\nfunc TestIteratorBlockIntervalFilter(t *testing.T) {\n\tvar mem vfs.FS\n\tvar d *DB\n\tdefer func() {\n\t\trequire.NoError(t, d.Close())\n\t}()\n\n\ttype collector struct {\n\t\tid     uint16\n\t\toffset int\n\t}\n\tcreateDB := func(collectors []collector) {\n\t\tif d != nil {\n\t\t\trequire.NoError(t, d.Close())\n\t\t}\n\n\t\tmem = vfs.NewMem()\n\t\trequire.NoError(t, mem.MkdirAll(\"ext\", 0755))\n\n\t\tvar bpCollectors []func() BlockPropertyCollector\n\t\tfor _, c := range collectors {\n\t\t\tcoll := c\n\t\t\tbpCollectors = append(bpCollectors, func() BlockPropertyCollector {\n\t\t\t\treturn sstable.NewBlockIntervalCollector(\n\t\t\t\t\tfmt.Sprintf(\"%d\", coll.id),\n\t\t\t\t\t&testBlockIntervalMapper{numLength: 2, offsetFromEnd: coll.offset},\n\t\t\t\t\tnil, /* range key collector */\n\t\t\t\t)\n\t\t\t})\n\t\t}\n\t\topts := &Options{\n\t\t\tFS:                      mem,\n\t\t\tFormatMajorVersion:      internalFormatNewest,\n\t\t\tBlockPropertyCollectors: bpCollectors,\n\t\t}\n\t\tlo := LevelOptions{BlockSize: 1, IndexBlockSize: 1}\n\t\topts.Levels = append(opts.Levels, lo)\n\n\t\t// Automatic compactions may compact away tombstones from L6, making\n\t\t// some testcases non-deterministic.\n\t\topts.DisableAutomaticCompactions = true\n\t\tvar err error\n\t\td, err = Open(\"\", opts)\n\t\trequire.NoError(t, err)\n\t}\n\n\tdatadriven.RunTest(\n\t\tt, \"testdata/iterator_block_interval_filter\", func(t *testing.T, td *datadriven.TestData) string {\n\t\t\tswitch td.Cmd {\n\t\t\tcase \"build\":\n\t\t\t\tvar collectors []collector\n\t\t\t\tfor _, arg := range td.CmdArgs {\n\t\t\t\t\tswitch arg.Key {\n\t\t\t\t\tcase \"id_offset\":\n\t\t\t\t\t\tif len(arg.Vals) != 2 {\n\t\t\t\t\t\t\treturn \"id and offset not provided\"\n\t\t\t\t\t\t}\n\t\t\t\t\t\tvar id, offset int\n\t\t\t\t\t\tvar err error\n\t\t\t\t\t\tif id, err = strconv.Atoi(arg.Vals[0]); err != nil {\n\t\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif offset, err = strconv.Atoi(arg.Vals[1]); err != nil {\n\t\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t\t}\n\t\t\t\t\t\tcollectors = append(collectors, collector{id: uint16(id), offset: offset})\n\t\t\t\t\tdefault:\n\t\t\t\t\t\treturn fmt.Sprintf(\"unknown key: %s\", arg.Key)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tcreateDB(collectors)\n\t\t\t\tb := d.NewBatch()\n\t\t\t\tif err := runBatchDefineCmd(td, b); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\tif err := b.Commit(nil); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\tif err := d.Flush(); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\treturn runLSMCmd(td, d)\n\n\t\t\tcase \"iter\":\n\t\t\t\tvar opts IterOptions\n\t\t\t\tfor _, arg := range td.CmdArgs {\n\t\t\t\t\tswitch arg.Key {\n\t\t\t\t\tcase \"id_lower_upper\":\n\t\t\t\t\t\tif len(arg.Vals) != 3 {\n\t\t\t\t\t\t\treturn \"id, lower, upper not provided\"\n\t\t\t\t\t\t}\n\t\t\t\t\t\tvar id, lower, upper int\n\t\t\t\t\t\tvar err error\n\t\t\t\t\t\tif id, err = strconv.Atoi(arg.Vals[0]); err != nil {\n\t\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif lower, err = strconv.Atoi(arg.Vals[1]); err != nil {\n\t\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif upper, err = strconv.Atoi(arg.Vals[2]); err != nil {\n\t\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t\t}\n\t\t\t\t\t\topts.PointKeyFilters = append(opts.PointKeyFilters,\n\t\t\t\t\t\t\tsstable.NewBlockIntervalFilter(fmt.Sprintf(\"%d\", id), uint64(lower), uint64(upper), nil))\n\t\t\t\t\tdefault:\n\t\t\t\t\t\treturn fmt.Sprintf(\"unknown key: %s\", arg.Key)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\trand.Shuffle(len(opts.PointKeyFilters), func(i, j int) {\n\t\t\t\t\topts.PointKeyFilters[i], opts.PointKeyFilters[j] =\n\t\t\t\t\t\topts.PointKeyFilters[j], opts.PointKeyFilters[i]\n\t\t\t\t})\n\t\t\t\titer, _ := d.NewIter(&opts)\n\t\t\t\treturn runIterCmd(td, iter, true)\n\n\t\t\tdefault:\n\t\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t\t}\n\t\t})\n}\n\nvar seed = flag.Uint64(\"seed\", 0, \"a pseudorandom number generator seed\")\n\nfunc randStr(fill []byte, rng *rand.Rand) {\n\tconst letters = \"abcdefghijklmnopqrstuvwxyz\"\n\tconst lettersLen = len(letters)\n\tfor i := 0; i < len(fill); i++ {\n\t\tfill[i] = letters[rng.IntN(lettersLen)]\n\t}\n}\n\nfunc randValue(n int, rng *rand.Rand) []byte {\n\tbuf := make([]byte, n)\n\trandStr(buf, rng)\n\treturn buf\n}\n\nfunc randKey(n int, rng *rand.Rand) ([]byte, int) {\n\tkeyPrefix := randValue(n, rng)\n\tsuffix := rng.IntN(100)\n\treturn append(keyPrefix, []byte(fmt.Sprintf(\"%02d\", suffix))...), suffix\n}\n\nfunc TestIteratorRandomizedBlockIntervalFilter(t *testing.T) {\n\tmem := vfs.NewMem()\n\topts := &Options{\n\t\tFS:                 mem,\n\t\tFormatMajorVersion: internalFormatNewest,\n\t\tBlockPropertyCollectors: []func() BlockPropertyCollector{\n\t\t\tfunc() BlockPropertyCollector {\n\t\t\t\treturn sstable.NewBlockIntervalCollector(\n\t\t\t\t\t\"0\", &testBlockIntervalMapper{numLength: 2}, nil, /* suffixReplacer */\n\t\t\t\t)\n\t\t\t},\n\t\t},\n\t}\n\tseed := *seed\n\tif seed == 0 {\n\t\tseed = uint64(time.Now().UnixNano())\n\t\tt.Logf(\"seed: %d\", seed)\n\t}\n\trng := rand.New(rand.NewPCG(seed, seed))\n\topts.FlushSplitBytes = 1 << rng.IntN(8)            // 1B - 256B\n\topts.L0CompactionThreshold = 1 << rng.IntN(2)      // 1-2\n\topts.L0CompactionFileThreshold = 1 << rng.IntN(11) // 1-1024\n\topts.LBaseMaxBytes = 1 << rng.IntN(11)             // 1B - 1KB\n\topts.MemTableSize = 2 << 10                        // 2KB\n\tvar lopts LevelOptions\n\tlopts.BlockSize = 1 << rng.IntN(8)      // 1B - 256B\n\tlopts.IndexBlockSize = 1 << rng.IntN(8) // 1B - 256B\n\topts.Levels = []LevelOptions{lopts}\n\n\td, err := Open(\"\", opts)\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, d.Close())\n\t}()\n\tmatchingKeyValues := make(map[string]string)\n\tlower := rng.IntN(100)\n\tupper := rng.IntN(100)\n\tif lower > upper {\n\t\tlower, upper = upper, lower\n\t}\n\tn := 2000\n\tfor i := 0; i < n; i++ {\n\t\tkey, suffix := randKey(20+rng.IntN(5), rng)\n\t\tvalue := randValue(50, rng)\n\t\tif lower <= suffix && suffix < upper {\n\t\t\tmatchingKeyValues[string(key)] = string(value)\n\t\t}\n\t\td.Set(key, value, nil)\n\t}\n\n\tvar iterOpts IterOptions\n\titerOpts.PointKeyFilters = []BlockPropertyFilter{\n\t\tsstable.NewBlockIntervalFilter(\"0\", uint64(lower), uint64(upper), nil),\n\t}\n\titer, _ := d.NewIter(&iterOpts)\n\tdefer func() {\n\t\trequire.NoError(t, iter.Close())\n\t}()\n\titer.First()\n\tfound := 0\n\tmatchingCount := len(matchingKeyValues)\n\tfor ; iter.Valid(); iter.Next() {\n\t\tfound++\n\t\tkey := string(iter.Key())\n\t\tvalue, ok := matchingKeyValues[key]\n\t\tif ok {\n\t\t\trequire.Equal(t, value, string(iter.Value()))\n\t\t\tdelete(matchingKeyValues, key)\n\t\t}\n\t}\n\tt.Logf(\"generated %d keys: %d matching, %d found\", n, matchingCount, found)\n\trequire.Equal(t, 0, len(matchingKeyValues))\n}\n\nfunc TestIteratorGuaranteedDurable(t *testing.T) {\n\tmem := vfs.NewMem()\n\topts := &Options{FS: mem}\n\td, err := Open(\"\", opts)\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\trequire.NoError(t, d.Close())\n\t}()\n\titerOptions := IterOptions{OnlyReadGuaranteedDurable: true}\n\tfailFunc := func(t *testing.T, reader Reader) {\n\t\tdefer func() {\n\t\t\tif r := recover(); r == nil {\n\t\t\t\trequire.Fail(t, \"expected panic\")\n\t\t\t}\n\t\t\treader.Close()\n\t\t}()\n\t\titer, _ := reader.NewIter(&iterOptions)\n\t\tdefer iter.Close()\n\t}\n\tt.Run(\"snapshot\", func(t *testing.T) {\n\t\tfailFunc(t, d.NewSnapshot())\n\t})\n\tt.Run(\"batch\", func(t *testing.T) {\n\t\tfailFunc(t, d.NewIndexedBatch())\n\t})\n\tt.Run(\"db\", func(t *testing.T) {\n\t\td.Set([]byte(\"k\"), []byte(\"v\"), nil)\n\t\tfoundKV := func(o *IterOptions) bool {\n\t\t\titer, _ := d.NewIter(o)\n\t\t\tdefer iter.Close()\n\t\t\titer.SeekGE([]byte(\"k\"))\n\t\t\treturn iter.Valid()\n\t\t}\n\t\trequire.True(t, foundKV(nil))\n\t\trequire.False(t, foundKV(&iterOptions))\n\t\trequire.NoError(t, d.Flush())\n\t\trequire.True(t, foundKV(nil))\n\t\trequire.True(t, foundKV(&iterOptions))\n\t})\n}\n\nfunc TestIteratorBoundsLifetimes(t *testing.T) {\n\trng := rand.New(rand.NewPCG(0, uint64(time.Now().UnixNano())))\n\td := newPointTestkeysDatabase(t, testkeys.Alpha(2))\n\tdefer func() { require.NoError(t, d.Close()) }()\n\n\tvar buf bytes.Buffer\n\titerators := map[string]*Iterator{}\n\tvar labels []string\n\tprintIters := func(w io.Writer) {\n\t\tlabels = labels[:0]\n\t\tfor label := range iterators {\n\t\t\tlabels = append(labels, label)\n\t\t}\n\t\tsort.Strings(labels)\n\t\tfor _, label := range labels {\n\t\t\tit := iterators[label]\n\t\t\tfmt.Fprintf(&buf, \"%s: (\", label)\n\t\t\tif it.opts.LowerBound == nil {\n\t\t\t\tfmt.Fprint(&buf, \"<nil>, \")\n\t\t\t} else {\n\t\t\t\tfmt.Fprintf(&buf, \"%q, \", it.opts.LowerBound)\n\t\t\t}\n\t\t\tif it.opts.UpperBound == nil {\n\t\t\t\tfmt.Fprint(&buf, \"<nil>)\")\n\t\t\t} else {\n\t\t\t\tfmt.Fprintf(&buf, \"%q)\", it.opts.UpperBound)\n\t\t\t}\n\t\t\tfmt.Fprintf(&buf, \" boundsBufIdx=%d\\n\", it.boundsBufIdx)\n\t\t}\n\t}\n\tparseBounds := func(td *datadriven.TestData) (lower, upper []byte) {\n\t\tfor _, arg := range td.CmdArgs {\n\t\t\tif arg.Key == \"lower\" {\n\t\t\t\tlower = []byte(arg.Vals[0])\n\t\t\t} else if arg.Key == \"upper\" {\n\t\t\t\tupper = []byte(arg.Vals[0])\n\t\t\t}\n\t\t}\n\t\treturn lower, upper\n\t}\n\ttrashBounds := func(bounds ...[]byte) {\n\t\tfor _, bound := range bounds {\n\t\t\tfor j := range bound {\n\t\t\t\tbound[j] = byte(rng.Uint32())\n\t\t\t}\n\t\t}\n\t}\n\n\tdatadriven.RunTest(t, \"testdata/iterator_bounds_lifetimes\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"define\":\n\t\t\tvar err error\n\t\t\tif d, err = runDBDefineCmd(td, d.opts); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\td.mu.Lock()\n\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\td.mu.Unlock()\n\t\t\treturn s\n\t\tcase \"new-iter\":\n\t\t\tvar label string\n\t\t\ttd.ScanArgs(t, \"label\", &label)\n\t\t\tlower, upper := parseBounds(td)\n\t\t\titerators[label], _ = d.NewIter(&IterOptions{\n\t\t\t\tLowerBound: lower,\n\t\t\t\tUpperBound: upper,\n\t\t\t})\n\t\t\ttrashBounds(lower, upper)\n\t\t\tbuf.Reset()\n\t\t\tprintIters(&buf)\n\t\t\treturn buf.String()\n\t\tcase \"clone\":\n\t\t\tvar from, to string\n\t\t\ttd.ScanArgs(t, \"from\", &from)\n\t\t\ttd.ScanArgs(t, \"to\", &to)\n\t\t\tvar err error\n\t\t\titerators[to], err = iterators[from].Clone(CloneOptions{})\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tbuf.Reset()\n\t\t\tprintIters(&buf)\n\t\t\treturn buf.String()\n\t\tcase \"close\":\n\t\t\tvar label string\n\t\t\ttd.ScanArgs(t, \"label\", &label)\n\t\t\titerators[label].Close()\n\t\t\tdelete(iterators, label)\n\t\t\tbuf.Reset()\n\t\t\tprintIters(&buf)\n\t\t\treturn buf.String()\n\t\tcase \"iter\":\n\t\t\tvar label string\n\t\t\ttd.ScanArgs(t, \"label\", &label)\n\t\t\treturn runIterCmd(td, iterators[label], false /* closeIter */)\n\t\tcase \"set-bounds\":\n\t\t\tvar label string\n\t\t\ttd.ScanArgs(t, \"label\", &label)\n\t\t\tlower, upper := parseBounds(td)\n\t\t\titerators[label].SetBounds(lower, upper)\n\t\t\ttrashBounds(lower, upper)\n\t\t\tbuf.Reset()\n\t\t\tprintIters(&buf)\n\t\t\treturn buf.String()\n\t\tcase \"set-options\":\n\t\t\tvar label string\n\t\t\ttd.ScanArgs(t, \"label\", &label)\n\t\t\topts := iterators[label].opts\n\t\t\tfor _, arg := range td.CmdArgs {\n\t\t\t\tif arg.Key == \"key-types\" {\n\t\t\t\t\tswitch arg.Vals[0] {\n\t\t\t\t\tcase \"points-only\":\n\t\t\t\t\t\topts.KeyTypes = IterKeyTypePointsOnly\n\t\t\t\t\tcase \"ranges-only\":\n\t\t\t\t\t\topts.KeyTypes = IterKeyTypeRangesOnly\n\t\t\t\t\tcase \"both\":\n\t\t\t\t\t\topts.KeyTypes = IterKeyTypePointsAndRanges\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tpanic(fmt.Sprintf(\"unrecognized key type %q\", arg.Vals[0]))\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\topts.LowerBound, opts.UpperBound = parseBounds(td)\n\t\t\titerators[label].SetOptions(&opts)\n\t\t\ttrashBounds(opts.LowerBound, opts.UpperBound)\n\t\t\tbuf.Reset()\n\t\t\tprintIters(&buf)\n\t\t\treturn buf.String()\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unrecognized command %q\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc TestIteratorStatsMerge(t *testing.T) {\n\ts := IteratorStats{\n\t\tForwardSeekCount: [NumStatsKind]int{1, 2},\n\t\tReverseSeekCount: [NumStatsKind]int{3, 4},\n\t\tForwardStepCount: [NumStatsKind]int{5, 6},\n\t\tReverseStepCount: [NumStatsKind]int{7, 8},\n\t\tInternalStats: InternalIteratorStats{\n\t\t\tBlockBytes:                     9,\n\t\t\tBlockBytesInCache:              10,\n\t\t\tBlockReadDuration:              3 * time.Millisecond,\n\t\t\tKeyBytes:                       11,\n\t\t\tValueBytes:                     12,\n\t\t\tPointCount:                     13,\n\t\t\tPointsCoveredByRangeTombstones: 14,\n\t\t},\n\t\tRangeKeyStats: RangeKeyIteratorStats{\n\t\t\tCount:           15,\n\t\t\tContainedPoints: 16,\n\t\t\tSkippedPoints:   17,\n\t\t},\n\t}\n\ts.InternalStats.SeparatedPointValue.Count = 1\n\ts.InternalStats.SeparatedPointValue.ValueBytes = 5\n\ts.InternalStats.SeparatedPointValue.ValueBytesFetched = 3\n\ts2 := IteratorStats{\n\t\tForwardSeekCount: [NumStatsKind]int{1, 2},\n\t\tReverseSeekCount: [NumStatsKind]int{3, 4},\n\t\tForwardStepCount: [NumStatsKind]int{5, 6},\n\t\tReverseStepCount: [NumStatsKind]int{7, 8},\n\t\tInternalStats: InternalIteratorStats{\n\t\t\tBlockBytes:                     9,\n\t\t\tBlockBytesInCache:              10,\n\t\t\tBlockReadDuration:              4 * time.Millisecond,\n\t\t\tKeyBytes:                       11,\n\t\t\tValueBytes:                     12,\n\t\t\tPointCount:                     13,\n\t\t\tPointsCoveredByRangeTombstones: 14,\n\t\t},\n\t\tRangeKeyStats: RangeKeyIteratorStats{\n\t\t\tCount:           15,\n\t\t\tContainedPoints: 16,\n\t\t\tSkippedPoints:   17,\n\t\t},\n\t}\n\ts2.InternalStats.SeparatedPointValue.Count = 2\n\ts2.InternalStats.SeparatedPointValue.ValueBytes = 10\n\ts2.InternalStats.SeparatedPointValue.ValueBytesFetched = 6\n\ts.Merge(s2)\n\texpected := IteratorStats{\n\t\tForwardSeekCount: [NumStatsKind]int{2, 4},\n\t\tReverseSeekCount: [NumStatsKind]int{6, 8},\n\t\tForwardStepCount: [NumStatsKind]int{10, 12},\n\t\tReverseStepCount: [NumStatsKind]int{14, 16},\n\t\tInternalStats: InternalIteratorStats{\n\t\t\tBlockBytes:                     18,\n\t\t\tBlockBytesInCache:              20,\n\t\t\tBlockReadDuration:              7 * time.Millisecond,\n\t\t\tKeyBytes:                       22,\n\t\t\tValueBytes:                     24,\n\t\t\tPointCount:                     26,\n\t\t\tPointsCoveredByRangeTombstones: 28,\n\t\t},\n\t\tRangeKeyStats: RangeKeyIteratorStats{\n\t\t\tCount:           30,\n\t\t\tContainedPoints: 32,\n\t\t\tSkippedPoints:   34,\n\t\t},\n\t}\n\texpected.InternalStats.SeparatedPointValue.Count = 3\n\texpected.InternalStats.SeparatedPointValue.ValueBytes = 15\n\texpected.InternalStats.SeparatedPointValue.ValueBytesFetched = 9\n\trequire.Equal(t, expected, s)\n}\n\n// TestSetOptionsEquivalence tests equivalence between SetOptions to mutate an\n// iterator and constructing a new iterator with NewIter. The long-lived\n// iterator and the new iterator should surface identical iterator states.\nfunc TestSetOptionsEquivalence(t *testing.T) {\n\tseed := uint64(time.Now().UnixNano())\n\t// Call a helper function with the seed so that the seed appears within\n\t// stack traces if there's a panic.\n\ttestSetOptionsEquivalence(t, seed)\n}\n\nfunc testSetOptionsEquivalence(t *testing.T, seed uint64) {\n\trng := rand.New(rand.NewPCG(seed, seed))\n\tks := testkeys.Alpha(2)\n\td := newTestkeysDatabase(t, ks, rng)\n\tdefer func() { require.NoError(t, d.Close()) }()\n\n\tvar o IterOptions\n\tgenerateNewOptions := func() {\n\t\t// TODO(jackson): Include test coverage for block property filters, etc.\n\t\tif rng.IntN(2) == 1 {\n\t\t\to.KeyTypes = IterKeyType(rng.IntN(3))\n\t\t}\n\t\tif rng.IntN(2) == 1 {\n\t\t\tif rng.IntN(2) == 1 {\n\t\t\t\to.LowerBound = nil\n\t\t\t\tif rng.IntN(2) == 1 {\n\t\t\t\t\to.LowerBound = testkeys.KeyAt(ks, rng.Int64N(ks.Count()), rng.Int64N(ks.Count()))\n\t\t\t\t}\n\t\t\t}\n\t\t\tif rng.IntN(2) == 1 {\n\t\t\t\to.UpperBound = nil\n\t\t\t\tif rng.IntN(2) == 1 {\n\t\t\t\t\to.UpperBound = testkeys.KeyAt(ks, rng.Int64N(ks.Count()), rng.Int64N(ks.Count()))\n\t\t\t\t}\n\t\t\t}\n\t\t\tif testkeys.Comparer.Compare(o.LowerBound, o.UpperBound) > 0 {\n\t\t\t\to.LowerBound, o.UpperBound = o.UpperBound, o.LowerBound\n\t\t\t}\n\t\t}\n\t\to.RangeKeyMasking.Suffix = nil\n\t\tif o.KeyTypes == IterKeyTypePointsAndRanges && rng.IntN(2) == 1 {\n\t\t\to.RangeKeyMasking.Suffix = testkeys.Suffix(rng.Int64N(ks.Count()))\n\t\t}\n\t}\n\n\tvar longLivedIter, newIter *Iterator\n\tvar history, longLivedBuf, newIterBuf bytes.Buffer\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tt.Log(history.String())\n\t\t\tpanic(r)\n\t\t}\n\t}()\n\tdefer func() {\n\t\tif longLivedIter != nil {\n\t\t\tlongLivedIter.Close()\n\t\t}\n\t\tif newIter != nil {\n\t\t\tnewIter.Close()\n\t\t}\n\t}()\n\n\ttype positioningOp struct {\n\t\tdesc string\n\t\trun  func(*Iterator) IterValidityState\n\t}\n\tpositioningOps := []func() positioningOp{\n\t\t// SeekGE\n\t\tfunc() positioningOp {\n\t\t\tk := testkeys.Key(ks, rng.Int64N(ks.Count()))\n\t\t\treturn positioningOp{\n\t\t\t\tdesc: fmt.Sprintf(\"SeekGE(%q)\", k),\n\t\t\t\trun: func(it *Iterator) IterValidityState {\n\t\t\t\t\treturn it.SeekGEWithLimit(k, nil)\n\t\t\t\t},\n\t\t\t}\n\t\t},\n\t\t// SeekLT\n\t\tfunc() positioningOp {\n\t\t\tk := testkeys.Key(ks, rng.Int64N(ks.Count()))\n\t\t\treturn positioningOp{\n\t\t\t\tdesc: fmt.Sprintf(\"SeekLT(%q)\", k),\n\t\t\t\trun: func(it *Iterator) IterValidityState {\n\t\t\t\t\treturn it.SeekLTWithLimit(k, nil)\n\t\t\t\t},\n\t\t\t}\n\t\t},\n\t\t// SeekPrefixGE\n\t\tfunc() positioningOp {\n\t\t\tk := testkeys.Key(ks, rng.Int64N(ks.Count()))\n\t\t\treturn positioningOp{\n\t\t\t\tdesc: fmt.Sprintf(\"SeekPrefixGE(%q)\", k),\n\t\t\t\trun: func(it *Iterator) IterValidityState {\n\t\t\t\t\tif it.SeekPrefixGE(k) {\n\t\t\t\t\t\treturn IterValid\n\t\t\t\t\t}\n\t\t\t\t\treturn IterExhausted\n\t\t\t\t},\n\t\t\t}\n\t\t},\n\t}\n\n\tfor i := 0; i < 10_000; i++ {\n\t\t// Generate new random options. The options in o will be mutated.\n\t\tgenerateNewOptions()\n\t\tfmt.Fprintf(&history, \"new options: %s\\n\", iterOptionsString(&o))\n\n\t\tnewIter, _ = d.NewIter(&o)\n\t\tif longLivedIter == nil {\n\t\t\tlongLivedIter, _ = d.NewIter(&o)\n\t\t} else {\n\t\t\tlongLivedIter.SetOptions(&o)\n\t\t}\n\n\t\t// Apply the same operation to both keys.\n\t\titerOp := positioningOps[rng.IntN(len(positioningOps))]()\n\t\tnewIterValidity := iterOp.run(newIter)\n\t\tlongLivedValidity := iterOp.run(longLivedIter)\n\n\t\tnewIterBuf.Reset()\n\t\tlongLivedBuf.Reset()\n\t\tprintIterState(&newIterBuf, newIter, newIterValidity, true /* printValidityState */)\n\t\tprintIterState(&longLivedBuf, longLivedIter, longLivedValidity, true /* printValidityState */)\n\t\tfmt.Fprintf(&history, \"%s = %s\\n\", iterOp.desc, newIterBuf.String())\n\n\t\tif newIterBuf.String() != longLivedBuf.String() {\n\t\t\tt.Logf(\"history:\\n%s\\n\", history.String())\n\t\t\tt.Logf(\"seed: %d\\n\", seed)\n\t\t\tt.Fatalf(\"expected %q, got %q\", newIterBuf.String(), longLivedBuf.String())\n\t\t}\n\t\t_ = newIter.Close()\n\n\t\tnewIter = nil\n\t}\n\tt.Logf(\"history:\\n%s\\n\", history.String())\n}\n\nfunc iterOptionsString(o *IterOptions) string {\n\tvar buf bytes.Buffer\n\tfmt.Fprintf(&buf, \"key-types=%s, lower=%q, upper=%q\",\n\t\to.KeyTypes, o.LowerBound, o.UpperBound)\n\tif o.OnlyReadGuaranteedDurable {\n\t\tfmt.Fprintf(&buf, \", only-durable\")\n\t}\n\tif o.UseL6Filters {\n\t\tfmt.Fprintf(&buf, \", use-L6-filters\")\n\t}\n\tfor i, pkf := range o.PointKeyFilters {\n\t\tfmt.Fprintf(&buf, \", point-key-filter[%d]=%q\", i, pkf.Name())\n\t}\n\tfor i, rkf := range o.RangeKeyFilters {\n\t\tfmt.Fprintf(&buf, \", range-key-filter[%d]=%q\", i, rkf.Name())\n\t}\n\treturn buf.String()\n}\n\nfunc newTestkeysDatabase(t *testing.T, ks testkeys.Keyspace, rng *rand.Rand) *DB {\n\tdbOpts := &Options{\n\t\tComparer: testkeys.Comparer,\n\t\tFS:       vfs.NewMem(),\n\t\tLogger:   panicLogger{},\n\t}\n\tdbOpts.testingRandomized(t)\n\td, err := Open(\"\", dbOpts)\n\trequire.NoError(t, err)\n\n\t// Randomize the order in which we write keys.\n\torder := rng.Perm(int(ks.Count()))\n\tb := d.NewBatch()\n\tkeyBuf := make([]byte, ks.MaxLen()+testkeys.MaxSuffixLen)\n\tkeyBuf2 := make([]byte, ks.MaxLen()+testkeys.MaxSuffixLen)\n\tfor i := 0; i < len(order); i++ {\n\t\tconst maxVersionsPerKey = 10\n\t\tkeyIndex := order[i]\n\t\tfor versions := rng.IntN(maxVersionsPerKey); versions > 0; versions-- {\n\t\t\tn := testkeys.WriteKeyAt(keyBuf, ks, int64(keyIndex), rng.Int64N(maxVersionsPerKey))\n\t\t\tb.Set(keyBuf[:n], keyBuf[:n], nil)\n\t\t}\n\n\t\t// Sometimes add a range key too.\n\t\tif rng.IntN(100) == 1 {\n\t\t\tstartIdx := rng.Int64N(ks.Count())\n\t\t\tendIdx := rng.Int64N(ks.Count())\n\t\t\tstartLen := testkeys.WriteKey(keyBuf, ks, startIdx)\n\t\t\tendLen := testkeys.WriteKey(keyBuf2, ks, endIdx)\n\t\t\tsuffixInt := rng.Int64N(maxVersionsPerKey)\n\t\t\trequire.NoError(t, b.RangeKeySet(\n\t\t\t\tkeyBuf[:startLen],\n\t\t\t\tkeyBuf2[:endLen],\n\t\t\t\ttestkeys.Suffix(suffixInt),\n\t\t\t\tnil,\n\t\t\t\tnil))\n\t\t}\n\n\t\t// Randomize the flush points.\n\t\tif !b.Empty() && rng.IntN(10) == 1 {\n\t\t\trequire.NoError(t, b.Commit(nil))\n\t\t\trequire.NoError(t, d.Flush())\n\t\t\tb = d.NewBatch()\n\t\t}\n\t}\n\tif !b.Empty() {\n\t\trequire.NoError(t, b.Commit(nil))\n\t}\n\treturn d\n}\n\nfunc newPointTestkeysDatabase(t *testing.T, ks testkeys.Keyspace) *DB {\n\tdbOpts := &Options{\n\t\tComparer: testkeys.Comparer,\n\t\tFS:       vfs.NewMem(),\n\t}\n\tdbOpts.testingRandomized(t)\n\td, err := Open(\"\", dbOpts)\n\trequire.NoError(t, err)\n\n\tb := d.NewBatch()\n\tkeyBuf := make([]byte, ks.MaxLen()+testkeys.MaxSuffixLen)\n\tfor i := int64(0); i < ks.Count(); i++ {\n\t\tn := testkeys.WriteKeyAt(keyBuf, ks, i, i)\n\t\tb.Set(keyBuf[:n], keyBuf[:n], nil)\n\t}\n\trequire.NoError(t, b.Commit(nil))\n\treturn d\n}\n\nfunc BenchmarkIteratorSeekGE(b *testing.B) {\n\tm, keys := buildMemTable(b)\n\titer := &Iterator{\n\t\tcomparer: *DefaultComparer,\n\t\titer:     m.newIter(nil),\n\t}\n\trng := rand.New(rand.NewPCG(0, uint64(time.Now().UnixNano())))\n\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\tkey := keys[rng.IntN(len(keys))]\n\t\titer.SeekGE(key)\n\t}\n}\n\nfunc BenchmarkIteratorNext(b *testing.B) {\n\tm, _ := buildMemTable(b)\n\titer := &Iterator{\n\t\tcomparer: *DefaultComparer,\n\t\titer:     m.newIter(nil),\n\t}\n\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\tif !iter.Valid() {\n\t\t\titer.First()\n\t\t}\n\t\titer.Next()\n\t}\n}\n\nfunc BenchmarkIteratorPrev(b *testing.B) {\n\tm, _ := buildMemTable(b)\n\titer := &Iterator{\n\t\tcomparer: *DefaultComparer,\n\t\titer:     m.newIter(nil),\n\t}\n\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\tif !iter.Valid() {\n\t\t\titer.Last()\n\t\t}\n\t\titer.Prev()\n\t}\n}\n\ntype twoLevelBloomTombstoneState struct {\n\tkeys        [][]byte\n\treaders     [8][][]*sstable.Reader\n\tlevelSlices [8][]manifest.LevelSlice\n\tindexFunc   func(twoLevelIndex bool, bloom bool, withTombstone bool) int\n}\n\nfunc setupForTwoLevelBloomTombstone(b *testing.B, keyOffset int) twoLevelBloomTombstoneState {\n\tconst blockSize = 32 << 10\n\tconst restartInterval = 16\n\tconst levelCount = 5\n\n\tvar readers [8][][]*sstable.Reader\n\tvar levelSlices [8][]manifest.LevelSlice\n\tvar keys [][]byte\n\tindexFunc := func(twoLevelIndex bool, bloom bool, withTombstone bool) int {\n\t\tindex := 0\n\t\tif twoLevelIndex {\n\t\t\tindex = 4\n\t\t}\n\t\tif bloom {\n\t\t\tindex += 2\n\t\t}\n\t\tif withTombstone {\n\t\t\tindex++\n\t\t}\n\t\treturn index\n\t}\n\tfor _, twoLevelIndex := range []bool{false, true} {\n\t\tfor _, bloom := range []bool{false, true} {\n\t\t\tfor _, withTombstone := range []bool{false, true} {\n\t\t\t\tindex := indexFunc(twoLevelIndex, bloom, withTombstone)\n\t\t\t\tlevels := levelCount\n\t\t\t\tif withTombstone {\n\t\t\t\t\tlevels = 1\n\t\t\t\t}\n\t\t\t\treaders[index], levelSlices[index], keys = buildLevelsForMergingIterSeqSeek(\n\t\t\t\t\tb, blockSize, restartInterval, levels, keyOffset, withTombstone, bloom, twoLevelIndex)\n\t\t\t}\n\t\t}\n\t}\n\treturn twoLevelBloomTombstoneState{\n\t\tkeys: keys, readers: readers, levelSlices: levelSlices, indexFunc: indexFunc}\n}\n\n// BenchmarkIteratorSeqSeekPrefixGENotFound exercises the case of SeekPrefixGE\n// specifying monotonic keys all of which precede actual keys present in L6 of\n// the DB. Moreover, with-tombstone=true exercises the sub-case where those\n// actual keys are deleted using a range tombstone that has not physically\n// deleted those keys due to the presence of a snapshot that needs to see\n// those keys. This sub-case needs to be efficient in (a) avoiding iteration\n// over all those deleted keys, including repeated iteration, (b) using the\n// next optimization, since the seeks are monotonic.\nfunc BenchmarkIteratorSeqSeekPrefixGENotFound(b *testing.B) {\n\tconst keyOffset = 100000\n\tstate := setupForTwoLevelBloomTombstone(b, keyOffset)\n\treaders := state.readers\n\tlevelSlices := state.levelSlices\n\tindexFunc := state.indexFunc\n\n\t// We will not be seeking to the keys that were written but instead to\n\t// keys before the written keys. This is to validate that the optimization\n\t// to use Next still functions when mergingIter checks for the prefix\n\t// match, and that mergingIter can avoid iterating over all the keys\n\t// deleted by a range tombstone when there is no possibility of matching\n\t// the prefix.\n\tvar keys [][]byte\n\tfor i := 0; i < keyOffset; i++ {\n\t\tkeys = append(keys, []byte(fmt.Sprintf(\"%08d\", i)))\n\t}\n\tfor _, skip := range []int{1, 2, 4} {\n\t\tfor _, twoLevelIndex := range []bool{false, true} {\n\t\t\tfor _, bloom := range []bool{false, true} {\n\t\t\t\tfor _, withTombstone := range []bool{false, true} {\n\t\t\t\t\tb.Run(fmt.Sprintf(\"skip=%d/two-level=%t/bloom=%t/with-tombstone=%t\",\n\t\t\t\t\t\tskip, twoLevelIndex, bloom, withTombstone),\n\t\t\t\t\t\tfunc(b *testing.B) {\n\t\t\t\t\t\t\tindex := indexFunc(twoLevelIndex, bloom, withTombstone)\n\t\t\t\t\t\t\treaders := readers[index]\n\t\t\t\t\t\t\tlevelSlices := levelSlices[index]\n\t\t\t\t\t\t\tm := buildMergingIter(readers, levelSlices)\n\t\t\t\t\t\t\titer := Iterator{\n\t\t\t\t\t\t\t\tcomparer: *testkeys.Comparer,\n\t\t\t\t\t\t\t\tmerge:    DefaultMerger.Merge,\n\t\t\t\t\t\t\t\titer:     m,\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tpos := 0\n\t\t\t\t\t\t\tb.ResetTimer()\n\t\t\t\t\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t\t\t\t\t// When withTombstone=true, and prior to the\n\t\t\t\t\t\t\t\t// optimization to stop early due to a range\n\t\t\t\t\t\t\t\t// tombstone, the iteration would continue into the\n\t\t\t\t\t\t\t\t// next file, and not be able to use Next at the lower\n\t\t\t\t\t\t\t\t// level in the next SeekPrefixGE call. So we would\n\t\t\t\t\t\t\t\t// incur the cost of iterating over all the deleted\n\t\t\t\t\t\t\t\t// keys for every seek. Note that it is not possible\n\t\t\t\t\t\t\t\t// to do a noop optimization in Iterator for the\n\t\t\t\t\t\t\t\t// prefix case, unlike SeekGE/SeekLT, since we don't\n\t\t\t\t\t\t\t\t// know if the iterators inside mergingIter are all\n\t\t\t\t\t\t\t\t// appropriately positioned -- some may not be due to\n\t\t\t\t\t\t\t\t// bloom filters not matching.\n\t\t\t\t\t\t\t\tvalid := iter.SeekPrefixGE(keys[pos])\n\t\t\t\t\t\t\t\tif valid {\n\t\t\t\t\t\t\t\t\tb.Fatalf(\"key should not be found\")\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\tpos += skip\n\t\t\t\t\t\t\t\tif pos >= keyOffset {\n\t\t\t\t\t\t\t\t\tpos = 0\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tb.StopTimer()\n\t\t\t\t\t\t\titer.Close()\n\t\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tfor _, r := range readers {\n\t\tfor i := range r {\n\t\t\tfor j := range r[i] {\n\t\t\t\tr[i][j].Close()\n\t\t\t}\n\t\t}\n\t}\n}\n\n// BenchmarkIteratorSeqSeekPrefixGEFound exercises the case of SeekPrefixGE\n// specifying monotonic keys that are present in L6 of the DB. Moreover,\n// with-tombstone=true exercises the sub-case where those actual keys are\n// deleted using a range tombstone that has not physically deleted those keys\n// due to the presence of a snapshot that needs to see those keys. This\n// sub-case needs to be efficient in (a) avoiding iteration over all those\n// deleted keys, including repeated iteration, (b) using the next\n// optimization, since the seeks are monotonic.\nfunc BenchmarkIteratorSeqSeekPrefixGEFound(b *testing.B) {\n\tstate := setupForTwoLevelBloomTombstone(b, 0)\n\tkeys := state.keys\n\treaders := state.readers\n\tlevelSlices := state.levelSlices\n\tindexFunc := state.indexFunc\n\n\tfor _, skip := range []int{1, 2, 4} {\n\t\tfor _, twoLevelIndex := range []bool{false, true} {\n\t\t\tfor _, bloom := range []bool{false, true} {\n\t\t\t\tfor _, withTombstone := range []bool{false, true} {\n\t\t\t\t\tb.Run(fmt.Sprintf(\"skip=%d/two-level=%t/bloom=%t/with-tombstone=%t\",\n\t\t\t\t\t\tskip, twoLevelIndex, bloom, withTombstone),\n\t\t\t\t\t\tfunc(b *testing.B) {\n\t\t\t\t\t\t\tindex := indexFunc(twoLevelIndex, bloom, withTombstone)\n\t\t\t\t\t\t\treaders := readers[index]\n\t\t\t\t\t\t\tlevelSlices := levelSlices[index]\n\t\t\t\t\t\t\tm := buildMergingIter(readers, levelSlices)\n\t\t\t\t\t\t\titer := Iterator{\n\t\t\t\t\t\t\t\tcomparer: *testkeys.Comparer,\n\t\t\t\t\t\t\t\tmerge:    DefaultMerger.Merge,\n\t\t\t\t\t\t\t\titer:     m,\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tpos := 0\n\t\t\t\t\t\t\tb.ResetTimer()\n\t\t\t\t\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t\t\t\t\t// When withTombstone=true, and prior to the\n\t\t\t\t\t\t\t\t// optimization to stop early due to a range\n\t\t\t\t\t\t\t\t// tombstone, the iteration would continue into the\n\t\t\t\t\t\t\t\t// next file, and not be able to use Next at the lower\n\t\t\t\t\t\t\t\t// level in the next SeekPrefixGE call. So we would\n\t\t\t\t\t\t\t\t// incur the cost of iterating over all the deleted\n\t\t\t\t\t\t\t\t// keys for every seek. Note that it is not possible\n\t\t\t\t\t\t\t\t// to do a noop optimization in Iterator for the\n\t\t\t\t\t\t\t\t// prefix case, unlike SeekGE/SeekLT, since we don't\n\t\t\t\t\t\t\t\t// know if the iterators inside mergingIter are all\n\t\t\t\t\t\t\t\t// appropriately positioned -- some may not be due to\n\t\t\t\t\t\t\t\t// bloom filters not matching.\n\t\t\t\t\t\t\t\t_ = iter.SeekPrefixGE(keys[pos])\n\t\t\t\t\t\t\t\tpos += skip\n\t\t\t\t\t\t\t\tif pos >= len(keys) {\n\t\t\t\t\t\t\t\t\tpos = 0\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tb.StopTimer()\n\t\t\t\t\t\t\titer.Close()\n\t\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tfor _, r := range readers {\n\t\tfor i := range r {\n\t\t\tfor j := range r[i] {\n\t\t\t\tr[i][j].Close()\n\t\t\t}\n\t\t}\n\t}\n}\n\n// BenchmarkIteratorSeqSeekGEWithBounds is analogous to\n// BenchmarkMergingIterSeqSeekGEWithBounds, except for using an Iterator,\n// which causes it to exercise the end-to-end code path.\nfunc BenchmarkIteratorSeqSeekGEWithBounds(b *testing.B) {\n\tconst blockSize = 32 << 10\n\tconst restartInterval = 16\n\tconst levelCount = 5\n\tfor _, twoLevelIndex := range []bool{false, true} {\n\t\tb.Run(fmt.Sprintf(\"two-level=%t\", twoLevelIndex),\n\t\t\tfunc(b *testing.B) {\n\t\t\t\treaders, levelSlices, keys := buildLevelsForMergingIterSeqSeek(\n\t\t\t\t\tb, blockSize, restartInterval, levelCount, 0, /* keyOffset */\n\t\t\t\t\tfalse, false, twoLevelIndex)\n\t\t\t\tm := buildMergingIter(readers, levelSlices)\n\t\t\t\titer := Iterator{\n\t\t\t\t\tcomparer: *testkeys.Comparer,\n\t\t\t\t\tmerge:    DefaultMerger.Merge,\n\t\t\t\t\titer:     m,\n\t\t\t\t}\n\t\t\t\tkeyCount := len(keys)\n\t\t\t\tb.ResetTimer()\n\t\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t\tpos := i % (keyCount - 1)\n\t\t\t\t\titer.SetBounds(keys[pos], keys[pos+1])\n\t\t\t\t\t// SeekGE will return keys[pos].\n\t\t\t\t\tvalid := iter.SeekGE(keys[pos])\n\t\t\t\t\tfor valid {\n\t\t\t\t\t\tvalid = iter.Next()\n\t\t\t\t\t}\n\t\t\t\t\tif iter.Error() != nil {\n\t\t\t\t\t\tb.Fatal(iter.Error().Error())\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\titer.Close()\n\t\t\t\tfor i := range readers {\n\t\t\t\t\tfor j := range readers[i] {\n\t\t\t\t\t\treaders[i][j].Close()\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t})\n\t}\n}\n\nfunc BenchmarkIteratorSeekGENoop(b *testing.B) {\n\tconst blockSize = 32 << 10\n\tconst restartInterval = 16\n\tconst levelCount = 5\n\tconst keyOffset = 10000\n\treaders, levelSlices, _ := buildLevelsForMergingIterSeqSeek(\n\t\tb, blockSize, restartInterval, levelCount, keyOffset, false, false, false)\n\tvar keys [][]byte\n\tfor i := 0; i < keyOffset; i++ {\n\t\tkeys = append(keys, []byte(fmt.Sprintf(\"%08d\", i)))\n\t}\n\tfor _, withLimit := range []bool{false, true} {\n\t\tb.Run(fmt.Sprintf(\"withLimit=%t\", withLimit), func(b *testing.B) {\n\t\t\tm := buildMergingIter(readers, levelSlices)\n\t\t\titer := Iterator{\n\t\t\t\tcomparer: *testkeys.Comparer,\n\t\t\t\tmerge:    DefaultMerger.Merge,\n\t\t\t\titer:     m,\n\t\t\t}\n\t\t\tb.ResetTimer()\n\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\tpos := i % (len(keys) - 1)\n\t\t\t\tif withLimit {\n\t\t\t\t\tif iter.SeekGEWithLimit(keys[pos], keys[pos+1]) != IterAtLimit {\n\t\t\t\t\t\tb.Fatal(\"should be at limit\")\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tif !iter.SeekGE(keys[pos]) {\n\t\t\t\t\t\tb.Fatal(\"should be valid\")\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\titer.Close()\n\t\t})\n\t}\n\tfor i := range readers {\n\t\tfor j := range readers[i] {\n\t\t\treaders[i][j].Close()\n\t\t}\n\t}\n}\n\nfunc BenchmarkBlockPropertyFilter(b *testing.B) {\n\trng := rand.New(rand.NewPCG(0, uint64(time.Now().UnixNano())))\n\tfor _, matchInterval := range []int{1, 10, 100, 1000} {\n\t\tb.Run(fmt.Sprintf(\"match-interval=%d\", matchInterval), func(b *testing.B) {\n\t\t\tmem := vfs.NewMem()\n\t\t\topts := &Options{\n\t\t\t\tFS:                 mem,\n\t\t\t\tFormatMajorVersion: FormatNewest,\n\t\t\t\tBlockPropertyCollectors: []func() BlockPropertyCollector{\n\t\t\t\t\tfunc() BlockPropertyCollector {\n\t\t\t\t\t\treturn sstable.NewBlockIntervalCollector(\n\t\t\t\t\t\t\t\"0\", &testBlockIntervalMapper{numLength: 3}, nil, /* range key collector */\n\t\t\t\t\t\t)\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t}\n\t\t\td, err := Open(\"\", opts)\n\t\t\trequire.NoError(b, err)\n\t\t\tdefer func() {\n\t\t\t\trequire.NoError(b, d.Close())\n\t\t\t}()\n\t\t\tbatch := d.NewBatch()\n\t\t\tconst numKeys = 20 * 1000\n\t\t\tconst valueSize = 1000\n\t\t\tfor i := 0; i < numKeys; i++ {\n\t\t\t\tkey := fmt.Sprintf(\"%06d%03d\", i, i%matchInterval)\n\t\t\t\tvalue := randValue(valueSize, rng)\n\t\t\t\trequire.NoError(b, batch.Set([]byte(key), value, nil))\n\t\t\t}\n\t\t\trequire.NoError(b, batch.Commit(nil))\n\t\t\trequire.NoError(b, d.Flush())\n\t\t\trequire.NoError(b, d.Compact(nil, []byte{0xFF}, false))\n\n\t\t\tfor _, filter := range []bool{false, true} {\n\t\t\t\tb.Run(fmt.Sprintf(\"filter=%t\", filter), func(b *testing.B) {\n\t\t\t\t\tvar iterOpts IterOptions\n\t\t\t\t\tif filter {\n\t\t\t\t\t\titerOpts.PointKeyFilters = []BlockPropertyFilter{\n\t\t\t\t\t\t\tsstable.NewBlockIntervalFilter(\"0\", uint64(0), uint64(1), nil),\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\titer, _ := d.NewIter(&iterOpts)\n\t\t\t\t\tb.ResetTimer()\n\t\t\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t\t\tvalid := iter.First()\n\t\t\t\t\t\tfor valid {\n\t\t\t\t\t\t\tvalid = iter.Next()\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tb.StopTimer()\n\t\t\t\t\trequire.NoError(b, iter.Close())\n\t\t\t\t})\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestRangeKeyMaskingRandomized(t *testing.T) {\n\tseed := *seed\n\tif seed == 0 {\n\t\tseed = uint64(time.Now().UnixNano())\n\t\tt.Logf(\"seed: %d\", seed)\n\t}\n\trng := rand.New(rand.NewPCG(0, seed))\n\n\t// Generate keyspace with point keys, and range keys which will\n\t// mask the point keys.\n\tvar timestamps []int64\n\tfor i := 0; i <= 100; i++ {\n\t\ttimestamps = append(timestamps, rng.Int64N(1000))\n\t}\n\n\tks := testkeys.Alpha(5)\n\tnumKeys := 1000 + rng.IntN(9000)\n\tkeys := make([][]byte, numKeys)\n\tkeyTimeStamps := make([]int64, numKeys) // ts associated with the keys.\n\tfor i := 0; i < numKeys; i++ {\n\t\tkeys[i] = make([]byte, 5+testkeys.MaxSuffixLen)\n\t\tkeyTimeStamps[i] = timestamps[rng.IntN(len(timestamps))]\n\t\tn := testkeys.WriteKeyAt(keys[i], ks, rng.Int64N(ks.Count()), keyTimeStamps[i])\n\t\tkeys[i] = keys[i][:n]\n\t}\n\n\tnumRangeKeys := rng.IntN(20)\n\ttype rkey struct {\n\t\tstart  []byte\n\t\tend    []byte\n\t\tsuffix []byte\n\t}\n\trkeys := make([]rkey, numRangeKeys)\n\tpointKeyHidden := make([]bool, numKeys)\n\tfor i := 0; i < numRangeKeys; i++ {\n\t\trkeys[i].start = make([]byte, 5)\n\t\trkeys[i].end = make([]byte, 5)\n\n\t\ttestkeys.WriteKey(rkeys[i].start[:5], ks, rng.Int64N(ks.Count()))\n\t\ttestkeys.WriteKey(rkeys[i].end[:5], ks, rng.Int64N(ks.Count()))\n\n\t\tfor bytes.Equal(rkeys[i].start[:5], rkeys[i].end[:5]) {\n\t\t\ttestkeys.WriteKey(rkeys[i].end[:5], ks, rng.Int64N(ks.Count()))\n\t\t}\n\n\t\tif bytes.Compare(rkeys[i].start[:5], rkeys[i].end[:5]) > 0 {\n\t\t\trkeys[i].start, rkeys[i].end = rkeys[i].end, rkeys[i].start\n\t\t}\n\n\t\trkeyTimestamp := timestamps[rng.IntN(len(timestamps))]\n\t\trkeys[i].suffix = []byte(\"@\" + strconv.FormatInt(rkeyTimestamp, 10))\n\n\t\t// Each time we create a range key, check if the range key masks any\n\t\t// point keys.\n\t\tfor j, pkey := range keys {\n\t\t\tif pointKeyHidden[j] {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tif keyTimeStamps[j] >= rkeyTimestamp {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tif testkeys.Comparer.Compare(pkey, rkeys[i].start) >= 0 &&\n\t\t\t\ttestkeys.Comparer.Compare(pkey, rkeys[i].end) < 0 {\n\t\t\t\tpointKeyHidden[j] = true\n\t\t\t}\n\t\t}\n\t}\n\n\t// Define a simple base testOpts, and a randomized testOpts. The results\n\t// of iteration will be compared.\n\ttype testOpts struct {\n\t\tlevelOpts []LevelOptions\n\t\tfilter    func() BlockPropertyFilterMask\n\t}\n\n\tbaseOpts := testOpts{\n\t\tlevelOpts: make([]LevelOptions, 7),\n\t}\n\tfor i := 0; i < len(baseOpts.levelOpts); i++ {\n\t\tbaseOpts.levelOpts[i].TargetFileSize = 1\n\t\tbaseOpts.levelOpts[i].BlockSize = 1\n\t}\n\n\trandomOpts := testOpts{\n\t\tlevelOpts: []LevelOptions{\n\t\t\t{\n\t\t\t\tTargetFileSize: int64(1 + rng.IntN(2<<20)), // Vary the L0 file size.\n\t\t\t\tBlockSize:      1 + rng.IntN(32<<10),\n\t\t\t},\n\t\t},\n\t}\n\tif rng.IntN(2) == 0 {\n\t\trandomOpts.filter = func() BlockPropertyFilterMask {\n\t\t\treturn sstable.NewTestKeysMaskingFilter()\n\t\t}\n\t}\n\n\tmaxProcs := runtime.GOMAXPROCS(0)\n\n\topts1 := &Options{\n\t\tFS:                       vfs.NewCrashableMem(),\n\t\tComparer:                 testkeys.Comparer,\n\t\tFormatMajorVersion:       FormatNewest,\n\t\tMaxConcurrentCompactions: func() int { return maxProcs/2 + 1 },\n\t\tBlockPropertyCollectors: []func() BlockPropertyCollector{\n\t\t\tsstable.NewTestKeysBlockPropertyCollector,\n\t\t},\n\t}\n\topts1.Levels = baseOpts.levelOpts\n\td1, err := Open(\"\", opts1)\n\trequire.NoError(t, err)\n\n\topts2 := &Options{\n\t\tFS:                       vfs.NewCrashableMem(),\n\t\tComparer:                 testkeys.Comparer,\n\t\tFormatMajorVersion:       FormatNewest,\n\t\tMaxConcurrentCompactions: func() int { return maxProcs/2 + 1 },\n\t\tBlockPropertyCollectors: []func() BlockPropertyCollector{\n\t\t\tsstable.NewTestKeysBlockPropertyCollector,\n\t\t},\n\t}\n\topts2.Levels = randomOpts.levelOpts\n\td2, err := Open(\"\", opts2)\n\trequire.NoError(t, err)\n\n\tdefer func() {\n\t\tif err := d1.Close(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif err := d2.Close(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\t// Run test\n\tvar batch1 *Batch\n\tvar batch2 *Batch\n\tconst keysPerBatch = 50\n\tfor i := 0; i < numKeys; i++ {\n\t\tif i%keysPerBatch == 0 {\n\t\t\tif batch1 != nil {\n\t\t\t\trequire.NoError(t, batch1.Commit(nil))\n\t\t\t\trequire.NoError(t, batch2.Commit(nil))\n\t\t\t}\n\t\t\tbatch1 = d1.NewBatch()\n\t\t\tbatch2 = d2.NewBatch()\n\t\t}\n\t\trequire.NoError(t, batch1.Set(keys[i], []byte{1}, nil))\n\t\trequire.NoError(t, batch2.Set(keys[i], []byte{1}, nil))\n\t}\n\n\tfor _, rkey := range rkeys {\n\t\trequire.NoError(t, d1.RangeKeySet(rkey.start, rkey.end, rkey.suffix, nil, nil))\n\t\trequire.NoError(t, d2.RangeKeySet(rkey.start, rkey.end, rkey.suffix, nil, nil))\n\t}\n\n\t// Scan the keyspace\n\titer1Opts := IterOptions{\n\t\tKeyTypes: IterKeyTypePointsAndRanges,\n\t\tRangeKeyMasking: RangeKeyMasking{\n\t\t\tSuffix: []byte(\"@1000\"),\n\t\t\tFilter: baseOpts.filter,\n\t\t},\n\t}\n\n\titer2Opts := IterOptions{\n\t\tKeyTypes: IterKeyTypePointsAndRanges,\n\t\tRangeKeyMasking: RangeKeyMasking{\n\t\t\tSuffix: []byte(\"@1000\"),\n\t\t\tFilter: randomOpts.filter,\n\t\t},\n\t}\n\n\titer1, _ := d1.NewIter(&iter1Opts)\n\titer2, _ := d2.NewIter(&iter2Opts)\n\tdefer func() {\n\t\tif err := iter1.Close(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif err := iter2.Close(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}()\n\n\tfor valid1, valid2 := iter1.First(), iter2.First(); valid1 || valid2; valid1, valid2 = iter1.Next(), iter2.Next() {\n\t\tif valid1 != valid2 {\n\t\t\tt.Fatalf(\"iteration didn't produce identical results\")\n\t\t}\n\n\t\t// Confirm exposed range key state is identical.\n\t\thasP1, hasR1 := iter1.HasPointAndRange()\n\t\thasP2, hasR2 := iter2.HasPointAndRange()\n\t\tif hasP1 != hasP2 || hasR1 != hasR2 {\n\t\t\tt.Fatalf(\"iteration didn't produce identical results\")\n\t\t}\n\t\tif hasP1 && !bytes.Equal(iter1.Key(), iter2.Key()) {\n\t\t\tt.Fatalf(\"iteration didn't produce identical point keys: %s, %s\", iter1.Key(), iter2.Key())\n\t\t}\n\t\tif hasR1 {\n\t\t\t// Confirm that the range key is the same.\n\t\t\tb1, e1 := iter1.RangeBounds()\n\t\t\tb2, e2 := iter2.RangeBounds()\n\t\t\tif !bytes.Equal(b1, b2) || !bytes.Equal(e1, e2) {\n\t\t\t\tt.Fatalf(\n\t\t\t\t\t\"iteration didn't produce identical range keys: [%s, %s], [%s, %s]\",\n\t\t\t\t\tb1, e1, b2, e2,\n\t\t\t\t)\n\t\t\t}\n\n\t\t}\n\n\t\t// Confirm that the returned point key wasn't hidden.\n\t\tfor j, pkey := range keys {\n\t\t\tif bytes.Equal(iter1.Key(), pkey) && pointKeyHidden[j] {\n\t\t\t\tt.Fatalf(\"hidden point key was exposed %s %d\", pkey, keyTimeStamps[j])\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestIteratorSeekPrefixGERandomized(t *testing.T) {\n\tseed := uint64(time.Now().UnixNano())\n\tt.Logf(\"seed: %d\", seed)\n\trng := rand.New(rand.NewPCG(seed, seed))\n\tks := testkeys.Alpha(2)\n\td := newTestkeysDatabase(t, ks, rng)\n\tdefer func() { require.NoError(t, d.Close()) }()\n\n\t// Scan through the keys and construct a map from unique prefix to the\n\t// largest key with that prefix.\n\tm := make(map[string]string) // prefix -> largest user key\n\titer, err := d.NewIter(nil)\n\trequire.NoError(t, err)\n\tdefer iter.Close()\n\tfor valid := iter.First(); valid; valid = iter.Next() {\n\t\tprefix := string(d.opts.Comparer.Split.Prefix(iter.Key()))\n\t\tif _, ok := m[prefix]; !ok {\n\t\t\tm[string(prefix)] = string(iter.Key())\n\t\t}\n\t}\n\t// Perform a few seeks for random prefixes and ensure that result matches\n\t// the map constructed.\n\tfor i := 0; i < 100; i++ {\n\t\tk := testkeys.Key(ks, rng.Int64N(ks.Count()))\n\t\tt.Logf(\"SeekPrefixGE(%q)\", k)\n\t\texists := iter.SeekPrefixGE(k)\n\t\texpectedKey, expectedOk := m[string(k)]\n\t\trequire.Equal(t, expectedOk, exists)\n\t\tif exists {\n\t\t\trequire.Equal(t, expectedKey, string(iter.Key()))\n\t\t}\n\t}\n}\n\n// BenchmarkIterator_RangeKeyMasking benchmarks a scan through a keyspace with\n// 10,000 random suffixed point keys, and three range keys covering most of the\n// keyspace. It varies the suffix of the range keys in subbenchmarks to exercise\n// varying amounts of masking. This benchmark does configure a block-property\n// filter, allowing for skipping blocks wholly contained within a range key and\n// consisting of points all with a suffix lower than the range key's.\nfunc BenchmarkIterator_RangeKeyMasking(b *testing.B) {\n\tconst (\n\t\tprefixLen    = 20\n\t\tvalueSize    = 1024\n\t\tbatches      = 200\n\t\tkeysPerBatch = 50\n\t)\n\tvar alloc bytealloc.A\n\trng := rand.New(rand.NewPCG(0, 1658872515083979000))\n\tkeyBuf := make([]byte, prefixLen+testkeys.MaxSuffixLen)\n\tvalBuf := make([]byte, valueSize)\n\n\tmem := vfs.NewMem()\n\tmaxProcs := runtime.GOMAXPROCS(0)\n\topts := &Options{\n\t\tFS:                       mem,\n\t\tComparer:                 testkeys.Comparer,\n\t\tFormatMajorVersion:       FormatNewest,\n\t\tMaxConcurrentCompactions: func() int { return maxProcs/2 + 1 },\n\t\tBlockPropertyCollectors: []func() BlockPropertyCollector{\n\t\t\tsstable.NewTestKeysBlockPropertyCollector,\n\t\t},\n\t}\n\td, err := Open(\"\", opts)\n\trequire.NoError(b, err)\n\n\tkeys := make([][]byte, keysPerBatch*batches)\n\tfor bi := 0; bi < batches; bi++ {\n\t\tbatch := d.NewBatch()\n\t\tfor k := 0; k < keysPerBatch; k++ {\n\t\t\trandStr(keyBuf[:prefixLen], rng)\n\t\t\tsuffix := rng.Int64N(100)\n\t\t\tsuffixLen := testkeys.WriteSuffix(keyBuf[prefixLen:], suffix)\n\t\t\trandStr(valBuf[:], rng)\n\n\t\t\tvar key []byte\n\t\t\talloc, key = alloc.Copy(keyBuf[:prefixLen+suffixLen])\n\t\t\tkeys[bi*keysPerBatch+k] = key\n\t\t\trequire.NoError(b, batch.Set(key, valBuf[:], nil))\n\t\t}\n\t\trequire.NoError(b, batch.Commit(nil))\n\t}\n\n\t// Wait for compactions to complete before starting benchmarks. We don't\n\t// want to benchmark while compactions are running.\n\td.mu.Lock()\n\tfor d.mu.compact.compactingCount > 0 {\n\t\td.mu.compact.cond.Wait()\n\t}\n\td.mu.Unlock()\n\tb.Log(d.Metrics().String())\n\trequire.NoError(b, d.Close())\n\n\t// TODO(jackson): Benchmark lazy-combined iteration versus not.\n\t// TODO(jackson): Benchmark seeks.\n\tfor _, rkSuffix := range []string{\"@10\", \"@50\", \"@75\", \"@100\"} {\n\t\tb.Run(fmt.Sprintf(\"range-keys-suffixes=%s\", rkSuffix), func(b *testing.B) {\n\t\t\t// Clone the filesystem so that each subbenchmark may mutate state.\n\t\t\topts := opts.Clone()\n\t\t\topts.FS = vfs.NewMem()\n\t\t\tok, err := vfs.Clone(mem, opts.FS, \"\", \"\")\n\t\t\trequire.NoError(b, err)\n\t\t\trequire.True(b, ok)\n\t\t\td, err := Open(\"\", opts)\n\t\t\trequire.NoError(b, err)\n\t\t\trequire.NoError(b, d.RangeKeySet([]byte(\"b\"), []byte(\"e\"), []byte(rkSuffix), nil, nil))\n\t\t\trequire.NoError(b, d.RangeKeySet([]byte(\"f\"), []byte(\"p\"), []byte(rkSuffix), nil, nil))\n\t\t\trequire.NoError(b, d.RangeKeySet([]byte(\"q\"), []byte(\"z\"), []byte(rkSuffix), nil, nil))\n\t\t\trequire.NoError(b, d.Flush())\n\n\t\t\t// Populate 3 range keys, covering most of the keyspace, at the\n\t\t\t// given suffix.\n\n\t\t\titerOpts := IterOptions{\n\t\t\t\tKeyTypes: IterKeyTypePointsAndRanges,\n\t\t\t\tRangeKeyMasking: RangeKeyMasking{\n\t\t\t\t\tSuffix: []byte(\"@100\"),\n\t\t\t\t\tFilter: func() BlockPropertyFilterMask {\n\t\t\t\t\t\treturn sstable.NewTestKeysMaskingFilter()\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t}\n\t\t\tb.Run(\"forward\", func(b *testing.B) {\n\t\t\t\tb.Run(\"seekprefix\", func(b *testing.B) {\n\t\t\t\t\tb.ResetTimer()\n\t\t\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t\t\titer, _ := d.NewIter(&iterOpts)\n\t\t\t\t\t\tcount := 0\n\t\t\t\t\t\tfor j := 0; j < len(keys); j++ {\n\t\t\t\t\t\t\tif !iter.SeekPrefixGE(keys[j]) {\n\t\t\t\t\t\t\t\tb.Errorf(\"unable to find %q\\n\", keys[j])\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif hasPoint, _ := iter.HasPointAndRange(); hasPoint {\n\t\t\t\t\t\t\t\tcount++\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif err := iter.Close(); err != nil {\n\t\t\t\t\t\t\tb.Fatal(err)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t})\n\t\t\t\tb.Run(\"next\", func(b *testing.B) {\n\t\t\t\t\tb.ResetTimer()\n\t\t\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t\t\titer, _ := d.NewIter(&iterOpts)\n\t\t\t\t\t\tcount := 0\n\t\t\t\t\t\tfor valid := iter.First(); valid; valid = iter.Next() {\n\t\t\t\t\t\t\tif hasPoint, _ := iter.HasPointAndRange(); hasPoint {\n\t\t\t\t\t\t\t\tcount++\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif err := iter.Close(); err != nil {\n\t\t\t\t\t\t\tb.Fatal(err)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t})\n\t\t\t})\n\t\t\tb.Run(\"backward\", func(b *testing.B) {\n\t\t\t\tb.ResetTimer()\n\t\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t\titer, _ := d.NewIter(&iterOpts)\n\t\t\t\t\tcount := 0\n\t\t\t\t\tfor valid := iter.Last(); valid; valid = iter.Prev() {\n\t\t\t\t\t\tif hasPoint, _ := iter.HasPointAndRange(); hasPoint {\n\t\t\t\t\t\t\tcount++\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tif err := iter.Close(); err != nil {\n\t\t\t\t\t\tb.Fatal(err)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t})\n\n\t\t\t// Reset the benchmark state at the end of each run to remove the\n\t\t\t// range keys we wrote.\n\t\t\tb.StopTimer()\n\t\t\trequire.NoError(b, d.Close())\n\t\t})\n\t}\n}\n\nfunc BenchmarkIteratorScan(b *testing.B) {\n\tconst maxPrefixLen = 8\n\tkeyBuf := make([]byte, maxPrefixLen+testkeys.MaxSuffixLen)\n\trng := rand.New(rand.NewPCG(0, uint64(time.Now().UnixNano())))\n\n\tfor _, keyCount := range []int64{100, 1000, 10000} {\n\t\tfor _, readAmp := range []int{1, 3, 7, 10} {\n\t\t\tfunc() {\n\t\t\t\topts := &Options{\n\t\t\t\t\tFS:                 vfs.NewMem(),\n\t\t\t\t\tFormatMajorVersion: FormatNewest,\n\t\t\t\t}\n\t\t\t\topts.DisableAutomaticCompactions = true\n\t\t\t\td, err := Open(\"\", opts)\n\t\t\t\trequire.NoError(b, err)\n\t\t\t\tdefer func() { require.NoError(b, d.Close()) }()\n\n\t\t\t\t// Take the very large keyspace consisting of alphabetic\n\t\t\t\t// characters of lengths up to `maxPrefixLen` and reduce it down\n\t\t\t\t// to `keyCount` keys by picking every 1 key every `keyCount` keys.\n\t\t\t\tkeys := testkeys.Alpha(maxPrefixLen)\n\t\t\t\tkeys = keys.EveryN(keys.Count() / keyCount)\n\t\t\t\tif keys.Count() < keyCount {\n\t\t\t\t\tb.Fatalf(\"expected %d keys, found %d\", keyCount, keys.Count())\n\t\t\t\t}\n\n\t\t\t\t// Portion the keys into `readAmp` overlapping key sets.\n\t\t\t\tfor _, ks := range testkeys.Divvy(keys, int64(readAmp)) {\n\t\t\t\t\tbatch := d.NewBatch()\n\t\t\t\t\tfor i := int64(0); i < ks.Count(); i++ {\n\t\t\t\t\t\tn := testkeys.WriteKeyAt(keyBuf[:], ks, i, rng.Int64N(100))\n\t\t\t\t\t\tbatch.Set(keyBuf[:n], keyBuf[:n], nil)\n\t\t\t\t\t}\n\t\t\t\t\trequire.NoError(b, batch.Commit(nil))\n\t\t\t\t\trequire.NoError(b, d.Flush())\n\t\t\t\t}\n\t\t\t\t// Each level is a sublevel.\n\t\t\t\tm := d.Metrics()\n\t\t\t\trequire.Equal(b, readAmp, m.ReadAmp())\n\n\t\t\t\tfor _, keyTypes := range []IterKeyType{IterKeyTypePointsOnly, IterKeyTypePointsAndRanges} {\n\t\t\t\t\titerOpts := IterOptions{KeyTypes: keyTypes}\n\t\t\t\t\tb.Run(fmt.Sprintf(\"keys=%d,r-amp=%d,key-types=%s\", keyCount, readAmp, keyTypes), func(b *testing.B) {\n\t\t\t\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t\t\t\tb.StartTimer()\n\t\t\t\t\t\t\titer, _ := d.NewIter(&iterOpts)\n\t\t\t\t\t\t\tvalid := iter.First()\n\t\t\t\t\t\t\tfor valid {\n\t\t\t\t\t\t\t\tvalid = iter.Next()\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tb.StopTimer()\n\t\t\t\t\t\t\trequire.NoError(b, iter.Close())\n\t\t\t\t\t\t}\n\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t}()\n\t\t}\n\t}\n}\n\nfunc BenchmarkIteratorScanNextPrefix(b *testing.B) {\n\tsetupBench := func(\n\t\tb *testing.B, maxKeysPerLevel, versCount, readAmp int, enableValueBlocks bool) *DB {\n\t\tkeyBuf := make([]byte, readAmp+testkeys.MaxSuffixLen)\n\t\topts := &Options{\n\t\t\tFS:                 vfs.NewMem(),\n\t\t\tComparer:           testkeys.Comparer,\n\t\t\tFormatMajorVersion: FormatNewest,\n\t\t}\n\t\topts.DisableAutomaticCompactions = true\n\t\topts.Experimental.EnableValueBlocks = func() bool { return enableValueBlocks }\n\t\td, err := Open(\"\", opts)\n\t\trequire.NoError(b, err)\n\n\t\t// Create `readAmp` levels. Prefixes in the top of the LSM are length 1.\n\t\t// Prefixes in the bottom of the LSM are length `readAmp`. Eg,:\n\t\t//\n\t\t//    a  b c...\n\t\t//    aa ab ac...\n\t\t//    aaa aab aac...\n\t\t//\n\t\tfor l := readAmp; l > 0; l-- {\n\t\t\tks := testkeys.Alpha(l)\n\t\t\tif step := ks.Count() / int64(maxKeysPerLevel); step > 1 {\n\t\t\t\tks = ks.EveryN(step)\n\t\t\t}\n\t\t\tif ks.Count() > int64(maxKeysPerLevel) {\n\t\t\t\tks = ks.Slice(0, int64(maxKeysPerLevel))\n\t\t\t}\n\n\t\t\tbatch := d.NewBatch()\n\t\t\tfor i := int64(0); i < ks.Count(); i++ {\n\t\t\t\tfor v := 0; v < versCount; v++ {\n\t\t\t\t\tn := testkeys.WriteKeyAt(keyBuf[:], ks, i, int64(versCount-v+1))\n\t\t\t\t\tbatch.Set(keyBuf[:n], keyBuf[:n], nil)\n\t\t\t\t}\n\t\t\t}\n\t\t\trequire.NoError(b, batch.Commit(nil))\n\t\t\trequire.NoError(b, d.Flush())\n\t\t}\n\n\t\t// Each level is a sublevel.\n\t\tm := d.Metrics()\n\t\trequire.Equal(b, readAmp, m.ReadAmp())\n\t\treturn d\n\t}\n\n\tfor _, keysPerLevel := range []int{10, 100, 1000} {\n\t\tb.Run(fmt.Sprintf(\"keysPerLevel=%d\", keysPerLevel), func(b *testing.B) {\n\t\t\tfor _, versionCount := range []int{1, 2, 10, 100} {\n\t\t\t\tb.Run(fmt.Sprintf(\"versions=%d\", versionCount), func(b *testing.B) {\n\t\t\t\t\tfor _, readAmp := range []int{1, 3, 7, 10} {\n\t\t\t\t\t\tb.Run(fmt.Sprintf(\"ramp=%d\", readAmp), func(b *testing.B) {\n\t\t\t\t\t\t\tfor _, enableValueBlocks := range []bool{false, true} {\n\t\t\t\t\t\t\t\tb.Run(fmt.Sprintf(\"value-blocks=%t\", enableValueBlocks), func(b *testing.B) {\n\t\t\t\t\t\t\t\t\td := setupBench(b, keysPerLevel, versionCount, readAmp, enableValueBlocks)\n\t\t\t\t\t\t\t\t\tdefer func() { require.NoError(b, d.Close()) }()\n\t\t\t\t\t\t\t\t\tfor _, keyTypes := range []IterKeyType{\n\t\t\t\t\t\t\t\t\t\tIterKeyTypePointsOnly, IterKeyTypePointsAndRanges} {\n\t\t\t\t\t\t\t\t\t\tb.Run(fmt.Sprintf(\"key-types=%s\", keyTypes), func(b *testing.B) {\n\t\t\t\t\t\t\t\t\t\t\titerOpts := IterOptions{KeyTypes: keyTypes}\n\t\t\t\t\t\t\t\t\t\t\titer, _ := d.NewIter(&iterOpts)\n\t\t\t\t\t\t\t\t\t\t\tvar valid bool\n\t\t\t\t\t\t\t\t\t\t\tb.ResetTimer()\n\t\t\t\t\t\t\t\t\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t\t\t\t\t\t\t\t\tif !valid {\n\t\t\t\t\t\t\t\t\t\t\t\t\tvalid = iter.First()\n\t\t\t\t\t\t\t\t\t\t\t\t\tif !valid {\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tb.Fatalf(\"iter must be valid\")\n\t\t\t\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\t\t\t\t\t\tvalid = iter.NextPrefix()\n\t\t\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t\t\tb.StopTimer()\n\t\t\t\t\t\t\t\t\t\t\trequire.NoError(b, iter.Close())\n\t\t\t\t\t\t\t\t\t\t})\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t})\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t})\n\t\t\t\t\t}\n\t\t\t\t})\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc BenchmarkCombinedIteratorSeek(b *testing.B) {\n\tfor _, withRangeKey := range []bool{false, true} {\n\t\tb.Run(fmt.Sprintf(\"range-key=%t\", withRangeKey), func(b *testing.B) {\n\t\t\trng := rand.New(rand.NewPCG(0, 1658872515083979000))\n\t\t\tks := testkeys.Alpha(1)\n\t\t\topts := &Options{\n\t\t\t\tFS:                 vfs.NewMem(),\n\t\t\t\tComparer:           testkeys.Comparer,\n\t\t\t\tFormatMajorVersion: FormatNewest,\n\t\t\t}\n\t\t\td, err := Open(\"\", opts)\n\t\t\trequire.NoError(b, err)\n\t\t\tdefer func() { require.NoError(b, d.Close()) }()\n\n\t\t\tkeys := make([][]byte, ks.Count())\n\t\t\tfor i := int64(0); i < ks.Count(); i++ {\n\t\t\t\tkeys[i] = testkeys.Key(ks, i)\n\t\t\t\tvar val [40]byte\n\t\t\t\tfor j := range val {\n\t\t\t\t\tval[j] = byte(rng.Uint32())\n\t\t\t\t}\n\t\t\t\trequire.NoError(b, d.Set(keys[i], val[:], nil))\n\t\t\t}\n\t\t\tif withRangeKey {\n\t\t\t\trequire.NoError(b, d.RangeKeySet([]byte(\"a\"), []byte{'z', 0x00}, []byte(\"@5\"), nil, nil))\n\t\t\t}\n\n\t\t\tbatch := d.NewIndexedBatch()\n\t\t\tdefer batch.Close()\n\n\t\t\tfor _, useBatch := range []bool{false, true} {\n\t\t\t\tb.Run(fmt.Sprintf(\"batch=%t\", useBatch), func(b *testing.B) {\n\t\t\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t\t\titerOpts := IterOptions{KeyTypes: IterKeyTypePointsAndRanges}\n\t\t\t\t\t\tvar it *Iterator\n\t\t\t\t\t\tif useBatch {\n\t\t\t\t\t\t\tit, _ = batch.NewIter(&iterOpts)\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tit, _ = d.NewIter(&iterOpts)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tfor j := 0; j < len(keys); j++ {\n\t\t\t\t\t\t\tif !it.SeekGE(keys[j]) {\n\t\t\t\t\t\t\t\tb.Errorf(\"key %q missing\", keys[j])\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\trequire.NoError(b, it.Close())\n\t\t\t\t\t}\n\t\t\t\t})\n\t\t\t}\n\t\t})\n\t}\n}\n\n// BenchmarkCombinedIteratorSeek_Bounded benchmarks a bounded iterator that\n// performs repeated seeks over 5% of the middle of a keyspace covered by a\n// range key that's fragmented across hundreds of files. The iterator bounds\n// should prevent defragmenting beyond the iterator's bounds.\nfunc BenchmarkCombinedIteratorSeek_Bounded(b *testing.B) {\n\td, keys := buildFragmentedRangeKey(b, 1658872515083979000)\n\n\tvar lower = len(keys) / 2\n\tvar upper = len(keys)/2 + len(keys)/20 // 5%\n\titerOpts := IterOptions{\n\t\tKeyTypes:   IterKeyTypePointsAndRanges,\n\t\tLowerBound: keys[lower],\n\t\tUpperBound: keys[upper],\n\t}\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\tit, _ := d.NewIter(&iterOpts)\n\t\tfor j := lower; j < upper; j++ {\n\t\t\tif !it.SeekGE(keys[j]) {\n\t\t\t\tb.Errorf(\"key %q missing\", keys[j])\n\t\t\t}\n\t\t}\n\t\trequire.NoError(b, it.Close())\n\t}\n}\n\n// BenchmarkCombinedIteratorSeekPrefix benchmarks an iterator that\n// performs repeated prefix seeks over 5% of the middle of a keyspace covered by a\n// range key that's fragmented across hundreds of files. The seek prefix should\n// avoid defragmenting beyond the seek prefixes.\nfunc BenchmarkCombinedIteratorSeekPrefix(b *testing.B) {\n\td, keys := buildFragmentedRangeKey(b, 1658872515083979000)\n\n\tvar lower = len(keys) / 2\n\tvar upper = len(keys)/2 + len(keys)/20 // 5%\n\titerOpts := IterOptions{\n\t\tKeyTypes: IterKeyTypePointsAndRanges,\n\t}\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\tit, _ := d.NewIter(&iterOpts)\n\t\tfor j := lower; j < upper; j++ {\n\t\t\tif !it.SeekPrefixGE(keys[j]) {\n\t\t\t\tb.Errorf(\"key %q missing\", keys[j])\n\t\t\t}\n\t\t}\n\t\trequire.NoError(b, it.Close())\n\t}\n}\n\nfunc buildFragmentedRangeKey(b testing.TB, seed uint64) (d *DB, keys [][]byte) {\n\trng := rand.New(rand.NewPCG(0, seed))\n\tks := testkeys.Alpha(2)\n\topts := &Options{\n\t\tFS:                        vfs.NewMem(),\n\t\tComparer:                  testkeys.Comparer,\n\t\tFormatMajorVersion:        FormatNewest,\n\t\tL0CompactionFileThreshold: 1,\n\t}\n\topts.EnsureDefaults()\n\tfor l := 0; l < len(opts.Levels); l++ {\n\t\topts.Levels[l].TargetFileSize = 1\n\t}\n\tvar err error\n\td, err = Open(\"\", opts)\n\trequire.NoError(b, err)\n\n\tkeys = make([][]byte, ks.Count())\n\tfor i := int64(0); i < ks.Count(); i++ {\n\t\tkeys[i] = testkeys.Key(ks, i)\n\t}\n\tfor i := 0; i < len(keys); i++ {\n\t\tvar val [40]byte\n\t\tfor j := range val {\n\t\t\tval[j] = byte(rng.Uint32())\n\t\t}\n\t\trequire.NoError(b, d.Set(keys[i], val[:], nil))\n\t\tif i < len(keys)-1 {\n\t\t\trequire.NoError(b, d.RangeKeySet(keys[i], keys[i+1], []byte(\"@5\"), nil, nil))\n\t\t}\n\t\trequire.NoError(b, d.Flush())\n\t}\n\n\td.mu.Lock()\n\tfor d.mu.compact.compactingCount > 0 {\n\t\td.mu.compact.cond.Wait()\n\t}\n\tv := d.mu.versions.currentVersion()\n\td.mu.Unlock()\n\trequire.GreaterOrEqualf(b, v.Levels[numLevels-1].Len(),\n\t\t700, \"expect many (≥700) L6 files but found %d\", v.Levels[numLevels-1].Len())\n\treturn d, keys\n}\n\n// BenchmarkSeekPrefixTombstones benchmarks a SeekPrefixGE into the beginning of\n// a series of sstables containing exclusively range tombstones. Previously,\n// such a seek would next through all the tombstone files until it arrived at a\n// point key or exhausted the level's files. The SeekPrefixGE should not next\n// beyond the files that contain the prefix.\n//\n// See cockroachdb/cockroach#89327.\nfunc BenchmarkSeekPrefixTombstones(b *testing.B) {\n\to := (&Options{\n\t\tFS:                 vfs.NewMem(),\n\t\tComparer:           testkeys.Comparer,\n\t\tFormatMajorVersion: FormatNewest,\n\t}).EnsureDefaults()\n\td, err := Open(\"\", o)\n\trequire.NoError(b, err)\n\tdefer func() { require.NoError(b, d.Close()) }()\n\twOpts := o.MakeWriterOptions(numLevels-1, d.TableFormat())\n\n\t// Keep a snapshot open for the duration of the test to prevent elision-only\n\t// compactions from removing the ingested files containing exclusively\n\t// elidable tombstones.\n\tdefer d.NewSnapshot().Close()\n\n\tks := testkeys.Alpha(2)\n\tfor i := int64(0); i < ks.Count()-1; i++ {\n\t\tfunc() {\n\t\t\tfilename := fmt.Sprintf(\"ext%2d\", i)\n\t\t\tf, err := o.FS.Create(filename, vfs.WriteCategoryUnspecified)\n\t\t\trequire.NoError(b, err)\n\t\t\tw := sstable.NewWriter(objstorageprovider.NewFileWritable(f), wOpts)\n\t\t\trequire.NoError(b, w.DeleteRange(testkeys.Key(ks, i), testkeys.Key(ks, i+1)))\n\t\t\trequire.NoError(b, w.Close())\n\t\t\trequire.NoError(b, d.Ingest(context.Background(), []string{filename}))\n\t\t}()\n\t}\n\n\td.mu.Lock()\n\trequire.Equal(b, int64(ks.Count()-1), d.mu.versions.metrics.Levels[numLevels-1].NumFiles)\n\td.mu.Unlock()\n\n\tseekKey := testkeys.Key(ks, 1)\n\titer, _ := d.NewIter(nil)\n\tdefer iter.Close()\n\tb.ResetTimer()\n\tdefer b.StopTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\titer.SeekPrefixGE(seekKey)\n\t}\n}\n\nfunc waitForCompactionsAndTableStats(d *DB) {\n\td.mu.Lock()\n\t// NB: Wait for table stats because some compaction types rely\n\t// on table stats to be collected.\n\td.waitTableStats()\n\tfor d.mu.compact.compactingCount > 0 {\n\t\td.mu.compact.cond.Wait()\n\t\td.waitTableStats()\n\t}\n\td.mu.Unlock()\n}\n\n// BenchmarkPointDeletedSwath benchmarks iterator operations on large-ish\n// (hundreds of MBs) databases containing broad swaths of keys removed by point\n// tombstones.\nfunc BenchmarkPointDeletedSwath(b *testing.B) {\n\tconst maxKeyLen = 5\n\tks := testkeys.Alpha(maxKeyLen)\n\n\topts := func() *Options {\n\t\treturn (&Options{\n\t\t\tFS:                 vfs.NewMem(),\n\t\t\tComparer:           testkeys.Comparer,\n\t\t\tFormatMajorVersion: FormatNewest,\n\t\t}).EnsureDefaults()\n\t}\n\ttype iteratorOp struct {\n\t\tname string\n\t\tfn   func(*Iterator, testkeys.Keyspace, *rand.Rand)\n\t}\n\tvar iterKeyBuf [maxKeyLen]byte\n\n\titerOps := []iteratorOp{\n\t\t{\n\t\t\tname: \"seek-prefix-ge\", fn: func(iter *Iterator, ks testkeys.Keyspace, rng *rand.Rand) {\n\t\t\t\tn := testkeys.WriteKey(iterKeyBuf[:], ks, int64(rng.IntN(int(ks.Count()))))\n\t\t\t\t_ = iter.SeekPrefixGE(iterKeyBuf[:n])\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"seek-ge\", fn: func(iter *Iterator, ks testkeys.Keyspace, rng *rand.Rand) {\n\t\t\t\tn := testkeys.WriteKey(iterKeyBuf[:], ks, int64(rng.IntN(int(ks.Count()))))\n\t\t\t\t_ = iter.SeekGE(iterKeyBuf[:n])\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"iterate\", fn: func(iter *Iterator, ks testkeys.Keyspace, rng *rand.Rand) {\n\t\t\t\tvalid := iter.Next()\n\t\t\t\tif !valid {\n\t\t\t\t\titer.First()\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\t}\n\n\t// Populate an initial database with point keys at every key in the `ks`\n\t// keyspace.\n\tpopulated := withStateSetup(b, vfs.NewMem(), opts(), populateKeyspaceSetup(ks))\n\tfor _, gapLength := range []int{100, 1_000, 10_000, 100_000, 200_000, 400_000, 1_000_000, 2_000_000, 5_000_000, 10_000_000} {\n\t\tb.Run(fmt.Sprintf(\"gap=%d\", gapLength), func(b *testing.B) {\n\t\t\t// Extend the `populated` initial database with DELs deleting all\n\t\t\t// the middle keys in the keyspace in a contiguous swath of\n\t\t\t// `gapLength` keys.\n\t\t\tgapDeleted := withStateSetup(b, populated, opts(), deleteGapSetup(ks, gapLength))\n\n\t\t\tfor _, op := range iterOps {\n\t\t\t\tb.Run(op.name, func(b *testing.B) {\n\t\t\t\t\t// Run each instance of the test in a fresh DB constructed\n\t\t\t\t\t// from `compacted`. This ensures background compactions\n\t\t\t\t\t// from one iterator operation don't affect another iterator\n\t\t\t\t\t// option.\n\t\t\t\t\twithStateSetup(b, gapDeleted, opts(), func(_ testing.TB, d *DB) {\n\t\t\t\t\t\trng := rand.New(rand.NewPCG(0, 1) /* fixed seed */)\n\t\t\t\t\t\titer, err := d.NewIter(nil)\n\t\t\t\t\t\trequire.NoError(b, err)\n\t\t\t\t\t\tb.ResetTimer()\n\t\t\t\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t\t\t\top.fn(iter, ks, rng)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tb.StopTimer()\n\t\t\t\t\t\trequire.NoError(b, iter.Close())\n\t\t\t\t\t})\n\t\t\t\t})\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc withStateSetup(\n\tt testing.TB, initial vfs.FS, opts *Options, setup func(testing.TB, *DB),\n) vfs.FS {\n\tok, err := vfs.Clone(initial, opts.FS, \"\", \"\", vfs.CloneSync)\n\trequire.NoError(t, err)\n\trequire.True(t, ok)\n\td, err := Open(\"\", opts)\n\trequire.NoError(t, err)\n\tdefer func() { require.NoError(t, d.Close()) }()\n\tsetup(t, d)\n\treturn opts.FS\n}\n\nfunc populateKeyspaceSetup(ks testkeys.Keyspace) func(testing.TB, *DB) {\n\tconst valSize = 256\n\treturn func(t testing.TB, d *DB) {\n\t\tt.Logf(\"Populating keyspace with %d keys, each with %d-byte values\", ks.Count(), valSize)\n\t\t// Parallelize population by divvying up the keyspace.\n\t\tvar grp errgroup.Group\n\t\tloadKeyspaces := testkeys.Divvy(ks, 20)\n\t\tvar progress atomic.Uint64\n\t\tfor l := 0; l < len(loadKeyspaces); l++ {\n\t\t\tl := l\n\t\t\tgrp.Go(func() error {\n\t\t\t\trng := rand.New(rand.NewPCG(1, 1))\n\t\t\t\tbatch := d.NewBatch()\n\t\t\t\tkey := make([]byte, ks.MaxLen())\n\t\t\t\tvar val [valSize]byte\n\t\t\t\tfor i := int64(0); i < loadKeyspaces[l].Count(); i++ {\n\t\t\t\t\tfor j := range val {\n\t\t\t\t\t\tval[j] = byte(rng.Uint32())\n\t\t\t\t\t}\n\t\t\t\t\tn := testkeys.WriteKey(key, loadKeyspaces[l], i)\n\t\t\t\t\tif err := batch.Set(key[:n], val[:], nil); err != nil {\n\t\t\t\t\t\treturn err\n\t\t\t\t\t}\n\t\t\t\t\tif batch.Len() >= 10<<10 /* 10 kib */ {\n\t\t\t\t\t\tcount := batch.Count()\n\t\t\t\t\t\trequire.NoError(t, batch.Commit(NoSync))\n\t\t\t\t\t\tif newTotal := progress.Add(uint64(count)); (newTotal / (uint64(ks.Count()) / 100)) != (newTotal-uint64(count))/uint64(ks.Count()/100) {\n\t\t\t\t\t\t\tt.Logf(\"%.1f%% populated\", 100.0*(float64(newTotal)/float64(ks.Count())))\n\t\t\t\t\t\t}\n\t\t\t\t\t\tbatch = d.NewBatch()\n\t\t\t\t\t\td.AsyncFlush()\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif !batch.Empty() {\n\t\t\t\t\treturn batch.Commit(NoSync)\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t})\n\t\t}\n\t\trequire.NoError(t, grp.Wait())\n\t}\n}\n\nfunc deleteGapSetup(ks testkeys.Keyspace, gapLength int) func(testing.TB, *DB) {\n\treturn func(t testing.TB, d *DB) {\n\t\tmidpoint := ks.Count() / 2\n\t\tgapStart := midpoint - int64(gapLength/2)\n\t\tgapEnd := midpoint + int64(gapLength/2+(gapLength%2))\n\n\t\tbatch := d.NewBatch()\n\t\tkey := make([]byte, ks.MaxLen())\n\t\tfor i := gapStart; i <= gapEnd; i++ {\n\t\t\tn := testkeys.WriteKey(key, ks, i)\n\t\t\tif err := batch.Delete(key[:n], nil); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tif batch.Len() >= 10<<10 /* 10 kib */ {\n\t\t\t\tif err := batch.Commit(NoSync); err != nil {\n\t\t\t\t\tt.Fatal(err)\n\t\t\t\t}\n\t\t\t\tbatch = d.NewBatch()\n\t\t\t}\n\t\t}\n\t\tif !batch.Empty() {\n\t\t\tif err := batch.Commit(NoSync); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t}\n\t\tif err := d.Flush(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\twaitForCompactionsAndTableStats(d)\n\t}\n}\n\nfunc runBenchmarkQueueWorkload(b *testing.B, deleteRatio float32, initOps int, valueSize int) {\n\tconst queueCount = 8\n\t// These should be large enough to assign a unique key to each item in the\n\t// queue.\n\tconst maxQueueIDLen = 1\n\tconst maxItemLen = 7\n\tconst maxKeyLen = maxQueueIDLen + 1 + maxItemLen\n\tqueueIDKeyspace := testkeys.Alpha(maxQueueIDLen)\n\titemKeyspace := testkeys.Alpha(maxItemLen)\n\tkey := make([]byte, maxKeyLen)\n\tval := make([]byte, valueSize)\n\trng := rand.New(rand.NewPCG(0, uint64(time.Now().UnixNano())))\n\n\tgetKey := func(q int, i int) []byte {\n\t\tn := testkeys.WriteKey(key, queueIDKeyspace, int64(q))\n\t\tkey[n] = '/'\n\t\tprefixLen := n + 1\n\t\tn = testkeys.WriteKey(key[prefixLen:], itemKeyspace, int64(i))\n\t\treturn key[:prefixLen+n]\n\t}\n\n\ttype Queue struct {\n\t\tstart int\n\t\tend   int // exclusive\n\t}\n\tvar queues = make([]*Queue, queueCount)\n\tfor i := 0; i < queueCount; i++ {\n\t\tqueues[i] = &Queue{}\n\t}\n\n\to := (&Options{\n\t\tCache:              cache.New(0), // disable cache\n\t\tDisableWAL:         true,\n\t\tFS:                 vfs.NewMem(),\n\t\tComparer:           testkeys.Comparer,\n\t\tFormatMajorVersion: FormatNewest,\n\t}).EnsureDefaults()\n\n\td, err := Open(\"\", o)\n\trequire.NoError(b, err)\n\n\tprocessQueueOnce := func(batch *Batch) {\n\t\tfor {\n\t\t\t// Randomly pick a queue to process.\n\t\t\tq := rng.IntN(queueCount)\n\t\t\tqueue := queues[q]\n\n\t\t\tisDelete := rng.Float32() < deleteRatio\n\n\t\t\tif isDelete {\n\t\t\t\t// Only process the queue if it's not empty. Otherwise, retry\n\t\t\t\t// with a different queue.\n\t\t\t\tif queue.start != queue.end {\n\t\t\t\t\trequire.NoError(b, batch.Delete(getKey(q, queue.start), nil))\n\t\t\t\t\tqueue.start = (queue.start + 1) % int(itemKeyspace.Count())\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// Append to the queue.\n\t\t\t\trequire.NoError(b, batch.Set(getKey(q, queue.end), val, nil))\n\t\t\t\tqueue.end = (queue.end + 1) % int(itemKeyspace.Count())\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\n\t// First, process queues initialOps times.\n\tbatch := d.NewBatch()\n\tfor i := 0; i < initOps; i++ {\n\t\tprocessQueueOnce(batch)\n\t\t// Use a large batch size to speed up initialization.\n\t\tif batch.Len() >= 10<<24 /* 167 MiB */ {\n\t\t\trequire.NoError(b, batch.Commit(NoSync))\n\t\t\tbatch = d.NewBatch()\n\t\t}\n\t}\n\trequire.NoError(b, batch.Commit(NoSync))\n\t// Manually flush in case the last batch was small.\n\t_, err = d.AsyncFlush()\n\trequire.NoError(b, err)\n\n\twaitForCompactionsAndTableStats(d)\n\n\t// Log the number of tombstones and live keys in each level after\n\t// background compactions are complete.\n\tb.Log(\"LSM after compactions:\")\n\tfirstIter, _ := d.NewIter(nil)\n\tfirstIter.First()\n\tlastIter, _ := d.NewIter(nil)\n\tlastIter.Last()\n\tstats, _ := d.ScanStatistics(context.Background(), firstIter.Key(), lastIter.Key(), ScanStatisticsOptions{})\n\trequire.NoError(b, firstIter.Close())\n\trequire.NoError(b, lastIter.Close())\n\tmetrics := d.Metrics()\n\tfor i := 0; i < numLevels; i++ {\n\t\tnumTombstones := stats.Levels[i].KindsCount[base.InternalKeyKindDelete]\n\t\tnumSets := stats.Levels[i].KindsCount[base.InternalKeyKindSet]\n\t\tnumTables := metrics.Levels[i].NumFiles\n\t\tif numSets > 0 {\n\t\t\tb.Logf(\"L%d: %d tombstones, %d sets, %d sstables\\n\", i, numTombstones, numSets, numTables)\n\t\t}\n\t}\n\n\t// Seek to the start of each queue.\n\tb.Run(\"seek\", func(b *testing.B) {\n\t\titer, _ := d.NewIter(nil)\n\t\tb.ResetTimer()\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\tfor q := 0; q < queueCount; q++ {\n\t\t\t\titer.SeekGE(getKey(q, 0))\n\t\t\t}\n\t\t}\n\t\tb.StopTimer()\n\t\trequire.NoError(b, iter.Close())\n\t})\n\n\trequire.NoError(b, d.Close())\n}\n\n// BenchmarkQueueWorkload benchmarks a workload consisting of multiple queues\n// that are all being processed at the same time. Processing a queue entails\n// either appending to the end of the queue (a Set operation) or deleting from\n// the start of the queue (a Delete operation). The goal is to detect cases\n// where we see a large buildup of point tombstones at the beginning of each\n// queue, which leads to the slowdown of SeekGE(<start of queue>). To that end,\n// the test subbenchmarks a series of configurations that each 1) process the\n// queues a certain number of times and then 2) benchmark both the queue\n// processing throughput and SeekGE performance. See\n// https://github.com/facebook/rocksdb/wiki/Implement-Queue-Service-Using-RocksDB\n// for more information.\nfunc BenchmarkQueueWorkload(b *testing.B) {\n\t// The portion of processing ops that will be deletes for each subbenchmark.\n\tvar deleteRatios = []float32{0.1, 0.3, 0.5}\n\t// The number of times queues will be processed before running each\n\t// subbenchmark.\n\tvar initOps = []int{400_000, 800_000, 1_200_000, 2_000_000, 3_500_000, 5_000_000, 7_500_000}\n\t// We vary the value size to identify how compaction behaves when the\n\t// relative sizes of tombstones and the keys they delete are different.\n\tvar valueSizes = []int{128, 2048}\n\n\tfor _, deleteRatio := range deleteRatios {\n\t\tfor _, valueSize := range valueSizes {\n\t\t\tfor _, numInitOps := range initOps {\n\t\t\t\tb.Run(fmt.Sprintf(\"initial_ops=%d/deleteRatio=%.2f/valueSize=%d\", numInitOps, deleteRatio, valueSize), func(b *testing.B) {\n\t\t\t\t\trunBenchmarkQueueWorkload(b, deleteRatio, numInitOps, valueSize)\n\t\t\t\t})\n\t\t\t}\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "keyspan_probe_test.go",
          "type": "blob",
          "size": 10.1181640625,
          "content": "// Copyright 2023 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"go/token\"\n\t\"io\"\n\t\"reflect\"\n\t\"strconv\"\n\t\"strings\"\n\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/dsl\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan\"\n\t\"github.com/cockroachdb/pebble/internal/treeprinter\"\n)\n\n// This file contains testing facilities for Spans and FragmentIterators. It's\n// a copy of the facilities defined in internal/keyspan/datadriven_test.go.\n//\n// TODO(jackson): Move keyspan.{Span,Key,FragmentIterator} into internal/base,\n// and then move the testing facilities to an independent package, eg\n// internal/itertest, where it may be shared by both uses.\n\n// keyspanProbe defines an interface for probes that may inspect or mutate internal\n// span iterator behavior.\ntype keyspanProbe interface {\n\t// probe inspects, and possibly manipulates, iterator operations' results.\n\tprobe(*keyspanProbeContext)\n}\n\nfunc parseKeyspanProbes(probeDSLs ...string) []keyspanProbe {\n\tprobes := make([]keyspanProbe, len(probeDSLs))\n\tvar err error\n\tfor i := range probeDSLs {\n\t\tprobes[i], err = probeParser.Parse(probeDSLs[i])\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t}\n\treturn probes\n}\n\nfunc attachKeyspanProbes(\n\titer keyspan.FragmentIterator, pctx keyspanProbeContext, probes ...keyspanProbe,\n) keyspan.FragmentIterator {\n\tif pctx.log == nil {\n\t\tpctx.log = io.Discard\n\t}\n\tfor i := range probes {\n\t\titer = &probeKeyspanIterator{\n\t\t\titer:     iter,\n\t\t\tprobe:    probes[i],\n\t\t\tprobeCtx: pctx,\n\t\t}\n\t}\n\treturn iter\n}\n\n// keyspanProbeContext provides the context within which a probe is run. It includes\n// information about the iterator operation in progress.\ntype keyspanProbeContext struct {\n\tkeyspanOp\n\tlog io.Writer\n}\n\ntype keyspanOp struct {\n\tKind    keyspanOpKind\n\tSeekKey []byte\n\tSpan    *keyspan.Span\n\tErr     error\n}\n\n// errInjected is an error artificially injected for testing.\nvar errInjected = &errorProbe{name: \"ErrInjected\", err: errors.New(\"injected error\")}\n\nvar probeParser = func() *dsl.Parser[keyspanProbe] {\n\tvaluerParser := dsl.NewParser[valuer]()\n\tvaluerParser.DefineConstant(\"StartKey\", func() valuer { return startKey{} })\n\tvaluerParser.DefineConstant(\"SeekKey\", func() valuer { return seekKey{} })\n\tvaluerParser.DefineFunc(\"Bytes\",\n\t\tfunc(p *dsl.Parser[valuer], s *dsl.Scanner) valuer {\n\t\t\tv := bytesConstant{bytes: []byte(s.ConsumeString())}\n\t\t\ts.Consume(token.RPAREN)\n\t\t\treturn v\n\t\t})\n\n\tpredicateParser := dsl.NewPredicateParser[*keyspanProbeContext]()\n\tpredicateParser.DefineFunc(\"Equal\",\n\t\tfunc(p *dsl.Parser[dsl.Predicate[*keyspanProbeContext]], s *dsl.Scanner) dsl.Predicate[*keyspanProbeContext] {\n\t\t\teq := equal{\n\t\t\t\tvaluerParser.ParseFromPos(s, s.Scan()),\n\t\t\t\tvaluerParser.ParseFromPos(s, s.Scan()),\n\t\t\t}\n\t\t\ts.Consume(token.RPAREN)\n\t\t\treturn eq\n\t\t})\n\tfor i, name := range opNames {\n\t\topKind := keyspanOpKind(i)\n\t\tpredicateParser.DefineConstant(name, func() dsl.Predicate[*keyspanProbeContext] {\n\t\t\t// An OpKind implements dsl.Predicate[*probeContext].\n\t\t\treturn opKind\n\t\t})\n\t}\n\tprobeParser := dsl.NewParser[keyspanProbe]()\n\tprobeParser.DefineConstant(\"ErrInjected\", func() keyspanProbe { return errInjected })\n\tprobeParser.DefineConstant(\"noop\", func() keyspanProbe { return noop{} })\n\tprobeParser.DefineFunc(\"If\",\n\t\tfunc(p *dsl.Parser[keyspanProbe], s *dsl.Scanner) keyspanProbe {\n\t\t\tprobe := ifProbe{\n\t\t\t\tpredicateParser.ParseFromPos(s, s.Scan()),\n\t\t\t\tprobeParser.ParseFromPos(s, s.Scan()),\n\t\t\t\tprobeParser.ParseFromPos(s, s.Scan()),\n\t\t\t}\n\t\t\ts.Consume(token.RPAREN)\n\t\t\treturn probe\n\t\t})\n\tprobeParser.DefineFunc(\"Return\",\n\t\tfunc(p *dsl.Parser[keyspanProbe], s *dsl.Scanner) (ret keyspanProbe) {\n\t\t\tswitch tok := s.Scan(); tok.Kind {\n\t\t\tcase token.STRING:\n\t\t\t\tstr, err := strconv.Unquote(tok.Lit)\n\t\t\t\tif err != nil {\n\t\t\t\t\tpanic(err)\n\t\t\t\t}\n\t\t\t\tspan := keyspan.ParseSpan(str)\n\t\t\t\tret = returnSpan{s: &span}\n\t\t\tcase token.IDENT:\n\t\t\t\tswitch tok.Lit {\n\t\t\t\tcase \"nil\":\n\t\t\t\t\tret = returnSpan{s: nil}\n\t\t\t\tdefault:\n\t\t\t\t\tpanic(errors.Newf(\"unrecognized return value %q\", tok.Lit))\n\t\t\t\t}\n\t\t\t}\n\t\t\ts.Consume(token.RPAREN)\n\t\t\treturn ret\n\t\t})\n\tprobeParser.DefineFunc(\"Log\",\n\t\tfunc(p *dsl.Parser[keyspanProbe], s *dsl.Scanner) (ret keyspanProbe) {\n\t\t\tret = loggingProbe{prefix: s.ConsumeString()}\n\t\t\ts.Consume(token.RPAREN)\n\t\t\treturn ret\n\t\t})\n\treturn probeParser\n}()\n\n// probe implementations\n\ntype errorProbe struct {\n\tname string\n\terr  error\n}\n\nfunc (p *errorProbe) String() string { return p.name }\nfunc (p *errorProbe) Error() error   { return p.err }\nfunc (p *errorProbe) probe(pctx *keyspanProbeContext) {\n\tpctx.keyspanOp.Err = p.err\n\tpctx.keyspanOp.Span = nil\n}\n\n// ifProbe is a conditional probe. If its predicate evaluates to true, it probes\n// using its Then probe. If its predicate evalutes to false, it probes using its\n// Else probe.\ntype ifProbe struct {\n\tPredicate dsl.Predicate[*keyspanProbeContext]\n\tThen      keyspanProbe\n\tElse      keyspanProbe\n}\n\nfunc (p ifProbe) String() string { return fmt.Sprintf(\"(If %s %s %s)\", p.Predicate, p.Then, p.Else) }\nfunc (p ifProbe) probe(pctx *keyspanProbeContext) {\n\tif p.Predicate.Evaluate(pctx) {\n\t\tp.Then.probe(pctx)\n\t} else {\n\t\tp.Else.probe(pctx)\n\t}\n}\n\ntype returnSpan struct {\n\ts *keyspan.Span\n}\n\nfunc (p returnSpan) String() string {\n\tif p.s == nil {\n\t\treturn \"(Return nil)\"\n\t}\n\treturn fmt.Sprintf(\"(Return %q)\", p.s.String())\n}\n\nfunc (p returnSpan) probe(pctx *keyspanProbeContext) {\n\tpctx.keyspanOp.Span = p.s\n\tpctx.keyspanOp.Err = nil\n}\n\ntype noop struct{}\n\nfunc (noop) String() string                  { return \"Noop\" }\nfunc (noop) probe(pctx *keyspanProbeContext) {}\n\ntype loggingProbe struct {\n\tprefix string\n}\n\nfunc (lp loggingProbe) String() string { return fmt.Sprintf(\"(Log %q)\", lp.prefix) }\nfunc (lp loggingProbe) probe(pctx *keyspanProbeContext) {\n\topStr := strings.TrimPrefix(pctx.keyspanOp.Kind.String(), \"Op\")\n\tfmt.Fprintf(pctx.log, \"%s%s(\", lp.prefix, opStr)\n\tif pctx.keyspanOp.SeekKey != nil {\n\t\tfmt.Fprintf(pctx.log, \"%q\", pctx.keyspanOp.SeekKey)\n\t}\n\tfmt.Fprint(pctx.log, \") = \")\n\tif pctx.keyspanOp.Span == nil {\n\t\tfmt.Fprint(pctx.log, \"nil\")\n\t\tif pctx.keyspanOp.Err != nil {\n\t\t\tfmt.Fprintf(pctx.log, \" <err=%q>\", pctx.keyspanOp.Err)\n\t\t}\n\t} else {\n\t\tfmt.Fprint(pctx.log, pctx.keyspanOp.Span.String())\n\t}\n\tfmt.Fprintln(pctx.log)\n}\n\n// dsl.Predicate[*probeContext] implementations.\n\ntype equal struct {\n\ta, b valuer\n}\n\nfunc (e equal) String() string { return fmt.Sprintf(\"(Equal %s %s)\", e.a, e.b) }\nfunc (e equal) Evaluate(pctx *keyspanProbeContext) bool {\n\treturn reflect.DeepEqual(e.a.value(pctx), e.b.value(pctx))\n}\n\n// keyspanOpKind indicates the type of iterator operation being performed.\ntype keyspanOpKind int8\n\nconst (\n\topSpanSeekGE keyspanOpKind = iota\n\topSpanSeekLT\n\topSpanFirst\n\topSpanLast\n\topSpanNext\n\topSpanPrev\n\topSpanClose\n\tnumKeyspanOpKinds\n)\n\nfunc (o keyspanOpKind) String() string                          { return opNames[o] }\nfunc (o keyspanOpKind) Evaluate(pctx *keyspanProbeContext) bool { return pctx.keyspanOp.Kind == o }\n\nvar opNames = [numKeyspanOpKinds]string{\n\topSpanSeekGE: \"opSpanSeekGE\",\n\topSpanSeekLT: \"opSpanSeekLT\",\n\topSpanFirst:  \"opSpanFirst\",\n\topSpanLast:   \"opSpanLast\",\n\topSpanNext:   \"opSpanNext\",\n\topSpanPrev:   \"opSpanPrev\",\n\topSpanClose:  \"opSpanClose\",\n}\n\n// valuer implementations\n\ntype valuer interface {\n\tfmt.Stringer\n\tvalue(pctx *keyspanProbeContext) any\n}\n\ntype bytesConstant struct {\n\tbytes []byte\n}\n\nfunc (b bytesConstant) String() string                      { return fmt.Sprintf(\"%q\", string(b.bytes)) }\nfunc (b bytesConstant) value(pctx *keyspanProbeContext) any { return b.bytes }\n\ntype startKey struct{}\n\nfunc (s startKey) String() string { return \"StartKey\" }\nfunc (s startKey) value(pctx *keyspanProbeContext) any {\n\tif pctx.keyspanOp.Span == nil {\n\t\treturn nil\n\t}\n\treturn pctx.keyspanOp.Span.Start\n}\n\ntype seekKey struct{}\n\nfunc (s seekKey) String() string { return \"SeekKey\" }\nfunc (s seekKey) value(pctx *keyspanProbeContext) any {\n\tif pctx.keyspanOp.SeekKey == nil {\n\t\treturn nil\n\t}\n\treturn pctx.keyspanOp.SeekKey\n}\n\ntype probeKeyspanIterator struct {\n\titer     keyspan.FragmentIterator\n\tprobe    keyspanProbe\n\tprobeCtx keyspanProbeContext\n}\n\n// Assert that probeIterator implements the fragment iterator interface.\nvar _ keyspan.FragmentIterator = (*probeKeyspanIterator)(nil)\n\nfunc (p *probeKeyspanIterator) handleOp(preProbeOp keyspanOp) (*keyspan.Span, error) {\n\tp.probeCtx.keyspanOp = preProbeOp\n\tp.probe.probe(&p.probeCtx)\n\treturn p.probeCtx.keyspanOp.Span, p.probeCtx.keyspanOp.Err\n}\n\nfunc (p *probeKeyspanIterator) SeekGE(key []byte) (*keyspan.Span, error) {\n\top := keyspanOp{\n\t\tKind:    opSpanSeekGE,\n\t\tSeekKey: key,\n\t}\n\tif p.iter != nil {\n\t\top.Span, op.Err = p.iter.SeekGE(key)\n\t}\n\treturn p.handleOp(op)\n}\n\nfunc (p *probeKeyspanIterator) SeekLT(key []byte) (*keyspan.Span, error) {\n\top := keyspanOp{\n\t\tKind:    opSpanSeekLT,\n\t\tSeekKey: key,\n\t}\n\tif p.iter != nil {\n\t\top.Span, op.Err = p.iter.SeekLT(key)\n\t}\n\treturn p.handleOp(op)\n}\n\nfunc (p *probeKeyspanIterator) First() (*keyspan.Span, error) {\n\top := keyspanOp{Kind: opSpanFirst}\n\tif p.iter != nil {\n\t\top.Span, op.Err = p.iter.First()\n\t}\n\treturn p.handleOp(op)\n}\n\nfunc (p *probeKeyspanIterator) Last() (*keyspan.Span, error) {\n\top := keyspanOp{Kind: opSpanLast}\n\tif p.iter != nil {\n\t\top.Span, op.Err = p.iter.Last()\n\t}\n\treturn p.handleOp(op)\n}\n\nfunc (p *probeKeyspanIterator) Next() (*keyspan.Span, error) {\n\top := keyspanOp{Kind: opSpanNext}\n\tif p.iter != nil {\n\t\top.Span, op.Err = p.iter.Next()\n\t}\n\treturn p.handleOp(op)\n}\n\nfunc (p *probeKeyspanIterator) Prev() (*keyspan.Span, error) {\n\top := keyspanOp{Kind: opSpanPrev}\n\tif p.iter != nil {\n\t\top.Span, op.Err = p.iter.Prev()\n\t}\n\treturn p.handleOp(op)\n}\n\nfunc (p *probeKeyspanIterator) WrapChildren(wrap keyspan.WrapFn) {\n\tp.iter = wrap(p.iter)\n}\n\n// DebugTree is part of the FragmentIterator interface.\nfunc (p *probeKeyspanIterator) DebugTree(tp treeprinter.Node) {\n\tn := tp.Childf(\"%T(%p)\", p, p)\n\tif p.iter != nil {\n\t\tp.iter.DebugTree(n)\n\t}\n}\n\n// SetContext is part of the FragmentIterator interface.\nfunc (p *probeKeyspanIterator) SetContext(ctx context.Context) {\n\tp.iter.SetContext(ctx)\n}\n\nfunc (p *probeKeyspanIterator) Close() {\n\top := keyspanOp{Kind: opSpanClose}\n\tif p.iter != nil {\n\t\tp.iter.Close()\n\t}\n\t_, _ = p.handleOp(op)\n}\n"
        },
        {
          "name": "level_checker.go",
          "type": "blob",
          "size": 21.958984375,
          "content": "// Copyright 2019 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"sort\"\n\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n)\n\n// This file implements DB.CheckLevels() which checks that every entry in the\n// DB is consistent with respect to the level invariant: any point (or the\n// infinite number of points in a range tombstone) has a seqnum such that a\n// point with the same UserKey at a lower level has a lower seqnum. This is an\n// expensive check since it involves iterating over all the entries in the DB,\n// hence only intended for tests or tools.\n//\n// If we ignore range tombstones, the consistency checking of points can be\n// done with a simplified version of mergingIter. simpleMergingIter is that\n// simplified version of mergingIter that only needs to step through points\n// (analogous to only doing Next()). It can also easily accommodate\n// consistency checking of points relative to range tombstones.\n// simpleMergingIter does not do any seek optimizations present in mergingIter\n// (it minimally needs to seek the range delete iterators to position them at\n// or past the current point) since it does not want to miss points for\n// purposes of consistency checking.\n//\n// Mutual consistency of range tombstones is non-trivial to check. One needs\n// to detect inversions of the form [a, c)#8 at higher level and [b, c)#10 at\n// a lower level. The start key of the former is not contained in the latter\n// and we can't use the exclusive end key, c, for a containment check since it\n// is the sentinel key. We observe that if these tombstones were fragmented\n// wrt each other we would have [a, b)#8 and [b, c)#8 at the higher level and\n// [b, c)#10 at the lower level and then it is is trivial to compare the two\n// [b, c) tombstones. Note that this fragmentation needs to take into account\n// that tombstones in a file may be untruncated and need to act within the\n// bounds of the file. This checking is performed by checkRangeTombstones()\n// and its helper functions.\n\n// The per-level structure used by simpleMergingIter.\ntype simpleMergingIterLevel struct {\n\titer         internalIterator\n\trangeDelIter keyspan.FragmentIterator\n\n\titerKV    *base.InternalKV\n\ttombstone *keyspan.Span\n}\n\nfunc (ml *simpleMergingIterLevel) setRangeDelIter(iter keyspan.FragmentIterator) {\n\tml.tombstone = nil\n\tif ml.rangeDelIter != nil {\n\t\tml.rangeDelIter.Close()\n\t}\n\tml.rangeDelIter = iter\n}\n\ntype simpleMergingIter struct {\n\tlevels   []simpleMergingIterLevel\n\tsnapshot base.SeqNum\n\theap     simpleMergingIterHeap\n\t// The last point's key and level. For validation.\n\tlastKey     InternalKey\n\tlastLevel   int\n\tlastIterMsg string\n\t// A non-nil valueMerger means MERGE record processing is ongoing.\n\tvalueMerger base.ValueMerger\n\t// The first error will cause step() to return false.\n\terr       error\n\tnumPoints int64\n\tmerge     Merge\n\tformatKey base.FormatKey\n}\n\nfunc (m *simpleMergingIter) init(\n\tmerge Merge,\n\tcmp Compare,\n\tsnapshot base.SeqNum,\n\tformatKey base.FormatKey,\n\tlevels ...simpleMergingIterLevel,\n) {\n\tm.levels = levels\n\tm.formatKey = formatKey\n\tm.merge = merge\n\tm.snapshot = snapshot\n\tm.lastLevel = -1\n\tm.heap.cmp = cmp\n\tm.heap.items = make([]simpleMergingIterItem, 0, len(levels))\n\tfor i := range m.levels {\n\t\tl := &m.levels[i]\n\t\tl.iterKV = l.iter.First()\n\t\tif l.iterKV != nil {\n\t\t\titem := simpleMergingIterItem{\n\t\t\t\tindex: i,\n\t\t\t\tvalue: l.iterKV.V,\n\t\t\t}\n\t\t\titem.key = l.iterKV.K.Clone()\n\t\t\tm.heap.items = append(m.heap.items, item)\n\t\t}\n\t}\n\tm.heap.init()\n\n\tif m.heap.len() == 0 {\n\t\treturn\n\t}\n\tm.positionRangeDels()\n}\n\n// Positions all the rangedel iterators at or past the current top of the\n// heap, using SeekGE().\nfunc (m *simpleMergingIter) positionRangeDels() {\n\titem := &m.heap.items[0]\n\tfor i := range m.levels {\n\t\tl := &m.levels[i]\n\t\tif l.rangeDelIter == nil {\n\t\t\tcontinue\n\t\t}\n\t\tt, err := l.rangeDelIter.SeekGE(item.key.UserKey)\n\t\tm.err = firstError(m.err, err)\n\t\tl.tombstone = t\n\t}\n}\n\n// Returns true if not yet done.\nfunc (m *simpleMergingIter) step() bool {\n\tif m.heap.len() == 0 || m.err != nil {\n\t\treturn false\n\t}\n\titem := &m.heap.items[0]\n\tl := &m.levels[item.index]\n\t// Sentinels are not relevant for this point checking.\n\tif !item.key.IsExclusiveSentinel() && item.key.Visible(m.snapshot, base.SeqNumMax) {\n\t\t// This is a visible point key.\n\t\tif !m.handleVisiblePoint(item, l) {\n\t\t\treturn false\n\t\t}\n\t}\n\n\t// The iterator for the current level may be closed in the following call to\n\t// Next(). We save its debug string for potential use after it is closed -\n\t// either in this current step() invocation or on the next invocation.\n\tm.lastIterMsg = l.iter.String()\n\n\t// Step to the next point.\n\tl.iterKV = l.iter.Next()\n\tif l.iterKV == nil {\n\t\tm.err = errors.CombineErrors(l.iter.Error(), l.iter.Close())\n\t\tl.iter = nil\n\t\tm.heap.pop()\n\t} else {\n\t\t// Check point keys in an sstable are ordered. Although not required, we check\n\t\t// for memtables as well. A subtle check here is that successive sstables of\n\t\t// L1 and higher levels are ordered. This happens when levelIter moves to the\n\t\t// next sstable in the level, in which case item.key is previous sstable's\n\t\t// last point key.\n\t\tif !l.iterKV.K.IsExclusiveSentinel() && base.InternalCompare(m.heap.cmp, item.key, l.iterKV.K) >= 0 {\n\t\t\tm.err = errors.Errorf(\"out of order keys %s >= %s in %s\",\n\t\t\t\titem.key.Pretty(m.formatKey), l.iterKV.K.Pretty(m.formatKey), l.iter)\n\t\t\treturn false\n\t\t}\n\t\titem.key = base.InternalKey{\n\t\t\tTrailer: l.iterKV.K.Trailer,\n\t\t\tUserKey: append(item.key.UserKey[:0], l.iterKV.K.UserKey...),\n\t\t}\n\t\titem.value = l.iterKV.V\n\t\tif m.heap.len() > 1 {\n\t\t\tm.heap.fix(0)\n\t\t}\n\t}\n\tif m.err != nil {\n\t\treturn false\n\t}\n\tif m.heap.len() == 0 {\n\t\t// If m.valueMerger != nil, the last record was a MERGE record.\n\t\tif m.valueMerger != nil {\n\t\t\tvar closer io.Closer\n\t\t\tvar err error\n\t\t\t_, closer, err = m.valueMerger.Finish(true /* includesBase */)\n\t\t\tif closer != nil {\n\t\t\t\terr = errors.CombineErrors(err, closer.Close())\n\t\t\t}\n\t\t\tif err != nil {\n\t\t\t\tm.err = errors.CombineErrors(m.err,\n\t\t\t\t\terrors.Wrapf(err, \"merge processing error on key %s in %s\",\n\t\t\t\t\t\titem.key.Pretty(m.formatKey), m.lastIterMsg))\n\t\t\t}\n\t\t\tm.valueMerger = nil\n\t\t}\n\t\treturn false\n\t}\n\tm.positionRangeDels()\n\treturn true\n}\n\n// handleVisiblePoint returns true if validation succeeded and level checking\n// can continue.\nfunc (m *simpleMergingIter) handleVisiblePoint(\n\titem *simpleMergingIterItem, l *simpleMergingIterLevel,\n) (ok bool) {\n\tm.numPoints++\n\tkeyChanged := m.heap.cmp(item.key.UserKey, m.lastKey.UserKey) != 0\n\tif !keyChanged {\n\t\t// At the same user key. We will see them in decreasing seqnum\n\t\t// order so the lastLevel must not be lower.\n\t\tif m.lastLevel > item.index {\n\t\t\tm.err = errors.Errorf(\"found InternalKey %s in %s and InternalKey %s in %s\",\n\t\t\t\titem.key.Pretty(m.formatKey), l.iter, m.lastKey.Pretty(m.formatKey),\n\t\t\t\tm.lastIterMsg)\n\t\t\treturn false\n\t\t}\n\t\tm.lastLevel = item.index\n\t} else {\n\t\t// The user key has changed.\n\t\tm.lastKey.Trailer = item.key.Trailer\n\t\tm.lastKey.UserKey = append(m.lastKey.UserKey[:0], item.key.UserKey...)\n\t\tm.lastLevel = item.index\n\t}\n\t// Ongoing series of MERGE records ends with a MERGE record.\n\tif keyChanged && m.valueMerger != nil {\n\t\tvar closer io.Closer\n\t\t_, closer, m.err = m.valueMerger.Finish(true /* includesBase */)\n\t\tif m.err == nil && closer != nil {\n\t\t\tm.err = closer.Close()\n\t\t}\n\t\tm.valueMerger = nil\n\t}\n\titemValue, _, err := item.value.Value(nil)\n\tif err != nil {\n\t\tm.err = err\n\t\treturn false\n\t}\n\tif m.valueMerger != nil {\n\t\t// Ongoing series of MERGE records.\n\t\tswitch item.key.Kind() {\n\t\tcase InternalKeyKindSingleDelete, InternalKeyKindDelete, InternalKeyKindDeleteSized:\n\t\t\tvar closer io.Closer\n\t\t\t_, closer, m.err = m.valueMerger.Finish(true /* includesBase */)\n\t\t\tif m.err == nil && closer != nil {\n\t\t\t\tm.err = closer.Close()\n\t\t\t}\n\t\t\tm.valueMerger = nil\n\t\tcase InternalKeyKindSet, InternalKeyKindSetWithDelete:\n\t\t\tm.err = m.valueMerger.MergeOlder(itemValue)\n\t\t\tif m.err == nil {\n\t\t\t\tvar closer io.Closer\n\t\t\t\t_, closer, m.err = m.valueMerger.Finish(true /* includesBase */)\n\t\t\t\tif m.err == nil && closer != nil {\n\t\t\t\t\tm.err = closer.Close()\n\t\t\t\t}\n\t\t\t}\n\t\t\tm.valueMerger = nil\n\t\tcase InternalKeyKindMerge:\n\t\t\tm.err = m.valueMerger.MergeOlder(itemValue)\n\t\tdefault:\n\t\t\tm.err = errors.Errorf(\"pebble: invalid internal key kind %s in %s\",\n\t\t\t\titem.key.Pretty(m.formatKey),\n\t\t\t\tl.iter)\n\t\t\treturn false\n\t\t}\n\t} else if item.key.Kind() == InternalKeyKindMerge && m.err == nil {\n\t\t// New series of MERGE records.\n\t\tm.valueMerger, m.err = m.merge(item.key.UserKey, itemValue)\n\t}\n\tif m.err != nil {\n\t\tm.err = errors.Wrapf(m.err, \"merge processing error on key %s in %s\",\n\t\t\titem.key.Pretty(m.formatKey), l.iter)\n\t\treturn false\n\t}\n\t// Is this point covered by a tombstone at a lower level? Note that all these\n\t// iterators must be positioned at a key > item.key.\n\tfor level := item.index + 1; level < len(m.levels); level++ {\n\t\tlvl := &m.levels[level]\n\t\tif lvl.rangeDelIter == nil || lvl.tombstone.Empty() {\n\t\t\tcontinue\n\t\t}\n\t\tif lvl.tombstone.Contains(m.heap.cmp, item.key.UserKey) && lvl.tombstone.CoversAt(m.snapshot, item.key.SeqNum()) {\n\t\t\tm.err = errors.Errorf(\"tombstone %s in %s deletes key %s in %s\",\n\t\t\t\tlvl.tombstone.Pretty(m.formatKey), lvl.iter, item.key.Pretty(m.formatKey),\n\t\t\t\tl.iter)\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// Checking that range tombstones are mutually consistent is performed by\n// checkRangeTombstones(). See the overview comment at the top of the file.\n//\n// We do this check as follows:\n// - Collect the tombstones for each level, put them into one pool of tombstones\n//   along with their level information (addTombstonesFromIter()).\n// - Collect the start and end user keys from all these tombstones\n//   (collectAllUserKey()) and use them to fragment all the tombstones\n//   (fragmentUsingUserKey()).\n// - Sort tombstones by start key and decreasing seqnum\n//   (tombstonesByStartKeyAndSeqnum) - all tombstones that have the same start\n//   key will have the same end key because they have been fragmented.\n// - Iterate and check (iterateAndCheckTombstones()).\n//\n// Note that this simple approach requires holding all the tombstones across all\n// levels in-memory. A more sophisticated incremental approach could be devised,\n// if necessary.\n\n// A tombstone and the corresponding level it was found in.\ntype tombstoneWithLevel struct {\n\tkeyspan.Span\n\tlevel int\n\t// The level in LSM. A -1 means it's a memtable.\n\tlsmLevel int\n\tfileNum  FileNum\n}\n\n// For sorting tombstoneWithLevels in increasing order of start UserKey and\n// for the same start UserKey in decreasing order of seqnum.\ntype tombstonesByStartKeyAndSeqnum struct {\n\tcmp Compare\n\tbuf []tombstoneWithLevel\n}\n\nfunc (v *tombstonesByStartKeyAndSeqnum) Len() int { return len(v.buf) }\nfunc (v *tombstonesByStartKeyAndSeqnum) Less(i, j int) bool {\n\tless := v.cmp(v.buf[i].Start, v.buf[j].Start)\n\tif less == 0 {\n\t\treturn v.buf[i].LargestSeqNum() > v.buf[j].LargestSeqNum()\n\t}\n\treturn less < 0\n}\nfunc (v *tombstonesByStartKeyAndSeqnum) Swap(i, j int) {\n\tv.buf[i], v.buf[j] = v.buf[j], v.buf[i]\n}\n\nfunc iterateAndCheckTombstones(\n\tcmp Compare, formatKey base.FormatKey, tombstones []tombstoneWithLevel,\n) error {\n\tsortBuf := tombstonesByStartKeyAndSeqnum{\n\t\tcmp: cmp,\n\t\tbuf: tombstones,\n\t}\n\tsort.Sort(&sortBuf)\n\n\t// For a sequence of tombstones that share the same start UserKey, we will\n\t// encounter them in non-increasing seqnum order and so should encounter them\n\t// in non-decreasing level order.\n\tlastTombstone := tombstoneWithLevel{}\n\tfor _, t := range tombstones {\n\t\tif cmp(lastTombstone.Start, t.Start) == 0 && lastTombstone.level > t.level {\n\t\t\treturn errors.Errorf(\"encountered tombstone %s in %s\"+\n\t\t\t\t\" that has a lower seqnum than the same tombstone in %s\",\n\t\t\t\tt.Span.Pretty(formatKey), levelOrMemtable(t.lsmLevel, t.fileNum),\n\t\t\t\tlevelOrMemtable(lastTombstone.lsmLevel, lastTombstone.fileNum))\n\t\t}\n\t\tlastTombstone = t\n\t}\n\treturn nil\n}\n\ntype checkConfig struct {\n\tlogger    Logger\n\tcomparer  *Comparer\n\treadState *readState\n\tnewIters  tableNewIters\n\tseqNum    base.SeqNum\n\tstats     *CheckLevelsStats\n\tmerge     Merge\n\tformatKey base.FormatKey\n}\n\n// cmp is shorthand for comparer.Compare.\nfunc (c *checkConfig) cmp(a, b []byte) int { return c.comparer.Compare(a, b) }\n\nfunc checkRangeTombstones(c *checkConfig) error {\n\tvar level int\n\tvar tombstones []tombstoneWithLevel\n\tvar err error\n\n\tmemtables := c.readState.memtables\n\tfor i := len(memtables) - 1; i >= 0; i-- {\n\t\titer := memtables[i].newRangeDelIter(nil)\n\t\tif iter == nil {\n\t\t\tcontinue\n\t\t}\n\t\ttombstones, err = addTombstonesFromIter(\n\t\t\titer, level, -1, 0, tombstones, c.seqNum, c.cmp, c.formatKey,\n\t\t)\n\t\titer.Close()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlevel++\n\t}\n\n\tcurrent := c.readState.current\n\taddTombstonesFromLevel := func(files manifest.LevelIterator, lsmLevel int) error {\n\t\tfor f := files.First(); f != nil; f = files.Next() {\n\t\t\tlf := files.Take()\n\t\t\titers, err := c.newIters(\n\t\t\t\tcontext.Background(), lf.FileMetadata, &IterOptions{layer: manifest.Level(lsmLevel)},\n\t\t\t\tinternalIterOpts{}, iterRangeDeletions)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\ttombstones, err = addTombstonesFromIter(iters.RangeDeletion(), level, lsmLevel, f.FileNum,\n\t\t\t\ttombstones, c.seqNum, c.cmp, c.formatKey)\n\t\t\titers.CloseAll()\n\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}\n\t// Now the levels with untruncated tombsones.\n\tfor i := len(current.L0SublevelFiles) - 1; i >= 0; i-- {\n\t\tif current.L0SublevelFiles[i].Empty() {\n\t\t\tcontinue\n\t\t}\n\t\terr := addTombstonesFromLevel(current.L0SublevelFiles[i].Iter(), 0)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlevel++\n\t}\n\tfor i := 1; i < len(current.Levels); i++ {\n\t\tif err := addTombstonesFromLevel(current.Levels[i].Iter(), i); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlevel++\n\t}\n\tif c.stats != nil {\n\t\tc.stats.NumTombstones = len(tombstones)\n\t}\n\t// We now have truncated tombstones.\n\t// Fragment them all.\n\tuserKeys := collectAllUserKeys(c.cmp, tombstones)\n\ttombstones = fragmentUsingUserKeys(c.cmp, tombstones, userKeys)\n\treturn iterateAndCheckTombstones(c.cmp, c.formatKey, tombstones)\n}\n\nfunc levelOrMemtable(lsmLevel int, fileNum FileNum) string {\n\tif lsmLevel == -1 {\n\t\treturn \"memtable\"\n\t}\n\treturn fmt.Sprintf(\"L%d: fileNum=%s\", lsmLevel, fileNum)\n}\n\nfunc addTombstonesFromIter(\n\titer keyspan.FragmentIterator,\n\tlevel int,\n\tlsmLevel int,\n\tfileNum FileNum,\n\ttombstones []tombstoneWithLevel,\n\tseqNum base.SeqNum,\n\tcmp Compare,\n\tformatKey base.FormatKey,\n) (_ []tombstoneWithLevel, err error) {\n\tvar prevTombstone keyspan.Span\n\ttomb, err := iter.First()\n\tfor ; tomb != nil; tomb, err = iter.Next() {\n\t\tt := tomb.Visible(seqNum)\n\t\tif t.Empty() {\n\t\t\tcontinue\n\t\t}\n\t\tt = t.Clone()\n\t\t// This is mainly a test for rangeDelV2 formatted blocks which are expected to\n\t\t// be ordered and fragmented on disk. But we anyways check for memtables,\n\t\t// rangeDelV1 as well.\n\t\tif cmp(prevTombstone.End, t.Start) > 0 {\n\t\t\treturn nil, errors.Errorf(\"unordered or unfragmented range delete tombstones %s, %s in %s\",\n\t\t\t\tprevTombstone.Pretty(formatKey), t.Pretty(formatKey), levelOrMemtable(lsmLevel, fileNum))\n\t\t}\n\t\tprevTombstone = t\n\n\t\tif !t.Empty() {\n\t\t\ttombstones = append(tombstones, tombstoneWithLevel{\n\t\t\t\tSpan:     t,\n\t\t\t\tlevel:    level,\n\t\t\t\tlsmLevel: lsmLevel,\n\t\t\t\tfileNum:  fileNum,\n\t\t\t})\n\t\t}\n\t}\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn tombstones, nil\n}\n\ntype userKeysSort struct {\n\tcmp Compare\n\tbuf [][]byte\n}\n\nfunc (v *userKeysSort) Len() int { return len(v.buf) }\nfunc (v *userKeysSort) Less(i, j int) bool {\n\treturn v.cmp(v.buf[i], v.buf[j]) < 0\n}\nfunc (v *userKeysSort) Swap(i, j int) {\n\tv.buf[i], v.buf[j] = v.buf[j], v.buf[i]\n}\nfunc collectAllUserKeys(cmp Compare, tombstones []tombstoneWithLevel) [][]byte {\n\tkeys := make([][]byte, 0, len(tombstones)*2)\n\tfor _, t := range tombstones {\n\t\tkeys = append(keys, t.Start)\n\t\tkeys = append(keys, t.End)\n\t}\n\tsorter := userKeysSort{\n\t\tcmp: cmp,\n\t\tbuf: keys,\n\t}\n\tsort.Sort(&sorter)\n\tvar last, curr int\n\tfor last, curr = -1, 0; curr < len(keys); curr++ {\n\t\tif last < 0 || cmp(keys[last], keys[curr]) != 0 {\n\t\t\tlast++\n\t\t\tkeys[last] = keys[curr]\n\t\t}\n\t}\n\tkeys = keys[:last+1]\n\treturn keys\n}\n\nfunc fragmentUsingUserKeys(\n\tcmp Compare, tombstones []tombstoneWithLevel, userKeys [][]byte,\n) []tombstoneWithLevel {\n\tvar buf []tombstoneWithLevel\n\tfor _, t := range tombstones {\n\t\t// Find the first position with tombstone start < user key\n\t\ti := sort.Search(len(userKeys), func(i int) bool {\n\t\t\treturn cmp(t.Start, userKeys[i]) < 0\n\t\t})\n\t\tfor ; i < len(userKeys); i++ {\n\t\t\tif cmp(userKeys[i], t.End) >= 0 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\ttPartial := t\n\t\t\ttPartial.End = userKeys[i]\n\t\t\tbuf = append(buf, tPartial)\n\t\t\tt.Start = userKeys[i]\n\t\t}\n\t\tbuf = append(buf, t)\n\t}\n\treturn buf\n}\n\n// CheckLevelsStats provides basic stats on points and tombstones encountered.\ntype CheckLevelsStats struct {\n\tNumPoints     int64\n\tNumTombstones int\n}\n\n// CheckLevels checks:\n//   - Every entry in the DB is consistent with the level invariant. See the\n//     comment at the top of the file.\n//   - Point keys in sstables are ordered.\n//   - Range delete tombstones in sstables are ordered and fragmented.\n//   - Successful processing of all MERGE records.\nfunc (d *DB) CheckLevels(stats *CheckLevelsStats) error {\n\t// Grab and reference the current readState.\n\treadState := d.loadReadState()\n\tdefer readState.unref()\n\n\t// Determine the seqnum to read at after grabbing the read state (current and\n\t// memtables) above.\n\tseqNum := d.mu.versions.visibleSeqNum.Load()\n\n\tcheckConfig := &checkConfig{\n\t\tlogger:    d.opts.Logger,\n\t\tcomparer:  d.opts.Comparer,\n\t\treadState: readState,\n\t\tnewIters:  d.newIters,\n\t\tseqNum:    seqNum,\n\t\tstats:     stats,\n\t\tmerge:     d.merge,\n\t\tformatKey: d.opts.Comparer.FormatKey,\n\t}\n\treturn checkLevelsInternal(checkConfig)\n}\n\nfunc checkLevelsInternal(c *checkConfig) (err error) {\n\t// Phase 1: Use a simpleMergingIter to step through all the points and ensure\n\t// that points with the same user key at different levels are not inverted\n\t// wrt sequence numbers and the same holds for tombstones that cover points.\n\t// To do this, one needs to construct a simpleMergingIter which is similar to\n\t// how one constructs a mergingIter.\n\n\t// Add mem tables from newest to oldest.\n\tvar mlevels []simpleMergingIterLevel\n\tdefer func() {\n\t\tfor i := range mlevels {\n\t\t\tl := &mlevels[i]\n\t\t\tif l.iter != nil {\n\t\t\t\terr = firstError(err, l.iter.Close())\n\t\t\t\tl.iter = nil\n\t\t\t}\n\t\t\tif l.rangeDelIter != nil {\n\t\t\t\tl.rangeDelIter.Close()\n\t\t\t\tl.rangeDelIter = nil\n\t\t\t}\n\t\t}\n\t}()\n\n\tmemtables := c.readState.memtables\n\tfor i := len(memtables) - 1; i >= 0; i-- {\n\t\tmem := memtables[i]\n\t\tmlevels = append(mlevels, simpleMergingIterLevel{\n\t\t\titer:         mem.newIter(nil),\n\t\t\trangeDelIter: mem.newRangeDelIter(nil),\n\t\t})\n\t}\n\n\tcurrent := c.readState.current\n\t// Determine the final size for mlevels so that there are no more\n\t// reallocations. levelIter will hold a pointer to elements in mlevels.\n\tstart := len(mlevels)\n\tfor sublevel := len(current.L0SublevelFiles) - 1; sublevel >= 0; sublevel-- {\n\t\tif current.L0SublevelFiles[sublevel].Empty() {\n\t\t\tcontinue\n\t\t}\n\t\tmlevels = append(mlevels, simpleMergingIterLevel{})\n\t}\n\tfor level := 1; level < len(current.Levels); level++ {\n\t\tif current.Levels[level].Empty() {\n\t\t\tcontinue\n\t\t}\n\t\tmlevels = append(mlevels, simpleMergingIterLevel{})\n\t}\n\tmlevelAlloc := mlevels[start:]\n\t// Add L0 files by sublevel.\n\tfor sublevel := len(current.L0SublevelFiles) - 1; sublevel >= 0; sublevel-- {\n\t\tif current.L0SublevelFiles[sublevel].Empty() {\n\t\t\tcontinue\n\t\t}\n\t\tmanifestIter := current.L0SublevelFiles[sublevel].Iter()\n\t\titerOpts := IterOptions{logger: c.logger}\n\t\tli := &levelIter{}\n\t\tli.init(context.Background(), iterOpts, c.comparer, c.newIters, manifestIter,\n\t\t\tmanifest.L0Sublevel(sublevel), internalIterOpts{})\n\t\tli.initRangeDel(&mlevelAlloc[0])\n\t\tmlevelAlloc[0].iter = li\n\t\tmlevelAlloc = mlevelAlloc[1:]\n\t}\n\tfor level := 1; level < len(current.Levels); level++ {\n\t\tif current.Levels[level].Empty() {\n\t\t\tcontinue\n\t\t}\n\n\t\titerOpts := IterOptions{logger: c.logger}\n\t\tli := &levelIter{}\n\t\tli.init(context.Background(), iterOpts, c.comparer, c.newIters,\n\t\t\tcurrent.Levels[level].Iter(), manifest.Level(level), internalIterOpts{})\n\t\tli.initRangeDel(&mlevelAlloc[0])\n\t\tmlevelAlloc[0].iter = li\n\t\tmlevelAlloc = mlevelAlloc[1:]\n\t}\n\n\tmergingIter := &simpleMergingIter{}\n\tmergingIter.init(c.merge, c.cmp, c.seqNum, c.formatKey, mlevels...)\n\tfor cont := mergingIter.step(); cont; cont = mergingIter.step() {\n\t}\n\tif err := mergingIter.err; err != nil {\n\t\treturn err\n\t}\n\tif c.stats != nil {\n\t\tc.stats.NumPoints = mergingIter.numPoints\n\t}\n\n\t// Phase 2: Check that the tombstones are mutually consistent.\n\treturn checkRangeTombstones(c)\n}\n\ntype simpleMergingIterItem struct {\n\tindex int\n\tkey   InternalKey\n\tvalue base.LazyValue\n}\n\ntype simpleMergingIterHeap struct {\n\tcmp     Compare\n\treverse bool\n\titems   []simpleMergingIterItem\n}\n\nfunc (h *simpleMergingIterHeap) len() int {\n\treturn len(h.items)\n}\n\nfunc (h *simpleMergingIterHeap) less(i, j int) bool {\n\tikey, jkey := h.items[i].key, h.items[j].key\n\tif c := h.cmp(ikey.UserKey, jkey.UserKey); c != 0 {\n\t\tif h.reverse {\n\t\t\treturn c > 0\n\t\t}\n\t\treturn c < 0\n\t}\n\tif h.reverse {\n\t\treturn ikey.Trailer < jkey.Trailer\n\t}\n\treturn ikey.Trailer > jkey.Trailer\n}\n\nfunc (h *simpleMergingIterHeap) swap(i, j int) {\n\th.items[i], h.items[j] = h.items[j], h.items[i]\n}\n\n// init, fix, up and down are copied from the go stdlib.\nfunc (h *simpleMergingIterHeap) init() {\n\t// heapify\n\tn := h.len()\n\tfor i := n/2 - 1; i >= 0; i-- {\n\t\th.down(i, n)\n\t}\n}\n\nfunc (h *simpleMergingIterHeap) fix(i int) {\n\tif !h.down(i, h.len()) {\n\t\th.up(i)\n\t}\n}\n\nfunc (h *simpleMergingIterHeap) pop() *simpleMergingIterItem {\n\tn := h.len() - 1\n\th.swap(0, n)\n\th.down(0, n)\n\titem := &h.items[n]\n\th.items = h.items[:n]\n\treturn item\n}\n\nfunc (h *simpleMergingIterHeap) up(j int) {\n\tfor {\n\t\ti := (j - 1) / 2 // parent\n\t\tif i == j || !h.less(j, i) {\n\t\t\tbreak\n\t\t}\n\t\th.swap(i, j)\n\t\tj = i\n\t}\n}\n\nfunc (h *simpleMergingIterHeap) down(i0, n int) bool {\n\ti := i0\n\tfor {\n\t\tj1 := 2*i + 1\n\t\tif j1 >= n || j1 < 0 { // j1 < 0 after int overflow\n\t\t\tbreak\n\t\t}\n\t\tj := j1 // left child\n\t\tif j2 := j1 + 1; j2 < n && h.less(j2, j1) {\n\t\t\tj = j2 // = 2*i + 2  // right child\n\t\t}\n\t\tif !h.less(j, i) {\n\t\t\tbreak\n\t\t}\n\t\th.swap(i, j)\n\t\ti = j\n\t}\n\treturn i > i0\n}\n"
        },
        {
          "name": "level_checker_test.go",
          "type": "blob",
          "size": 8.2041015625,
          "content": "// Copyright 2019 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/cockroachdb/datadriven\"\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/invalidating\"\n\t\"github.com/cockroachdb/pebble/internal/invariants\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n\t\"github.com/cockroachdb/pebble/internal/sstableinternal\"\n\t\"github.com/cockroachdb/pebble/internal/testkeys\"\n\t\"github.com/cockroachdb/pebble/objstorage/objstorageprovider\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n\t\"github.com/cockroachdb/pebble/sstable/colblk\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestCheckLevelsBasics(t *testing.T) {\n\ttestCases := []string{\"db-stage-1\", \"db-stage-2\", \"db-stage-3\", \"db-stage-4\"}\n\tfor _, tc := range testCases {\n\t\tt.Run(tc, func(t *testing.T) {\n\t\t\tt.Logf(\"%s\", t.Name())\n\t\t\tfs := vfs.NewMem()\n\t\t\t_, err := vfs.Clone(vfs.Default, fs, filepath.Join(\"testdata\", tc), tc)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"%s: cloneFileSystem failed: %v\", tc, err)\n\t\t\t}\n\t\t\td, err := Open(tc, &Options{\n\t\t\t\tFS:     fs,\n\t\t\t\tLogger: testLogger{t},\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"%s: Open failed: %v\", tc, err)\n\t\t\t}\n\t\t\trequire.NoError(t, d.CheckLevels(nil))\n\t\t\trequire.NoError(t, d.Close())\n\t\t})\n\t}\n}\n\ntype failMerger struct {\n\tlastBuf    []byte\n\tcloseCount int\n}\n\nfunc (f *failMerger) MergeNewer(value []byte) error {\n\treturn nil\n}\n\nfunc (f *failMerger) MergeOlder(value []byte) error {\n\tif string(value) == \"fail-merge\" {\n\t\tf.lastBuf = nil\n\t\treturn errors.New(\"merge failed\")\n\t}\n\tf.lastBuf = append(f.lastBuf[:0], value...)\n\treturn nil\n}\n\nfunc (f *failMerger) Finish(includesBase bool) ([]byte, io.Closer, error) {\n\tif string(f.lastBuf) == \"fail-finish\" {\n\t\tf.lastBuf = nil\n\t\treturn nil, nil, errors.New(\"finish failed\")\n\t}\n\tf.closeCount++\n\treturn nil, f, nil\n}\n\nfunc (f *failMerger) Close() error {\n\tf.closeCount--\n\tf.lastBuf = nil\n\treturn nil\n}\n\nfunc TestCheckLevelsCornerCases(t *testing.T) {\n\tif invariants.Enabled {\n\t\tt.Skip(\"disabled under invariants; relies on violating invariants to detect them\")\n\t}\n\n\tmemFS := vfs.NewMem()\n\tvar levels [][]*fileMetadata\n\tformatKey := testkeys.Comparer.FormatKey\n\t// Indexed by fileNum\n\tvar readers []*sstable.Reader\n\tdefer func() {\n\t\tfor _, r := range readers {\n\t\t\tr.Close()\n\t\t}\n\t}()\n\n\tvar fileNum FileNum\n\tnewIters :=\n\t\tfunc(_ context.Context, file *manifest.FileMetadata, _ *IterOptions, _ internalIterOpts, _ iterKinds) (iterSet, error) {\n\t\t\tr := readers[file.FileNum]\n\t\t\trangeDelIter, err := r.NewRawRangeDelIter(context.Background(), sstable.NoFragmentTransforms)\n\t\t\tif err != nil {\n\t\t\t\treturn iterSet{}, err\n\t\t\t}\n\t\t\titer, err := r.NewIter(sstable.NoTransforms, nil /* lower */, nil /* upper */)\n\t\t\tif err != nil {\n\t\t\t\treturn iterSet{}, err\n\t\t\t}\n\n\t\t\treturn iterSet{\n\t\t\t\tpoint:         invalidating.MaybeWrapIfInvariants(iter),\n\t\t\t\trangeDeletion: rangeDelIter,\n\t\t\t}, nil\n\t\t}\n\n\tfm := &failMerger{}\n\tdefer require.Equal(t, 0, fm.closeCount)\n\n\tfailMerger := &Merger{\n\t\tMerge: func(key, value []byte) (ValueMerger, error) {\n\t\t\tfm.lastBuf = append(fm.lastBuf[:0], value...)\n\t\t\treturn fm, nil\n\t\t},\n\n\t\tName: \"fail-merger\",\n\t}\n\n\tdatadriven.RunTest(t, \"testdata/level_checker\", func(t *testing.T, d *datadriven.TestData) string {\n\t\tswitch d.Cmd {\n\t\tcase \"define\":\n\t\t\tlines := strings.Split(d.Input, \"\\n\")\n\t\t\tlevels = levels[:0]\n\t\t\tfor i := 0; i < len(lines); i++ {\n\t\t\t\tline := lines[i]\n\t\t\t\tline = strings.TrimSpace(line)\n\t\t\t\tif line == \"L\" {\n\t\t\t\t\t// start next level\n\t\t\t\t\tlevels = append(levels, nil)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tli := &levels[len(levels)-1]\n\t\t\t\tkeys := strings.Fields(line)\n\t\t\t\tsmallestKey := base.ParseInternalKey(keys[0])\n\t\t\t\tlargestKey := base.ParseInternalKey(keys[1])\n\t\t\t\tm := (&fileMetadata{\n\t\t\t\t\tFileNum: fileNum,\n\t\t\t\t}).ExtendPointKeyBounds(testkeys.Comparer.Compare, smallestKey, largestKey)\n\t\t\t\tm.InitPhysicalBacking()\n\t\t\t\t*li = append(*li, m)\n\n\t\t\t\ti++\n\t\t\t\tline = lines[i]\n\t\t\t\tline = strings.TrimSpace(line)\n\t\t\t\tname := fmt.Sprint(fileNum)\n\t\t\t\tfileNum++\n\t\t\t\tf, err := memFS.Create(name, vfs.WriteCategoryUnspecified)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\twriteUnfragmented := false\n\t\t\t\tdisableKeyOrderChecks := false\n\t\t\t\tfor _, arg := range d.CmdArgs {\n\t\t\t\t\tswitch arg.Key {\n\t\t\t\t\tcase \"disable-key-order-checks\":\n\t\t\t\t\t\tdisableKeyOrderChecks = true\n\t\t\t\t\tcase \"write-unfragmented\":\n\t\t\t\t\t\twriteUnfragmented = true\n\t\t\t\t\tdefault:\n\t\t\t\t\t\treturn fmt.Sprintf(\"unknown arg: %s\", arg.Key)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tkeySchema := colblk.DefaultKeySchema(testkeys.Comparer, 16)\n\t\t\t\twriterOpts := sstable.WriterOptions{\n\t\t\t\t\tComparer:    testkeys.Comparer,\n\t\t\t\t\tKeySchema:   &keySchema,\n\t\t\t\t\tTableFormat: FormatNewest.MaxTableFormat(),\n\t\t\t\t}\n\t\t\t\twriterOpts.SetInternal(sstableinternal.WriterOptions{\n\t\t\t\t\tDisableKeyOrderChecks: disableKeyOrderChecks,\n\t\t\t\t})\n\t\t\t\tw := sstable.NewRawWriter(objstorageprovider.NewFileWritable(f), writerOpts)\n\t\t\t\tvar tombstones []keyspan.Span\n\t\t\t\tfrag := keyspan.Fragmenter{\n\t\t\t\t\tCmp:    testkeys.Comparer.Compare,\n\t\t\t\t\tFormat: formatKey,\n\t\t\t\t\tEmit: func(fragmented keyspan.Span) {\n\t\t\t\t\t\ttombstones = append(tombstones, fragmented)\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t\tkeyValues := strings.Fields(line)\n\t\t\t\tfor _, kv := range keyValues {\n\t\t\t\t\tif strings.HasPrefix(kv, \"EncodeSpan:\") {\n\t\t\t\t\t\tkv = kv[len(\"EncodeSpan:\"):]\n\t\t\t\t\t\ts := keyspan.ParseSpan(kv)\n\t\t\t\t\t\tif writeUnfragmented {\n\t\t\t\t\t\t\tif err = w.EncodeSpan(s); err != nil {\n\t\t\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t} else if s.Keys[0].Kind() == base.InternalKeyKindRangeDelete {\n\t\t\t\t\t\t\tfrag.Add(s)\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tt.Fatalf(\"unexpected span: %s\", s.Pretty(testkeys.Comparer.FormatKey))\n\t\t\t\t\t\t}\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tj := strings.Index(kv, \":\")\n\t\t\t\t\tikey := base.ParseInternalKey(kv[:j])\n\t\t\t\t\tvalue := []byte(kv[j+1:])\n\t\t\t\t\terr = w.AddWithForceObsolete(ikey, value, false /* forceObsolete */)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfrag.Finish()\n\t\t\t\tfor _, v := range tombstones {\n\t\t\t\t\tif err := w.EncodeSpan(v); err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif err := w.Close(); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\tf, err = memFS.Open(name)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\treadable, err := sstable.NewSimpleReadable(f)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\t// Set FileNum for logging purposes.\n\t\t\t\treaderOpts := sstable.ReaderOptions{\n\t\t\t\t\tComparer:   testkeys.Comparer,\n\t\t\t\t\tKeySchemas: sstable.KeySchemas{writerOpts.KeySchema.Name: writerOpts.KeySchema},\n\t\t\t\t}\n\t\t\t\treaderOpts.SetInternalCacheOpts(sstableinternal.CacheOptions{FileNum: base.DiskFileNum(fileNum - 1)})\n\t\t\t\tr, err := sstable.NewReader(context.Background(), readable, readerOpts)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\treaders = append(readers, r)\n\t\t\t}\n\t\t\t// TODO(sbhola): clean this up by wrapping levels in a Version and using\n\t\t\t// Version.DebugString().\n\t\t\tvar buf bytes.Buffer\n\t\t\tfor i, l := range levels {\n\t\t\t\tfmt.Fprintf(&buf, \"Level %d\\n\", i+1)\n\t\t\t\tfor j, f := range l {\n\t\t\t\t\tfmt.Fprintf(&buf, \"  file %d: [%s-%s]\\n\", j, f.Smallest.String(), f.Largest.String())\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn buf.String()\n\t\tcase \"check\":\n\t\t\tmerge := DefaultMerger.Merge\n\t\t\tfor _, arg := range d.CmdArgs {\n\t\t\t\tswitch arg.Key {\n\t\t\t\tcase \"merger\":\n\t\t\t\t\tif len(arg.Vals) != 1 {\n\t\t\t\t\t\treturn fmt.Sprintf(\"expected one arg value, got %d\", len(arg.Vals))\n\t\t\t\t\t}\n\t\t\t\t\tif arg.Vals[0] != failMerger.Name {\n\t\t\t\t\t\treturn \"unsupported merger\"\n\t\t\t\t\t}\n\t\t\t\t\tmerge = failMerger.Merge\n\t\t\t\tdefault:\n\t\t\t\t\treturn fmt.Sprintf(\"unknown arg: %s\", arg.Key)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tvar files [numLevels][]*fileMetadata\n\t\t\tfor i := range levels {\n\t\t\t\t// Start from level 1 in this test.\n\t\t\t\tfiles[i+1] = levels[i]\n\t\t\t}\n\t\t\tversion := manifest.NewVersion(\n\t\t\t\ttestkeys.Comparer,\n\t\t\t\t0,\n\t\t\t\tfiles)\n\t\t\treadState := &readState{current: version}\n\t\t\tc := &checkConfig{\n\t\t\t\tcomparer:  testkeys.Comparer,\n\t\t\t\treadState: readState,\n\t\t\t\tnewIters:  newIters,\n\t\t\t\tseqNum:    base.SeqNumMax,\n\t\t\t\tmerge:     merge,\n\t\t\t\tformatKey: formatKey,\n\t\t\t}\n\t\t\tif err := checkLevelsInternal(c); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", d.Cmd)\n\t\t}\n\t})\n}\n"
        },
        {
          "name": "level_iter.go",
          "type": "blob",
          "size": 34.9697265625,
          "content": "// Copyright 2018 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"runtime/debug\"\n\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/invariants\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n\t\"github.com/cockroachdb/pebble/internal/treeprinter\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n)\n\ntype internalIterOpts struct {\n\t// if compaction is set, sstable-level iterators will be created using\n\t// NewCompactionIter; these iterators have a more constrained interface\n\t// and are optimized for the sequential scan of a compaction.\n\tcompaction           bool\n\tbufferPool           *sstable.BufferPool\n\tstats                *base.InternalIteratorStats\n\titerStatsAccumulator sstable.IterStatsAccumulator\n\tboundLimitedFilter   sstable.BoundLimitedBlockPropertyFilter\n}\n\n// levelIter provides a merged view of the sstables in a level.\n//\n// levelIter is used during compaction and as part of the Iterator\n// implementation. When used as part of the Iterator implementation, level\n// iteration needs to \"pause\" at range deletion boundaries if file contains\n// range deletions. In this case, the levelIter uses a keyspan.InterleavingIter\n// to materialize InternalKVs at start and end boundaries of range deletions.\n// This prevents mergingIter from advancing past the sstable until the sstable\n// contains the smallest (or largest for reverse iteration) key in the merged\n// heap. Note that mergingIter treats a range deletion tombstone returned by the\n// point iterator as a no-op.\ntype levelIter struct {\n\t// The context is stored here since (a) iterators are expected to be\n\t// short-lived (since they pin sstables), (b) plumbing a context into every\n\t// method is very painful, (c) they do not (yet) respect context\n\t// cancellation and are only used for tracing.\n\tctx      context.Context\n\tlogger   Logger\n\tcomparer *Comparer\n\tcmp      Compare\n\tsplit    Split\n\t// The lower/upper bounds for iteration as specified at creation or the most\n\t// recent call to SetBounds.\n\tlower []byte\n\tupper []byte\n\t// prefix holds the iteration prefix when the most recent absolute\n\t// positioning method was a SeekPrefixGE.\n\tprefix []byte\n\t// The iterator options for the currently open table. If\n\t// tableOpts.{Lower,Upper}Bound are nil, the corresponding iteration boundary\n\t// does not lie within the table bounds.\n\ttableOpts IterOptions\n\t// The layer this levelIter is initialized for. This can be either\n\t// a level L1+, an L0 sublevel, or a flushable ingests layer.\n\tlayer manifest.Layer\n\t// combinedIterState may be set when a levelIter is used during user\n\t// iteration. Although levelIter only iterates over point keys, it's also\n\t// responsible for lazily constructing the combined range & point iterator\n\t// when it observes a file containing range keys. If the combined iter\n\t// state's initialized field is true, the iterator is already using combined\n\t// iterator, OR the iterator is not configured to use combined iteration. If\n\t// it's false, the levelIter must set the `triggered` and `key` fields when\n\t// the levelIter passes over a file containing range keys. See the\n\t// lazyCombinedIter for more details.\n\tcombinedIterState *combinedIterState\n\t// The iter for the current file. It is nil under any of the following conditions:\n\t// - files.Current() == nil\n\t// - err != nil\n\t// - some other constraint, like the bounds in opts, caused the file at index to not\n\t//   be relevant to the iteration.\n\titer internalIterator\n\t// iterFile holds the current file. It is always equal to l.files.Current().\n\titerFile *fileMetadata\n\tnewIters tableNewIters\n\tfiles    manifest.LevelIterator\n\terr      error\n\n\t// When rangeDelIterSetter != nil, the caller requires that this function\n\t// gets called with a range deletion iterator whenever the current file\n\t// changes.  The iterator is relinquished to the caller which is responsible\n\t// for closing it.\n\t//\n\t// When rangeDelIterSetter != nil, the levelIter will also interleave the\n\t// boundaries of range deletions among point keys.\n\trangeDelIterSetter rangeDelIterSetter\n\n\t// interleaving is used when rangeDelIterFn != nil to interleave the\n\t// boundaries of range deletions among point keys. When the leve iterator is\n\t// used by a merging iterator, this ensures that we don't advance to a new\n\t// file until the range deletions are no longer needed by other levels.\n\tinterleaving keyspan.InterleavingIter\n\n\t// internalOpts holds the internal iterator options to pass to the table\n\t// cache when constructing new table iterators.\n\tinternalOpts internalIterOpts\n\n\t// Scratch space for the obsolete keys filter, when there are no other block\n\t// property filters specified. See the performance note where\n\t// IterOptions.PointKeyFilters is declared.\n\tfiltersBuf [1]BlockPropertyFilter\n\n\t// exhaustedDir is set to +1 or -1 when the levelIter has been exhausted in\n\t// the forward or backward direction respectively. It is set when the\n\t// underlying data is exhausted or when iteration has reached the upper or\n\t// lower boundary and interleaved a synthetic iterator bound key. When the\n\t// iterator is exhausted and Next or Prev is called, the levelIter uses\n\t// exhaustedDir to determine whether the iterator should step on to the\n\t// first or last key within iteration bounds.\n\texhaustedDir int8\n\n\t// Disable invariant checks even if they are otherwise enabled. Used by tests\n\t// which construct \"impossible\" situations (e.g. seeking to a key before the\n\t// lower bound).\n\tdisableInvariants bool\n}\n\ntype rangeDelIterSetter interface {\n\tsetRangeDelIter(rangeDelIter keyspan.FragmentIterator)\n}\n\n// levelIter implements the base.InternalIterator interface.\nvar _ base.InternalIterator = (*levelIter)(nil)\n\n// newLevelIter returns a levelIter. It is permissible to pass a nil split\n// parameter if the caller is never going to call SeekPrefixGE.\nfunc newLevelIter(\n\tctx context.Context,\n\topts IterOptions,\n\tcomparer *Comparer,\n\tnewIters tableNewIters,\n\tfiles manifest.LevelIterator,\n\tlayer manifest.Layer,\n\tinternalOpts internalIterOpts,\n) *levelIter {\n\tl := &levelIter{}\n\tl.init(ctx, opts, comparer, newIters, files, layer, internalOpts)\n\treturn l\n}\n\nfunc (l *levelIter) init(\n\tctx context.Context,\n\topts IterOptions,\n\tcomparer *Comparer,\n\tnewIters tableNewIters,\n\tfiles manifest.LevelIterator,\n\tlayer manifest.Layer,\n\tinternalOpts internalIterOpts,\n) {\n\tl.ctx = ctx\n\tl.err = nil\n\tl.layer = layer\n\tl.logger = opts.getLogger()\n\tl.prefix = nil\n\tl.lower = opts.LowerBound\n\tl.upper = opts.UpperBound\n\tl.tableOpts.PointKeyFilters = opts.PointKeyFilters\n\tif len(opts.PointKeyFilters) == 0 {\n\t\tl.tableOpts.PointKeyFilters = l.filtersBuf[:0:1]\n\t}\n\tl.tableOpts.UseL6Filters = opts.UseL6Filters\n\tl.tableOpts.Category = opts.Category\n\tl.tableOpts.layer = l.layer\n\tl.tableOpts.snapshotForHideObsoletePoints = opts.snapshotForHideObsoletePoints\n\tl.comparer = comparer\n\tl.cmp = comparer.Compare\n\tl.split = comparer.Split\n\tl.iterFile = nil\n\tl.newIters = newIters\n\tl.files = files\n\tl.exhaustedDir = 0\n\tl.internalOpts = internalOpts\n}\n\n// initRangeDel puts the level iterator into a mode where it interleaves range\n// deletion boundaries with point keys and provides a range deletion iterator\n// (through rangeDelIterFn) whenever the current file changes.\n//\n// The range deletion iterator passed to rangeDelIterFn is relinquished to the\n// implementor who is responsible for closing it.\nfunc (l *levelIter) initRangeDel(rangeDelSetter rangeDelIterSetter) {\n\tl.rangeDelIterSetter = rangeDelSetter\n}\n\nfunc (l *levelIter) initCombinedIterState(state *combinedIterState) {\n\tl.combinedIterState = state\n}\n\nfunc (l *levelIter) maybeTriggerCombinedIteration(file *fileMetadata, dir int) {\n\t// If we encounter a file that contains range keys, we may need to\n\t// trigger a switch to combined range-key and point-key iteration,\n\t// if the *pebble.Iterator is configured for it. This switch is done\n\t// lazily because range keys are intended to be rare, and\n\t// constructing the range-key iterator substantially adds to the\n\t// cost of iterator construction and seeking.\n\t//\n\t// If l.combinedIterState.initialized is already true, either the\n\t// iterator is already using combined iteration or the iterator is not\n\t// configured to observe range keys. Either way, there's nothing to do.\n\t// If false, trigger the switch to combined iteration, using the the\n\t// file's bounds to seek the range-key iterator appropriately.\n\t//\n\t// We only need to trigger combined iteration if the file contains\n\t// RangeKeySets: if there are only Unsets and Dels, the user will observe no\n\t// range keys regardless. If this file has table stats available, they'll\n\t// tell us whether the file has any RangeKeySets. Otherwise, we must\n\t// fallback to assuming it does if HasRangeKeys=true.\n\tif file != nil && file.HasRangeKeys && l.combinedIterState != nil && !l.combinedIterState.initialized &&\n\t\t(l.upper == nil || l.cmp(file.SmallestRangeKey.UserKey, l.upper) < 0) &&\n\t\t(l.lower == nil || l.cmp(file.LargestRangeKey.UserKey, l.lower) > 0) &&\n\t\t(!file.StatsValid() || file.Stats.NumRangeKeySets > 0) {\n\t\t// The file contains range keys, and we're not using combined iteration yet.\n\t\t// Trigger a switch to combined iteration. It's possible that a switch has\n\t\t// already been triggered if multiple levels encounter files containing\n\t\t// range keys while executing a single mergingIter operation. In this case,\n\t\t// we need to compare the existing key recorded to l.combinedIterState.key,\n\t\t// adjusting it if our key is smaller (forward iteration) or larger\n\t\t// (backward iteration) than the existing key.\n\t\t//\n\t\t// These key comparisons are only required during a single high-level\n\t\t// iterator operation. When the high-level iter op completes,\n\t\t// iinitialized will be true, and future calls to this function will be\n\t\t// no-ops.\n\t\tswitch dir {\n\t\tcase +1:\n\t\t\tif !l.combinedIterState.triggered {\n\t\t\t\tl.combinedIterState.triggered = true\n\t\t\t\tl.combinedIterState.key = file.SmallestRangeKey.UserKey\n\t\t\t} else if l.cmp(l.combinedIterState.key, file.SmallestRangeKey.UserKey) > 0 {\n\t\t\t\tl.combinedIterState.key = file.SmallestRangeKey.UserKey\n\t\t\t}\n\t\tcase -1:\n\t\t\tif !l.combinedIterState.triggered {\n\t\t\t\tl.combinedIterState.triggered = true\n\t\t\t\tl.combinedIterState.key = file.LargestRangeKey.UserKey\n\t\t\t} else if l.cmp(l.combinedIterState.key, file.LargestRangeKey.UserKey) < 0 {\n\t\t\t\tl.combinedIterState.key = file.LargestRangeKey.UserKey\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (l *levelIter) findFileGE(key []byte, flags base.SeekGEFlags) *fileMetadata {\n\t// Find the earliest file whose largest key is >= key.\n\n\t// NB: if flags.TrySeekUsingNext()=true, the levelIter must respect it. If\n\t// the levelIter is positioned at the key P, it must return a key ≥ P. If\n\t// used within a merging iterator, the merging iterator will depend on the\n\t// levelIter only moving forward to maintain heap invariants.\n\n\t// Ordinarily we seek the LevelIterator using SeekGE. In some instances, we\n\t// Next instead. In other instances, we try Next-ing first, falling back to\n\t// seek:\n\t//   a) flags.TrySeekUsingNext(): The top-level Iterator knows we're seeking\n\t//      to a key later than the current iterator position. We don't know how\n\t//      much later the seek key is, so it's possible there are many sstables\n\t//      between the current position and the seek key. However in most real-\n\t//      world use cases, the seek key is likely to be nearby. Rather than\n\t//      performing a log(N) seek through the file metadata, we next a few\n\t//      times from our existing location. If we don't find a file whose\n\t//      largest is >= key within a few nexts, we fall back to seeking.\n\t//\n\t//      Note that in this case, the file returned by findFileGE may be\n\t//      different than the file returned by a raw binary search (eg, when\n\t//      TrySeekUsingNext=false). This is possible because the most recent\n\t//      positioning operation may have already determined that previous\n\t//      files' keys that are ≥ key are all deleted. This information is\n\t//      encoded within the iterator's current iterator position and is\n\t//      unavailable to a fresh binary search.\n\t//\n\t//   b) flags.RelativeSeek(): The merging iterator decided to re-seek this\n\t//      level according to a range tombstone. When lazy combined iteration\n\t//      is enabled, the level iterator is responsible for watching for\n\t//      files containing range keys and triggering the switch to combined\n\t//      iteration when such a file is observed. If a range deletion was\n\t//      observed in a higher level causing the merging iterator to seek the\n\t//      level to the range deletion's end key, we need to check whether all\n\t//      of the files between the old position and the new position contain\n\t//      any range keys.\n\t//\n\t//      In this scenario, we don't seek the LevelIterator and instead we\n\t//      Next it, one file at a time, checking each for range keys. The\n\t//      merging iterator sets this flag to inform us that we're moving\n\t//      forward relative to the existing position and that we must examine\n\t//      each intermediate sstable's metadata for lazy-combined iteration.\n\t//      In this case, we only Next and never Seek. We set nextsUntilSeek=-1\n\t//      to signal this intention.\n\t//\n\t// NB: At most one of flags.RelativeSeek() and flags.TrySeekUsingNext() may\n\t// be set, because the merging iterator re-seeks relative seeks with\n\t// explicitly only the RelativeSeek flag set.\n\tvar nextsUntilSeek int\n\tvar nextInsteadOfSeek bool\n\tif flags.TrySeekUsingNext() {\n\t\tnextInsteadOfSeek = true\n\t\tnextsUntilSeek = 4 // arbitrary\n\t}\n\tif flags.RelativeSeek() && l.combinedIterState != nil && !l.combinedIterState.initialized {\n\t\tnextInsteadOfSeek = true\n\t\tnextsUntilSeek = -1\n\t}\n\n\tvar m *fileMetadata\n\tif nextInsteadOfSeek {\n\t\tm = l.iterFile\n\t} else {\n\t\tm = l.files.SeekGE(l.cmp, key)\n\t}\n\t// The below loop has a bit of an unusual organization. There are several\n\t// conditions under which we need to Next to a later file. If none of those\n\t// conditions are met, the file in `m` is okay to return. The loop body is\n\t// structured with a series of if statements, each of which may continue the\n\t// loop to the next file. If none of the statements are met, the end of the\n\t// loop body is a break.\n\tfor m != nil {\n\t\tif m.HasRangeKeys {\n\t\t\tl.maybeTriggerCombinedIteration(m, +1)\n\n\t\t\t// Some files may only contain range keys, which we can skip.\n\t\t\t// NB: HasPointKeys=true if the file contains any points or range\n\t\t\t// deletions (which delete points).\n\t\t\tif !m.HasPointKeys {\n\t\t\t\tm = l.files.Next()\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\t// This file has point keys.\n\t\t//\n\t\t// However, there are a couple reasons why `m` may not be positioned ≥\n\t\t// `key` yet:\n\t\t//\n\t\t// 1. If SeekGE(key) landed on a file containing range keys, the file\n\t\t//    may contain range keys ≥ `key` but no point keys ≥ `key`.\n\t\t// 2. When nexting instead of seeking, we must check to see whether\n\t\t//    we've nexted sufficiently far, or we need to next again.\n\t\t//\n\t\t// If the file does not contain point keys ≥ `key`, next to continue\n\t\t// looking for a file that does.\n\t\tif (m.HasRangeKeys || nextInsteadOfSeek) && l.cmp(m.LargestPointKey.UserKey, key) < 0 {\n\t\t\t// If nextInsteadOfSeek is set and nextsUntilSeek is non-negative,\n\t\t\t// the iterator has been nexting hoping to discover the relevant\n\t\t\t// file without seeking. It's exhausted the allotted nextsUntilSeek\n\t\t\t// and should seek to the sought key.\n\t\t\tif nextInsteadOfSeek && nextsUntilSeek == 0 {\n\t\t\t\tnextInsteadOfSeek = false\n\t\t\t\tm = l.files.SeekGE(l.cmp, key)\n\t\t\t\tcontinue\n\t\t\t} else if nextsUntilSeek > 0 {\n\t\t\t\tnextsUntilSeek--\n\t\t\t}\n\t\t\tm = l.files.Next()\n\t\t\tcontinue\n\t\t}\n\n\t\t// This file has a point key bound ≥ `key`. But the largest point key\n\t\t// bound may still be a range deletion sentinel, which is exclusive.  In\n\t\t// this case, the file doesn't actually contain any point keys equal to\n\t\t// `key`. We next to keep searching for a file that actually contains\n\t\t// point keys ≥ key.\n\t\t//\n\t\t// Additionally, this prevents loading untruncated range deletions from\n\t\t// a table which can't possibly contain the target key and is required\n\t\t// for correctness by mergingIter.SeekGE (see the comment in that\n\t\t// function).\n\t\tif m.LargestPointKey.IsExclusiveSentinel() && l.cmp(m.LargestPointKey.UserKey, key) == 0 {\n\t\t\tm = l.files.Next()\n\t\t\tcontinue\n\t\t}\n\n\t\t// This file contains point keys ≥ `key`. Break and return it.\n\t\tbreak\n\t}\n\treturn m\n}\n\nfunc (l *levelIter) findFileLT(key []byte, flags base.SeekLTFlags) *fileMetadata {\n\t// Find the last file whose smallest key is < ikey.\n\n\t// Ordinarily we seek the LevelIterator using SeekLT.\n\t//\n\t// When lazy combined iteration is enabled, there's a complication. The\n\t// level iterator is responsible for watching for files containing range\n\t// keys and triggering the switch to combined iteration when such a file is\n\t// observed. If a range deletion was observed in a higher level causing the\n\t// merging iterator to seek the level to the range deletion's start key, we\n\t// need to check whether all of the files between the old position and the\n\t// new position contain any range keys.\n\t//\n\t// In this scenario, we don't seek the LevelIterator and instead we Prev it,\n\t// one file at a time, checking each for range keys.\n\tprevInsteadOfSeek := flags.RelativeSeek() && l.combinedIterState != nil && !l.combinedIterState.initialized\n\n\tvar m *fileMetadata\n\tif prevInsteadOfSeek {\n\t\tm = l.iterFile\n\t} else {\n\t\tm = l.files.SeekLT(l.cmp, key)\n\t}\n\t// The below loop has a bit of an unusual organization. There are several\n\t// conditions under which we need to Prev to a previous file. If none of\n\t// those conditions are met, the file in `m` is okay to return. The loop\n\t// body is structured with a series of if statements, each of which may\n\t// continue the loop to the previous file. If none of the statements are\n\t// met, the end of the loop body is a break.\n\tfor m != nil {\n\t\tif m.HasRangeKeys {\n\t\t\tl.maybeTriggerCombinedIteration(m, -1)\n\n\t\t\t// Some files may only contain range keys, which we can skip.\n\t\t\t// NB: HasPointKeys=true if the file contains any points or range\n\t\t\t// deletions (which delete points).\n\t\t\tif !m.HasPointKeys {\n\t\t\t\tm = l.files.Prev()\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\t// This file has point keys.\n\t\t//\n\t\t// However, there are a couple reasons why `m` may not be positioned <\n\t\t// `key` yet:\n\t\t//\n\t\t// 1. If SeekLT(key) landed on a file containing range keys, the file\n\t\t//    may contain range keys < `key` but no point keys < `key`.\n\t\t// 2. When preving instead of seeking, we must check to see whether\n\t\t//    we've preved sufficiently far, or we need to prev again.\n\t\t//\n\t\t// If the file does not contain point keys < `key`, prev to continue\n\t\t// looking for a file that does.\n\t\tif (m.HasRangeKeys || prevInsteadOfSeek) && l.cmp(m.SmallestPointKey.UserKey, key) >= 0 {\n\t\t\tm = l.files.Prev()\n\t\t\tcontinue\n\t\t}\n\n\t\t// This file contains point keys < `key`. Break and return it.\n\t\tbreak\n\t}\n\treturn m\n}\n\n// Init the iteration bounds for the current table. Returns -1 if the table\n// lies fully before the lower bound, +1 if the table lies fully after the\n// upper bound, and 0 if the table overlaps the iteration bounds.\nfunc (l *levelIter) initTableBounds(f *fileMetadata) int {\n\tl.tableOpts.LowerBound = l.lower\n\tif l.tableOpts.LowerBound != nil {\n\t\tif l.cmp(f.LargestPointKey.UserKey, l.tableOpts.LowerBound) < 0 {\n\t\t\t// The largest key in the sstable is smaller than the lower bound.\n\t\t\treturn -1\n\t\t}\n\t\tif l.cmp(l.tableOpts.LowerBound, f.SmallestPointKey.UserKey) <= 0 {\n\t\t\t// The lower bound is smaller or equal to the smallest key in the\n\t\t\t// table. Iteration within the table does not need to check the lower\n\t\t\t// bound.\n\t\t\tl.tableOpts.LowerBound = nil\n\t\t}\n\t}\n\tl.tableOpts.UpperBound = l.upper\n\tif l.tableOpts.UpperBound != nil {\n\t\tif l.cmp(f.SmallestPointKey.UserKey, l.tableOpts.UpperBound) >= 0 {\n\t\t\t// The smallest key in the sstable is greater than or equal to the upper\n\t\t\t// bound.\n\t\t\treturn 1\n\t\t}\n\t\tif l.cmp(l.tableOpts.UpperBound, f.LargestPointKey.UserKey) > 0 {\n\t\t\t// The upper bound is greater than the largest key in the\n\t\t\t// table. Iteration within the table does not need to check the upper\n\t\t\t// bound. NB: tableOpts.UpperBound is exclusive and f.LargestPointKey is\n\t\t\t// inclusive.\n\t\t\tl.tableOpts.UpperBound = nil\n\t\t}\n\t}\n\treturn 0\n}\n\ntype loadFileReturnIndicator int8\n\nconst (\n\tnoFileLoaded loadFileReturnIndicator = iota\n\tfileAlreadyLoaded\n\tnewFileLoaded\n)\n\nfunc (l *levelIter) loadFile(file *fileMetadata, dir int) loadFileReturnIndicator {\n\tif l.iterFile == file {\n\t\tif l.err != nil {\n\t\t\treturn noFileLoaded\n\t\t}\n\t\tif l.iter != nil {\n\t\t\t// We don't bother comparing the file bounds with the iteration bounds when we have\n\t\t\t// an already open iterator. It is possible that the iter may not be relevant given the\n\t\t\t// current iteration bounds, but it knows those bounds, so it will enforce them.\n\n\t\t\t// There are a few reasons we might not have triggered combined\n\t\t\t// iteration yet, even though we already had `file` open.\n\t\t\t// 1. If the bounds changed, we might have previously avoided\n\t\t\t//    switching to combined iteration because the bounds excluded\n\t\t\t//    the range keys contained in this file.\n\t\t\t// 2. If an existing iterator was reconfigured to iterate over range\n\t\t\t//    keys (eg, using SetOptions), then we wouldn't have triggered\n\t\t\t//    the switch to combined iteration yet.\n\t\t\tl.maybeTriggerCombinedIteration(file, dir)\n\t\t\treturn fileAlreadyLoaded\n\t\t}\n\t\t// We were already at file, but don't have an iterator, probably because the file was\n\t\t// beyond the iteration bounds. It may still be, but it is also possible that the bounds\n\t\t// have changed. We handle that below.\n\t}\n\n\t// Close iter and send a nil iterator through rangeDelIterFn.rangeDelIterFn.\n\tif err := l.Close(); err != nil {\n\t\treturn noFileLoaded\n\t}\n\n\tfor {\n\t\tl.iterFile = file\n\t\tif file == nil {\n\t\t\treturn noFileLoaded\n\t\t}\n\n\t\tl.maybeTriggerCombinedIteration(file, dir)\n\t\tif !file.HasPointKeys {\n\t\t\tswitch dir {\n\t\t\tcase +1:\n\t\t\t\tfile = l.files.Next()\n\t\t\t\tcontinue\n\t\t\tcase -1:\n\t\t\t\tfile = l.files.Prev()\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\tswitch l.initTableBounds(file) {\n\t\tcase -1:\n\t\t\t// The largest key in the sstable is smaller than the lower bound.\n\t\t\tif dir < 0 {\n\t\t\t\treturn noFileLoaded\n\t\t\t}\n\t\t\tfile = l.files.Next()\n\t\t\tcontinue\n\t\tcase +1:\n\t\t\t// The smallest key in the sstable is greater than or equal to the upper\n\t\t\t// bound.\n\t\t\tif dir > 0 {\n\t\t\t\treturn noFileLoaded\n\t\t\t}\n\t\t\tfile = l.files.Prev()\n\t\t\tcontinue\n\t\t}\n\t\t// If we're in prefix iteration, it's possible this file's smallest\n\t\t// boundary is large enough to prove the file cannot possibly contain\n\t\t// any keys within the iteration prefix. Loading the next file is\n\t\t// unnecessary. This has been observed in practice on slow shared\n\t\t// storage. See #3575.\n\t\tif l.prefix != nil && l.cmp(l.split.Prefix(file.SmallestPointKey.UserKey), l.prefix) > 0 {\n\t\t\t// Note that because l.iter is nil, a subsequent call to\n\t\t\t// SeekPrefixGE with TrySeekUsingNext()=true will load the file\n\t\t\t// (returning newFileLoaded) and disable TrySeekUsingNext before\n\t\t\t// performing a seek in the file.\n\t\t\treturn noFileLoaded\n\t\t}\n\n\t\titerKinds := iterPointKeys\n\t\tif l.rangeDelIterSetter != nil {\n\t\t\titerKinds |= iterRangeDeletions\n\t\t}\n\n\t\tvar iters iterSet\n\t\titers, l.err = l.newIters(l.ctx, l.iterFile, &l.tableOpts, l.internalOpts, iterKinds)\n\t\tif l.err != nil {\n\t\t\tif l.rangeDelIterSetter != nil {\n\t\t\t\tl.rangeDelIterSetter.setRangeDelIter(nil)\n\t\t\t}\n\t\t\treturn noFileLoaded\n\t\t}\n\t\tl.iter = iters.Point()\n\t\tif l.rangeDelIterSetter != nil && iters.rangeDeletion != nil {\n\t\t\t// If this file has range deletions, interleave the bounds of the\n\t\t\t// range deletions among the point keys. When used with a\n\t\t\t// mergingIter, this ensures we don't move beyond a file with range\n\t\t\t// deletions until its range deletions are no longer relevant.\n\t\t\t//\n\t\t\t// For now, we open a second range deletion iterator. Future work\n\t\t\t// will avoid the need to open a second range deletion iterator, and\n\t\t\t// avoid surfacing the file's range deletion iterator via rangeDelIterFn.\n\t\t\titersForBounds, err := l.newIters(l.ctx, l.iterFile, &l.tableOpts, l.internalOpts, iterRangeDeletions)\n\t\t\tif err != nil {\n\t\t\t\tl.iter = nil\n\t\t\t\tl.err = errors.CombineErrors(err, iters.CloseAll())\n\t\t\t\treturn noFileLoaded\n\t\t\t}\n\t\t\tl.interleaving.Init(l.comparer, l.iter, itersForBounds.RangeDeletion(), keyspan.InterleavingIterOpts{\n\t\t\t\tLowerBound:        l.tableOpts.LowerBound,\n\t\t\t\tUpperBound:        l.tableOpts.UpperBound,\n\t\t\t\tInterleaveEndKeys: true,\n\t\t\t})\n\t\t\tl.iter = &l.interleaving\n\n\t\t\t// Relinquish iters.rangeDeletion to the caller.\n\t\t\tl.rangeDelIterSetter.setRangeDelIter(iters.rangeDeletion)\n\t\t}\n\t\treturn newFileLoaded\n\t}\n}\n\n// In race builds we verify that the keys returned by levelIter lie within\n// [lower,upper).\nfunc (l *levelIter) verify(kv *base.InternalKV) *base.InternalKV {\n\t// Note that invariants.Enabled is a compile time constant, which means the\n\t// block of code will be compiled out of normal builds making this method\n\t// eligible for inlining. Do not change this to use a variable.\n\tif invariants.Enabled && !l.disableInvariants && kv != nil {\n\t\t// We allow returning a boundary key that is outside of the lower/upper\n\t\t// bounds as such keys are always range tombstones which will be skipped\n\t\t// by the Iterator.\n\t\tif l.lower != nil && kv != nil && !kv.K.IsExclusiveSentinel() && l.cmp(kv.K.UserKey, l.lower) < 0 {\n\t\t\tl.logger.Fatalf(\"levelIter %s: lower bound violation: %s < %s\\n%s\", l.layer, kv, l.lower, debug.Stack())\n\t\t}\n\t\tif l.upper != nil && kv != nil && !kv.K.IsExclusiveSentinel() && l.cmp(kv.K.UserKey, l.upper) > 0 {\n\t\t\tl.logger.Fatalf(\"levelIter %s: upper bound violation: %s > %s\\n%s\", l.layer, kv, l.upper, debug.Stack())\n\t\t}\n\t}\n\treturn kv\n}\n\nfunc (l *levelIter) SeekGE(key []byte, flags base.SeekGEFlags) *base.InternalKV {\n\tif invariants.Enabled && l.lower != nil && l.cmp(key, l.lower) < 0 {\n\t\tpanic(errors.AssertionFailedf(\"levelIter SeekGE to key %q violates lower bound %q\", key, l.lower))\n\t}\n\n\tl.err = nil // clear cached iteration error\n\tl.exhaustedDir = 0\n\tl.prefix = nil\n\t// NB: the top-level Iterator has already adjusted key based on\n\t// IterOptions.LowerBound.\n\tloadFileIndicator := l.loadFile(l.findFileGE(key, flags), +1)\n\tif loadFileIndicator == noFileLoaded {\n\t\tl.exhaustedForward()\n\t\treturn nil\n\t}\n\tif loadFileIndicator == newFileLoaded {\n\t\t// File changed, so l.iter has changed, and that iterator is not\n\t\t// positioned appropriately.\n\t\tflags = flags.DisableTrySeekUsingNext()\n\t}\n\tif kv := l.iter.SeekGE(key, flags); kv != nil {\n\t\treturn l.verify(kv)\n\t}\n\treturn l.verify(l.skipEmptyFileForward())\n}\n\nfunc (l *levelIter) SeekPrefixGE(prefix, key []byte, flags base.SeekGEFlags) *base.InternalKV {\n\tif invariants.Enabled && l.lower != nil && l.cmp(key, l.lower) < 0 {\n\t\tpanic(errors.AssertionFailedf(\"levelIter SeekGE to key %q violates lower bound %q\", key, l.lower))\n\t}\n\tl.err = nil // clear cached iteration error\n\tl.exhaustedDir = 0\n\tl.prefix = prefix\n\n\t// NB: the top-level Iterator has already adjusted key based on\n\t// IterOptions.LowerBound.\n\tloadFileIndicator := l.loadFile(l.findFileGE(key, flags), +1)\n\tif loadFileIndicator == noFileLoaded {\n\t\tl.exhaustedForward()\n\t\treturn nil\n\t}\n\tif loadFileIndicator == newFileLoaded {\n\t\t// File changed, so l.iter has changed, and that iterator is not\n\t\t// positioned appropriately.\n\t\tflags = flags.DisableTrySeekUsingNext()\n\t}\n\tif kv := l.iter.SeekPrefixGE(prefix, key, flags); kv != nil {\n\t\treturn l.verify(kv)\n\t}\n\tif err := l.iter.Error(); err != nil {\n\t\treturn nil\n\t}\n\treturn l.verify(l.skipEmptyFileForward())\n}\n\nfunc (l *levelIter) SeekLT(key []byte, flags base.SeekLTFlags) *base.InternalKV {\n\tif invariants.Enabled && l.upper != nil && l.cmp(key, l.upper) > 0 {\n\t\tpanic(errors.AssertionFailedf(\"levelIter SeekLT to key %q violates upper bound %q\", key, l.upper))\n\t}\n\n\tl.err = nil // clear cached iteration error\n\tl.exhaustedDir = 0\n\tl.prefix = nil\n\n\t// NB: the top-level Iterator has already adjusted key based on\n\t// IterOptions.UpperBound.\n\tif l.loadFile(l.findFileLT(key, flags), -1) == noFileLoaded {\n\t\tl.exhaustedBackward()\n\t\treturn nil\n\t}\n\tif kv := l.iter.SeekLT(key, flags); kv != nil {\n\t\treturn l.verify(kv)\n\t}\n\treturn l.verify(l.skipEmptyFileBackward())\n}\n\nfunc (l *levelIter) First() *base.InternalKV {\n\tif invariants.Enabled && l.lower != nil {\n\t\tpanic(errors.AssertionFailedf(\"levelIter First called while lower bound %q is set\", l.lower))\n\t}\n\n\tl.err = nil // clear cached iteration error\n\tl.exhaustedDir = 0\n\tl.prefix = nil\n\n\t// NB: the top-level Iterator will call SeekGE if IterOptions.LowerBound is\n\t// set.\n\tif l.loadFile(l.files.First(), +1) == noFileLoaded {\n\t\tl.exhaustedForward()\n\t\treturn nil\n\t}\n\tif kv := l.iter.First(); kv != nil {\n\t\treturn l.verify(kv)\n\t}\n\treturn l.verify(l.skipEmptyFileForward())\n}\n\nfunc (l *levelIter) Last() *base.InternalKV {\n\tif invariants.Enabled && l.upper != nil {\n\t\tpanic(errors.AssertionFailedf(\"levelIter Last called while upper bound %q is set\", l.upper))\n\t}\n\n\tl.err = nil // clear cached iteration error\n\tl.exhaustedDir = 0\n\tl.prefix = nil\n\n\t// NB: the top-level Iterator will call SeekLT if IterOptions.UpperBound is\n\t// set.\n\tif l.loadFile(l.files.Last(), -1) == noFileLoaded {\n\t\tl.exhaustedBackward()\n\t\treturn nil\n\t}\n\tif kv := l.iter.Last(); kv != nil {\n\t\treturn l.verify(kv)\n\t}\n\treturn l.verify(l.skipEmptyFileBackward())\n}\n\nfunc (l *levelIter) Next() *base.InternalKV {\n\tif l.exhaustedDir == -1 {\n\t\tif l.lower != nil {\n\t\t\treturn l.SeekGE(l.lower, base.SeekGEFlagsNone)\n\t\t}\n\t\treturn l.First()\n\t}\n\tif l.err != nil || l.iter == nil {\n\t\treturn nil\n\t}\n\tif kv := l.iter.Next(); kv != nil {\n\t\treturn l.verify(kv)\n\t}\n\treturn l.verify(l.skipEmptyFileForward())\n}\n\nfunc (l *levelIter) NextPrefix(succKey []byte) *base.InternalKV {\n\tif l.err != nil || l.iter == nil {\n\t\treturn nil\n\t}\n\n\tif kv := l.iter.NextPrefix(succKey); kv != nil {\n\t\treturn l.verify(kv)\n\t}\n\tif l.iter.Error() != nil {\n\t\treturn nil\n\t}\n\tif l.tableOpts.UpperBound != nil {\n\t\t// The UpperBound was within this file, so don't load the next file.\n\t\tl.exhaustedForward()\n\t\treturn nil\n\t}\n\n\t// Seek the manifest level iterator using TrySeekUsingNext=true and\n\t// RelativeSeek=true so that we take advantage of the knowledge that\n\t// `succKey` can only be contained in later files.\n\tmetadataSeekFlags := base.SeekGEFlagsNone.EnableTrySeekUsingNext().EnableRelativeSeek()\n\tif l.loadFile(l.findFileGE(succKey, metadataSeekFlags), +1) != noFileLoaded {\n\t\t// NB: The SeekGE on the file's iterator must not set TrySeekUsingNext,\n\t\t// because l.iter is unpositioned.\n\t\tif kv := l.iter.SeekGE(succKey, base.SeekGEFlagsNone); kv != nil {\n\t\t\treturn l.verify(kv)\n\t\t}\n\t\treturn l.verify(l.skipEmptyFileForward())\n\t}\n\tl.exhaustedForward()\n\treturn nil\n}\n\nfunc (l *levelIter) Prev() *base.InternalKV {\n\tif l.exhaustedDir == +1 {\n\t\tif l.upper != nil {\n\t\t\treturn l.SeekLT(l.upper, base.SeekLTFlagsNone)\n\t\t}\n\t\treturn l.Last()\n\t}\n\tif l.err != nil || l.iter == nil {\n\t\treturn nil\n\t}\n\tif kv := l.iter.Prev(); kv != nil {\n\t\treturn l.verify(kv)\n\t}\n\treturn l.verify(l.skipEmptyFileBackward())\n}\n\nfunc (l *levelIter) skipEmptyFileForward() *base.InternalKV {\n\tvar kv *base.InternalKV\n\t// The first iteration of this loop starts with an already exhausted l.iter.\n\t// The reason for the exhaustion is either that we iterated to the end of\n\t// the sstable, or our iteration was terminated early due to the presence of\n\t// an upper-bound or the use of SeekPrefixGE.\n\t//\n\t// Subsequent iterations will examine consecutive files such that the first\n\t// file that does not have an exhausted iterator causes the code to return\n\t// that key.\n\tfor ; kv == nil; kv = l.iter.First() {\n\t\tif l.iter.Error() != nil {\n\t\t\treturn nil\n\t\t}\n\t\t// If an upper bound is present and the upper bound lies within the\n\t\t// current sstable, then we will have reached the upper bound rather\n\t\t// than the end of the sstable.\n\t\tif l.tableOpts.UpperBound != nil {\n\t\t\tl.exhaustedForward()\n\t\t\treturn nil\n\t\t}\n\n\t\t// If the iterator is in prefix iteration mode, it's possible that we\n\t\t// are here because bloom filter matching failed. In that case it is\n\t\t// likely that all keys matching the prefix are wholly within the\n\t\t// current file and cannot be in a subsequent file. In that case we\n\t\t// don't want to go to the next file, since loading and seeking in there\n\t\t// has some cost.\n\t\t//\n\t\t// This is not just an optimization. We must not advance to the next\n\t\t// file if the current file might possibly contain keys relevant to any\n\t\t// prefix greater than our current iteration prefix. If we did, a\n\t\t// subsequent SeekPrefixGE with TrySeekUsingNext could mistakenly skip\n\t\t// the file's relevant keys.\n\t\tif l.prefix != nil {\n\t\t\tif l.cmp(l.split.Prefix(l.iterFile.LargestPointKey.UserKey), l.prefix) > 0 {\n\t\t\t\tl.exhaustedForward()\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\n\t\t// Current file was exhausted. Move to the next file.\n\t\tif l.loadFile(l.files.Next(), +1) == noFileLoaded {\n\t\t\tl.exhaustedForward()\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn kv\n}\n\nfunc (l *levelIter) skipEmptyFileBackward() *base.InternalKV {\n\tvar kv *base.InternalKV\n\t// The first iteration of this loop starts with an already exhausted\n\t// l.iter. The reason for the exhaustion is either that we iterated to the\n\t// end of the sstable, or our iteration was terminated early due to the\n\t// presence of a lower-bound.\n\t//\n\t// Subsequent iterations will examine consecutive files such that the first\n\t// file that does not have an exhausted iterator causes the code to return\n\t// that key.\n\tfor ; kv == nil; kv = l.iter.Last() {\n\t\tif l.iter.Error() != nil {\n\t\t\treturn nil\n\t\t}\n\t\t// If a lower bound is present and the lower bound lies within the\n\t\t// current sstable, then we will have reached the lowerr bound rather\n\t\t// than the end of the sstable.\n\t\tif l.tableOpts.LowerBound != nil {\n\t\t\tl.exhaustedBackward()\n\t\t\treturn nil\n\t\t}\n\t\t// Current file was exhausted. Move to the previous file.\n\t\tif l.loadFile(l.files.Prev(), -1) == noFileLoaded {\n\t\t\tl.exhaustedBackward()\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn kv\n}\n\nfunc (l *levelIter) exhaustedForward() {\n\tl.exhaustedDir = +1\n}\n\nfunc (l *levelIter) exhaustedBackward() {\n\tl.exhaustedDir = -1\n}\n\nfunc (l *levelIter) Error() error {\n\tif l.err != nil || l.iter == nil {\n\t\treturn l.err\n\t}\n\treturn l.iter.Error()\n}\n\nfunc (l *levelIter) Close() error {\n\tif l.iter != nil {\n\t\tl.err = l.iter.Close()\n\t\tl.iter = nil\n\t}\n\tif l.rangeDelIterSetter != nil {\n\t\tl.rangeDelIterSetter.setRangeDelIter(nil)\n\t}\n\treturn l.err\n}\n\nfunc (l *levelIter) SetBounds(lower, upper []byte) {\n\tl.lower = lower\n\tl.upper = upper\n\n\tif l.iter == nil {\n\t\treturn\n\t}\n\n\t// Update tableOpts.{Lower,Upper}Bound in case the new boundaries fall within\n\t// the boundaries of the current table.\n\tif l.initTableBounds(l.iterFile) != 0 {\n\t\t// The table does not overlap the bounds. Close() will set levelIter.err if\n\t\t// an error occurs.\n\t\t_ = l.Close()\n\t\treturn\n\t}\n\n\tl.iter.SetBounds(l.tableOpts.LowerBound, l.tableOpts.UpperBound)\n}\n\nfunc (l *levelIter) SetContext(ctx context.Context) {\n\tl.ctx = ctx\n\tif l.iter != nil {\n\t\t// TODO(sumeer): this is losing the ctx = objiotracing.WithLevel(ctx,\n\t\t// manifest.LevelToInt(opts.level)) that happens in table_cache.go.\n\t\tl.iter.SetContext(ctx)\n\t}\n}\n\n// DebugTree is part of the InternalIterator interface.\nfunc (l *levelIter) DebugTree(tp treeprinter.Node) {\n\tn := tp.Childf(\"%T(%p) %s\", l, l, l.String())\n\tif l.iter != nil {\n\t\tl.iter.DebugTree(n)\n\t}\n}\n\nfunc (l *levelIter) String() string {\n\tif l.iterFile != nil {\n\t\treturn fmt.Sprintf(\"%s: fileNum=%s\", l.layer, l.iterFile.FileNum.String())\n\t}\n\treturn fmt.Sprintf(\"%s: fileNum=<nil>\", l.layer)\n}\n\nvar _ internalIterator = &levelIter{}\n"
        },
        {
          "name": "level_iter_test.go",
          "type": "blob",
          "size": 23.119140625,
          "content": "// Copyright 2018 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"math/rand/v2\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/datadriven\"\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/bloom\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/itertest\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n\t\"github.com/cockroachdb/pebble/internal/rangedel\"\n\t\"github.com/cockroachdb/pebble/internal/rangekey\"\n\t\"github.com/cockroachdb/pebble/internal/sstableinternal\"\n\t\"github.com/cockroachdb/pebble/internal/testkeys\"\n\t\"github.com/cockroachdb/pebble/objstorage/objstorageprovider\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nconst (\n\tlevel = 1\n)\n\nfunc TestLevelIter(t *testing.T) {\n\tvar iterKVs [][]base.InternalKV\n\tvar files manifest.LevelSlice\n\n\tnewIters := func(\n\t\t_ context.Context, file *manifest.FileMetadata, opts *IterOptions, _ internalIterOpts, _ iterKinds,\n\t) (iterSet, error) {\n\t\tf := base.NewFakeIter(iterKVs[file.FileNum])\n\t\tf.SetBounds(opts.GetLowerBound(), opts.GetUpperBound())\n\t\treturn iterSet{point: f}, nil\n\t}\n\n\tdatadriven.RunTest(t, \"testdata/level_iter\", func(t *testing.T, d *datadriven.TestData) string {\n\t\tswitch d.Cmd {\n\t\tcase \"define\":\n\t\t\titerKVs = nil\n\t\t\tvar metas []*fileMetadata\n\t\t\tfor _, line := range strings.Split(d.Input, \"\\n\") {\n\t\t\t\tvar kvs []base.InternalKV\n\t\t\t\tfor _, key := range strings.Fields(line) {\n\t\t\t\t\tj := strings.Index(key, \":\")\n\t\t\t\t\tkvs = append(kvs, base.MakeInternalKV(base.ParseInternalKey(key[:j]), []byte(key[j+1:])))\n\t\t\t\t}\n\t\t\t\titerKVs = append(iterKVs, kvs)\n\n\t\t\t\tmeta := (&fileMetadata{\n\t\t\t\t\tFileNum: FileNum(len(metas)),\n\t\t\t\t}).ExtendPointKeyBounds(\n\t\t\t\t\tDefaultComparer.Compare,\n\t\t\t\t\tkvs[0].K,\n\t\t\t\t\tkvs[len(kvs)-1].K,\n\t\t\t\t)\n\t\t\t\tmeta.InitPhysicalBacking()\n\t\t\t\tmetas = append(metas, meta)\n\t\t\t}\n\t\t\tfiles = manifest.NewLevelSliceKeySorted(base.DefaultComparer.Compare, metas)\n\n\t\t\treturn \"\"\n\n\t\tcase \"iter\":\n\t\t\tvar opts IterOptions\n\t\t\tfor _, arg := range d.CmdArgs {\n\t\t\t\tif len(arg.Vals) != 1 {\n\t\t\t\t\treturn fmt.Sprintf(\"%s: %s=<value>\", d.Cmd, arg.Key)\n\t\t\t\t}\n\t\t\t\tswitch arg.Key {\n\t\t\t\tcase \"lower\":\n\t\t\t\t\topts.LowerBound = []byte(arg.Vals[0])\n\t\t\t\tcase \"upper\":\n\t\t\t\t\topts.UpperBound = []byte(arg.Vals[0])\n\t\t\t\tdefault:\n\t\t\t\t\treturn fmt.Sprintf(\"%s: unknown arg: %s\", d.Cmd, arg.Key)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\titer := newLevelIter(context.Background(), opts, testkeys.Comparer, newIters, files.Iter(), manifest.Level(level), internalIterOpts{})\n\t\t\tdefer iter.Close()\n\t\t\t// Fake up the range deletion initialization.\n\t\t\titer.initRangeDel(rangeDelIterSetterFunc(func(rangeDelIter keyspan.FragmentIterator) {\n\t\t\t\tif rangeDelIter != nil {\n\t\t\t\t\trangeDelIter.Close()\n\t\t\t\t}\n\t\t\t}))\n\t\t\titer.disableInvariants = true\n\t\t\treturn itertest.RunInternalIterCmd(t, d, iter, itertest.Verbose)\n\n\t\tcase \"load\":\n\t\t\t// The \"load\" command allows testing the iterator options passed to load\n\t\t\t// sstables.\n\t\t\t//\n\t\t\t// load <key> [lower=<key>] [upper=<key>]\n\t\t\tvar opts IterOptions\n\t\t\tvar key string\n\t\t\tfor _, arg := range d.CmdArgs {\n\t\t\t\tif len(arg.Vals) == 0 {\n\t\t\t\t\tkey = arg.Key\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif len(arg.Vals) != 1 {\n\t\t\t\t\treturn fmt.Sprintf(\"%s: %s=<value>\", d.Cmd, arg.Key)\n\t\t\t\t}\n\t\t\t\tswitch arg.Key {\n\t\t\t\tcase \"lower\":\n\t\t\t\t\topts.LowerBound = []byte(arg.Vals[0])\n\t\t\t\tcase \"upper\":\n\t\t\t\t\topts.UpperBound = []byte(arg.Vals[0])\n\t\t\t\tdefault:\n\t\t\t\t\treturn fmt.Sprintf(\"%s: unknown arg: %s\", d.Cmd, arg.Key)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tvar tableOpts *IterOptions\n\t\t\tnewIters2 := func(\n\t\t\t\tctx context.Context, file *manifest.FileMetadata, opts *IterOptions,\n\t\t\t\tinternalOpts internalIterOpts, kinds iterKinds,\n\t\t\t) (iterSet, error) {\n\t\t\t\ttableOpts = opts\n\t\t\t\treturn newIters(ctx, file, opts, internalOpts, kinds)\n\t\t\t}\n\n\t\t\titer := newLevelIter(context.Background(), opts, testkeys.Comparer, newIters2, files.Iter(), manifest.Level(level), internalIterOpts{})\n\t\t\titer.SeekGE([]byte(key), base.SeekGEFlagsNone)\n\t\t\tlower, upper := tableOpts.GetLowerBound(), tableOpts.GetUpperBound()\n\t\t\treturn fmt.Sprintf(\"[%s,%s]\\n\", lower, upper)\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", d.Cmd)\n\t\t}\n\t})\n}\n\ntype rangeDelIterSetterFunc func(rangeDelIter keyspan.FragmentIterator)\n\n// Assert that rangeDelIterSetterFunc implements the rangeDelIterSetter interface.\nvar _ rangeDelIterSetter = rangeDelIterSetterFunc(nil)\n\nfunc (fn rangeDelIterSetterFunc) setRangeDelIter(rangeDelIter keyspan.FragmentIterator) {\n\tfn(rangeDelIter)\n}\n\ntype levelIterTest struct {\n\tcmp          base.Comparer\n\tmem          vfs.FS\n\treaders      []*sstable.Reader\n\tmetas        []*fileMetadata\n\titersCreated int\n}\n\nfunc newLevelIterTest() *levelIterTest {\n\tlt := &levelIterTest{\n\t\tcmp: *DefaultComparer,\n\t\tmem: vfs.NewMem(),\n\t}\n\tlt.cmp.Split = func(a []byte) int { return len(a) }\n\treturn lt\n}\n\nfunc (lt *levelIterTest) newIters(\n\tctx context.Context,\n\tfile *manifest.FileMetadata,\n\topts *IterOptions,\n\tiio internalIterOpts,\n\tkinds iterKinds,\n) (iterSet, error) {\n\tlt.itersCreated++\n\ttransforms := file.IterTransforms()\n\tvar set iterSet\n\tif kinds.Point() {\n\t\titer, err := lt.readers[file.FileNum].NewPointIter(\n\t\t\tctx, transforms,\n\t\t\topts.LowerBound, opts.UpperBound, nil, sstable.AlwaysUseFilterBlock, iio.stats,\n\t\t\tnil, sstable.MakeTrivialReaderProvider(lt.readers[file.FileNum]))\n\t\tif err != nil {\n\t\t\treturn iterSet{}, errors.CombineErrors(err, set.CloseAll())\n\t\t}\n\t\tset.point = iter\n\t}\n\tif kinds.RangeDeletion() {\n\t\trangeDelIter, err := lt.readers[file.FileNum].NewRawRangeDelIter(context.Background(), file.FragmentIterTransforms())\n\t\tif err != nil {\n\t\t\treturn iterSet{}, errors.CombineErrors(err, set.CloseAll())\n\t\t}\n\t\tset.rangeDeletion = rangeDelIter\n\t}\n\treturn set, nil\n}\n\nfunc (lt *levelIterTest) runClear() string {\n\tlt.mem = vfs.NewMem()\n\tfor _, r := range lt.readers {\n\t\tr.Close()\n\t}\n\tlt.readers = nil\n\tlt.metas = nil\n\tlt.itersCreated = 0\n\treturn \"\"\n}\n\nfunc (lt *levelIterTest) runBuild(d *datadriven.TestData) string {\n\tfileNum := FileNum(len(lt.readers))\n\tname := fmt.Sprint(fileNum)\n\tf0, err := lt.mem.Create(name, vfs.WriteCategoryUnspecified)\n\tif err != nil {\n\t\treturn err.Error()\n\t}\n\n\ttableFormat := sstable.TableFormatMinSupported\n\tfor _, arg := range d.CmdArgs {\n\t\tif arg.Key == \"format\" {\n\t\t\tswitch arg.Vals[0] {\n\t\t\tcase \"pebblev2\":\n\t\t\t\ttableFormat = sstable.TableFormatPebblev2\n\t\t\t}\n\t\t}\n\t}\n\tfp := bloom.FilterPolicy(10)\n\tw := sstable.NewRawWriter(objstorageprovider.NewFileWritable(f0), sstable.WriterOptions{\n\t\tComparer:     &lt.cmp,\n\t\tFilterPolicy: fp,\n\t\tTableFormat:  tableFormat,\n\t})\n\tvar tombstones []keyspan.Span\n\tf := keyspan.Fragmenter{\n\t\tCmp:    lt.cmp.Compare,\n\t\tFormat: lt.cmp.FormatKey,\n\t\tEmit: func(fragmented keyspan.Span) {\n\t\t\ttombstones = append(tombstones, fragmented)\n\t\t},\n\t}\n\tfor _, key := range strings.Split(d.Input, \"\\n\") {\n\t\tj := strings.Index(key, \":\")\n\t\tikey := base.ParseInternalKey(key[:j])\n\t\tvalue := []byte(key[j+1:])\n\t\tswitch ikey.Kind() {\n\t\tcase InternalKeyKindRangeDelete:\n\t\t\tf.Add(rangedel.Decode(ikey, value, nil))\n\t\tcase InternalKeyKindRangeKeySet, InternalKeyKindRangeKeyUnset, InternalKeyKindRangeKeyDelete:\n\t\t\tspan, err := rangekey.Decode(ikey, value, nil)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tif err := w.EncodeSpan(span); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\tdefault:\n\t\t\tif err := w.AddWithForceObsolete(ikey, value, false /* forceObsolete */); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t}\n\t}\n\tf.Finish()\n\tfor _, v := range tombstones {\n\t\tif err := w.EncodeSpan(v); err != nil {\n\t\t\treturn err.Error()\n\t\t}\n\t}\n\tif err := w.Close(); err != nil {\n\t\treturn err.Error()\n\t}\n\tmeta, err := w.Metadata()\n\tif err != nil {\n\t\treturn err.Error()\n\t}\n\n\tf1, err := lt.mem.Open(name)\n\tif err != nil {\n\t\treturn err.Error()\n\t}\n\treadable, err := sstable.NewSimpleReadable(f1)\n\tif err != nil {\n\t\treturn err.Error()\n\t}\n\tr, err := sstable.NewReader(context.Background(), readable, sstable.ReaderOptions{\n\t\tFilters: map[string]FilterPolicy{\n\t\t\tfp.Name(): fp,\n\t\t},\n\t})\n\tif err != nil {\n\t\treturn err.Error()\n\t}\n\tlt.readers = append(lt.readers, r)\n\tm := &fileMetadata{FileNum: fileNum}\n\tif meta.HasPointKeys {\n\t\tm.ExtendPointKeyBounds(lt.cmp.Compare, meta.SmallestPoint, meta.LargestPoint)\n\t}\n\tif meta.HasRangeDelKeys {\n\t\tm.ExtendPointKeyBounds(lt.cmp.Compare, meta.SmallestRangeDel, meta.LargestRangeDel)\n\t}\n\tif meta.HasRangeKeys {\n\t\tm.ExtendRangeKeyBounds(lt.cmp.Compare, meta.SmallestRangeKey, meta.LargestRangeKey)\n\t}\n\tm.InitPhysicalBacking()\n\tlt.metas = append(lt.metas, m)\n\n\tvar buf bytes.Buffer\n\tfor _, f := range lt.metas {\n\t\tfmt.Fprintf(&buf, \"%d: %s-%s\\n\", f.FileNum, f.Smallest, f.Largest)\n\t}\n\treturn buf.String()\n}\n\nfunc TestLevelIterBoundaries(t *testing.T) {\n\tlt := newLevelIterTest()\n\tdefer lt.runClear()\n\n\tvar iter *levelIter\n\tdatadriven.RunTest(t, \"testdata/level_iter_boundaries\", func(t *testing.T, d *datadriven.TestData) string {\n\t\tswitch d.Cmd {\n\t\tcase \"clear\":\n\t\t\treturn lt.runClear()\n\n\t\tcase \"build\":\n\t\t\treturn lt.runBuild(d)\n\n\t\tcase \"iter\":\n\t\t\t// The save and continue parameters allow us to save the iterator\n\t\t\t// for later continued use.\n\t\t\tsave := false\n\t\t\tcont := false\n\t\t\tfor _, arg := range d.CmdArgs {\n\t\t\t\tswitch arg.Key {\n\t\t\t\tcase \"save\":\n\t\t\t\t\tsave = true\n\t\t\t\tcase \"continue\":\n\t\t\t\t\tcont = true\n\t\t\t\tdefault:\n\t\t\t\t\treturn fmt.Sprintf(\"%s: unknown arg: %s\", d.Cmd, arg.Key)\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !cont && iter != nil {\n\t\t\t\treturn \"preceding iter was not closed\"\n\t\t\t}\n\t\t\tif cont && iter == nil {\n\t\t\t\treturn \"no existing iter\"\n\t\t\t}\n\t\t\tif iter == nil {\n\t\t\t\tslice := manifest.NewLevelSliceKeySorted(lt.cmp.Compare, lt.metas)\n\t\t\t\titer = newLevelIter(context.Background(), IterOptions{}, testkeys.Comparer, lt.newIters, slice.Iter(), manifest.Level(level), internalIterOpts{})\n\t\t\t\t// Fake up the range deletion initialization.\n\t\t\t\titer.initRangeDel(rangeDelIterSetterFunc(func(rangeDelIter keyspan.FragmentIterator) {\n\t\t\t\t\tif rangeDelIter != nil {\n\t\t\t\t\t\trangeDelIter.Close()\n\t\t\t\t\t}\n\t\t\t\t}))\n\t\t\t}\n\t\t\tif !save {\n\t\t\t\tdefer func() {\n\t\t\t\t\titer.Close()\n\t\t\t\t\titer = nil\n\t\t\t\t}()\n\t\t\t}\n\t\t\treturn itertest.RunInternalIterCmd(t, d, iter, itertest.Verbose)\n\n\t\tcase \"file-pos\":\n\t\t\t// Returns the FileNum at which the iterator is positioned.\n\t\t\tif iter == nil {\n\t\t\t\treturn \"nil levelIter\"\n\t\t\t}\n\t\t\tif iter.iterFile == nil {\n\t\t\t\treturn \"nil iterFile\"\n\t\t\t}\n\t\t\tif iter.iter != nil {\n\t\t\t\treturn fmt.Sprintf(\"file %s [loaded]\", iter.iterFile.FileNum)\n\t\t\t}\n\t\t\treturn fmt.Sprintf(\"file %s [not loaded]\", iter.iterFile.FileNum)\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", d.Cmd)\n\t\t}\n\t})\n}\n\n// levelIterTestIter allows a datadriven test to use runInternalIterCmd and\n// perform parallel operations on both both a levelIter and rangeDelIter.\ntype levelIterTestIter struct {\n\t*levelIter\n\trangeDelIter keyspan.FragmentIterator\n\t// rangeDel is only set on seeks: SeekGE, SeekLT, SeekPrefixGE.\n\t// TODO(jackson): Clean this up when #2863 is resolved.\n\trangeDel *keyspan.Span\n}\n\nfunc must(err error) {\n\tif err != nil {\n\t\tpanic(err)\n\t}\n}\n\nfunc (i *levelIterTestIter) getRangeDel() *keyspan.Span {\n\treturn i.rangeDel\n}\n\nfunc (i *levelIterTestIter) rangeDelSeek(\n\tkey []byte, kv *base.InternalKV, dir int,\n) *base.InternalKV {\n\ti.rangeDel = nil\n\tif i.rangeDelIter != nil {\n\t\tvar t *keyspan.Span\n\t\tvar err error\n\t\tif dir < 0 {\n\t\t\tt, err = keyspan.SeekLE(i.levelIter.cmp, i.rangeDelIter, key)\n\t\t} else {\n\t\t\tt, err = i.rangeDelIter.SeekGE(key)\n\t\t}\n\t\t// TODO(jackson): Clean this up when the InternalIterator interface\n\t\t// is refactored to return an error return value from all\n\t\t// positioning methods.\n\t\tmust(err)\n\t\tif t != nil {\n\t\t\ti.rangeDel = new(keyspan.Span)\n\t\t\t*i.rangeDel = t.Visible(1000)\n\t\t}\n\t}\n\treturn kv\n}\n\nfunc (i *levelIterTestIter) Close() error {\n\tif i.rangeDelIter != nil {\n\t\ti.rangeDelIter.Close()\n\t\ti.rangeDelIter = nil\n\t}\n\treturn i.levelIter.Close()\n}\n\nfunc (i *levelIterTestIter) String() string {\n\treturn \"level-iter-test\"\n}\n\nfunc (i *levelIterTestIter) SeekGE(key []byte, flags base.SeekGEFlags) *base.InternalKV {\n\tkv := i.levelIter.SeekGE(key, flags)\n\treturn i.rangeDelSeek(key, kv, 1)\n}\n\nfunc (i *levelIterTestIter) SeekPrefixGE(\n\tprefix, key []byte, flags base.SeekGEFlags,\n) *base.InternalKV {\n\tkv := i.levelIter.SeekPrefixGE(prefix, key, flags)\n\treturn i.rangeDelSeek(key, kv, 1)\n}\n\nfunc (i *levelIterTestIter) SeekLT(key []byte, flags base.SeekLTFlags) *base.InternalKV {\n\tkv := i.levelIter.SeekLT(key, flags)\n\treturn i.rangeDelSeek(key, kv, -1)\n}\n\nfunc (i *levelIterTestIter) Next() *base.InternalKV {\n\ti.rangeDel = nil\n\treturn i.levelIter.Next()\n}\n\nfunc (i *levelIterTestIter) Prev() *base.InternalKV {\n\ti.rangeDel = nil\n\treturn i.levelIter.Prev()\n}\n\nfunc TestLevelIterSeek(t *testing.T) {\n\tlt := newLevelIterTest()\n\tdefer lt.runClear()\n\n\tdatadriven.RunTest(t, \"testdata/level_iter_seek\", func(t *testing.T, d *datadriven.TestData) string {\n\t\tswitch d.Cmd {\n\t\tcase \"clear\":\n\t\t\treturn lt.runClear()\n\n\t\tcase \"build\":\n\t\t\treturn lt.runBuild(d)\n\n\t\tcase \"iter\":\n\t\t\tvar stats base.InternalIteratorStats\n\t\t\tslice := manifest.NewLevelSliceKeySorted(lt.cmp.Compare, lt.metas)\n\t\t\titer := &levelIterTestIter{levelIter: &levelIter{}}\n\t\t\titer.init(context.Background(), IterOptions{}, testkeys.Comparer, lt.newIters, slice.Iter(),\n\t\t\t\tmanifest.Level(level), internalIterOpts{stats: &stats})\n\t\t\tdefer iter.Close()\n\t\t\titer.initRangeDel(rangeDelIterSetterFunc(func(rangeDelIter keyspan.FragmentIterator) {\n\t\t\t\tif iter.rangeDelIter != nil {\n\t\t\t\t\titer.rangeDelIter.Close()\n\t\t\t\t}\n\t\t\t\titer.rangeDelIter = rangeDelIter\n\t\t\t}))\n\t\t\treturn itertest.RunInternalIterCmd(t, d, iter, itertest.Verbose, itertest.WithSpan(iter.getRangeDel), itertest.WithStats(&stats))\n\n\t\tcase \"iters-created\":\n\t\t\treturn fmt.Sprintf(\"%d\", lt.itersCreated)\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", d.Cmd)\n\t\t}\n\t})\n}\n\nfunc buildLevelIterTables(\n\tb *testing.B, blockSize, restartInterval, count int,\n) ([]*sstable.Reader, manifest.LevelSlice, [][]byte, func()) {\n\tmem := vfs.NewMem()\n\tfiles := make([]vfs.File, count)\n\tfor i := range files {\n\t\tf, err := mem.Create(fmt.Sprintf(\"bench%d\", i), vfs.WriteCategoryUnspecified)\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t\tfiles[i] = f\n\t}\n\n\twriters := make([]sstable.RawWriter, len(files))\n\tfor i := range files {\n\t\twriters[i] = sstable.NewRawWriter(objstorageprovider.NewFileWritable(files[i]), sstable.WriterOptions{\n\t\t\tBlockRestartInterval: restartInterval,\n\t\t\tBlockSize:            blockSize,\n\t\t\tCompression:          NoCompression,\n\t\t})\n\t}\n\n\tvar keys [][]byte\n\tvar i int\n\tconst targetSize = 2 << 20\n\tfor _, w := range writers {\n\t\tfor ; w.EstimatedSize() < targetSize; i++ {\n\t\t\tkey := []byte(fmt.Sprintf(\"%08d\", i))\n\t\t\tkeys = append(keys, key)\n\t\t\tikey := base.MakeInternalKey(key, 0, InternalKeyKindSet)\n\t\t\trequire.NoError(b, w.AddWithForceObsolete(ikey, nil, false /* forceObsolete */))\n\t\t}\n\t\tif err := w.Close(); err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n\tc := NewCache(128 << 20 /* 128MB */)\n\tdefer c.Unref()\n\topts := sstable.ReaderOptions{Comparer: DefaultComparer}\n\topts.SetInternalCacheOpts(sstableinternal.CacheOptions{\n\t\tCache: c,\n\t})\n\treaders := make([]*sstable.Reader, len(files))\n\tfor i := range files {\n\t\tf, err := mem.Open(fmt.Sprintf(\"bench%d\", i))\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t\treadable, err := sstable.NewSimpleReadable(f)\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t\treaders[i], err = sstable.NewReader(context.Background(), readable, opts)\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n\n\tcleanup := func() {\n\t\tfor _, r := range readers {\n\t\t\trequire.NoError(b, r.Close())\n\t\t}\n\t}\n\n\tmeta := make([]*fileMetadata, len(readers))\n\tfor i := range readers {\n\t\titer, err := readers[i].NewIter(sstable.NoTransforms, nil /* lower */, nil /* upper */)\n\t\trequire.NoError(b, err)\n\t\tsmallest := iter.First()\n\t\tmeta[i] = &fileMetadata{}\n\t\tmeta[i].FileNum = FileNum(i)\n\t\tlargest := iter.Last()\n\t\tmeta[i].ExtendPointKeyBounds(opts.Comparer.Compare, smallest.K.Clone(), largest.K.Clone())\n\t\tmeta[i].InitPhysicalBacking()\n\t}\n\tslice := manifest.NewLevelSliceKeySorted(base.DefaultComparer.Compare, meta)\n\treturn readers, slice, keys, cleanup\n}\n\nfunc BenchmarkLevelIterSeekGE(b *testing.B) {\n\tconst blockSize = 32 << 10\n\n\tfor _, restartInterval := range []int{16} {\n\t\tb.Run(fmt.Sprintf(\"restart=%d\", restartInterval),\n\t\t\tfunc(b *testing.B) {\n\t\t\t\tfor _, count := range []int{5} {\n\t\t\t\t\tb.Run(fmt.Sprintf(\"count=%d\", count),\n\t\t\t\t\t\tfunc(b *testing.B) {\n\t\t\t\t\t\t\treaders, metas, keys, cleanup := buildLevelIterTables(b, blockSize, restartInterval, count)\n\t\t\t\t\t\t\tdefer cleanup()\n\t\t\t\t\t\t\tnewIters := func(\n\t\t\t\t\t\t\t\t_ context.Context, file *manifest.FileMetadata, _ *IterOptions, _ internalIterOpts, _ iterKinds,\n\t\t\t\t\t\t\t) (iterSet, error) {\n\t\t\t\t\t\t\t\titer, err := readers[file.FileNum].NewIter(sstable.NoTransforms, nil /* lower */, nil /* upper */)\n\t\t\t\t\t\t\t\treturn iterSet{point: iter}, err\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tl := newLevelIter(context.Background(), IterOptions{}, DefaultComparer, newIters, metas.Iter(), manifest.Level(level), internalIterOpts{})\n\t\t\t\t\t\t\trng := rand.New(rand.NewPCG(0, uint64(time.Now().UnixNano())))\n\n\t\t\t\t\t\t\tb.ResetTimer()\n\t\t\t\t\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t\t\t\t\tl.SeekGE(keys[rng.IntN(len(keys))], base.SeekGEFlagsNone)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tl.Close()\n\t\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t})\n\t}\n}\n\n// A benchmark that simulates the behavior of a levelIter being used as part\n// of a mergingIter where narrow bounds are repeatedly set and used to Seek\n// and then iterate over the keys within the bounds. This resembles MVCC\n// scanning by CockroachDB when doing a lookup/index join with a large number\n// of left rows, that are batched and reuse the same iterator, and which can\n// have good locality of access. This results in the successive bounds being\n// in the same file.\nfunc BenchmarkLevelIterSeqSeekGEWithBounds(b *testing.B) {\n\tconst blockSize = 32 << 10\n\n\tfor _, restartInterval := range []int{16} {\n\t\tb.Run(fmt.Sprintf(\"restart=%d\", restartInterval),\n\t\t\tfunc(b *testing.B) {\n\t\t\t\tfor _, count := range []int{5} {\n\t\t\t\t\tb.Run(fmt.Sprintf(\"count=%d\", count),\n\t\t\t\t\t\tfunc(b *testing.B) {\n\t\t\t\t\t\t\treaders, metas, keys, cleanup :=\n\t\t\t\t\t\t\t\tbuildLevelIterTables(b, blockSize, restartInterval, count)\n\t\t\t\t\t\t\tdefer cleanup()\n\t\t\t\t\t\t\t// This newIters is cheaper than in practice since it does not do\n\t\t\t\t\t\t\t// fileCacheShard.findNode.\n\t\t\t\t\t\t\tnewIters := func(\n\t\t\t\t\t\t\t\t_ context.Context, file *manifest.FileMetadata, opts *IterOptions, _ internalIterOpts, _ iterKinds,\n\t\t\t\t\t\t\t) (iterSet, error) {\n\t\t\t\t\t\t\t\titer, err := readers[file.FileNum].NewIter(\n\t\t\t\t\t\t\t\t\tsstable.NoTransforms,\n\t\t\t\t\t\t\t\t\topts.LowerBound, opts.UpperBound)\n\t\t\t\t\t\t\t\treturn iterSet{point: iter}, err\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tl := newLevelIter(context.Background(), IterOptions{}, DefaultComparer, newIters, metas.Iter(), manifest.Level(level), internalIterOpts{})\n\t\t\t\t\t\t\t// Fake up the range deletion initialization, to resemble the usage\n\t\t\t\t\t\t\t// in a mergingIter.\n\t\t\t\t\t\t\tl.initRangeDel(rangeDelIterSetterFunc(func(rangeDelIter keyspan.FragmentIterator) {\n\t\t\t\t\t\t\t\tif rangeDelIter != nil {\n\t\t\t\t\t\t\t\t\trangeDelIter.Close()\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}))\n\t\t\t\t\t\t\tkeyCount := len(keys)\n\t\t\t\t\t\t\tb.ResetTimer()\n\t\t\t\t\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t\t\t\t\tpos := i % (keyCount - 1)\n\t\t\t\t\t\t\t\tl.SetBounds(keys[pos], keys[pos+1])\n\t\t\t\t\t\t\t\t// SeekGE will return keys[pos].\n\t\t\t\t\t\t\t\tkv := l.SeekGE(keys[pos], base.SeekGEFlagsNone)\n\t\t\t\t\t\t\t\t// Next() will get called once and return nil.\n\t\t\t\t\t\t\t\tfor kv != nil {\n\t\t\t\t\t\t\t\t\tkv = l.Next()\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tl.Close()\n\t\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t})\n\t}\n}\n\n// BenchmarkLevelIterSeqSeekPrefixGE simulates the behavior of a levelIter\n// being used as part of a mergingIter where SeekPrefixGE is used to seek in a\n// monotonically increasing manner. This resembles key-value lookups done by\n// CockroachDB when evaluating Put operations.\nfunc BenchmarkLevelIterSeqSeekPrefixGE(b *testing.B) {\n\tconst blockSize = 32 << 10\n\tconst restartInterval = 16\n\treaders, metas, keys, cleanup :=\n\t\tbuildLevelIterTables(b, blockSize, restartInterval, 5)\n\tdefer cleanup()\n\t// This newIters is cheaper than in practice since it does not do\n\t// fileCacheShard.findNode.\n\tnewIters := func(\n\t\t_ context.Context, file *manifest.FileMetadata, opts *IterOptions, _ internalIterOpts, _ iterKinds,\n\t) (iterSet, error) {\n\t\titer, err := readers[file.FileNum].NewIter(\n\t\t\tsstable.NoTransforms,\n\t\t\topts.LowerBound, opts.UpperBound)\n\t\treturn iterSet{point: iter}, err\n\t}\n\n\tfor _, skip := range []int{1, 2, 4, 8, 16} {\n\t\tfor _, useNext := range []bool{false, true} {\n\t\t\tb.Run(fmt.Sprintf(\"skip=%d/use-next=%t\", skip, useNext),\n\t\t\t\tfunc(b *testing.B) {\n\t\t\t\t\tl := newLevelIter(context.Background(), IterOptions{}, testkeys.Comparer, newIters, metas.Iter(), manifest.Level(level), internalIterOpts{})\n\t\t\t\t\t// Fake up the range deletion initialization, to resemble the usage\n\t\t\t\t\t// in a mergingIter.\n\t\t\t\t\tl.initRangeDel(rangeDelIterSetterFunc(func(rangeDelIter keyspan.FragmentIterator) {\n\t\t\t\t\t\tif rangeDelIter != nil {\n\t\t\t\t\t\t\trangeDelIter.Close()\n\t\t\t\t\t\t}\n\t\t\t\t\t}))\n\t\t\t\t\tkeyCount := len(keys)\n\t\t\t\t\tpos := 0\n\t\t\t\t\tl.SeekPrefixGE(keys[pos], keys[pos], base.SeekGEFlagsNone)\n\t\t\t\t\tb.ResetTimer()\n\t\t\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t\t\tpos += skip\n\t\t\t\t\t\tvar flags base.SeekGEFlags\n\t\t\t\t\t\tif useNext {\n\t\t\t\t\t\t\tflags = flags.EnableTrySeekUsingNext()\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif pos >= keyCount {\n\t\t\t\t\t\t\tpos = 0\n\t\t\t\t\t\t\tflags = flags.DisableTrySeekUsingNext()\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// SeekPrefixGE will return keys[pos].\n\t\t\t\t\t\tl.SeekPrefixGE(keys[pos], keys[pos], flags)\n\t\t\t\t\t}\n\t\t\t\t\tb.StopTimer()\n\t\t\t\t\tl.Close()\n\t\t\t\t})\n\t\t}\n\t}\n}\n\nfunc BenchmarkLevelIterNext(b *testing.B) {\n\tconst blockSize = 32 << 10\n\n\tfor _, restartInterval := range []int{16} {\n\t\tb.Run(fmt.Sprintf(\"restart=%d\", restartInterval),\n\t\t\tfunc(b *testing.B) {\n\t\t\t\tfor _, count := range []int{5} {\n\t\t\t\t\tb.Run(fmt.Sprintf(\"count=%d\", count),\n\t\t\t\t\t\tfunc(b *testing.B) {\n\t\t\t\t\t\t\treaders, metas, _, cleanup := buildLevelIterTables(b, blockSize, restartInterval, count)\n\t\t\t\t\t\t\tdefer cleanup()\n\t\t\t\t\t\t\tnewIters := func(\n\t\t\t\t\t\t\t\t_ context.Context, file *manifest.FileMetadata, _ *IterOptions, _ internalIterOpts, _ iterKinds,\n\t\t\t\t\t\t\t) (iterSet, error) {\n\t\t\t\t\t\t\t\titer, err := readers[file.FileNum].NewIter(sstable.NoTransforms, nil /* lower */, nil /* upper */)\n\t\t\t\t\t\t\t\treturn iterSet{point: iter}, err\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tl := newLevelIter(context.Background(), IterOptions{}, testkeys.Comparer, newIters, metas.Iter(), manifest.Level(level), internalIterOpts{})\n\n\t\t\t\t\t\t\tb.ResetTimer()\n\t\t\t\t\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t\t\t\t\tkv := l.Next()\n\t\t\t\t\t\t\t\tif kv == nil {\n\t\t\t\t\t\t\t\t\tkv = l.First()\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t_ = kv\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tl.Close()\n\t\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t})\n\t}\n}\n\nfunc BenchmarkLevelIterPrev(b *testing.B) {\n\tconst blockSize = 32 << 10\n\n\tfor _, restartInterval := range []int{16} {\n\t\tb.Run(fmt.Sprintf(\"restart=%d\", restartInterval),\n\t\t\tfunc(b *testing.B) {\n\t\t\t\tfor _, count := range []int{5} {\n\t\t\t\t\tb.Run(fmt.Sprintf(\"count=%d\", count),\n\t\t\t\t\t\tfunc(b *testing.B) {\n\t\t\t\t\t\t\treaders, metas, _, cleanup := buildLevelIterTables(b, blockSize, restartInterval, count)\n\t\t\t\t\t\t\tdefer cleanup()\n\t\t\t\t\t\t\tnewIters := func(\n\t\t\t\t\t\t\t\t_ context.Context, file *manifest.FileMetadata, _ *IterOptions, _ internalIterOpts, _ iterKinds,\n\t\t\t\t\t\t\t) (iterSet, error) {\n\t\t\t\t\t\t\t\titer, err := readers[file.FileNum].NewIter(sstable.NoTransforms, nil /* lower */, nil /* upper */)\n\t\t\t\t\t\t\t\treturn iterSet{point: iter}, err\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tl := newLevelIter(context.Background(), IterOptions{}, DefaultComparer, newIters, metas.Iter(), manifest.Level(level), internalIterOpts{})\n\n\t\t\t\t\t\t\tb.ResetTimer()\n\t\t\t\t\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t\t\t\t\tkv := l.Prev()\n\t\t\t\t\t\t\t\tif kv == nil {\n\t\t\t\t\t\t\t\t\tkv = l.Last()\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t_ = kv\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tl.Close()\n\t\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t})\n\t}\n}\n"
        },
        {
          "name": "logger.go",
          "type": "blob",
          "size": 0.5146484375,
          "content": "// Copyright 2011 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport \"github.com/cockroachdb/pebble/internal/base\"\n\n// Logger defines an interface for writing log messages.\ntype Logger = base.Logger\n\n// DefaultLogger logs to the Go stdlib logs.\nvar DefaultLogger = base.DefaultLogger\n\n// LoggerAndTracer defines an interface for logging and tracing.\ntype LoggerAndTracer = base.LoggerAndTracer\n"
        },
        {
          "name": "lsm_view.go",
          "type": "blob",
          "size": 7.3671875,
          "content": "// Copyright 2024 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"slices\"\n\t\"strings\"\n\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/humanize\"\n\t\"github.com/cockroachdb/pebble/internal/lsmview\"\n\t\"github.com/cockroachdb/pebble/objstorage\"\n)\n\n// LSMViewURL returns an URL which shows a diagram of the LSM.\nfunc (d *DB) LSMViewURL() string {\n\tv := func() *version {\n\t\td.mu.Lock()\n\t\tdefer d.mu.Unlock()\n\n\t\tv := d.mu.versions.currentVersion()\n\t\tv.Ref()\n\t\treturn v\n\t}()\n\tdefer v.Unref()\n\n\tb := lsmViewBuilder{\n\t\tcmp:    d.opts.Comparer.Compare,\n\t\tfmtKey: d.opts.Comparer.FormatKey,\n\t}\n\tif b.fmtKey == nil {\n\t\tb.fmtKey = DefaultComparer.FormatKey\n\t}\n\tb.InitLevels(v)\n\tb.PopulateKeys()\n\tdata := b.Build(d.objProvider, d.newIters)\n\turl, err := lsmview.GenerateURL(data)\n\tif err != nil {\n\t\treturn fmt.Sprintf(\"error: %s\", err)\n\t}\n\treturn url.String()\n}\n\ntype lsmViewBuilder struct {\n\tcmp    base.Compare\n\tfmtKey base.FormatKey\n\n\tlevelNames []string\n\tlevels     [][]*fileMetadata\n\n\t// The keys that appear as Smallest/Largest, sorted and formatted.\n\tsortedKeys []string\n\t// keys[k] is the position of key k in the sortedKeys list.\n\tkeys map[string]int\n\n\t// scanTables is set during Build. If we don't have too many tables, we will\n\t// create iterators and show some of the keys.\n\tscanTables bool\n}\n\n// InitLevels gets the metadata for the tables in the LSM and populates\n// levelNames and levels.\nfunc (b *lsmViewBuilder) InitLevels(v *version) {\n\tvar levelNames []string\n\tvar levels [][]*fileMetadata\n\tfor sublevel := len(v.L0SublevelFiles) - 1; sublevel >= 0; sublevel-- {\n\t\tvar files []*fileMetadata\n\t\tv.L0SublevelFiles[sublevel].Each(func(f *fileMetadata) {\n\t\t\tfiles = append(files, f)\n\t\t})\n\n\t\tlevelNames = append(levelNames, fmt.Sprintf(\"L0.%d\", sublevel))\n\t\tlevels = append(levels, files)\n\t}\n\tif len(levels) == 0 {\n\t\tlevelNames = append(levelNames, \"L0\")\n\t\tlevels = append(levels, nil)\n\t}\n\tfor level := 1; level < len(v.Levels); level++ {\n\t\tvar files []*fileMetadata\n\t\tv.Levels[level].Slice().Each(func(f *fileMetadata) {\n\t\t\tfiles = append(files, f)\n\t\t})\n\t\tlevelNames = append(levelNames, fmt.Sprintf(\"L%d\", level))\n\t\tlevels = append(levels, files)\n\t}\n\tb.levelNames = levelNames\n\tb.levels = levels\n}\n\n// PopulateKeys initializes the sortedKeys and keys fields.\nfunc (b *lsmViewBuilder) PopulateKeys() {\n\t// keys[k] will hold the position of k into sortedKeys.\n\tkeys := make(map[string]int)\n\tfor _, l := range b.levels {\n\t\tfor _, f := range l {\n\t\t\tkeys[string(f.Smallest.UserKey)] = -1\n\t\t\tkeys[string(f.Largest.UserKey)] = -1\n\t\t}\n\t}\n\n\tsortedKeys := make([]string, 0, len(keys))\n\tfor s := range keys {\n\t\tsortedKeys = append(sortedKeys, s)\n\t}\n\tslices.SortFunc(sortedKeys, func(k1, k2 string) int {\n\t\treturn b.cmp([]byte(k1), []byte(k2))\n\t})\n\tsortedKeys = slices.CompactFunc(sortedKeys, func(k1, k2 string) bool {\n\t\treturn b.cmp([]byte(k1), []byte(k2)) == 0\n\t})\n\tfor i, k := range sortedKeys {\n\t\tkeys[k] = i\n\t}\n\tfor i := range sortedKeys {\n\t\tsortedKeys[i] = fmt.Sprintf(\"%v\", b.fmtKey([]byte(sortedKeys[i])))\n\t}\n\tb.sortedKeys = sortedKeys\n\tb.keys = keys\n}\n\nfunc (b *lsmViewBuilder) Build(\n\tobjProvider objstorage.Provider, newIters tableNewIters,\n) lsmview.Data {\n\tn := 0\n\tfor _, l := range b.levels {\n\t\tn += len(l)\n\t}\n\tconst scanTablesThreshold = 100\n\tb.scanTables = n <= scanTablesThreshold\n\n\tvar data lsmview.Data\n\tdata.Keys = b.sortedKeys\n\tdata.Levels = make([]lsmview.Level, len(b.levels))\n\tfor i, files := range b.levels {\n\t\tl := &data.Levels[i]\n\t\tl.Name = b.levelNames[i]\n\t\tl.Tables = make([]lsmview.Table, len(files))\n\t\tfor j, f := range files {\n\t\t\tt := &l.Tables[j]\n\t\t\tif !f.Virtual {\n\t\t\t\tt.Label = fmt.Sprintf(\"%d\", f.FileNum)\n\t\t\t} else {\n\t\t\t\tt.Label = fmt.Sprintf(\"%d (%d)\", f.FileNum, f.FileBacking.DiskFileNum)\n\t\t\t}\n\n\t\t\tt.Size = f.Size\n\t\t\tt.SmallestKey = b.keys[string(f.Smallest.UserKey)]\n\t\t\tt.LargestKey = b.keys[string(f.Largest.UserKey)]\n\t\t\tt.Details = b.tableDetails(f, objProvider, newIters)\n\t\t}\n\t}\n\treturn data\n}\n\nfunc (b *lsmViewBuilder) tableDetails(\n\tm *fileMetadata, objProvider objstorage.Provider, newIters tableNewIters,\n) []string {\n\tres := make([]string, 0, 10)\n\toutf := func(format string, args ...any) {\n\t\tres = append(res, fmt.Sprintf(format, args...))\n\t}\n\n\toutf(\"%s: %s - %s\", m.FileNum, m.Smallest.Pretty(b.fmtKey), m.Largest.Pretty(b.fmtKey))\n\toutf(\"size: %s\", humanize.Bytes.Uint64(m.Size))\n\tif m.Virtual {\n\t\tmeta, err := objProvider.Lookup(base.FileTypeTable, m.FileBacking.DiskFileNum)\n\t\tvar backingInfo string\n\t\tswitch {\n\t\tcase err != nil:\n\t\t\tbackingInfo = fmt.Sprintf(\" (error looking up object: %v)\", err)\n\t\tcase meta.IsShared():\n\t\t\tbackingInfo = \"shared; \"\n\t\tcase meta.IsExternal():\n\t\t\tbackingInfo = \"external; \"\n\t\t}\n\t\toutf(\"virtual; backed by %s (%ssize: %s)\", m.FileBacking.DiskFileNum, backingInfo, humanize.Bytes.Uint64(m.FileBacking.Size))\n\t}\n\toutf(\"seqnums: %d - %d\", m.SmallestSeqNum, m.LargestSeqNum)\n\tif m.SyntheticPrefixAndSuffix.HasPrefix() {\n\t\t// Note: we are abusing the key formatter by passing just the prefix.\n\t\toutf(\"synthetic prefix: %s\", b.fmtKey(m.SyntheticPrefixAndSuffix.Prefix()))\n\t}\n\tif m.SyntheticPrefixAndSuffix.HasSuffix() {\n\t\t// Note: we are abusing the key formatter by passing just the suffix.\n\t\toutf(\"synthetic suffix: %s\", b.fmtKey(m.SyntheticPrefixAndSuffix.Suffix()))\n\t}\n\tvar iters iterSet\n\tif b.scanTables {\n\t\tvar err error\n\t\titers, err = newIters(context.Background(), m, nil /* opts */, internalIterOpts{}, iterPointKeys|iterRangeDeletions|iterRangeKeys)\n\t\tif err != nil {\n\t\t\toutf(\"error opening table: %v\", err)\n\t\t} else {\n\t\t\tdefer iters.CloseAll()\n\t\t}\n\t}\n\tconst maxPoints = 14\n\tconst maxRangeDels = 10\n\tconst maxRangeKeys = 10\n\tif m.HasPointKeys {\n\t\toutf(\"points: %s - %s\", m.SmallestPointKey.Pretty(b.fmtKey), m.LargestPointKey.Pretty(b.fmtKey))\n\t\tif b.scanTables {\n\t\t\tn := 0\n\t\t\tif it := iters.point; it != nil {\n\t\t\t\tfor kv := it.First(); kv != nil; kv = it.Next() {\n\t\t\t\t\tif n == maxPoints {\n\t\t\t\t\t\toutf(\"  ...\")\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t\toutf(\"  %s\", kv.K.Pretty(b.fmtKey))\n\t\t\t\t\tn++\n\t\t\t\t}\n\t\t\t\tif err := it.Error(); err != nil {\n\t\t\t\t\toutf(\"  error scanning points: %v\", err)\n\t\t\t\t}\n\t\t\t}\n\t\t\tif n == 0 {\n\t\t\t\toutf(\"  no points\")\n\t\t\t}\n\n\t\t\tn = 0\n\t\t\tif it := iters.rangeDeletion; it != nil {\n\t\t\t\tspan, err := it.First()\n\t\t\t\tfor ; span != nil; span, err = it.Next() {\n\t\t\t\t\tif n == maxRangeDels {\n\t\t\t\t\t\toutf(\" ...\")\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t\tseqNums := make([]string, len(span.Keys))\n\t\t\t\t\tfor i, k := range span.Keys {\n\t\t\t\t\t\tseqNums[i] = fmt.Sprintf(\"#%d\", k.SeqNum())\n\t\t\t\t\t}\n\t\t\t\t\toutf(\"  [%s - %s): %s\", b.fmtKey(span.Start), b.fmtKey(span.End), strings.Join(seqNums, \",\"))\n\t\t\t\t\tn++\n\t\t\t\t}\n\t\t\t\tif err != nil {\n\t\t\t\t\toutf(\"error scanning range dels: %v\", err)\n\t\t\t\t}\n\t\t\t}\n\t\t\tif n == 0 {\n\t\t\t\toutf(\"  no range dels\")\n\t\t\t}\n\t\t}\n\t}\n\tif m.HasRangeKeys {\n\t\toutf(\"range keys: %s - %s\", m.SmallestRangeKey.Pretty(b.fmtKey), m.LargestRangeKey.Pretty(b.fmtKey))\n\t\tn := 0\n\t\tif it := iters.rangeKey; it != nil {\n\t\t\tspan, err := it.First()\n\t\t\tfor ; span != nil; span, err = it.Next() {\n\t\t\t\tif n == maxRangeKeys {\n\t\t\t\t\toutf(\" ...\")\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tkeys := make([]string, len(span.Keys))\n\t\t\t\tfor i, k := range span.Keys {\n\t\t\t\t\tkeys[i] = k.String()\n\t\t\t\t}\n\t\t\t\toutf(\"  [%s, %s): {%s}\", b.fmtKey(span.Start), b.fmtKey(span.End), strings.Join(keys, \" \"))\n\t\t\t\tn++\n\t\t\t}\n\t\t\tif err != nil {\n\t\t\t\toutf(\"error scanning range keys: %v\", err)\n\t\t\t}\n\t\t}\n\t\tif n == 0 {\n\t\t\toutf(\"  no range keys\")\n\t\t}\n\t}\n\n\treturn res\n}\n"
        },
        {
          "name": "lsm_view_test.go",
          "type": "blob",
          "size": 0.658203125,
          "content": "// Copyright 2024 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"testing\"\n\n\t\"github.com/cockroachdb/datadriven\"\n)\n\nfunc TestLSMViewURL(t *testing.T) {\n\tdatadriven.RunTest(t, \"testdata/lsm_view\",\n\t\tfunc(t *testing.T, td *datadriven.TestData) string {\n\t\t\tswitch td.Cmd {\n\t\t\tcase \"define\":\n\t\t\t\td, err := runDBDefineCmd(td, nil /* options */)\n\t\t\t\tif err != nil {\n\t\t\t\t\ttd.Fatalf(t, \"error: %s\", err)\n\t\t\t\t}\n\t\t\t\tdefer d.Close()\n\t\t\t\treturn d.LSMViewURL()\n\n\t\t\tdefault:\n\t\t\t\ttd.Fatalf(t, \"unknown command %q\", td.Cmd)\n\t\t\t\treturn \"\"\n\t\t\t}\n\t\t})\n}\n"
        },
        {
          "name": "mem_table.go",
          "type": "blob",
          "size": 14.3388671875,
          "content": "// Copyright 2011 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"os\"\n\t\"sync\"\n\t\"sync/atomic\"\n\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/arenaskl\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan\"\n\t\"github.com/cockroachdb/pebble/internal/manual\"\n\t\"github.com/cockroachdb/pebble/internal/rangedel\"\n\t\"github.com/cockroachdb/pebble/internal/rangekey\"\n)\n\nfunc memTableEntrySize(keyBytes, valueBytes int) uint64 {\n\treturn arenaskl.MaxNodeSize(uint32(keyBytes)+8, uint32(valueBytes))\n}\n\n// memTableEmptySize is the amount of allocated space in the arena when the\n// memtable is empty.\nvar memTableEmptySize = func() uint32 {\n\tvar pointSkl arenaskl.Skiplist\n\tvar rangeDelSkl arenaskl.Skiplist\n\tvar rangeKeySkl arenaskl.Skiplist\n\tarena := arenaskl.NewArena(make([]byte, 16<<10 /* 16 KB */))\n\tpointSkl.Reset(arena, bytes.Compare)\n\trangeDelSkl.Reset(arena, bytes.Compare)\n\trangeKeySkl.Reset(arena, bytes.Compare)\n\treturn arena.Size()\n}()\n\n// A memTable implements an in-memory layer of the LSM. A memTable is mutable,\n// but append-only. Records are added, but never removed. Deletion is supported\n// via tombstones, but it is up to higher level code (see Iterator) to support\n// processing those tombstones.\n//\n// A memTable is implemented on top of a lock-free arena-backed skiplist. An\n// arena is a fixed size contiguous chunk of memory (see\n// Options.MemTableSize). A memTable's memory consumption is thus fixed at the\n// time of creation (with the exception of the cached fragmented range\n// tombstones). The arena-backed skiplist provides both forward and reverse\n// links which makes forward and reverse iteration the same speed.\n//\n// A batch is \"applied\" to a memTable in a two step process: prepare(batch) ->\n// apply(batch). memTable.prepare() is not thread-safe and must be called with\n// external synchronization. Preparation reserves space in the memTable for the\n// batch. Note that we pessimistically compute how much space a batch will\n// consume in the memTable (see memTableEntrySize and\n// Batch.memTableSize). Preparation is an O(1) operation. Applying a batch to\n// the memTable can be performed concurrently with other apply\n// operations. Applying a batch is an O(n logm) operation where N is the number\n// of records in the batch and M is the number of records in the memtable. The\n// commitPipeline serializes batch preparation, and allows batch application to\n// proceed concurrently.\n//\n// It is safe to call get, apply, newIter, and newRangeDelIter concurrently.\ntype memTable struct {\n\tcmp         Compare\n\tformatKey   base.FormatKey\n\tequal       Equal\n\tarenaBuf    manual.Buf\n\tskl         arenaskl.Skiplist\n\trangeDelSkl arenaskl.Skiplist\n\trangeKeySkl arenaskl.Skiplist\n\t// reserved tracks the amount of space used by the memtable, both by actual\n\t// data stored in the memtable as well as inflight batch commit\n\t// operations. This value is incremented pessimistically by prepare() in\n\t// order to account for the space needed by a batch.\n\treserved uint32\n\t// writerRefs tracks the write references on the memtable. The two sources of\n\t// writer references are the memtable being on DB.mu.mem.queue and from\n\t// inflight mutations that have reserved space in the memtable but not yet\n\t// applied. The memtable cannot be flushed to disk until the writer refs\n\t// drops to zero.\n\twriterRefs atomic.Int32\n\ttombstones keySpanCache\n\trangeKeys  keySpanCache\n\t// The current logSeqNum at the time the memtable was created. This is\n\t// guaranteed to be less than or equal to any seqnum stored in the memtable.\n\tlogSeqNum                    base.SeqNum\n\treleaseAccountingReservation func()\n}\n\nfunc (m *memTable) free() {\n\tif m != nil {\n\t\tm.releaseAccountingReservation()\n\t\tmanual.Free(manual.MemTable, m.arenaBuf)\n\t\tm.arenaBuf = manual.Buf{}\n\t}\n}\n\n// memTableOptions holds configuration used when creating a memTable. All of\n// the fields are optional and will be filled with defaults if not specified\n// which is used by tests.\ntype memTableOptions struct {\n\t*Options\n\tarenaBuf                     manual.Buf\n\tsize                         int\n\tlogSeqNum                    base.SeqNum\n\treleaseAccountingReservation func()\n}\n\nfunc checkMemTable(obj interface{}) {\n\tm := obj.(*memTable)\n\tif m.arenaBuf.Data() != nil {\n\t\tfmt.Fprintf(os.Stderr, \"%v: memTable buffer was not freed\\n\", m.arenaBuf)\n\t\tos.Exit(1)\n\t}\n}\n\n// newMemTable returns a new MemTable of the specified size. If size is zero,\n// Options.MemTableSize is used instead.\nfunc newMemTable(opts memTableOptions) *memTable {\n\topts.Options = opts.Options.EnsureDefaults()\n\tm := new(memTable)\n\tm.init(opts)\n\treturn m\n}\n\nfunc (m *memTable) init(opts memTableOptions) {\n\tif opts.size == 0 {\n\t\topts.size = int(opts.MemTableSize)\n\t}\n\t*m = memTable{\n\t\tcmp:                          opts.Comparer.Compare,\n\t\tformatKey:                    opts.Comparer.FormatKey,\n\t\tequal:                        opts.Comparer.Equal,\n\t\tarenaBuf:                     opts.arenaBuf,\n\t\tlogSeqNum:                    opts.logSeqNum,\n\t\treleaseAccountingReservation: opts.releaseAccountingReservation,\n\t}\n\tm.writerRefs.Store(1)\n\tm.tombstones = keySpanCache{\n\t\tcmp:           m.cmp,\n\t\tformatKey:     m.formatKey,\n\t\tskl:           &m.rangeDelSkl,\n\t\tconstructSpan: rangeDelConstructSpan,\n\t}\n\tm.rangeKeys = keySpanCache{\n\t\tcmp:           m.cmp,\n\t\tformatKey:     m.formatKey,\n\t\tskl:           &m.rangeKeySkl,\n\t\tconstructSpan: rangekey.Decode,\n\t}\n\n\tif m.arenaBuf.Data() == nil {\n\t\tm.arenaBuf = manual.New(manual.MemTable, uintptr(opts.size))\n\t}\n\n\tarena := arenaskl.NewArena(m.arenaBuf.Slice())\n\tm.skl.Reset(arena, m.cmp)\n\tm.rangeDelSkl.Reset(arena, m.cmp)\n\tm.rangeKeySkl.Reset(arena, m.cmp)\n\tm.reserved = arena.Size()\n}\n\nfunc (m *memTable) writerRef() {\n\tswitch v := m.writerRefs.Add(1); {\n\tcase v <= 1:\n\t\tpanic(fmt.Sprintf(\"pebble: inconsistent reference count: %d\", v))\n\t}\n}\n\n// writerUnref drops a ref on the memtable. Returns true if this was the last ref.\nfunc (m *memTable) writerUnref() (wasLastRef bool) {\n\tswitch v := m.writerRefs.Add(-1); {\n\tcase v < 0:\n\t\tpanic(fmt.Sprintf(\"pebble: inconsistent reference count: %d\", v))\n\tcase v == 0:\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\n// readyForFlush is part of the flushable interface.\nfunc (m *memTable) readyForFlush() bool {\n\treturn m.writerRefs.Load() == 0\n}\n\n// Prepare reserves space for the batch in the memtable and references the\n// memtable preventing it from being flushed until the batch is applied. Note\n// that prepare is not thread-safe, while apply is. The caller must call\n// writerUnref() after the batch has been applied.\nfunc (m *memTable) prepare(batch *Batch) error {\n\tavail := m.availBytes()\n\tif batch.memTableSize > uint64(avail) {\n\t\treturn arenaskl.ErrArenaFull\n\t}\n\tm.reserved += uint32(batch.memTableSize)\n\n\tm.writerRef()\n\treturn nil\n}\n\nfunc (m *memTable) apply(batch *Batch, seqNum base.SeqNum) error {\n\tif seqNum < m.logSeqNum {\n\t\treturn base.CorruptionErrorf(\"pebble: batch seqnum %d is less than memtable creation seqnum %d\",\n\t\t\terrors.Safe(seqNum), errors.Safe(m.logSeqNum))\n\t}\n\n\tvar ins arenaskl.Inserter\n\tvar tombstoneCount, rangeKeyCount uint32\n\tstartSeqNum := seqNum\n\tfor r := batch.Reader(); ; seqNum++ {\n\t\tkind, ukey, value, ok, err := r.Next()\n\t\tif !ok {\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tbreak\n\t\t}\n\t\tikey := base.MakeInternalKey(ukey, seqNum, kind)\n\t\tswitch kind {\n\t\tcase InternalKeyKindRangeDelete:\n\t\t\terr = m.rangeDelSkl.Add(ikey, value)\n\t\t\ttombstoneCount++\n\t\tcase InternalKeyKindRangeKeySet, InternalKeyKindRangeKeyUnset, InternalKeyKindRangeKeyDelete:\n\t\t\terr = m.rangeKeySkl.Add(ikey, value)\n\t\t\trangeKeyCount++\n\t\tcase InternalKeyKindLogData:\n\t\t\t// Don't increment seqNum for LogData, since these are not applied\n\t\t\t// to the memtable.\n\t\t\tseqNum--\n\t\tcase InternalKeyKindIngestSST, InternalKeyKindExcise:\n\t\t\tpanic(\"pebble: cannot apply ingested sstable or excise kind keys to memtable\")\n\t\tdefault:\n\t\t\terr = ins.Add(&m.skl, ikey, value)\n\t\t}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif seqNum != startSeqNum+base.SeqNum(batch.Count()) {\n\t\treturn base.CorruptionErrorf(\"pebble: inconsistent batch count: %d vs %d\",\n\t\t\terrors.Safe(seqNum), errors.Safe(startSeqNum+base.SeqNum(batch.Count())))\n\t}\n\tif tombstoneCount != 0 {\n\t\tm.tombstones.invalidate(tombstoneCount)\n\t}\n\tif rangeKeyCount != 0 {\n\t\tm.rangeKeys.invalidate(rangeKeyCount)\n\t}\n\treturn nil\n}\n\n// newIter is part of the flushable interface. It returns an iterator that is\n// unpositioned (Iterator.Valid() will return false). The iterator can be\n// positioned via a call to SeekGE, SeekLT, First or Last.\nfunc (m *memTable) newIter(o *IterOptions) internalIterator {\n\treturn m.skl.NewIter(o.GetLowerBound(), o.GetUpperBound())\n}\n\n// newFlushIter is part of the flushable interface.\nfunc (m *memTable) newFlushIter(o *IterOptions) internalIterator {\n\treturn m.skl.NewFlushIter()\n}\n\n// newRangeDelIter is part of the flushable interface.\nfunc (m *memTable) newRangeDelIter(*IterOptions) keyspan.FragmentIterator {\n\ttombstones := m.tombstones.get()\n\tif tombstones == nil {\n\t\treturn nil\n\t}\n\treturn keyspan.NewIter(m.cmp, tombstones)\n}\n\n// newRangeKeyIter is part of the flushable interface.\nfunc (m *memTable) newRangeKeyIter(*IterOptions) keyspan.FragmentIterator {\n\trangeKeys := m.rangeKeys.get()\n\tif rangeKeys == nil {\n\t\treturn nil\n\t}\n\treturn keyspan.NewIter(m.cmp, rangeKeys)\n}\n\n// containsRangeKeys is part of the flushable interface.\nfunc (m *memTable) containsRangeKeys() bool {\n\treturn m.rangeKeys.count.Load() > 0\n}\n\nfunc (m *memTable) availBytes() uint32 {\n\ta := m.skl.Arena()\n\tif m.writerRefs.Load() == 1 {\n\t\t// Note that one ref is maintained as long as the memtable is the\n\t\t// current mutable memtable, so when evaluating whether the current\n\t\t// mutable memtable has sufficient space for committing a batch, it is\n\t\t// guaranteed that m.writerRefs() >= 1. This means a writerRefs() of 1\n\t\t// indicates there are no other concurrent apply operations.\n\t\t//\n\t\t// If there are no other concurrent apply operations, we can update the\n\t\t// reserved bytes setting to accurately reflect how many bytes of been\n\t\t// allocated vs the over-estimation present in memTableEntrySize.\n\t\tm.reserved = a.Size()\n\t}\n\treturn a.Capacity() - m.reserved\n}\n\n// inuseBytes is part of the flushable interface.\nfunc (m *memTable) inuseBytes() uint64 {\n\treturn uint64(m.skl.Size() - memTableEmptySize)\n}\n\n// totalBytes is part of the flushable interface.\nfunc (m *memTable) totalBytes() uint64 {\n\treturn uint64(m.skl.Arena().Capacity())\n}\n\n// empty returns whether the MemTable has no key/value pairs.\nfunc (m *memTable) empty() bool {\n\treturn m.skl.Size() == memTableEmptySize\n}\n\n// computePossibleOverlaps is part of the flushable interface.\nfunc (m *memTable) computePossibleOverlaps(fn func(bounded) shouldContinue, bounded ...bounded) {\n\tcomputePossibleOverlapsGenericImpl[*memTable](m, m.cmp, fn, bounded)\n}\n\n// A keySpanFrags holds a set of fragmented keyspan.Spans with a particular key\n// kind at a particular moment for a memtable.\n//\n// When a new span of a particular kind is added to the memtable, it may overlap\n// with other spans of the same kind. Instead of performing the fragmentation\n// whenever an iterator requires it, fragments are cached within a keySpanCache\n// type. The keySpanCache uses keySpanFrags to hold the cached fragmented spans.\n//\n// The count of keys (and keys of any given kind) in a memtable only\n// monotonically increases. The count of key spans of a particular kind is used\n// as a stand-in for a 'sequence number'. A keySpanFrags represents the\n// fragmented state of the memtable's keys of a given kind at the moment while\n// there existed `count` keys of that kind in the memtable.\n//\n// It's currently only used to contain fragmented range deletion tombstones.\ntype keySpanFrags struct {\n\tcount uint32\n\tonce  sync.Once\n\tspans []keyspan.Span\n}\n\ntype constructSpan func(ik base.InternalKey, v []byte, keysDst []keyspan.Key) (keyspan.Span, error)\n\nfunc rangeDelConstructSpan(\n\tik base.InternalKey, v []byte, keysDst []keyspan.Key,\n) (keyspan.Span, error) {\n\treturn rangedel.Decode(ik, v, keysDst), nil\n}\n\n// get retrieves the fragmented spans, populating them if necessary. Note that\n// the populated span fragments may be built from more than f.count memTable\n// spans, but that is ok for correctness. All we're requiring is that the\n// memTable contains at least f.count keys of the configured kind. This\n// situation can occur if there are multiple concurrent additions of the key\n// kind and a concurrent reader. The reader can load a keySpanFrags and populate\n// it even though is has been invalidated (i.e. replaced with a newer\n// keySpanFrags).\nfunc (f *keySpanFrags) get(\n\tskl *arenaskl.Skiplist, cmp Compare, formatKey base.FormatKey, constructSpan constructSpan,\n) []keyspan.Span {\n\tf.once.Do(func() {\n\t\tfrag := &keyspan.Fragmenter{\n\t\t\tCmp:    cmp,\n\t\t\tFormat: formatKey,\n\t\t\tEmit: func(fragmented keyspan.Span) {\n\t\t\t\tf.spans = append(f.spans, fragmented)\n\t\t\t},\n\t\t}\n\t\tit := skl.NewIter(nil, nil)\n\t\tvar keysDst []keyspan.Key\n\t\tfor kv := it.First(); kv != nil; kv = it.Next() {\n\t\t\ts, err := constructSpan(kv.K, kv.InPlaceValue(), keysDst)\n\t\t\tif err != nil {\n\t\t\t\tpanic(err)\n\t\t\t}\n\t\t\tfrag.Add(s)\n\t\t\tkeysDst = s.Keys[len(s.Keys):]\n\t\t}\n\t\tfrag.Finish()\n\t})\n\treturn f.spans\n}\n\n// A keySpanCache is used to cache a set of fragmented spans. The cache is\n// invalidated whenever a key of the same kind is added to a memTable, and\n// populated when empty when a span iterator of that key kind is created.\ntype keySpanCache struct {\n\tcount         atomic.Uint32\n\tfrags         atomic.Pointer[keySpanFrags]\n\tcmp           Compare\n\tformatKey     base.FormatKey\n\tconstructSpan constructSpan\n\tskl           *arenaskl.Skiplist\n}\n\n// Invalidate the current set of cached spans, indicating the number of\n// spans that were added.\nfunc (c *keySpanCache) invalidate(count uint32) {\n\tnewCount := c.count.Add(count)\n\tvar frags *keySpanFrags\n\n\tfor {\n\t\toldFrags := c.frags.Load()\n\t\tif oldFrags != nil && oldFrags.count >= newCount {\n\t\t\t// Someone else invalidated the cache before us and their invalidation\n\t\t\t// subsumes ours.\n\t\t\tbreak\n\t\t}\n\t\tif frags == nil {\n\t\t\tfrags = &keySpanFrags{count: newCount}\n\t\t}\n\t\tif c.frags.CompareAndSwap(oldFrags, frags) {\n\t\t\t// We successfully invalidated the cache.\n\t\t\tbreak\n\t\t}\n\t\t// Someone else invalidated the cache. Loop and try again.\n\t}\n}\n\nfunc (c *keySpanCache) get() []keyspan.Span {\n\tfrags := c.frags.Load()\n\tif frags == nil {\n\t\treturn nil\n\t}\n\treturn frags.get(c.skl, c.cmp, c.formatKey, c.constructSpan)\n}\n"
        },
        {
          "name": "mem_table_test.go",
          "type": "blob",
          "size": 14.7607421875,
          "content": "// Copyright 2011 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"math/rand/v2\"\n\t\"strconv\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n\t\"unicode\"\n\n\t\"github.com/cockroachdb/datadriven\"\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/arenaskl\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/itertest\"\n\t\"github.com/cockroachdb/pebble/internal/rangekey\"\n\t\"github.com/stretchr/testify/require\"\n\t\"golang.org/x/sync/errgroup\"\n)\n\n// get gets the value for the given key. It returns ErrNotFound if the DB does\n// not contain the key.\nfunc (m *memTable) get(key []byte) (value []byte, err error) {\n\tit := m.skl.NewIter(nil, nil)\n\tkv := it.SeekGE(key, base.SeekGEFlagsNone)\n\tif kv == nil {\n\t\treturn nil, ErrNotFound\n\t}\n\tif !m.equal(key, kv.K.UserKey) {\n\t\treturn nil, ErrNotFound\n\t}\n\tswitch kv.Kind() {\n\tcase InternalKeyKindDelete, InternalKeyKindSingleDelete, InternalKeyKindDeleteSized:\n\t\treturn nil, ErrNotFound\n\tdefault:\n\t\treturn kv.InPlaceValue(), nil\n\t}\n}\n\n// Set sets the value for the given key. It overwrites any previous value for\n// that key; a DB is not a multi-map. NB: this might have unexpected\n// interaction with prepare/apply. Caveat emptor!\nfunc (m *memTable) set(key InternalKey, value []byte) error {\n\tif key.Kind() == InternalKeyKindRangeDelete {\n\t\tif err := m.rangeDelSkl.Add(key, value); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tm.tombstones.invalidate(1)\n\t\treturn nil\n\t}\n\tif rangekey.IsRangeKey(key.Kind()) {\n\t\tif err := m.rangeKeySkl.Add(key, value); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tm.rangeKeys.invalidate(1)\n\t\treturn nil\n\t}\n\treturn m.skl.Add(key, value)\n}\n\n// count returns the number of entries in a DB.\nfunc (m *memTable) count() (n int) {\n\tx := m.newIter(nil)\n\tfor kv := x.First(); kv != nil; kv = x.Next() {\n\t\tn++\n\t}\n\tif x.Close() != nil {\n\t\treturn -1\n\t}\n\treturn n\n}\n\nfunc ikey(s string) InternalKey {\n\treturn base.MakeInternalKey([]byte(s), 0, InternalKeyKindSet)\n}\n\nfunc TestMemTableBasic(t *testing.T) {\n\t// Check the empty DB.\n\tm := newMemTable(memTableOptions{})\n\tif got, want := m.count(), 0; got != want {\n\t\tt.Fatalf(\"0.count: got %v, want %v\", got, want)\n\t}\n\tv, err := m.get([]byte(\"cherry\"))\n\tif string(v) != \"\" || err != ErrNotFound {\n\t\tt.Fatalf(\"1.get: got (%q, %v), want (%q, %v)\", v, err, \"\", ErrNotFound)\n\t}\n\t// Add some key/value pairs.\n\tm.set(ikey(\"cherry\"), []byte(\"red\"))\n\tm.set(ikey(\"peach\"), []byte(\"yellow\"))\n\tm.set(ikey(\"grape\"), []byte(\"red\"))\n\tm.set(ikey(\"grape\"), []byte(\"green\"))\n\tm.set(ikey(\"plum\"), []byte(\"purple\"))\n\tif got, want := m.count(), 4; got != want {\n\t\tt.Fatalf(\"2.count: got %v, want %v\", got, want)\n\t}\n\t// Get keys that are and aren't in the DB.\n\tv, err = m.get([]byte(\"plum\"))\n\tif string(v) != \"purple\" || err != nil {\n\t\tt.Fatalf(\"6.get: got (%q, %v), want (%q, %v)\", v, err, \"purple\", error(nil))\n\t}\n\tv, err = m.get([]byte(\"lychee\"))\n\tif string(v) != \"\" || err != ErrNotFound {\n\t\tt.Fatalf(\"7.get: got (%q, %v), want (%q, %v)\", v, err, \"\", ErrNotFound)\n\t}\n\t// Check an iterator.\n\ts, x := \"\", m.newIter(nil)\n\tfor kv := x.SeekGE([]byte(\"mango\"), base.SeekGEFlagsNone); kv != nil; kv = x.Next() {\n\t\tv, _, err := kv.Value(nil)\n\t\trequire.NoError(t, err)\n\t\ts += fmt.Sprintf(\"%s/%s.\", kv.K.UserKey, v)\n\t}\n\tif want := \"peach/yellow.plum/purple.\"; s != want {\n\t\tt.Fatalf(\"8.iter: got %q, want %q\", s, want)\n\t}\n\tif err = x.Close(); err != nil {\n\t\tt.Fatalf(\"9.close: %v\", err)\n\t}\n\t// Check some more sets and deletes.\n\tif err := m.set(ikey(\"apricot\"), []byte(\"orange\")); err != nil {\n\t\tt.Fatalf(\"12.set: %v\", err)\n\t}\n\tif got, want := m.count(), 5; got != want {\n\t\tt.Fatalf(\"13.count: got %v, want %v\", got, want)\n\t}\n}\n\nfunc TestMemTableCount(t *testing.T) {\n\tm := newMemTable(memTableOptions{})\n\tfor i := 0; i < 200; i++ {\n\t\tif j := m.count(); j != i {\n\t\t\tt.Fatalf(\"count: got %d, want %d\", j, i)\n\t\t}\n\t\tm.set(InternalKey{UserKey: []byte{byte(i)}}, nil)\n\t}\n}\n\nfunc TestMemTableEmpty(t *testing.T) {\n\tm := newMemTable(memTableOptions{})\n\tif !m.empty() {\n\t\tt.Errorf(\"got !empty, want empty\")\n\t}\n\t// Add one key/value pair with an empty key and empty value.\n\tm.set(InternalKey{}, nil)\n\tif m.empty() {\n\t\tt.Errorf(\"got empty, want !empty\")\n\t}\n}\n\nfunc TestMemTable1000Entries(t *testing.T) {\n\t// Initialize the DB.\n\tconst N = 1000\n\tm0 := newMemTable(memTableOptions{})\n\tfor i := 0; i < N; i++ {\n\t\tk := ikey(strconv.Itoa(i))\n\t\tv := []byte(strings.Repeat(\"x\", i))\n\t\tm0.set(k, v)\n\t}\n\t// Check the DB count.\n\tif got, want := m0.count(), 1000; got != want {\n\t\tt.Fatalf(\"count: got %v, want %v\", got, want)\n\t}\n\t// Check random-access lookup.\n\tr := rand.New(rand.NewPCG(0, 0))\n\tfor i := 0; i < 3*N; i++ {\n\t\tj := r.IntN(N)\n\t\tk := []byte(strconv.Itoa(j))\n\t\tv, err := m0.get(k)\n\t\trequire.NoError(t, err)\n\t\tif len(v) != cap(v) {\n\t\t\tt.Fatalf(\"get: j=%d, got len(v)=%d, cap(v)=%d\", j, len(v), cap(v))\n\t\t}\n\t\tvar c uint8\n\t\tif len(v) != 0 {\n\t\t\tc = v[0]\n\t\t} else {\n\t\t\tc = 'x'\n\t\t}\n\t\tif len(v) != j || c != 'x' {\n\t\t\tt.Fatalf(\"get: j=%d, got len(v)=%d,c=%c, want %d,%c\", j, len(v), c, j, 'x')\n\t\t}\n\t}\n\t// Check that iterating through the middle of the DB looks OK.\n\t// Keys are in lexicographic order, not numerical order.\n\t// Multiples of 3 are not present.\n\twants := []string{\n\t\t\"499\",\n\t\t\"5\",\n\t\t\"50\",\n\t\t\"500\",\n\t\t\"501\",\n\t\t\"502\",\n\t\t\"503\",\n\t\t\"504\",\n\t\t\"505\",\n\t\t\"506\",\n\t\t\"507\",\n\t}\n\tx := m0.newIter(nil)\n\tkv := x.SeekGE([]byte(wants[0]), base.SeekGEFlagsNone)\n\tfor _, want := range wants {\n\t\tif kv == nil {\n\t\t\tt.Fatalf(\"iter: next failed, want=%q\", want)\n\t\t}\n\t\tif got := string(kv.K.UserKey); got != want {\n\t\t\tt.Fatalf(\"iter: got %q, want %q\", got, want)\n\t\t}\n\t\tif k := kv.K.UserKey; len(k) != cap(k) {\n\t\t\tt.Fatalf(\"iter: len(k)=%d, cap(k)=%d\", len(k), cap(k))\n\t\t}\n\t\tv, _, err := kv.Value(nil)\n\t\trequire.NoError(t, err)\n\t\tif len(v) != cap(v) {\n\t\t\tt.Fatalf(\"iter: len(v)=%d, cap(v)=%d\", len(v), cap(v))\n\t\t}\n\t\tx.Next()\n\t}\n\tif err := x.Close(); err != nil {\n\t\tt.Fatalf(\"close: %v\", err)\n\t}\n}\n\nfunc TestMemTableIter(t *testing.T) {\n\tvar mem *memTable\n\tfor _, testdata := range []string{\n\t\t\"testdata/internal_iter_next\", \"testdata/internal_iter_bounds\"} {\n\t\tdatadriven.RunTest(t, testdata, func(t *testing.T, d *datadriven.TestData) string {\n\t\t\tswitch d.Cmd {\n\t\t\tcase \"define\":\n\t\t\t\tmem = newMemTable(memTableOptions{})\n\t\t\t\tfor _, key := range strings.Split(d.Input, \"\\n\") {\n\t\t\t\t\tj := strings.Index(key, \":\")\n\t\t\t\t\tif err := mem.set(base.ParseInternalKey(key[:j]), []byte(key[j+1:])); err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn \"\"\n\n\t\t\tcase \"iter\":\n\t\t\t\tvar options IterOptions\n\t\t\t\tfor _, arg := range d.CmdArgs {\n\t\t\t\t\tswitch arg.Key {\n\t\t\t\t\tcase \"lower\":\n\t\t\t\t\t\tif len(arg.Vals) != 1 {\n\t\t\t\t\t\t\treturn fmt.Sprintf(\n\t\t\t\t\t\t\t\t\"%s expects at most 1 value for lower\", d.Cmd)\n\t\t\t\t\t\t}\n\t\t\t\t\t\toptions.LowerBound = []byte(arg.Vals[0])\n\t\t\t\t\tcase \"upper\":\n\t\t\t\t\t\tif len(arg.Vals) != 1 {\n\t\t\t\t\t\t\treturn fmt.Sprintf(\n\t\t\t\t\t\t\t\t\"%s expects at most 1 value for upper\", d.Cmd)\n\t\t\t\t\t\t}\n\t\t\t\t\t\toptions.UpperBound = []byte(arg.Vals[0])\n\t\t\t\t\tdefault:\n\t\t\t\t\t\treturn fmt.Sprintf(\"unknown arg: %s\", arg.Key)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\titer := mem.newIter(&options)\n\t\t\t\tdefer iter.Close()\n\t\t\t\treturn itertest.RunInternalIterCmd(t, d, iter)\n\n\t\t\tdefault:\n\t\t\t\treturn fmt.Sprintf(\"unknown command: %s\", d.Cmd)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestMemTableDeleteRange(t *testing.T) {\n\tvar mem *memTable\n\tvar seqNum base.SeqNum\n\n\tdatadriven.RunTest(t, \"testdata/delete_range\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"clear\":\n\t\t\tmem = nil\n\t\t\tseqNum = 0\n\t\t\treturn \"\"\n\n\t\tcase \"define\":\n\t\t\tb := newBatch(nil)\n\t\t\tif err := runBatchDefineCmd(td, b); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tif mem == nil {\n\t\t\t\tmem = newMemTable(memTableOptions{})\n\t\t\t}\n\t\t\tif err := mem.apply(b, seqNum); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tseqNum += base.SeqNum(b.Count())\n\t\t\treturn \"\"\n\n\t\tcase \"scan\":\n\t\t\tvar buf bytes.Buffer\n\t\t\tif td.HasArg(\"range-del\") {\n\t\t\t\titer := mem.newRangeDelIter(nil)\n\t\t\t\tdefer iter.Close()\n\t\t\t\tscanKeyspanIterator(&buf, iter)\n\t\t\t} else {\n\t\t\t\titer := mem.newIter(nil)\n\t\t\t\tdefer iter.Close()\n\t\t\t\tscanInternalIter(&buf, iter)\n\t\t\t}\n\t\t\treturn buf.String()\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc TestMemTableConcurrentDeleteRange(t *testing.T) {\n\t// Concurrently write and read range tombstones. Workers add range\n\t// tombstones, and then immediately retrieve them verifying that the\n\t// tombstones they've added are all present.\n\n\tm := newMemTable(memTableOptions{Options: &Options{MemTableSize: 64 << 20}})\n\n\tconst workers = 10\n\teg, _ := errgroup.WithContext(context.Background())\n\tvar seqNum base.AtomicSeqNum\n\tseqNum.Store(1)\n\tfor i := 0; i < workers; i++ {\n\t\ti := i\n\t\teg.Go(func() error {\n\t\t\tstart := ([]byte)(fmt.Sprintf(\"%03d\", i))\n\t\t\tend := ([]byte)(fmt.Sprintf(\"%03d\", i+1))\n\t\t\tfor j := 0; j < 100; j++ {\n\t\t\t\tb := newBatch(nil)\n\t\t\t\tb.DeleteRange(start, end, nil)\n\t\t\t\tn := seqNum.Add(1) - 1\n\t\t\t\trequire.NoError(t, m.apply(b, n))\n\t\t\t\tb.Close()\n\n\t\t\t\tvar count int\n\t\t\t\tit := m.newRangeDelIter(nil)\n\t\t\t\ts, err := it.SeekGE(start)\n\t\t\t\tfor ; s != nil; s, err = it.Next() {\n\t\t\t\t\tif m.cmp(s.Start, end) >= 0 {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t\tcount += len(s.Keys)\n\t\t\t\t}\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tif j+1 != count {\n\t\t\t\t\treturn errors.Errorf(\"%d: expected %d tombstones, but found %d\", i, j+1, count)\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\t}\n\terr := eg.Wait()\n\tif err != nil {\n\t\tt.Error(err)\n\t}\n}\n\nfunc TestMemTableReserved(t *testing.T) {\n\tm := newMemTable(memTableOptions{size: 5000})\n\t// Increase to 2 references.\n\tm.writerRef()\n\t// The initial reservation accounts for the already allocated bytes from the\n\t// arena.\n\trequire.Equal(t, m.reserved, m.skl.Arena().Size())\n\tb := newBatch(nil)\n\tb.Set([]byte(\"blueberry\"), []byte(\"pie\"), nil)\n\trequire.NotEqual(t, 0, int(b.memTableSize))\n\tprevReserved := m.reserved\n\tm.prepare(b)\n\trequire.Equal(t, int(m.reserved), int(b.memTableSize)+int(prevReserved))\n}\n\nfunc TestMemTable(t *testing.T) {\n\tvar m *memTable\n\tvar buf bytes.Buffer\n\tbatches := map[string]*Batch{}\n\n\tsummary := func() string {\n\t\treturn fmt.Sprintf(\"%d of %d bytes available\",\n\t\t\tm.availBytes(), m.totalBytes())\n\t}\n\n\tdatadriven.RunTest(t, \"testdata/mem_table\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tbuf.Reset()\n\t\tswitch td.Cmd {\n\t\tcase \"new\":\n\t\t\tvar o memTableOptions\n\t\t\ttd.MaybeScanArgs(t, \"size\", &o.size)\n\t\t\tm = newMemTable(o)\n\t\t\treturn \"\"\n\t\tcase \"prepare\":\n\t\t\tvar name string\n\t\t\ttd.ScanArgs(t, \"name\", &name)\n\t\t\tb := newBatch(nil)\n\t\t\tif err := runBatchDefineCmd(td, b); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tbatches[name] = b\n\t\t\tif err := m.prepare(b); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn summary()\n\t\tcase \"apply\":\n\t\t\tvar name string\n\t\t\tvar seqNum uint64\n\t\t\ttd.ScanArgs(t, \"name\", &name)\n\t\t\ttd.ScanArgs(t, \"seq\", &seqNum)\n\t\t\tif err := m.apply(batches[name], base.SeqNum(seqNum)); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tdelete(batches, name)\n\t\t\treturn summary()\n\t\tcase \"computePossibleOverlaps\":\n\t\t\tstopAfterFirst := td.HasArg(\"stop-after-first\")\n\n\t\t\tvar keyRanges []bounded\n\t\t\tfor _, l := range strings.Split(td.Input, \"\\n\") {\n\t\t\t\ts := strings.FieldsFunc(l, func(r rune) bool { return unicode.IsSpace(r) || r == '-' })\n\t\t\t\tkeyRanges = append(keyRanges, KeyRange{Start: []byte(s[0]), End: []byte(s[1])})\n\t\t\t}\n\n\t\t\tm.computePossibleOverlaps(func(b bounded) shouldContinue {\n\t\t\t\tfmt.Fprintf(&buf, \"%s\\n\", b)\n\t\t\t\tif stopAfterFirst {\n\t\t\t\t\treturn stopIteration\n\t\t\t\t}\n\t\t\t\treturn continueIteration\n\t\t\t}, keyRanges...)\n\n\t\t\treturn buf.String()\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unrecognized command %q\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc buildMemTable(b *testing.B) (*memTable, [][]byte) {\n\tm := newMemTable(memTableOptions{})\n\tvar keys [][]byte\n\tvar ikey InternalKey\n\tfor i := 0; ; i++ {\n\t\tkey := []byte(fmt.Sprintf(\"%08d\", i))\n\t\tkeys = append(keys, key)\n\t\tikey = base.MakeInternalKey(key, 0, InternalKeyKindSet)\n\t\tif m.set(ikey, nil) == arenaskl.ErrArenaFull {\n\t\t\tbreak\n\t\t}\n\t}\n\treturn m, keys\n}\n\nfunc BenchmarkMemTableIterSeekGE(b *testing.B) {\n\tm, keys := buildMemTable(b)\n\titer := m.newIter(nil)\n\trng := rand.New(rand.NewPCG(0, uint64(time.Now().UnixNano())))\n\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\titer.SeekGE(keys[rng.IntN(len(keys))], base.SeekGEFlagsNone)\n\t}\n}\n\nfunc BenchmarkMemTableIterSeqSeekGEWithBounds(b *testing.B) {\n\tm, keys := buildMemTable(b)\n\trng := rand.New(rand.NewPCG(0, uint64(17136275210000)))\n\t// Set bounds to restrict iteration to the middle 50% of keys.\n\titer := m.newIter(&IterOptions{\n\t\tLowerBound: keys[len(keys)/4],\n\t\tUpperBound: keys[3*len(keys)/4],\n\t})\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\titer.SeekGE(keys[rng.IntN(len(keys))], base.SeekGEFlagsNone)\n\t}\n}\n\n// BenchmarkMemTableIterSeekGESuccessiveWithBounds benchmarks a particular case\n// where an upper bound excludes the majority of the memtable keys and the user\n// seeks the iterator with successively increasing keys. This pattern is\n// expected to be common in CockroachDB: eg, intent resolution with an upper\n// bound at the end of the lock table span, or a MVCC iterator with an upper\n// bound restricting constraining iteration to a single CockroachDB Range.\nfunc BenchmarkMemTableIterSeekGESuccessiveWithBounds(b *testing.B) {\n\tm, keys := buildMemTable(b)\n\titer := m.newIter(&IterOptions{\n\t\tUpperBound: keys[1],\n\t})\n\tflags := base.SeekGEFlagsNone.EnableTrySeekUsingNext()\n\n\tseekKeys := make([][]byte, 256)\n\tfor i := 1; i < len(seekKeys); i++ {\n\t\tseekKeys[i] = append(append([]byte(nil), keys[0]...), byte(i-1))\n\t}\n\n\tb.ResetTimer()\n\titer.SeekGE(keys[0], base.SeekGEFlagsNone)\n\tfor i := 0; i < b.N-1; i++ {\n\t\titer.SeekGE(seekKeys[i%len(seekKeys)], flags)\n\t}\n}\n\nfunc BenchmarkMemTableIterNext(b *testing.B) {\n\tm, _ := buildMemTable(b)\n\titer := m.newIter(nil)\n\t_ = iter.First()\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\tkv := iter.Next()\n\t\tif kv == nil {\n\t\t\tkv = iter.First()\n\t\t}\n\t\t_ = kv\n\t}\n}\n\nfunc BenchmarkMemTableIterNextWithBounds(b *testing.B) {\n\tm, keys := buildMemTable(b)\n\t// Set bounds to restrict iteration to the middle 50% of keys.\n\topts := &IterOptions{\n\t\tLowerBound: keys[len(keys)/4],\n\t\tUpperBound: keys[3*len(keys)/4],\n\t}\n\titer := m.newIter(opts)\n\t_ = iter.SeekGE(opts.LowerBound, base.SeekGEFlagsNone)\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\tkv := iter.Next()\n\t\tif kv == nil {\n\t\t\tkv = iter.SeekGE(opts.LowerBound, base.SeekGEFlagsNone)\n\t\t}\n\t\t_ = kv\n\t}\n}\n\nfunc BenchmarkMemTableIterPrev(b *testing.B) {\n\tm, _ := buildMemTable(b)\n\titer := m.newIter(nil)\n\t_ = iter.Last()\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\tkv := iter.Prev()\n\t\tif kv == nil {\n\t\t\tkv = iter.Last()\n\t\t}\n\t\t_ = kv\n\t}\n}\n\nfunc BenchmarkMemTableIterPrevWithBounds(b *testing.B) {\n\tm, keys := buildMemTable(b)\n\t// Set bounds to restrict iteration to the middle 50% of keys.\n\topts := &IterOptions{\n\t\tLowerBound: keys[len(keys)/4],\n\t\tUpperBound: keys[3*len(keys)/4],\n\t}\n\titer := m.newIter(opts)\n\t_ = iter.SeekLT(opts.UpperBound, base.SeekLTFlagsNone)\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\tkv := iter.Prev()\n\t\tif kv == nil {\n\t\t\tkv = iter.SeekLT(opts.UpperBound, base.SeekLTFlagsNone)\n\t\t}\n\t\t_ = kv\n\t}\n}\n"
        },
        {
          "name": "merger.go",
          "type": "blob",
          "size": 1.0302734375,
          "content": "// Copyright 2018 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"io\"\n\n\t\"github.com/cockroachdb/pebble/internal/base\"\n)\n\n// Merge exports the base.Merge type.\ntype Merge = base.Merge\n\n// Merger exports the base.Merger type.\ntype Merger = base.Merger\n\n// ValueMerger exports the base.ValueMerger type.\ntype ValueMerger = base.ValueMerger\n\n// DeletableValueMerger exports the base.DeletableValueMerger type.\ntype DeletableValueMerger = base.DeletableValueMerger\n\n// DefaultMerger exports the base.DefaultMerger variable.\nvar DefaultMerger = base.DefaultMerger\n\nfunc finishValueMerger(\n\tvalueMerger ValueMerger, includesBase bool,\n) (value []byte, needDelete bool, closer io.Closer, err error) {\n\tif valueMerger2, ok := valueMerger.(DeletableValueMerger); ok {\n\t\tvalue, needDelete, closer, err = valueMerger2.DeletableFinish(includesBase)\n\t} else {\n\t\tvalue, closer, err = valueMerger.Finish(includesBase)\n\t}\n\treturn\n}\n"
        },
        {
          "name": "merging_iter.go",
          "type": "blob",
          "size": 50.9619140625,
          "content": "// Copyright 2018 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"runtime/debug\"\n\t\"unsafe\"\n\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/invariants\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan\"\n\t\"github.com/cockroachdb/pebble/internal/treeprinter\"\n)\n\ntype mergingIterLevel struct {\n\tindex int\n\titer  internalIterator\n\t// rangeDelIter is set to the range-deletion iterator for the level. When\n\t// configured with a levelIter, this pointer changes as sstable boundaries\n\t// are crossed. See levelIter.initRangeDel and the Range Deletions comment\n\t// below.\n\trangeDelIter keyspan.FragmentIterator\n\t// rangeDelIterGeneration is incremented whenever rangeDelIter changes.\n\trangeDelIterGeneration int\n\t// iterKV caches the current key-value pair iter points to.\n\titerKV *base.InternalKV\n\t// levelIter is non-nil if this level's iter is ultimately backed by a\n\t// *levelIter. The handle in iter may have wrapped the levelIter with\n\t// intermediary internalIterator implementations.\n\tlevelIter *levelIter\n\n\t// tombstone caches the tombstone rangeDelIter is currently pointed at. If\n\t// tombstone is nil, there are no further tombstones within the\n\t// current sstable in the current iterator direction. The cached tombstone is\n\t// only valid for the levels in the range [0,heap[0].index]. This avoids\n\t// positioning tombstones at lower levels which cannot possibly shadow the\n\t// current key.\n\ttombstone *keyspan.Span\n}\n\n// Assert that *mergingIterLevel implements rangeDelIterSetter.\nvar _ rangeDelIterSetter = (*mergingIterLevel)(nil)\n\nfunc (ml *mergingIterLevel) setRangeDelIter(iter keyspan.FragmentIterator) {\n\tml.tombstone = nil\n\tif ml.rangeDelIter != nil {\n\t\tml.rangeDelIter.Close()\n\t}\n\tml.rangeDelIter = iter\n\tml.rangeDelIterGeneration++\n}\n\n// mergingIter provides a merged view of multiple iterators from different\n// levels of the LSM.\n//\n// The core of a mergingIter is a heap of internalIterators (see\n// mergingIterHeap). The heap can operate as either a min-heap, used during\n// forward iteration (First, SeekGE, Next) or a max-heap, used during reverse\n// iteration (Last, SeekLT, Prev). The heap is initialized in calls to First,\n// Last, SeekGE, and SeekLT. A call to Next or Prev takes the current top\n// element on the heap, advances its iterator, and then \"fixes\" the heap\n// property. When one of the child iterators is exhausted during Next/Prev\n// iteration, it is removed from the heap.\n//\n// # Range Deletions\n//\n// A mergingIter can optionally be configured with a slice of range deletion\n// iterators. The range deletion iterator slice must exactly parallel the point\n// iterators and the range deletion iterator must correspond to the same level\n// in the LSM as the point iterator. Note that each memtable and each table in\n// L0 is a different \"level\" from the mergingIter perspective. So level 0 below\n// does not correspond to L0 in the LSM.\n//\n// A range deletion iterator iterates over fragmented range tombstones. Range\n// tombstones are fragmented by splitting them at any overlapping points. This\n// fragmentation guarantees that within an sstable tombstones will either be\n// distinct or will have identical start and end user keys. While range\n// tombstones are fragmented within an sstable, the start and end keys are not truncated\n// to sstable boundaries. This is necessary because the tombstone end key is\n// exclusive and does not have a sequence number. Consider an sstable\n// containing the range tombstone [a,c)#9 and the key \"b#8\". The tombstone must\n// delete \"b#8\", yet older versions of \"b\" might spill over to the next\n// sstable. So the boundary key for this sstable must be \"b#8\". Adjusting the\n// end key of tombstones to be optionally inclusive or contain a sequence\n// number would be possible solutions (such solutions have potentially serious\n// issues: tombstones have exclusive end keys since an inclusive deletion end can\n// be converted to an exclusive one while the reverse transformation is not possible;\n// the semantics of a sequence number for the end key of a range tombstone are murky).\n//\n// The approach taken here performs an\n// implicit truncation of the tombstone to the sstable boundaries.\n//\n// During initialization of a mergingIter, the range deletion iterators for\n// batches, memtables, and L0 tables are populated up front. Note that Batches\n// and memtables index unfragmented tombstones.  Batch.newRangeDelIter() and\n// memTable.newRangeDelIter() fragment and cache the tombstones on demand. The\n// L1-L6 range deletion iterators are populated by levelIter. When configured\n// to load range deletion iterators, whenever a levelIter loads a table it\n// loads both the point iterator and the range deletion\n// iterator. levelIter.rangeDelIter is configured to point to the right entry\n// in mergingIter.levels. The effect of this setup is that\n// mergingIter.levels[i].rangeDelIter always contains the fragmented range\n// tombstone for the current table in level i that the levelIter has open.\n//\n// Another crucial mechanism of levelIter is that it materializes fake point\n// entries for the table boundaries if the boundary is range deletion\n// key. Consider a table that contains only a range tombstone [a-e)#10. The\n// sstable boundaries for this table will be a#10,15 and\n// e#72057594037927935,15. During forward iteration levelIter will return\n// e#72057594037927935,15 as a key. During reverse iteration levelIter will\n// return a#10,15 as a key. These sentinel keys act as bookends to point\n// iteration and allow mergingIter to keep a table and its associated range\n// tombstones loaded as long as there are keys at lower levels that are within\n// the bounds of the table.\n//\n// The final piece to the range deletion puzzle is the LSM invariant that for a\n// given key K newer versions of K can only exist earlier in the level, or at\n// higher levels of the tree. For example, if K#4 exists in L3, k#5 can only\n// exist earlier in the L3 or in L0, L1, L2 or a memtable. Get very explicitly\n// uses this invariant to find the value for a key by walking the LSM level by\n// level. For range deletions, this invariant means that a range deletion at\n// level N will necessarily shadow any keys within its bounds in level Y where\n// Y > N. One wrinkle to this statement is that it only applies to keys that\n// lie within the sstable bounds as well, but we get that guarantee due to the\n// way the range deletion iterator and point iterator are bound together by a\n// levelIter.\n//\n// Tying the above all together, we get a picture where each level (index in\n// mergingIter.levels) is composed of both point operations (pX) and range\n// deletions (rX). The range deletions for level X shadow both the point\n// operations and range deletions for level Y where Y > X allowing mergingIter\n// to skip processing entries in that shadow. For example, consider the\n// scenario:\n//\n//\tr0: a---e\n//\tr1:    d---h\n//\tr2:       g---k\n//\tr3:          j---n\n//\tr4:             m---q\n//\n// This is showing 5 levels of range deletions. Consider what happens upon\n// SeekGE(\"b\"). We first seek the point iterator for level 0 (the point values\n// are not shown above) and we then seek the range deletion iterator. That\n// returns the tombstone [a,e). This tombstone tells us that all keys in the\n// range [a,e) in lower levels are deleted so we can skip them. So we can\n// adjust the seek key to \"e\", the tombstone end key. For level 1 we seek to\n// \"e\" and find the range tombstone [d,h) and similar logic holds. By the time\n// we get to level 4 we're seeking to \"n\".\n//\n// One consequence of not truncating tombstone end keys to sstable boundaries\n// is the seeking process described above cannot always seek to the tombstone\n// end key in the older level. For example, imagine in the above example r3 is\n// a partitioned level (i.e., L1+ in our LSM), and the sstable containing [j,\n// n) has \"k\" as its upper boundary. In this situation, compactions involving\n// keys at or after \"k\" can output those keys to r4+, even if they're newer\n// than our tombstone [j, n). So instead of seeking to \"n\" in r4 we can only\n// seek to \"k\".  To achieve this, the instance variable `largestUserKey.`\n// maintains the upper bounds of the current sstables in the partitioned\n// levels. In this example, `levels[3].largestUserKey` holds \"k\", telling us to\n// limit the seek triggered by a tombstone in r3 to \"k\".\n//\n// During actual iteration levels can contain both point operations and range\n// deletions. Within a level, when a range deletion contains a point operation\n// the sequence numbers must be checked to determine if the point operation is\n// newer or older than the range deletion tombstone. The mergingIter maintains\n// the invariant that the range deletion iterators for all levels newer that\n// the current iteration key (L < m.heap.items[0].index) are positioned at the\n// next (or previous during reverse iteration) range deletion tombstone. We\n// know those levels don't contain a range deletion tombstone that covers the\n// current key because if they did the current key would be deleted. The range\n// deletion iterator for the current key's level is positioned at a range\n// tombstone covering or past the current key. The position of all of other\n// range deletion iterators is unspecified. Whenever a key from those levels\n// becomes the current key, their range deletion iterators need to be\n// positioned. This lazy positioning avoids seeking the range deletion\n// iterators for keys that are never considered. (A similar bit of lazy\n// evaluation can be done for the point iterators, but is still TBD).\n//\n// For a full example, consider the following setup:\n//\n//\tp0:               o\n//\tr0:             m---q\n//\n//\tp1:              n p\n//\tr1:       g---k\n//\n//\tp2:  b d    i\n//\tr2: a---e           q----v\n//\n//\tp3:     e\n//\tr3:\n//\n// If we start iterating from the beginning, the first key we encounter is \"b\"\n// in p2. When the mergingIter is pointing at a valid entry, the range deletion\n// iterators for all of the levels < m.heap.items[0].index are positioned at\n// the next range tombstone past the current key. So r0 will point at [m,q) and\n// r1 at [g,k). When the key \"b\" is encountered, we check to see if the current\n// tombstone for r0 or r1 contains it, and whether the tombstone for r2, [a,e),\n// contains and is newer than \"b\".\n//\n// Advancing the iterator finds the next key at \"d\". This is in the same level\n// as the previous key \"b\" so we don't have to reposition any of the range\n// deletion iterators, but merely check whether \"d\" is now contained by any of\n// the range tombstones at higher levels or has stepped past the range\n// tombstone in its own level or higher levels. In this case, there is nothing to be done.\n//\n// Advancing the iterator again finds \"e\". Since \"e\" comes from p3, we have to\n// position the r3 range deletion iterator, which is empty. \"e\" is past the r2\n// tombstone of [a,e) so we need to advance the r2 range deletion iterator to\n// [q,v).\n//\n// The next key is \"i\". Because this key is in p2, a level above \"e\", we don't\n// have to reposition any range deletion iterators and instead see that \"i\" is\n// covered by the range tombstone [g,k). The iterator is immediately advanced\n// to \"n\" which is covered by the range tombstone [m,q) causing the iterator to\n// advance to \"o\" which is visible.\n//\n// # Error handling\n//\n// Any iterator operation may fail. The InternalIterator contract dictates that\n// an iterator must return a nil internal key when an error occurs, and a\n// subsequent call to Error() should return the error value. The exported\n// merging iterator positioning methods must adhere to this contract by setting\n// m.err to hold any error encountered by the individual level iterators and\n// returning a nil internal key. Some internal helpers (eg,\n// find[Next|Prev]Entry) also adhere to this contract, setting m.err directly).\n// Other internal functions return an explicit error return value and DO NOT set\n// m.err, relying on the caller to set m.err appropriately.\n//\n// TODO(jackson): Update the InternalIterator interface to return explicit error\n// return values (and an *InternalKV pointer).\n//\n// TODO(peter,rangedel): For testing, advance the iterator through various\n// scenarios and have each step display the current state (i.e. the current\n// heap and range-del iterator positioning).\ntype mergingIter struct {\n\tlogger        Logger\n\tsplit         Split\n\tdir           int\n\tsnapshot      base.SeqNum\n\tbatchSnapshot base.SeqNum\n\tlevels        []mergingIterLevel\n\theap          mergingIterHeap\n\terr           error\n\tprefix        []byte\n\tlower         []byte\n\tupper         []byte\n\tstats         *InternalIteratorStats\n\tseekKeyBuf    []byte\n\n\t// levelsPositioned, if non-nil, is a slice of the same length as levels.\n\t// It's used by NextPrefix to record which levels have already been\n\t// repositioned. It's created lazily by the first call to NextPrefix.\n\tlevelsPositioned []bool\n\n\tcombinedIterState *combinedIterState\n\n\t// Used in some tests to disable the random disabling of seek optimizations.\n\tforceEnableSeekOpt bool\n}\n\n// mergingIter implements the base.InternalIterator interface.\nvar _ base.InternalIterator = (*mergingIter)(nil)\n\n// newMergingIter returns an iterator that merges its input. Walking the\n// resultant iterator will return all key/value pairs of all input iterators\n// in strictly increasing key order, as defined by cmp. It is permissible to\n// pass a nil split parameter if the caller is never going to call\n// SeekPrefixGE.\n//\n// The input's key ranges may overlap, but there are assumed to be no duplicate\n// keys: if iters[i] contains a key k then iters[j] will not contain that key k.\n//\n// None of the iters may be nil.\nfunc newMergingIter(\n\tlogger Logger,\n\tstats *base.InternalIteratorStats,\n\tcmp Compare,\n\tsplit Split,\n\titers ...internalIterator,\n) *mergingIter {\n\tm := &mergingIter{}\n\tlevels := make([]mergingIterLevel, len(iters))\n\tfor i := range levels {\n\t\tlevels[i].iter = iters[i]\n\t}\n\tm.init(&IterOptions{logger: logger}, stats, cmp, split, levels...)\n\treturn m\n}\n\nfunc (m *mergingIter) init(\n\topts *IterOptions,\n\tstats *base.InternalIteratorStats,\n\tcmp Compare,\n\tsplit Split,\n\tlevels ...mergingIterLevel,\n) {\n\tm.err = nil // clear cached iteration error\n\tm.logger = opts.getLogger()\n\tif opts != nil {\n\t\tm.lower = opts.LowerBound\n\t\tm.upper = opts.UpperBound\n\t}\n\tm.snapshot = base.SeqNumMax\n\tm.batchSnapshot = base.SeqNumMax\n\tm.levels = levels\n\tm.heap.cmp = cmp\n\tm.split = split\n\tm.stats = stats\n\tif cap(m.heap.items) < len(levels) {\n\t\tm.heap.items = make([]*mergingIterLevel, 0, len(levels))\n\t} else {\n\t\tm.heap.items = m.heap.items[:0]\n\t}\n\tfor l := range m.levels {\n\t\tm.levels[l].index = l\n\t}\n}\n\nfunc (m *mergingIter) initHeap() {\n\tm.heap.items = m.heap.items[:0]\n\tfor i := range m.levels {\n\t\tif l := &m.levels[i]; l.iterKV != nil {\n\t\t\tm.heap.items = append(m.heap.items, l)\n\t\t}\n\t}\n\tm.heap.init()\n}\n\nfunc (m *mergingIter) initMinHeap() error {\n\tm.dir = 1\n\tm.heap.reverse = false\n\tm.initHeap()\n\treturn m.initMinRangeDelIters(-1)\n}\n\n// The level of the previous top element was oldTopLevel. Note that all range delete\n// iterators < oldTopLevel are positioned past the key of the previous top element and\n// the range delete iterator == oldTopLevel is positioned at or past the key of the\n// previous top element. We need to position the range delete iterators from oldTopLevel + 1\n// to the level of the current top element.\nfunc (m *mergingIter) initMinRangeDelIters(oldTopLevel int) error {\n\tif m.heap.len() == 0 {\n\t\treturn nil\n\t}\n\n\t// Position the range-del iterators at levels <= m.heap.items[0].index.\n\titem := m.heap.items[0]\n\tfor level := oldTopLevel + 1; level <= item.index; level++ {\n\t\tl := &m.levels[level]\n\t\tif l.rangeDelIter == nil {\n\t\t\tcontinue\n\t\t}\n\t\tvar err error\n\t\tl.tombstone, err = l.rangeDelIter.SeekGE(item.iterKV.K.UserKey)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (m *mergingIter) initMaxHeap() error {\n\tm.dir = -1\n\tm.heap.reverse = true\n\tm.initHeap()\n\treturn m.initMaxRangeDelIters(-1)\n}\n\n// The level of the previous top element was oldTopLevel. Note that all range delete\n// iterators < oldTopLevel are positioned before the key of the previous top element and\n// the range delete iterator == oldTopLevel is positioned at or before the key of the\n// previous top element. We need to position the range delete iterators from oldTopLevel + 1\n// to the level of the current top element.\nfunc (m *mergingIter) initMaxRangeDelIters(oldTopLevel int) error {\n\tif m.heap.len() == 0 {\n\t\treturn nil\n\t}\n\t// Position the range-del iterators at levels <= m.heap.items[0].index.\n\titem := m.heap.items[0]\n\tfor level := oldTopLevel + 1; level <= item.index; level++ {\n\t\tl := &m.levels[level]\n\t\tif l.rangeDelIter == nil {\n\t\t\tcontinue\n\t\t}\n\t\ttomb, err := keyspan.SeekLE(m.heap.cmp, l.rangeDelIter, item.iterKV.K.UserKey)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tl.tombstone = tomb\n\t}\n\treturn nil\n}\n\nfunc (m *mergingIter) switchToMinHeap() error {\n\tif m.heap.len() == 0 {\n\t\tif m.lower != nil {\n\t\t\tm.SeekGE(m.lower, base.SeekGEFlagsNone)\n\t\t} else {\n\t\t\tm.First()\n\t\t}\n\t\treturn m.err\n\t}\n\n\t// We're switching from using a max heap to a min heap. We need to advance\n\t// any iterator that is less than or equal to the current key. Consider the\n\t// scenario where we have 2 iterators being merged (user-key:seq-num):\n\t//\n\t// i1:     *a:2     b:2\n\t// i2: a:1      b:1\n\t//\n\t// The current key is a:2 and i2 is pointed at a:1. When we switch to forward\n\t// iteration, we want to return a key that is greater than a:2.\n\n\tkey := m.heap.items[0].iterKV.K\n\tcur := m.heap.items[0]\n\n\tfor i := range m.levels {\n\t\tl := &m.levels[i]\n\t\tif l == cur {\n\t\t\tcontinue\n\t\t}\n\t\tfor l.iterKV = l.iter.Next(); l.iterKV != nil; l.iterKV = l.iter.Next() {\n\t\t\tif base.InternalCompare(m.heap.cmp, key, l.iterKV.K) < 0 {\n\t\t\t\t// key < iter-key\n\t\t\t\tbreak\n\t\t\t}\n\t\t\t// key >= iter-key\n\t\t}\n\t\tif l.iterKV == nil {\n\t\t\tif err := l.iter.Error(); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\t// Special handling for the current iterator because we were using its key\n\t// above.\n\tcur.iterKV = cur.iter.Next()\n\tif cur.iterKV == nil {\n\t\tif err := cur.iter.Error(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn m.initMinHeap()\n}\n\nfunc (m *mergingIter) switchToMaxHeap() error {\n\tif m.heap.len() == 0 {\n\t\tif m.upper != nil {\n\t\t\tm.SeekLT(m.upper, base.SeekLTFlagsNone)\n\t\t} else {\n\t\t\tm.Last()\n\t\t}\n\t\treturn m.err\n\t}\n\n\t// We're switching from using a min heap to a max heap. We need to backup any\n\t// iterator that is greater than or equal to the current key. Consider the\n\t// scenario where we have 2 iterators being merged (user-key:seq-num):\n\t//\n\t// i1: a:2     *b:2\n\t// i2:     a:1      b:1\n\t//\n\t// The current key is b:2 and i2 is pointing at b:1. When we switch to\n\t// reverse iteration, we want to return a key that is less than b:2.\n\tkey := m.heap.items[0].iterKV.K\n\tcur := m.heap.items[0]\n\n\tfor i := range m.levels {\n\t\tl := &m.levels[i]\n\t\tif l == cur {\n\t\t\tcontinue\n\t\t}\n\n\t\tfor l.iterKV = l.iter.Prev(); l.iterKV != nil; l.iterKV = l.iter.Prev() {\n\t\t\tif base.InternalCompare(m.heap.cmp, key, l.iterKV.K) > 0 {\n\t\t\t\t// key > iter-key\n\t\t\t\tbreak\n\t\t\t}\n\t\t\t// key <= iter-key\n\t\t}\n\t\tif l.iterKV == nil {\n\t\t\tif err := l.iter.Error(); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\t// Special handling for the current iterator because we were using its key\n\t// above.\n\tcur.iterKV = cur.iter.Prev()\n\tif cur.iterKV == nil {\n\t\tif err := cur.iter.Error(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn m.initMaxHeap()\n}\n\n// nextEntry unconditionally steps to the next entry. item is the current top\n// item in the heap.\nfunc (m *mergingIter) nextEntry(l *mergingIterLevel, succKey []byte) error {\n\t// INVARIANT: If in prefix iteration mode, item.iterKey must have a prefix equal\n\t// to m.prefix. This invariant is important for ensuring TrySeekUsingNext\n\t// optimizations behave correctly.\n\t//\n\t// During prefix iteration, the iterator does not have a full view of the\n\t// LSM. Some level iterators may omit keys that are known to fall outside\n\t// the seek prefix (eg, due to sstable bloom filter exclusion). It's\n\t// important that in such cases we don't position any iterators beyond\n\t// m.prefix, because doing so may interfere with future seeks.\n\t//\n\t// Let prefixes P1 < P2 < P3. Imagine a SeekPrefixGE to prefix P1, followed\n\t// by a SeekPrefixGE to prefix P2. Imagine there exist live keys at prefix\n\t// P2, but they're not visible to the SeekPrefixGE(P1) (because of\n\t// bloom-filter exclusion or a range tombstone that deletes prefix P1 but\n\t// not P2). If the SeekPrefixGE(P1) is allowed to move any level iterators\n\t// to P3, the SeekPrefixGE(P2, TrySeekUsingNext=true) may mistakenly think\n\t// the level contains no point keys or range tombstones within the prefix\n\t// P2. Care is taken to avoid ever advancing the iterator beyond the current\n\t// prefix. If nextEntry is ever invoked while we're already beyond the\n\t// current prefix, we're violating the invariant.\n\tif invariants.Enabled && m.prefix != nil {\n\t\tif p := m.split.Prefix(l.iterKV.K.UserKey); !bytes.Equal(m.prefix, p) {\n\t\t\tm.logger.Fatalf(\"mergingIter: prefix violation: nexting beyond prefix %q; existing heap root %q\\n%s\",\n\t\t\t\tm.prefix, l.iterKV, debug.Stack())\n\t\t}\n\t}\n\n\toldTopLevel := l.index\n\toldRangeDelIterGeneration := l.rangeDelIterGeneration\n\n\tif succKey == nil {\n\t\tl.iterKV = l.iter.Next()\n\t} else {\n\t\tl.iterKV = l.iter.NextPrefix(succKey)\n\t}\n\n\tif l.iterKV == nil {\n\t\tif err := l.iter.Error(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tm.heap.pop()\n\t} else {\n\t\tif m.prefix != nil && !bytes.Equal(m.prefix, m.split.Prefix(l.iterKV.K.UserKey)) {\n\t\t\t// Set keys without a matching prefix to their zero values when in prefix\n\t\t\t// iteration mode and remove iterated level from heap.\n\t\t\tl.iterKV = nil\n\t\t\tm.heap.pop()\n\t\t} else if m.heap.len() > 1 {\n\t\t\tm.heap.fix(0)\n\t\t}\n\t\tif l.rangeDelIterGeneration != oldRangeDelIterGeneration {\n\t\t\t// The rangeDelIter changed which indicates that the l.iter moved to the\n\t\t\t// next sstable. We have to update the tombstone for oldTopLevel as well.\n\t\t\toldTopLevel--\n\t\t}\n\t}\n\n\t// The cached tombstones are only valid for the levels\n\t// [0,oldTopLevel]. Updated the cached tombstones for any levels in the range\n\t// [oldTopLevel+1,heap[0].index].\n\treturn m.initMinRangeDelIters(oldTopLevel)\n}\n\n// isNextEntryDeleted starts from the current entry (as the next entry) and if\n// it is deleted, moves the iterators forward as needed and returns true, else\n// it returns false. item is the top item in the heap. If any of the required\n// iterator operations error, the error is returned without updating m.err.\n//\n// During prefix iteration mode, isNextEntryDeleted will exhaust the iterator by\n// clearing the heap if the deleted key(s) extend beyond the iteration prefix\n// during prefix-iteration mode.\nfunc (m *mergingIter) isNextEntryDeleted(item *mergingIterLevel) (bool, error) {\n\t// Look for a range deletion tombstone containing item.iterKV at higher\n\t// levels (level < item.index). If we find such a range tombstone we know\n\t// it deletes the key in the current level. Also look for a range\n\t// deletion at the current level (level == item.index). If we find such a\n\t// range deletion we need to check whether it is newer than the current\n\t// entry.\n\tfor level := 0; level <= item.index; level++ {\n\t\tl := &m.levels[level]\n\t\tif l.rangeDelIter == nil || l.tombstone == nil {\n\t\t\t// If l.tombstone is nil, there are no further tombstones\n\t\t\t// in the current sstable in the current (forward) iteration\n\t\t\t// direction.\n\t\t\tcontinue\n\t\t}\n\t\tif m.heap.cmp(l.tombstone.End, item.iterKV.K.UserKey) <= 0 {\n\t\t\t// The current key is at or past the tombstone end key.\n\t\t\t//\n\t\t\t// NB: for the case that this l.rangeDelIter is provided by a levelIter we know that\n\t\t\t// the levelIter must be positioned at a key >= item.iterKV. So it is sufficient to seek the\n\t\t\t// current l.rangeDelIter (since any range del iterators that will be provided by the\n\t\t\t// levelIter in the future cannot contain item.iterKV). Also, it is possible that we\n\t\t\t// will encounter parts of the range delete that should be ignored -- we handle that\n\t\t\t// below.\n\t\t\tvar err error\n\t\t\tl.tombstone, err = l.rangeDelIter.SeekGE(item.iterKV.K.UserKey)\n\t\t\tif err != nil {\n\t\t\t\treturn false, err\n\t\t\t}\n\t\t}\n\t\tif l.tombstone == nil {\n\t\t\tcontinue\n\t\t}\n\n\t\tif l.tombstone.VisibleAt(m.snapshot) && m.heap.cmp(l.tombstone.Start, item.iterKV.K.UserKey) <= 0 {\n\t\t\tif level < item.index {\n\t\t\t\t// We could also do m.seekGE(..., level + 1). The levels from\n\t\t\t\t// [level + 1, item.index) are already after item.iterKV so seeking them may be\n\t\t\t\t// wasteful.\n\n\t\t\t\t// We can seek up to tombstone.End.\n\t\t\t\t//\n\t\t\t\t// Progress argument: Since this file is at a higher level than item.iterKV we know\n\t\t\t\t// that the iterator in this file must be positioned within its bounds and at a key\n\t\t\t\t// X > item.iterKV (otherwise it would be the min of the heap). It is not\n\t\t\t\t// possible for X.UserKey == item.iterKV.UserKey, since it is incompatible with\n\t\t\t\t// X > item.iterKV (a lower version cannot be in a higher sstable), so it must be that\n\t\t\t\t// X.UserKey > item.iterKV.UserKey. Which means l.largestUserKey > item.key.UserKey.\n\t\t\t\t// We also know that l.tombstone.End > item.iterKV.UserKey. So the min of these,\n\t\t\t\t// seekKey, computed below, is > item.iterKV.UserKey, so the call to seekGE() will\n\t\t\t\t// make forward progress.\n\t\t\t\tm.seekKeyBuf = append(m.seekKeyBuf[:0], l.tombstone.End...)\n\t\t\t\tseekKey := m.seekKeyBuf\n\t\t\t\t// This seek is not directly due to a SeekGE call, so we don't know\n\t\t\t\t// enough about the underlying iterator positions, and so we keep the\n\t\t\t\t// try-seek-using-next optimization disabled. Additionally, if we're in\n\t\t\t\t// prefix-seek mode and a re-seek would have moved us past the original\n\t\t\t\t// prefix, we can remove all merging iter levels below the rangedel\n\t\t\t\t// tombstone's level and return immediately instead of re-seeking. This\n\t\t\t\t// is correct since those levels cannot provide a key that matches the\n\t\t\t\t// prefix, and is also visible. Additionally, this is important to make\n\t\t\t\t// subsequent `TrySeekUsingNext` work correctly, as a re-seek on a\n\t\t\t\t// different prefix could have resulted in this iterator skipping visible\n\t\t\t\t// keys at prefixes in between m.prefix and seekKey, that are currently\n\t\t\t\t// not in the heap due to a bloom filter mismatch.\n\t\t\t\t//\n\t\t\t\t// Additionally, we set the relative-seek flag. This is\n\t\t\t\t// important when iterating with lazy combined iteration. If\n\t\t\t\t// there's a range key between this level's current file and the\n\t\t\t\t// file the seek will land on, we need to detect it in order to\n\t\t\t\t// trigger construction of the combined iterator.\n\t\t\t\tif m.prefix != nil {\n\t\t\t\t\tif !bytes.Equal(m.prefix, m.split.Prefix(seekKey)) {\n\t\t\t\t\t\tfor i := item.index; i < len(m.levels); i++ {\n\t\t\t\t\t\t\t// Remove this level from the heap. Setting iterKV\n\t\t\t\t\t\t\t// to nil should be sufficient for initMinHeap to\n\t\t\t\t\t\t\t// not re-initialize the heap with them in it. Other\n\t\t\t\t\t\t\t// fields in mergingIterLevel can remain as-is; the\n\t\t\t\t\t\t\t// iter/rangeDelIter needs to stay intact for future\n\t\t\t\t\t\t\t// trySeekUsingNexts to work, the level iter\n\t\t\t\t\t\t\t// boundary context is owned by the levelIter which\n\t\t\t\t\t\t\t// is not being repositioned, and any tombstones in\n\t\t\t\t\t\t\t// these levels will be irrelevant for us anyway.\n\t\t\t\t\t\t\tm.levels[i].iterKV = nil\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// TODO(bilal): Consider a more efficient way of removing levels from\n\t\t\t\t\t\t// the heap without reinitializing all of it. This would likely\n\t\t\t\t\t\t// necessitate tracking the heap positions of each mergingIterHeap\n\t\t\t\t\t\t// item in the mergingIterLevel, and then swapping that item in the\n\t\t\t\t\t\t// heap with the last-positioned heap item, and shrinking the heap by\n\t\t\t\t\t\t// one.\n\t\t\t\t\t\tif err := m.initMinHeap(); err != nil {\n\t\t\t\t\t\t\treturn false, err\n\t\t\t\t\t\t}\n\t\t\t\t\t\treturn true, nil\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif err := m.seekGE(seekKey, item.index, base.SeekGEFlagsNone.EnableRelativeSeek()); err != nil {\n\t\t\t\t\treturn false, err\n\t\t\t\t}\n\t\t\t\treturn true, nil\n\t\t\t}\n\t\t\tif l.tombstone.CoversAt(m.snapshot, item.iterKV.SeqNum()) {\n\t\t\t\tif err := m.nextEntry(item, nil /* succKey */); err != nil {\n\t\t\t\t\treturn false, err\n\t\t\t\t}\n\t\t\t\treturn true, nil\n\t\t\t}\n\t\t}\n\t}\n\treturn false, nil\n}\n\n// Starting from the current entry, finds the first (next) entry that can be returned.\n//\n// If an error occurs, m.err is updated to hold the error and findNextentry\n// returns a nil internal key.\nfunc (m *mergingIter) findNextEntry() *base.InternalKV {\n\tfor m.heap.len() > 0 && m.err == nil {\n\t\titem := m.heap.items[0]\n\n\t\t// The levelIter internal iterator will interleave exclusive sentinel\n\t\t// keys to keep files open until their range deletions are no longer\n\t\t// necessary. Sometimes these are interleaved with the user key of a\n\t\t// file's largest key, in which case they may simply be stepped over to\n\t\t// move to the next file in the forward direction. Other times they're\n\t\t// interleaved at the user key of the user-iteration boundary, if that\n\t\t// falls within the bounds of a file. In the latter case, there are no\n\t\t// more keys < m.upper, and we can stop iterating.\n\t\t//\n\t\t// We perform a key comparison to differentiate between these two cases.\n\t\t// This key comparison is considered okay because it only happens for\n\t\t// sentinel keys. It may be eliminated after #2863.\n\t\tif m.levels[item.index].iterKV.K.IsExclusiveSentinel() {\n\t\t\tif m.upper != nil && m.heap.cmp(m.levels[item.index].iterKV.K.UserKey, m.upper) >= 0 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\t// This key is the largest boundary of a file and can be skipped now\n\t\t\t// that the file's range deletions are no longer relevant.\n\t\t\tm.err = m.nextEntry(item, nil /* succKey */)\n\t\t\tif m.err != nil {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\tm.addItemStats(item)\n\n\t\t// Check if the heap root key is deleted by a range tombstone in a\n\t\t// higher level. If it is, isNextEntryDeleted will advance the iterator\n\t\t// to a later key (through seeking or nexting).\n\t\tisDeleted, err := m.isNextEntryDeleted(item)\n\t\tif err != nil {\n\t\t\tm.err = err\n\t\t\treturn nil\n\t\t} else if isDeleted {\n\t\t\tm.stats.PointsCoveredByRangeTombstones++\n\t\t\tcontinue\n\t\t}\n\n\t\t// Check if the key is visible at the iterator sequence numbers.\n\t\tif !item.iterKV.Visible(m.snapshot, m.batchSnapshot) {\n\t\t\tm.err = m.nextEntry(item, nil /* succKey */)\n\t\t\tif m.err != nil {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\t// The heap root is visible and not deleted by any range tombstones.\n\t\t// Return it.\n\t\treturn item.iterKV\n\t}\n\treturn nil\n}\n\n// Steps to the prev entry. item is the current top item in the heap.\nfunc (m *mergingIter) prevEntry(l *mergingIterLevel) error {\n\toldTopLevel := l.index\n\toldRangeDelIterGeneration := l.rangeDelIterGeneration\n\tif l.iterKV = l.iter.Prev(); l.iterKV != nil {\n\t\tif m.heap.len() > 1 {\n\t\t\tm.heap.fix(0)\n\t\t}\n\t\tif l.rangeDelIterGeneration != oldRangeDelIterGeneration && l.rangeDelIter != nil {\n\t\t\t// The rangeDelIter changed which indicates that the l.iter moved to the\n\t\t\t// previous sstable. We have to update the tombstone for oldTopLevel as\n\t\t\t// well.\n\t\t\toldTopLevel--\n\t\t}\n\t} else {\n\t\tif err := l.iter.Error(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tm.heap.pop()\n\t}\n\n\t// The cached tombstones are only valid for the levels\n\t// [0,oldTopLevel]. Updated the cached tombstones for any levels in the range\n\t// [oldTopLevel+1,heap[0].index].\n\treturn m.initMaxRangeDelIters(oldTopLevel)\n}\n\n// isPrevEntryDeleted() starts from the current entry (as the prev entry) and if it is deleted,\n// moves the iterators backward as needed and returns true, else it returns false. item is the top\n// item in the heap.\nfunc (m *mergingIter) isPrevEntryDeleted(item *mergingIterLevel) (bool, error) {\n\t// Look for a range deletion tombstone containing item.iterKV at higher\n\t// levels (level < item.index). If we find such a range tombstone we know\n\t// it deletes the key in the current level. Also look for a range\n\t// deletion at the current level (level == item.index). If we find such a\n\t// range deletion we need to check whether it is newer than the current\n\t// entry.\n\tfor level := 0; level <= item.index; level++ {\n\t\tl := &m.levels[level]\n\t\tif l.rangeDelIter == nil || l.tombstone == nil {\n\t\t\t// If l.tombstone is nil, there are no further tombstones\n\t\t\t// in the current sstable in the current (reverse) iteration\n\t\t\t// direction.\n\t\t\tcontinue\n\t\t}\n\t\tif m.heap.cmp(item.iterKV.K.UserKey, l.tombstone.Start) < 0 {\n\t\t\t// The current key is before the tombstone start key.\n\t\t\t//\n\t\t\t// NB: for the case that this l.rangeDelIter is provided by a levelIter we know that\n\t\t\t// the levelIter must be positioned at a key < item.iterKV. So it is sufficient to seek the\n\t\t\t// current l.rangeDelIter (since any range del iterators that will be provided by the\n\t\t\t// levelIter in the future cannot contain item.iterKV). Also, it is it is possible that we\n\t\t\t// will encounter parts of the range delete that should be ignored -- we handle that\n\t\t\t// below.\n\n\t\t\ttomb, err := keyspan.SeekLE(m.heap.cmp, l.rangeDelIter, item.iterKV.K.UserKey)\n\t\t\tif err != nil {\n\t\t\t\treturn false, err\n\t\t\t}\n\t\t\tl.tombstone = tomb\n\t\t}\n\t\tif l.tombstone == nil {\n\t\t\tcontinue\n\t\t}\n\t\tif l.tombstone.VisibleAt(m.snapshot) && m.heap.cmp(l.tombstone.End, item.iterKV.K.UserKey) > 0 {\n\t\t\tif level < item.index {\n\t\t\t\t// We could also do m.seekLT(..., level + 1). The levels from\n\t\t\t\t// [level + 1, item.index) are already before item.iterKV so seeking them may be\n\t\t\t\t// wasteful.\n\n\t\t\t\t// We can seek up to tombstone.Start.UserKey.\n\t\t\t\t//\n\t\t\t\t// Progress argument: We know that the iterator in this file is positioned within\n\t\t\t\t// its bounds and at a key X < item.iterKV (otherwise it would be the max of the heap).\n\t\t\t\t// So smallestUserKey <= item.iterKV.UserKey and we already know that\n\t\t\t\t// l.tombstone.Start.UserKey <= item.iterKV.UserKey. So the seekKey computed below\n\t\t\t\t// is <= item.iterKV.UserKey, and since we do a seekLT() we will make backwards\n\t\t\t\t// progress.\n\t\t\t\tm.seekKeyBuf = append(m.seekKeyBuf[:0], l.tombstone.Start...)\n\t\t\t\tseekKey := m.seekKeyBuf\n\t\t\t\t// We set the relative-seek flag. This is important when\n\t\t\t\t// iterating with lazy combined iteration. If there's a range\n\t\t\t\t// key between this level's current file and the file the seek\n\t\t\t\t// will land on, we need to detect it in order to trigger\n\t\t\t\t// construction of the combined iterator.\n\t\t\t\tif err := m.seekLT(seekKey, item.index, base.SeekLTFlagsNone.EnableRelativeSeek()); err != nil {\n\t\t\t\t\treturn false, err\n\t\t\t\t}\n\t\t\t\treturn true, nil\n\t\t\t}\n\t\t\tif l.tombstone.CoversAt(m.snapshot, item.iterKV.SeqNum()) {\n\t\t\t\tif err := m.prevEntry(item); err != nil {\n\t\t\t\t\treturn false, err\n\t\t\t\t}\n\t\t\t\treturn true, nil\n\t\t\t}\n\t\t}\n\t}\n\treturn false, nil\n}\n\n// Starting from the current entry, finds the first (prev) entry that can be returned.\n//\n// If an error occurs, m.err is updated to hold the error and findNextentry\n// returns a nil internal key.\nfunc (m *mergingIter) findPrevEntry() *base.InternalKV {\n\tfor m.heap.len() > 0 && m.err == nil {\n\t\titem := m.heap.items[0]\n\n\t\t// The levelIter internal iterator will interleave exclusive sentinel\n\t\t// keys to keep files open until their range deletions are no longer\n\t\t// necessary. Sometimes these are interleaved with the user key of a\n\t\t// file's smallest key, in which case they may simply be stepped over to\n\t\t// move to the next file in the backward direction. Other times they're\n\t\t// interleaved at the user key of the user-iteration boundary, if that\n\t\t// falls within the bounds of a file. In the latter case, there are no\n\t\t// more keys ≥ m.lower, and we can stop iterating.\n\t\t//\n\t\t// We perform a key comparison to differentiate between these two cases.\n\t\t// This key comparison is considered okay because it only happens for\n\t\t// sentinel keys. It may be eliminated after #2863.\n\t\tif m.levels[item.index].iterKV.K.IsExclusiveSentinel() {\n\t\t\tif m.lower != nil && m.heap.cmp(m.levels[item.index].iterKV.K.UserKey, m.lower) <= 0 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\t// This key is the smallest boundary of a file and can be skipped\n\t\t\t// now that the file's range deletions are no longer relevant.\n\t\t\tm.err = m.prevEntry(item)\n\t\t\tif m.err != nil {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\tm.addItemStats(item)\n\t\tif isDeleted, err := m.isPrevEntryDeleted(item); err != nil {\n\t\t\tm.err = err\n\t\t\treturn nil\n\t\t} else if isDeleted {\n\t\t\tm.stats.PointsCoveredByRangeTombstones++\n\t\t\tcontinue\n\t\t}\n\t\tif item.iterKV.Visible(m.snapshot, m.batchSnapshot) {\n\t\t\treturn item.iterKV\n\t\t}\n\t\tm.err = m.prevEntry(item)\n\t}\n\treturn nil\n}\n\n// Seeks levels >= level to >= key. Additionally uses range tombstones to extend the seeks.\n//\n// If an error occurs, seekGE returns the error without setting m.err.\nfunc (m *mergingIter) seekGE(key []byte, level int, flags base.SeekGEFlags) error {\n\t// When seeking, we can use tombstones to adjust the key we seek to on each\n\t// level. Consider the series of range tombstones:\n\t//\n\t//   1: a---e\n\t//   2:    d---h\n\t//   3:       g---k\n\t//   4:          j---n\n\t//   5:             m---q\n\t//\n\t// If we SeekGE(\"b\") we also find the tombstone \"b\" resides within in the\n\t// first level which is [a,e). Regardless of whether this tombstone deletes\n\t// \"b\" in that level, we know it deletes \"b\" in all lower levels, so we\n\t// adjust the search key in the next level to the tombstone end key \"e\". We\n\t// then SeekGE(\"e\") in the second level and find the corresponding tombstone\n\t// [d,h). This process continues and we end up seeking for \"h\" in the 3rd\n\t// level, \"k\" in the 4th level and \"n\" in the last level.\n\t//\n\t// TODO(peter,rangedel): In addition to the above we can delay seeking a\n\t// level (and any lower levels) when the current iterator position is\n\t// contained within a range tombstone at a higher level.\n\n\t// Deterministically disable the TrySeekUsingNext optimizations sometimes in\n\t// invariant builds to encourage the metamorphic tests to surface bugs. Note\n\t// that we cannot disable the optimization within individual levels. It must\n\t// be disabled for all levels or none. If one lower-level iterator performs\n\t// a fresh seek whereas another takes advantage of its current iterator\n\t// position, the heap can become inconsistent. Consider the following\n\t// example:\n\t//\n\t//     L5:  [ [b-c) ]  [ d ]*\n\t//     L6:  [  b ]           [e]*\n\t//\n\t// Imagine a SeekGE(a). The [b-c) range tombstone deletes the L6 point key\n\t// 'b', resulting in the iterator positioned at d with the heap:\n\t//\n\t//     {L5: d, L6: e}\n\t//\n\t// A subsequent SeekGE(b) is seeking to a larger key, so the caller may set\n\t// TrySeekUsingNext()=true. If the L5 iterator used the TrySeekUsingNext\n\t// optimization but the L6 iterator did not, the iterator would have the\n\t// heap:\n\t//\n\t//     {L6: b, L5: d}\n\t//\n\t// Because the L5 iterator has already advanced to the next sstable, the\n\t// merging iterator cannot observe the [b-c) range tombstone and will\n\t// mistakenly return L6's deleted point key 'b'.\n\tif testingDisableSeekOpt(key, uintptr(unsafe.Pointer(m))) && !m.forceEnableSeekOpt {\n\t\tflags = flags.DisableTrySeekUsingNext()\n\t}\n\n\tfor ; level < len(m.levels); level++ {\n\t\tif invariants.Enabled && m.lower != nil && m.heap.cmp(key, m.lower) < 0 {\n\t\t\tm.logger.Fatalf(\"mergingIter: lower bound violation: %s < %s\\n%s\", key, m.lower, debug.Stack())\n\t\t}\n\n\t\tl := &m.levels[level]\n\t\tif m.prefix != nil {\n\t\t\tl.iterKV = l.iter.SeekPrefixGE(m.prefix, key, flags)\n\t\t\tif l.iterKV != nil {\n\t\t\t\tif !bytes.Equal(m.prefix, m.split.Prefix(l.iterKV.K.UserKey)) {\n\t\t\t\t\t// Prevent keys without a matching prefix from being added to the heap by setting\n\t\t\t\t\t// iterKey and iterValue to their zero values before calling initMinHeap.\n\t\t\t\t\tl.iterKV = nil\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tl.iterKV = l.iter.SeekGE(key, flags)\n\t\t}\n\t\tif l.iterKV == nil {\n\t\t\tif err := l.iter.Error(); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\n\t\t// If this level contains overlapping range tombstones, alter the seek\n\t\t// key accordingly. Caveat: If we're performing lazy-combined iteration,\n\t\t// we cannot alter the seek key: Range tombstones don't delete range\n\t\t// keys, and there might exist live range keys within the range\n\t\t// tombstone's span that need to be observed to trigger a switch to\n\t\t// combined iteration.\n\t\tif rangeDelIter := l.rangeDelIter; rangeDelIter != nil &&\n\t\t\t(m.combinedIterState == nil || m.combinedIterState.initialized) {\n\t\t\t// The level has a range-del iterator. Find the tombstone containing\n\t\t\t// the search key.\n\t\t\tvar err error\n\t\t\tl.tombstone, err = rangeDelIter.SeekGE(key)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif l.tombstone != nil && l.tombstone.VisibleAt(m.snapshot) && m.heap.cmp(l.tombstone.Start, key) <= 0 {\n\t\t\t\t// Based on the containment condition tombstone.End > key, so\n\t\t\t\t// the assignment to key results in a monotonically\n\t\t\t\t// non-decreasing key across iterations of this loop.\n\t\t\t\t//\n\t\t\t\t// The adjustment of key here can only move it to a larger key.\n\t\t\t\t// Since the caller of seekGE guaranteed that the original key\n\t\t\t\t// was greater than or equal to m.lower, the new key will\n\t\t\t\t// continue to be greater than or equal to m.lower.\n\t\t\t\tkey = l.tombstone.End\n\t\t\t}\n\t\t}\n\t}\n\treturn m.initMinHeap()\n}\n\nfunc (m *mergingIter) String() string {\n\treturn \"merging\"\n}\n\n// SeekGE implements base.InternalIterator.SeekGE. Note that SeekGE only checks\n// the upper bound. It is up to the caller to ensure that key is greater than\n// or equal to the lower bound.\nfunc (m *mergingIter) SeekGE(key []byte, flags base.SeekGEFlags) *base.InternalKV {\n\tm.prefix = nil\n\tm.err = m.seekGE(key, 0 /* start level */, flags)\n\tif m.err != nil {\n\t\treturn nil\n\t}\n\treturn m.findNextEntry()\n}\n\n// SeekPrefixGE implements base.InternalIterator.SeekPrefixGE.\nfunc (m *mergingIter) SeekPrefixGE(prefix, key []byte, flags base.SeekGEFlags) *base.InternalKV {\n\treturn m.SeekPrefixGEStrict(prefix, key, flags)\n}\n\n// SeekPrefixGEStrict implements topLevelIterator.SeekPrefixGEStrict. Note that\n// SeekPrefixGEStrict explicitly checks that the key has a matching prefix.\nfunc (m *mergingIter) SeekPrefixGEStrict(\n\tprefix, key []byte, flags base.SeekGEFlags,\n) *base.InternalKV {\n\tm.prefix = prefix\n\tm.err = m.seekGE(key, 0 /* start level */, flags)\n\tif m.err != nil {\n\t\treturn nil\n\t}\n\n\titerKV := m.findNextEntry()\n\tif invariants.Enabled && iterKV != nil {\n\t\tif !bytes.Equal(m.prefix, m.split.Prefix(iterKV.K.UserKey)) {\n\t\t\tm.logger.Fatalf(\"mergingIter: prefix violation: returning key %q without prefix %q\\n\", iterKV, m.prefix)\n\t\t}\n\t}\n\treturn iterKV\n}\n\n// Seeks levels >= level to < key. Additionally uses range tombstones to extend the seeks.\nfunc (m *mergingIter) seekLT(key []byte, level int, flags base.SeekLTFlags) error {\n\t// See the comment in seekGE regarding using tombstones to adjust the seek\n\t// target per level.\n\tm.prefix = nil\n\tfor ; level < len(m.levels); level++ {\n\t\tif invariants.Enabled && m.upper != nil && m.heap.cmp(key, m.upper) > 0 {\n\t\t\tm.logger.Fatalf(\"mergingIter: upper bound violation: %s > %s\\n%s\", key, m.upper, debug.Stack())\n\t\t}\n\n\t\tl := &m.levels[level]\n\t\tl.iterKV = l.iter.SeekLT(key, flags)\n\t\tif l.iterKV == nil {\n\t\t\tif err := l.iter.Error(); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\n\t\t// If this level contains overlapping range tombstones, alter the seek\n\t\t// key accordingly. Caveat: If we're performing lazy-combined iteration,\n\t\t// we cannot alter the seek key: Range tombstones don't delete range\n\t\t// keys, and there might exist live range keys within the range\n\t\t// tombstone's span that need to be observed to trigger a switch to\n\t\t// combined iteration.\n\t\tif rangeDelIter := l.rangeDelIter; rangeDelIter != nil &&\n\t\t\t(m.combinedIterState == nil || m.combinedIterState.initialized) {\n\t\t\t// The level has a range-del iterator. Find the tombstone containing\n\t\t\t// the search key.\n\t\t\ttomb, err := keyspan.SeekLE(m.heap.cmp, rangeDelIter, key)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tl.tombstone = tomb\n\t\t\t// Since SeekLT is exclusive on `key` and a tombstone's end key is\n\t\t\t// also exclusive, a seek key equal to a tombstone's end key still\n\t\t\t// enables the seek optimization (Note this is different than the\n\t\t\t// check performed by (*keyspan.Span).Contains).\n\t\t\tif l.tombstone != nil && l.tombstone.VisibleAt(m.snapshot) &&\n\t\t\t\tm.heap.cmp(key, l.tombstone.End) <= 0 {\n\t\t\t\t// NB: Based on the containment condition\n\t\t\t\t// tombstone.Start.UserKey <= key, so the assignment to key\n\t\t\t\t// results in a monotonically non-increasing key across\n\t\t\t\t// iterations of this loop.\n\t\t\t\t//\n\t\t\t\t// The adjustment of key here can only move it to a smaller key.\n\t\t\t\t// Since the caller of seekLT guaranteed that the original key\n\t\t\t\t// was less than or equal to m.upper, the new key will continue\n\t\t\t\t// to be less than or equal to m.upper.\n\t\t\t\tkey = l.tombstone.Start\n\t\t\t}\n\t\t}\n\t}\n\n\treturn m.initMaxHeap()\n}\n\n// SeekLT implements base.InternalIterator.SeekLT. Note that SeekLT only checks\n// the lower bound. It is up to the caller to ensure that key is less than the\n// upper bound.\nfunc (m *mergingIter) SeekLT(key []byte, flags base.SeekLTFlags) *base.InternalKV {\n\tm.prefix = nil\n\tm.err = m.seekLT(key, 0 /* start level */, flags)\n\tif m.err != nil {\n\t\treturn nil\n\t}\n\treturn m.findPrevEntry()\n}\n\n// First implements base.InternalIterator.First. Note that First only checks\n// the upper bound. It is up to the caller to ensure that key is greater than\n// or equal to the lower bound (e.g. via a call to SeekGE(lower)).\nfunc (m *mergingIter) First() *base.InternalKV {\n\tm.err = nil // clear cached iteration error\n\tm.prefix = nil\n\tm.heap.items = m.heap.items[:0]\n\tfor i := range m.levels {\n\t\tl := &m.levels[i]\n\t\tl.iterKV = l.iter.First()\n\t\tif l.iterKV == nil {\n\t\t\tif m.err = l.iter.Error(); m.err != nil {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\t}\n\tif m.err = m.initMinHeap(); m.err != nil {\n\t\treturn nil\n\t}\n\treturn m.findNextEntry()\n}\n\n// Last implements base.InternalIterator.Last. Note that Last only checks the\n// lower bound. It is up to the caller to ensure that key is less than the\n// upper bound (e.g. via a call to SeekLT(upper))\nfunc (m *mergingIter) Last() *base.InternalKV {\n\tm.err = nil // clear cached iteration error\n\tm.prefix = nil\n\tfor i := range m.levels {\n\t\tl := &m.levels[i]\n\t\tl.iterKV = l.iter.Last()\n\t\tif l.iterKV == nil {\n\t\t\tif m.err = l.iter.Error(); m.err != nil {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\t}\n\tif m.err = m.initMaxHeap(); m.err != nil {\n\t\treturn nil\n\t}\n\treturn m.findPrevEntry()\n}\n\nfunc (m *mergingIter) Next() *base.InternalKV {\n\tif m.err != nil {\n\t\treturn nil\n\t}\n\n\tif m.dir != 1 {\n\t\tif m.err = m.switchToMinHeap(); m.err != nil {\n\t\t\treturn nil\n\t\t}\n\t\treturn m.findNextEntry()\n\t}\n\n\tif m.heap.len() == 0 {\n\t\treturn nil\n\t}\n\n\t// NB: It's okay to call nextEntry directly even during prefix iteration\n\t// mode. During prefix iteration mode, we rely on the caller to not call\n\t// Next if the iterator has already advanced beyond the iteration prefix.\n\t// See the comment above the base.InternalIterator interface.\n\tif m.err = m.nextEntry(m.heap.items[0], nil /* succKey */); m.err != nil {\n\t\treturn nil\n\t}\n\n\titerKV := m.findNextEntry()\n\tif invariants.Enabled && m.prefix != nil && iterKV != nil {\n\t\tif !bytes.Equal(m.prefix, m.split.Prefix(iterKV.K.UserKey)) {\n\t\t\tm.logger.Fatalf(\"mergingIter: prefix violation: returning key %q without prefix %q\\n\", iterKV, m.prefix)\n\t\t}\n\t}\n\treturn iterKV\n}\n\nfunc (m *mergingIter) NextPrefix(succKey []byte) *base.InternalKV {\n\tif m.dir != 1 {\n\t\tpanic(\"pebble: cannot switch directions with NextPrefix\")\n\t}\n\tif m.err != nil || m.heap.len() == 0 {\n\t\treturn nil\n\t}\n\tif m.levelsPositioned == nil {\n\t\tm.levelsPositioned = make([]bool, len(m.levels))\n\t} else {\n\t\tfor i := range m.levelsPositioned {\n\t\t\tm.levelsPositioned[i] = false\n\t\t}\n\t}\n\n\t// The heap root necessarily must be positioned at a key < succKey, because\n\t// NextPrefix was invoked.\n\troot := m.heap.items[0]\n\tif invariants.Enabled && m.heap.cmp((*root).iterKV.K.UserKey, succKey) >= 0 {\n\t\tm.logger.Fatalf(\"pebble: invariant violation: NextPrefix(%q) called on merging iterator already positioned at %q\",\n\t\t\tsuccKey, (*root).iterKV)\n\t}\n\t// NB: root is the heap root before we call nextEntry; nextEntry may change\n\t// the heap root, so we must not `root` to still be the root of the heap, or\n\t// even to be in the heap if the level's iterator becomes exhausted.\n\tif m.err = m.nextEntry(root, succKey); m.err != nil {\n\t\treturn nil\n\t}\n\t// We only consider the level to be conclusively positioned at the next\n\t// prefix if our call to nextEntry did not advance the level onto a range\n\t// deletion's boundary. Range deletions may have bounds within the prefix\n\t// that are still surfaced by NextPrefix.\n\tm.levelsPositioned[root.index] = root.iterKV == nil || !root.iterKV.K.IsExclusiveSentinel()\n\n\tfor m.heap.len() > 0 {\n\t\troot := m.heap.items[0]\n\t\tif m.levelsPositioned[root.index] {\n\t\t\t// A level we've previously positioned is at the top of the heap, so\n\t\t\t// there are no other levels positioned at keys < succKey. We've\n\t\t\t// advanced as far as we need to.\n\t\t\tbreak\n\t\t}\n\t\t// If the current heap root is a sentinel key, we need to skip it.\n\t\t// Calling NextPrefix while positioned at a sentinel key is not\n\t\t// supported.\n\t\tif root.iterKV.K.IsExclusiveSentinel() {\n\t\t\tif m.err = m.nextEntry(root, nil); m.err != nil {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\t// Since this level was not the original heap root when NextPrefix was\n\t\t// called, we don't know whether this level's current key has the\n\t\t// previous prefix or a new one.\n\t\tif m.heap.cmp(root.iterKV.K.UserKey, succKey) >= 0 {\n\t\t\tbreak\n\t\t}\n\t\tif m.err = m.nextEntry(root, succKey); m.err != nil {\n\t\t\treturn nil\n\t\t}\n\t\t// We only consider the level to be conclusively positioned at the next\n\t\t// prefix if our call to nextEntry did not land onto a range deletion's\n\t\t// boundary. Range deletions may have bounds within the prefix that are\n\t\t// still surfaced by NextPrefix.\n\t\tm.levelsPositioned[root.index] = root.iterKV == nil || !root.iterKV.K.IsExclusiveSentinel()\n\t}\n\treturn m.findNextEntry()\n}\n\nfunc (m *mergingIter) Prev() *base.InternalKV {\n\tif m.err != nil {\n\t\treturn nil\n\t}\n\n\tif m.dir != -1 {\n\t\tif m.prefix != nil {\n\t\t\tm.err = errors.New(\"pebble: unsupported reverse prefix iteration\")\n\t\t\treturn nil\n\t\t}\n\t\tif m.err = m.switchToMaxHeap(); m.err != nil {\n\t\t\treturn nil\n\t\t}\n\t\treturn m.findPrevEntry()\n\t}\n\n\tif m.heap.len() == 0 {\n\t\treturn nil\n\t}\n\tif m.err = m.prevEntry(m.heap.items[0]); m.err != nil {\n\t\treturn nil\n\t}\n\treturn m.findPrevEntry()\n}\n\nfunc (m *mergingIter) Error() error {\n\tif m.heap.len() == 0 || m.err != nil {\n\t\treturn m.err\n\t}\n\treturn m.levels[m.heap.items[0].index].iter.Error()\n}\n\nfunc (m *mergingIter) Close() error {\n\tfor i := range m.levels {\n\t\titer := m.levels[i].iter\n\t\tif err := iter.Close(); err != nil && m.err == nil {\n\t\t\tm.err = err\n\t\t}\n\t\tm.levels[i].setRangeDelIter(nil)\n\t}\n\tm.levels = nil\n\tm.heap.items = m.heap.items[:0]\n\treturn m.err\n}\n\nfunc (m *mergingIter) SetBounds(lower, upper []byte) {\n\tm.prefix = nil\n\tm.lower = lower\n\tm.upper = upper\n\tfor i := range m.levels {\n\t\tm.levels[i].iter.SetBounds(lower, upper)\n\t}\n\tm.heap.clear()\n}\n\nfunc (m *mergingIter) SetContext(ctx context.Context) {\n\tfor i := range m.levels {\n\t\tm.levels[i].iter.SetContext(ctx)\n\t}\n}\n\n// DebugTree is part of the InternalIterator interface.\nfunc (m *mergingIter) DebugTree(tp treeprinter.Node) {\n\tn := tp.Childf(\"%T(%p)\", m, m)\n\tfor i := range m.levels {\n\t\tif iter := m.levels[i].iter; iter != nil {\n\t\t\titer.DebugTree(n)\n\t\t}\n\t}\n}\n\nfunc (m *mergingIter) DebugString() string {\n\tvar buf bytes.Buffer\n\tsep := \"\"\n\tfor m.heap.len() > 0 {\n\t\titem := m.heap.pop()\n\t\tfmt.Fprintf(&buf, \"%s%s\", sep, item.iterKV.K)\n\t\tsep = \" \"\n\t}\n\tvar err error\n\tif m.dir == 1 {\n\t\terr = m.initMinHeap()\n\t} else {\n\t\terr = m.initMaxHeap()\n\t}\n\tif err != nil {\n\t\tfmt.Fprintf(&buf, \"err=<%s>\", err)\n\t}\n\treturn buf.String()\n}\n\nfunc (m *mergingIter) ForEachLevelIter(fn func(li *levelIter) bool) {\n\tfor _, ml := range m.levels {\n\t\tif ml.levelIter != nil {\n\t\t\tif done := fn(ml.levelIter); done {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (m *mergingIter) addItemStats(l *mergingIterLevel) {\n\tm.stats.PointCount++\n\tm.stats.KeyBytes += uint64(len(l.iterKV.K.UserKey))\n\tm.stats.ValueBytes += uint64(len(l.iterKV.V.ValueOrHandle))\n}\n\nvar _ internalIterator = &mergingIter{}\n"
        },
        {
          "name": "merging_iter_heap.go",
          "type": "blob",
          "size": 1.6689453125,
          "content": "// Copyright 2018 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\ntype mergingIterHeap struct {\n\tcmp     Compare\n\treverse bool\n\titems   []*mergingIterLevel\n}\n\nfunc (h *mergingIterHeap) len() int {\n\treturn len(h.items)\n}\n\nfunc (h *mergingIterHeap) clear() {\n\th.items = h.items[:0]\n}\n\nfunc (h *mergingIterHeap) less(i, j int) bool {\n\tikv, jkv := h.items[i].iterKV, h.items[j].iterKV\n\tif c := h.cmp(ikv.K.UserKey, jkv.K.UserKey); c != 0 {\n\t\tif h.reverse {\n\t\t\treturn c > 0\n\t\t}\n\t\treturn c < 0\n\t}\n\tif h.reverse {\n\t\treturn ikv.K.Trailer < jkv.K.Trailer\n\t}\n\treturn ikv.K.Trailer > jkv.K.Trailer\n}\n\nfunc (h *mergingIterHeap) swap(i, j int) {\n\th.items[i], h.items[j] = h.items[j], h.items[i]\n}\n\n// init, fix, up and down are copied from the go stdlib.\nfunc (h *mergingIterHeap) init() {\n\t// heapify\n\tn := h.len()\n\tfor i := n/2 - 1; i >= 0; i-- {\n\t\th.down(i, n)\n\t}\n}\n\nfunc (h *mergingIterHeap) fix(i int) {\n\tif !h.down(i, h.len()) {\n\t\th.up(i)\n\t}\n}\n\nfunc (h *mergingIterHeap) pop() *mergingIterLevel {\n\tn := h.len() - 1\n\th.swap(0, n)\n\th.down(0, n)\n\titem := h.items[n]\n\th.items = h.items[:n]\n\treturn item\n}\n\nfunc (h *mergingIterHeap) up(j int) {\n\tfor {\n\t\ti := (j - 1) / 2 // parent\n\t\tif i == j || !h.less(j, i) {\n\t\t\tbreak\n\t\t}\n\t\th.swap(i, j)\n\t\tj = i\n\t}\n}\n\nfunc (h *mergingIterHeap) down(i0, n int) bool {\n\ti := i0\n\tfor {\n\t\tj1 := 2*i + 1\n\t\tif j1 >= n || j1 < 0 { // j1 < 0 after int overflow\n\t\t\tbreak\n\t\t}\n\t\tj := j1 // left child\n\t\tif j2 := j1 + 1; j2 < n && h.less(j2, j1) {\n\t\t\tj = j2 // = 2*i + 2  // right child\n\t\t}\n\t\tif !h.less(j, i) {\n\t\t\tbreak\n\t\t}\n\t\th.swap(i, j)\n\t\ti = j\n\t}\n\treturn i > i0\n}\n"
        },
        {
          "name": "merging_iter_test.go",
          "type": "blob",
          "size": 23.7099609375,
          "content": "// Copyright 2018 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"math/rand/v2\"\n\t\"strconv\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/datadriven\"\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/bloom\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/itertest\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n\t\"github.com/cockroachdb/pebble/internal/sstableinternal\"\n\t\"github.com/cockroachdb/pebble/internal/testkeys\"\n\t\"github.com/cockroachdb/pebble/internal/testutils/indenttree\"\n\t\"github.com/cockroachdb/pebble/objstorage/objstorageprovider\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestMergingIter(t *testing.T) {\n\tvar stats base.InternalIteratorStats\n\tnewFunc := func(iters ...internalIterator) internalIterator {\n\t\treturn newMergingIter(nil /* logger */, &stats, DefaultComparer.Compare,\n\t\t\tfunc(a []byte) int { return len(a) }, iters...)\n\t}\n\ttestIterator(t, newFunc, func(r *rand.Rand) [][]string {\n\t\t// Shuffle testKeyValuePairs into one or more splits. Each individual\n\t\t// split is in increasing order, but different splits may overlap in\n\t\t// range. Some of the splits may be empty.\n\t\tsplits := make([][]string, 1+r.IntN(2+len(testKeyValuePairs)))\n\t\tfor _, kv := range testKeyValuePairs {\n\t\t\tj := r.IntN(len(splits))\n\t\t\tsplits[j] = append(splits[j], kv)\n\t\t}\n\t\treturn splits\n\t})\n}\n\nfunc TestMergingIterSeek(t *testing.T) {\n\tvar def string\n\tdatadriven.RunTest(t, \"testdata/merging_iter_seek\", func(t *testing.T, d *datadriven.TestData) string {\n\t\tswitch d.Cmd {\n\t\tcase \"define\":\n\t\t\tdef = d.Input\n\t\t\treturn \"\"\n\n\t\tcase \"iter\":\n\t\t\tvar iters []internalIterator\n\t\t\tfor _, line := range strings.Split(def, \"\\n\") {\n\t\t\t\tvar kvs []base.InternalKV\n\t\t\t\tfor _, key := range strings.Fields(line) {\n\t\t\t\t\tj := strings.Index(key, \":\")\n\t\t\t\t\tkvs = append(kvs, base.MakeInternalKV(base.ParseInternalKey(key[:j]), []byte(key[j+1:])))\n\t\t\t\t}\n\t\t\t\titers = append(iters, base.NewFakeIter(kvs))\n\t\t\t}\n\n\t\t\tvar stats base.InternalIteratorStats\n\t\t\titer := newMergingIter(nil /* logger */, &stats, DefaultComparer.Compare,\n\t\t\t\tfunc(a []byte) int { return len(a) }, iters...)\n\t\t\tdefer iter.Close()\n\t\t\treturn itertest.RunInternalIterCmd(t, d, iter)\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", d.Cmd)\n\t\t}\n\t})\n}\n\nfunc TestMergingIterNextPrev(t *testing.T) {\n\t// The data is the same in each of these cases, but divided up amongst the\n\t// iterators differently. This data must match the definition in\n\t// testdata/internal_iter_next.\n\titerCases := [][]string{\n\t\t{\n\t\t\t\"a.SET.2:2 a.SET.1:1 b.SET.2:2 b.SET.1:1 c.SET.2:2 c.SET.1:1\",\n\t\t},\n\t\t{\n\t\t\t\"a.SET.2:2 b.SET.2:2 c.SET.2:2\",\n\t\t\t\"a.SET.1:1 b.SET.1:1 c.SET.1:1\",\n\t\t},\n\t\t{\n\t\t\t\"a.SET.2:2 b.SET.2:2\",\n\t\t\t\"a.SET.1:1 b.SET.1:1\",\n\t\t\t\"c.SET.2:2 c.SET.1:1\",\n\t\t},\n\t\t{\n\t\t\t\"a.SET.2:2\",\n\t\t\t\"a.SET.1:1\",\n\t\t\t\"b.SET.2:2\",\n\t\t\t\"b.SET.1:1\",\n\t\t\t\"c.SET.2:2\",\n\t\t\t\"c.SET.1:1\",\n\t\t},\n\t}\n\n\tfor _, c := range iterCases {\n\t\tt.Run(\"\", func(t *testing.T) {\n\t\t\tdatadriven.RunTest(t, \"testdata/internal_iter_next\", func(t *testing.T, d *datadriven.TestData) string {\n\t\t\t\tswitch d.Cmd {\n\t\t\t\tcase \"define\":\n\t\t\t\t\t// Ignore. We've defined the iterator data above.\n\t\t\t\t\treturn \"\"\n\n\t\t\t\tcase \"iter\":\n\t\t\t\t\titers := make([]internalIterator, len(c))\n\t\t\t\t\tfor i := range c {\n\t\t\t\t\t\tvar kvs []base.InternalKV\n\t\t\t\t\t\tfor _, key := range strings.Fields(c[i]) {\n\t\t\t\t\t\t\tj := strings.Index(key, \":\")\n\t\t\t\t\t\t\tkvs = append(kvs, base.MakeInternalKV(base.ParseInternalKey(key[:j]), []byte(key[j+1:])))\n\t\t\t\t\t\t}\n\t\t\t\t\t\titers[i] = base.NewFakeIter(kvs)\n\t\t\t\t\t}\n\n\t\t\t\t\tvar stats base.InternalIteratorStats\n\t\t\t\t\titer := newMergingIter(nil /* logger */, &stats, DefaultComparer.Compare,\n\t\t\t\t\t\tfunc(a []byte) int { return len(a) }, iters...)\n\t\t\t\t\tdefer iter.Close()\n\t\t\t\t\treturn itertest.RunInternalIterCmd(t, d, iter)\n\n\t\t\t\tdefault:\n\t\t\t\t\treturn fmt.Sprintf(\"unknown command: %s\", d.Cmd)\n\t\t\t\t}\n\t\t\t})\n\t\t})\n\t}\n}\n\nfunc TestMergingIterDataDriven(t *testing.T) {\n\tmemFS := vfs.NewMem()\n\tcmp := DefaultComparer.Compare\n\tfmtKey := DefaultComparer.FormatKey\n\topts := (*Options)(nil).EnsureDefaults()\n\tvar v *version\n\tvar buf bytes.Buffer\n\n\t// Indexed by FileNum.\n\treaders := make(map[base.FileNum]*sstable.Reader)\n\tdefer func() {\n\t\tfor _, r := range readers {\n\t\t\tr.Close()\n\t\t}\n\t}()\n\tparser := itertest.NewParser()\n\n\tvar (\n\t\tpointProbes    map[base.FileNum][]itertest.Probe\n\t\trangeDelProbes map[base.FileNum][]keyspanProbe\n\t\tfileNum        base.FileNum\n\t)\n\tnewIters :=\n\t\tfunc(_ context.Context, file *manifest.FileMetadata, opts *IterOptions, iio internalIterOpts, kinds iterKinds,\n\t\t) (iterSet, error) {\n\t\t\tvar set iterSet\n\t\t\tvar err error\n\t\t\tr := readers[file.FileNum]\n\t\t\tif kinds.RangeDeletion() {\n\t\t\t\tset.rangeDeletion, err = r.NewRawRangeDelIter(context.Background(), sstable.NoFragmentTransforms)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn iterSet{}, errors.CombineErrors(err, set.CloseAll())\n\t\t\t\t}\n\t\t\t}\n\t\t\tif kinds.Point() {\n\t\t\t\tset.point, err = r.NewPointIter(\n\t\t\t\t\tcontext.Background(),\n\t\t\t\t\tsstable.NoTransforms,\n\t\t\t\t\topts.GetLowerBound(), opts.GetUpperBound(), nil, sstable.AlwaysUseFilterBlock, iio.stats,\n\t\t\t\t\tnil, sstable.MakeTrivialReaderProvider(r))\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn iterSet{}, errors.CombineErrors(err, set.CloseAll())\n\t\t\t\t}\n\t\t\t}\n\t\t\tset.point = itertest.Attach(set.point, itertest.ProbeState{Log: &buf}, pointProbes[file.FileNum]...)\n\t\t\tset.rangeDeletion = attachKeyspanProbes(set.rangeDeletion, keyspanProbeContext{log: &buf}, rangeDelProbes[file.FileNum]...)\n\t\t\treturn set, nil\n\t\t}\n\n\tdatadriven.RunTest(t, \"testdata/merging_iter\", func(t *testing.T, d *datadriven.TestData) string {\n\t\tswitch d.Cmd {\n\t\tcase \"define\":\n\t\t\tlevels, err := indenttree.Parse(d.Input)\n\t\t\tif err != nil {\n\t\t\t\td.Fatalf(t, \"%v\", err)\n\t\t\t}\n\t\t\tvar files [numLevels][]*fileMetadata\n\t\t\tfor l := range levels {\n\t\t\t\tif levels[l].Value() != \"L\" {\n\t\t\t\t\td.Fatalf(t, \"top-level strings should be L\")\n\t\t\t\t}\n\t\t\t\tfor _, file := range levels[l].Children() {\n\t\t\t\t\tm, err := manifest.ParseFileMetadataDebug(file.Value())\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\td.Fatalf(t, \"file metadata: %s\", err)\n\t\t\t\t\t}\n\t\t\t\t\tfiles[l+1] = append(files[l+1], m)\n\n\t\t\t\t\tname := fmt.Sprint(fileNum)\n\t\t\t\t\tfileNum++\n\t\t\t\t\tf, err := memFS.Create(name, vfs.WriteCategoryUnspecified)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t\tw := sstable.NewRawWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{})\n\t\t\t\t\tvar tombstones []keyspan.Span\n\t\t\t\t\tfrag := keyspan.Fragmenter{\n\t\t\t\t\t\tCmp:    cmp,\n\t\t\t\t\t\tFormat: fmtKey,\n\t\t\t\t\t\tEmit: func(fragmented keyspan.Span) {\n\t\t\t\t\t\t\ttombstones = append(tombstones, fragmented)\n\t\t\t\t\t\t},\n\t\t\t\t\t}\n\n\t\t\t\t\tfor _, kvRow := range file.Children() {\n\t\t\t\t\t\tfor _, kv := range strings.Fields(kvRow.Value()) {\n\t\t\t\t\t\t\tj := strings.Index(kv, \":\")\n\t\t\t\t\t\t\tikey := base.ParseInternalKey(kv[:j])\n\t\t\t\t\t\t\tvalue := []byte(kv[j+1:])\n\t\t\t\t\t\t\tswitch ikey.Kind() {\n\t\t\t\t\t\t\tcase InternalKeyKindRangeDelete:\n\t\t\t\t\t\t\t\tfrag.Add(keyspan.Span{Start: ikey.UserKey, End: value, Keys: []keyspan.Key{{Trailer: ikey.Trailer}}})\n\t\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t\t\tif err := w.AddWithForceObsolete(ikey, value, false /* forceObsolete */); err != nil {\n\t\t\t\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tfrag.Finish()\n\t\t\t\t\tfor _, v := range tombstones {\n\t\t\t\t\t\tif err := w.EncodeSpan(v); err != nil {\n\t\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tif err := w.Close(); err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t\tf, err = memFS.Open(name)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t\treadable, err := sstable.NewSimpleReadable(f)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t\tr, err := sstable.NewReader(context.Background(), readable, opts.MakeReaderOptions())\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t\treaders[m.FileNum] = r\n\t\t\t\t}\n\t\t\t}\n\t\t\tv = newVersion(opts, files)\n\t\t\treturn v.String()\n\t\tcase \"iter\":\n\t\t\tbuf.Reset()\n\t\t\tpointProbes = make(map[base.FileNum][]itertest.Probe, len(v.Levels))\n\t\t\trangeDelProbes = make(map[base.FileNum][]keyspanProbe, len(v.Levels))\n\t\t\tfor _, cmdArg := range d.CmdArgs {\n\t\t\t\tswitch key := cmdArg.Key; key {\n\t\t\t\tcase \"probe-points\":\n\t\t\t\t\ti, err := strconv.Atoi(cmdArg.Vals[0][1:])\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\trequire.NoError(t, err)\n\t\t\t\t\t}\n\t\t\t\t\tpointProbes[base.FileNum(i)] = itertest.MustParseProbes(parser, cmdArg.Vals[1:]...)\n\t\t\t\tcase \"probe-rangedels\":\n\t\t\t\t\ti, err := strconv.Atoi(cmdArg.Vals[0][1:])\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\trequire.NoError(t, err)\n\t\t\t\t\t}\n\t\t\t\t\trangeDelProbes[base.FileNum(i)] = parseKeyspanProbes(cmdArg.Vals[1:]...)\n\t\t\t\tdefault:\n\t\t\t\t\t// Might be a command understood by the RunInternalIterCmd\n\t\t\t\t\t// command, so don't error.\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tlevelIters := make([]mergingIterLevel, 0, len(v.Levels))\n\t\t\tvar stats base.InternalIteratorStats\n\t\t\tfor i, l := range v.Levels {\n\t\t\t\tslice := l.Slice()\n\t\t\t\tif slice.Empty() {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tli := &levelIter{}\n\t\t\t\tli.init(context.Background(), IterOptions{}, testkeys.Comparer,\n\t\t\t\t\tnewIters, slice.Iter(), manifest.Level(i), internalIterOpts{stats: &stats})\n\n\t\t\t\ti := len(levelIters)\n\t\t\t\tlevelIters = append(levelIters, mergingIterLevel{iter: li})\n\t\t\t\tli.initRangeDel(&levelIters[i])\n\t\t\t}\n\t\t\tmiter := &mergingIter{}\n\t\t\tmiter.init(nil /* opts */, &stats, cmp, func(a []byte) int { return len(a) }, levelIters...)\n\t\t\tdefer miter.Close()\n\t\t\tmiter.forceEnableSeekOpt = true\n\t\t\t// Exercise SetContext for fun\n\t\t\t// (https://github.com/cockroachdb/pebble/pull/3037 caused a SIGSEGV due\n\t\t\t// to a nil pointer dereference).\n\t\t\tmiter.SetContext(context.Background())\n\t\t\titertest.RunInternalIterCmdWriter(t, &buf, d, miter,\n\t\t\t\titertest.Verbose, itertest.WithStats(&stats))\n\t\t\treturn buf.String()\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", d.Cmd)\n\t\t}\n\t})\n}\n\nfunc buildMergingIterTables(\n\tb *testing.B, blockSize, restartInterval, count int,\n) ([]*sstable.Reader, [][]byte, func()) {\n\tmem := vfs.NewMem()\n\tfiles := make([]vfs.File, count)\n\tfor i := range files {\n\t\tf, err := mem.Create(fmt.Sprintf(\"bench%d\", i), vfs.WriteCategoryUnspecified)\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t\tfiles[i] = f\n\t}\n\n\twriters := make([]sstable.RawWriter, len(files))\n\tfor i := range files {\n\t\twriters[i] = sstable.NewRawWriter(objstorageprovider.NewFileWritable(files[i]), sstable.WriterOptions{\n\t\t\tBlockRestartInterval: restartInterval,\n\t\t\tBlockSize:            blockSize,\n\t\t\tCompression:          NoCompression,\n\t\t})\n\t}\n\n\testimatedSize := func() uint64 {\n\t\tvar sum uint64\n\t\tfor _, w := range writers {\n\t\t\tsum += w.EstimatedSize()\n\t\t}\n\t\treturn sum\n\t}\n\n\tvar keys [][]byte\n\tvar ikey InternalKey\n\ttargetSize := uint64(count * (2 << 20))\n\tfor i := 0; estimatedSize() < targetSize; i++ {\n\t\tkey := []byte(fmt.Sprintf(\"%08d\", i))\n\t\tkeys = append(keys, key)\n\t\tikey.UserKey = key\n\t\tj := rand.IntN(len(writers))\n\t\tw := writers[j]\n\t\tw.AddWithForceObsolete(ikey, nil, false /* forceObsolete */)\n\t}\n\n\tfor _, w := range writers {\n\t\tif err := w.Close(); err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n\tc := NewCache(128 << 20 /* 128MB */)\n\tdefer c.Unref()\n\n\tvar opts sstable.ReaderOptions\n\topts.SetInternalCacheOpts(sstableinternal.CacheOptions{\n\t\tCache: c,\n\t})\n\n\treaders := make([]*sstable.Reader, len(files))\n\tfor i := range files {\n\t\tf, err := mem.Open(fmt.Sprintf(\"bench%d\", i))\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t\treadable, err := sstable.NewSimpleReadable(f)\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t\treaders[i], err = sstable.NewReader(context.Background(), readable, opts)\n\t\tif err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n\treturn readers, keys, func() {\n\t\tfor _, r := range readers {\n\t\t\trequire.NoError(b, r.Close())\n\t\t}\n\t}\n}\n\nfunc BenchmarkMergingIterSeekGE(b *testing.B) {\n\tconst blockSize = 32 << 10\n\n\tfor _, restartInterval := range []int{16} {\n\t\tb.Run(fmt.Sprintf(\"restart=%d\", restartInterval),\n\t\t\tfunc(b *testing.B) {\n\t\t\t\tfor _, count := range []int{1, 2, 3, 4, 5} {\n\t\t\t\t\tb.Run(fmt.Sprintf(\"count=%d\", count),\n\t\t\t\t\t\tfunc(b *testing.B) {\n\t\t\t\t\t\t\treaders, keys, cleanup := buildMergingIterTables(b, blockSize, restartInterval, count)\n\t\t\t\t\t\t\tdefer cleanup()\n\t\t\t\t\t\t\titers := make([]internalIterator, len(readers))\n\t\t\t\t\t\t\tfor i := range readers {\n\t\t\t\t\t\t\t\tvar err error\n\t\t\t\t\t\t\t\titers[i], err = readers[i].NewIter(sstable.NoTransforms, nil /* lower */, nil /* upper */)\n\t\t\t\t\t\t\t\trequire.NoError(b, err)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tvar stats base.InternalIteratorStats\n\t\t\t\t\t\t\tm := newMergingIter(nil /* logger */, &stats, DefaultComparer.Compare,\n\t\t\t\t\t\t\t\tfunc(a []byte) int { return len(a) }, iters...)\n\t\t\t\t\t\t\trng := rand.New(rand.NewPCG(0, uint64(time.Now().UnixNano())))\n\n\t\t\t\t\t\t\tb.ResetTimer()\n\t\t\t\t\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t\t\t\t\tm.SeekGE(keys[rng.IntN(len(keys))], base.SeekGEFlagsNone)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tm.Close()\n\t\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t})\n\t}\n}\n\nfunc BenchmarkMergingIterNext(b *testing.B) {\n\tconst blockSize = 32 << 10\n\n\tfor _, restartInterval := range []int{16} {\n\t\tb.Run(fmt.Sprintf(\"restart=%d\", restartInterval),\n\t\t\tfunc(b *testing.B) {\n\t\t\t\tfor _, count := range []int{1, 2, 3, 4, 5} {\n\t\t\t\t\tb.Run(fmt.Sprintf(\"count=%d\", count),\n\t\t\t\t\t\tfunc(b *testing.B) {\n\t\t\t\t\t\t\treaders, _, cleanup := buildMergingIterTables(b, blockSize, restartInterval, count)\n\t\t\t\t\t\t\tdefer cleanup()\n\t\t\t\t\t\t\titers := make([]internalIterator, len(readers))\n\t\t\t\t\t\t\tfor i := range readers {\n\t\t\t\t\t\t\t\tvar err error\n\t\t\t\t\t\t\t\titers[i], err = readers[i].NewIter(sstable.NoTransforms, nil /* lower */, nil /* upper */)\n\t\t\t\t\t\t\t\trequire.NoError(b, err)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tvar stats base.InternalIteratorStats\n\t\t\t\t\t\t\tm := newMergingIter(nil /* logger */, &stats, DefaultComparer.Compare,\n\t\t\t\t\t\t\t\tfunc(a []byte) int { return len(a) }, iters...)\n\n\t\t\t\t\t\t\tb.ResetTimer()\n\t\t\t\t\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t\t\t\t\tkv := m.Next()\n\t\t\t\t\t\t\t\tif kv == nil {\n\t\t\t\t\t\t\t\t\tkv = m.First()\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t_ = kv\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tm.Close()\n\t\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t})\n\t}\n}\n\nfunc BenchmarkMergingIterPrev(b *testing.B) {\n\tconst blockSize = 32 << 10\n\n\tfor _, restartInterval := range []int{16} {\n\t\tb.Run(fmt.Sprintf(\"restart=%d\", restartInterval),\n\t\t\tfunc(b *testing.B) {\n\t\t\t\tfor _, count := range []int{1, 2, 3, 4, 5} {\n\t\t\t\t\tb.Run(fmt.Sprintf(\"count=%d\", count),\n\t\t\t\t\t\tfunc(b *testing.B) {\n\t\t\t\t\t\t\treaders, _, cleanup := buildMergingIterTables(b, blockSize, restartInterval, count)\n\t\t\t\t\t\t\tdefer cleanup()\n\t\t\t\t\t\t\titers := make([]internalIterator, len(readers))\n\t\t\t\t\t\t\tfor i := range readers {\n\t\t\t\t\t\t\t\tvar err error\n\t\t\t\t\t\t\t\titers[i], err = readers[i].NewIter(sstable.NoTransforms, nil /* lower */, nil /* upper */)\n\t\t\t\t\t\t\t\trequire.NoError(b, err)\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tvar stats base.InternalIteratorStats\n\t\t\t\t\t\t\tm := newMergingIter(nil /* logger */, &stats, DefaultComparer.Compare,\n\t\t\t\t\t\t\t\tfunc(a []byte) int { return len(a) }, iters...)\n\n\t\t\t\t\t\t\tb.ResetTimer()\n\t\t\t\t\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t\t\t\t\tkv := m.Prev()\n\t\t\t\t\t\t\t\tif kv == nil {\n\t\t\t\t\t\t\t\t\tkv = m.Last()\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t_ = kv\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tm.Close()\n\t\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t})\n\t}\n}\n\n// Builds levels for BenchmarkMergingIterSeqSeekGEWithBounds. The lowest level,\n// index 0 here, contains most of the data. Each level has 2 files, to allow for\n// stepping into the second file if needed. The lowest level has all the keys in\n// the file 0, and a single \"lastIKey\" in file 1. File 0 in all other levels have\n// only the first and last key of file 0 of the aforementioned level -- this\n// simulates sparseness of data, but not necessarily of file width, in higher\n// levels. File 1 in other levels is similar to File 1 in the aforementioned level\n// since it is only for stepping into. If writeRangeTombstoneToLowestLevel is\n// true, a range tombstone is written to the first lowest level file that\n// deletes all the keys in it, and no other levels should be written.\nfunc buildLevelsForMergingIterSeqSeek(\n\tb *testing.B,\n\tblockSize, restartInterval, levelCount int,\n\tkeyOffset int,\n\twriteRangeTombstoneToLowestLevel bool,\n\twriteBloomFilters bool,\n\tforceTwoLevelIndex bool,\n) (readers [][]*sstable.Reader, levelSlices []manifest.LevelSlice, keys [][]byte) {\n\tmem := vfs.NewMem()\n\tif writeRangeTombstoneToLowestLevel && levelCount != 1 {\n\t\tpanic(\"expect to write only 1 level\")\n\t}\n\tfiles := make([][]vfs.File, levelCount)\n\tfor i := range files {\n\t\tfor j := 0; j < 2; j++ {\n\t\t\tf, err := mem.Create(fmt.Sprintf(\"bench%d_%d\", i, j), vfs.WriteCategoryUnspecified)\n\t\t\tif err != nil {\n\t\t\t\tb.Fatal(err)\n\t\t\t}\n\t\t\tfiles[i] = append(files[i], f)\n\t\t}\n\t}\n\n\tconst targetL6FirstFileSize = 2 << 20\n\twriters := make([][]sstable.RawWriter, levelCount)\n\t// A policy unlikely to have false positives.\n\tfilterPolicy := bloom.FilterPolicy(100)\n\tfor i := range files {\n\t\tfor j := range files[i] {\n\t\t\twriterOptions := sstable.WriterOptions{\n\t\t\t\tBlockRestartInterval: restartInterval,\n\t\t\t\tBlockSize:            blockSize,\n\t\t\t\tCompression:          NoCompression,\n\t\t\t}\n\t\t\tif writeBloomFilters {\n\t\t\t\twriterOptions.FilterPolicy = filterPolicy\n\t\t\t\twriterOptions.FilterType = base.TableFilter\n\t\t\t}\n\t\t\tif forceTwoLevelIndex {\n\t\t\t\tif i == 0 && j == 0 {\n\t\t\t\t\t// Ignoring compression, approximate number of blocks\n\t\t\t\t\tnumDataBlocks := targetL6FirstFileSize / blockSize\n\t\t\t\t\tif numDataBlocks < 4 {\n\t\t\t\t\t\tb.Fatalf(\"cannot produce two level index\")\n\t\t\t\t\t}\n\t\t\t\t\t// Produce ~2 lower-level index blocks.\n\t\t\t\t\twriterOptions.IndexBlockSize = (numDataBlocks / 2) * 8\n\t\t\t\t} else if j == 0 {\n\t\t\t\t\t// Only 2 keys in these files, so to produce two level indexes we\n\t\t\t\t\t// set the block sizes to 1.\n\t\t\t\t\twriterOptions.BlockSize = 1\n\t\t\t\t\twriterOptions.IndexBlockSize = 1\n\t\t\t\t}\n\t\t\t}\n\t\t\twriters[i] = append(writers[i], sstable.NewRawWriter(objstorageprovider.NewFileWritable(files[i][j]), writerOptions))\n\t\t}\n\t}\n\n\ti := keyOffset\n\tw := writers[0][0]\n\tfor ; w.EstimatedSize() < targetL6FirstFileSize; i++ {\n\t\tkey := []byte(fmt.Sprintf(\"%08d\", i))\n\t\tkeys = append(keys, key)\n\t\tikey := base.MakeInternalKey(key, 0, InternalKeyKindSet)\n\t\trequire.NoError(b, w.AddWithForceObsolete(ikey, nil, false /* forceObsolete */))\n\t}\n\tif writeRangeTombstoneToLowestLevel {\n\t\trequire.NoError(b, w.EncodeSpan(keyspan.Span{\n\t\t\tStart: keys[0],\n\t\t\tEnd:   []byte(fmt.Sprintf(\"%08d\", i)),\n\t\t\tKeys: []keyspan.Key{{\n\t\t\t\tTrailer: base.MakeTrailer(1, InternalKeyKindRangeDelete),\n\t\t\t}},\n\t\t}))\n\t}\n\tfor j := 1; j < len(files); j++ {\n\t\tfor _, k := range []int{0, len(keys) - 1} {\n\t\t\tikey := base.MakeInternalKey(keys[k], base.SeqNum(j), InternalKeyKindSet)\n\t\t\trequire.NoError(b, writers[j][0].AddWithForceObsolete(ikey, nil, false /* forceObsolete */))\n\t\t}\n\t}\n\tlastKey := []byte(fmt.Sprintf(\"%08d\", i))\n\tkeys = append(keys, lastKey)\n\tfor j := 0; j < len(files); j++ {\n\t\tlastIKey := base.MakeInternalKey(lastKey, base.SeqNum(j), InternalKeyKindSet)\n\t\trequire.NoError(b, writers[j][1].AddWithForceObsolete(lastIKey, nil, false /* forceObsolete */))\n\t}\n\tfor _, levelWriters := range writers {\n\t\tfor j, w := range levelWriters {\n\t\t\tif err := w.Close(); err != nil {\n\t\t\t\tb.Fatal(err)\n\t\t\t}\n\t\t\tmeta, err := w.Metadata()\n\t\t\trequire.NoError(b, err)\n\t\t\tif forceTwoLevelIndex && j == 0 && meta.Properties.IndexType != 2 {\n\t\t\t\tb.Fatalf(\"did not produce two level index\")\n\t\t\t}\n\t\t}\n\t}\n\n\tc := NewCache(128 << 20 /* 128MB */)\n\tdefer c.Unref()\n\n\topts := sstable.ReaderOptions{Comparer: DefaultComparer}\n\topts.SetInternalCacheOpts(sstableinternal.CacheOptions{\n\t\tCache: c,\n\t})\n\n\tif writeBloomFilters {\n\t\topts.Filters = make(map[string]FilterPolicy)\n\t\topts.Filters[filterPolicy.Name()] = filterPolicy\n\t}\n\n\treaders = make([][]*sstable.Reader, levelCount)\n\tfor i := range files {\n\t\tfor j := range files[i] {\n\t\t\tf, err := mem.Open(fmt.Sprintf(\"bench%d_%d\", i, j))\n\t\t\tif err != nil {\n\t\t\t\tb.Fatal(err)\n\t\t\t}\n\t\t\treadable, err := sstable.NewSimpleReadable(f)\n\t\t\tif err != nil {\n\t\t\t\tb.Fatal(err)\n\t\t\t}\n\t\t\tr, err := sstable.NewReader(context.Background(), readable, opts)\n\t\t\tif err != nil {\n\t\t\t\tb.Fatal(err)\n\t\t\t}\n\t\t\treaders[i] = append(readers[i], r)\n\t\t}\n\t}\n\tlevelSlices = make([]manifest.LevelSlice, levelCount)\n\tfor i := range readers {\n\t\tmeta := make([]*fileMetadata, len(readers[i]))\n\t\tfor j := range readers[i] {\n\t\t\titer, err := readers[i][j].NewIter(sstable.NoTransforms, nil /* lower */, nil /* upper */)\n\t\t\trequire.NoError(b, err)\n\t\t\tsmallest := iter.First()\n\t\t\tmeta[j] = &fileMetadata{}\n\t\t\t// The same FileNum is being reused across different levels, which\n\t\t\t// is harmless for the benchmark since each level has its own iterator\n\t\t\t// creation func.\n\t\t\tmeta[j].FileNum = FileNum(j)\n\t\t\tlargest := iter.Last()\n\t\t\tmeta[j].ExtendPointKeyBounds(opts.Comparer.Compare, smallest.K.Clone(), largest.K.Clone())\n\t\t\tmeta[j].InitPhysicalBacking()\n\t\t}\n\t\tlevelSlices[i] = manifest.NewLevelSliceSpecificOrder(meta)\n\t}\n\treturn readers, levelSlices, keys\n}\n\nfunc buildMergingIter(readers [][]*sstable.Reader, levelSlices []manifest.LevelSlice) *mergingIter {\n\tmils := make([]mergingIterLevel, len(levelSlices))\n\tfor i := len(readers) - 1; i >= 0; i-- {\n\t\tlevelIndex := i\n\t\tlevel := len(readers) - 1 - i\n\t\tnewIters := func(\n\t\t\t_ context.Context, file *manifest.FileMetadata, opts *IterOptions, _ internalIterOpts, _ iterKinds,\n\t\t) (iterSet, error) {\n\t\t\titer, err := readers[levelIndex][file.FileNum].NewIter(\n\t\t\t\tsstable.NoTransforms, opts.LowerBound, opts.UpperBound)\n\t\t\tif err != nil {\n\t\t\t\treturn iterSet{}, err\n\t\t\t}\n\t\t\trdIter, err := readers[levelIndex][file.FileNum].NewRawRangeDelIter(context.Background(), sstable.NoFragmentTransforms)\n\t\t\tif err != nil {\n\t\t\t\titer.Close()\n\t\t\t\treturn iterSet{}, err\n\t\t\t}\n\t\t\treturn iterSet{point: iter, rangeDeletion: rdIter}, err\n\t\t}\n\t\tl := newLevelIter(\n\t\t\tcontext.Background(), IterOptions{}, testkeys.Comparer, newIters, levelSlices[i].Iter(),\n\t\t\tmanifest.Level(level), internalIterOpts{})\n\t\tl.initRangeDel(&mils[level])\n\t\tmils[level].iter = l\n\t}\n\tvar stats base.InternalIteratorStats\n\tm := &mergingIter{}\n\tm.init(nil /* logger */, &stats, testkeys.Comparer.Compare,\n\t\tfunc(a []byte) int { return len(a) }, mils...)\n\treturn m\n}\n\n// A benchmark that simulates the behavior of a mergingIter where\n// monotonically increasing narrow bounds are repeatedly set and used to Seek\n// and then iterate over the keys within the bounds. This resembles MVCC\n// scanning by CockroachDB when doing a lookup/index join with a large number\n// of left rows, that are batched and reuse the same iterator, and which can\n// have good locality of access. This results in the successive bounds being\n// in the same file.\nfunc BenchmarkMergingIterSeqSeekGEWithBounds(b *testing.B) {\n\tconst blockSize = 32 << 10\n\n\trestartInterval := 16\n\tfor _, levelCount := range []int{5} {\n\t\tb.Run(fmt.Sprintf(\"levelCount=%d\", levelCount),\n\t\t\tfunc(b *testing.B) {\n\t\t\t\treaders, levelSlices, keys := buildLevelsForMergingIterSeqSeek(\n\t\t\t\t\tb, blockSize, restartInterval, levelCount, 0 /* keyOffset */, false, false, false)\n\t\t\t\tm := buildMergingIter(readers, levelSlices)\n\t\t\t\tkeyCount := len(keys)\n\t\t\t\tb.ResetTimer()\n\t\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t\tpos := i % (keyCount - 1)\n\t\t\t\t\tm.SetBounds(keys[pos], keys[pos+1])\n\t\t\t\t\t// SeekGE will return keys[pos].\n\t\t\t\t\tk := m.SeekGE(keys[pos], base.SeekGEFlagsNone)\n\t\t\t\t\tfor k != nil {\n\t\t\t\t\t\tk = m.Next()\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tm.Close()\n\t\t\t\tfor i := range readers {\n\t\t\t\t\tfor j := range readers[i] {\n\t\t\t\t\t\treaders[i][j].Close()\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t})\n\t}\n}\n\nfunc BenchmarkMergingIterSeqSeekPrefixGE(b *testing.B) {\n\tconst blockSize = 32 << 10\n\tconst restartInterval = 16\n\tconst levelCount = 5\n\treaders, levelSlices, keys := buildLevelsForMergingIterSeqSeek(\n\t\tb, blockSize, restartInterval, levelCount, 0 /* keyOffset */, false, false, false)\n\n\tfor _, skip := range []int{1, 2, 4, 8, 16} {\n\t\tfor _, useNext := range []bool{false, true} {\n\t\t\tb.Run(fmt.Sprintf(\"skip=%d/use-next=%t\", skip, useNext),\n\t\t\t\tfunc(b *testing.B) {\n\t\t\t\t\tm := buildMergingIter(readers, levelSlices)\n\t\t\t\t\tkeyCount := len(keys)\n\t\t\t\t\tpos := 0\n\n\t\t\t\t\tm.SeekPrefixGE(keys[pos], keys[pos], base.SeekGEFlagsNone)\n\t\t\t\t\tb.ResetTimer()\n\t\t\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t\t\tpos += skip\n\t\t\t\t\t\tvar flags base.SeekGEFlags\n\t\t\t\t\t\tif useNext {\n\t\t\t\t\t\t\tflags = flags.EnableTrySeekUsingNext()\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif pos >= keyCount {\n\t\t\t\t\t\t\tpos = 0\n\t\t\t\t\t\t\tflags = flags.DisableTrySeekUsingNext()\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// SeekPrefixGE will return keys[pos].\n\t\t\t\t\t\tm.SeekPrefixGE(keys[pos], keys[pos], flags)\n\t\t\t\t\t}\n\t\t\t\t\tb.StopTimer()\n\t\t\t\t\tm.Close()\n\t\t\t\t})\n\t\t}\n\t}\n\tfor i := range readers {\n\t\tfor j := range readers[i] {\n\t\t\treaders[i][j].Close()\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "metamorphic",
          "type": "tree",
          "content": null
        },
        {
          "name": "metrics.go",
          "type": "blob",
          "size": 26.591796875,
          "content": "// Copyright 2019 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/cache\"\n\t\"github.com/cockroachdb/pebble/internal/humanize\"\n\t\"github.com/cockroachdb/pebble/internal/manual\"\n\t\"github.com/cockroachdb/pebble/objstorage/objstorageprovider/sharedcache\"\n\t\"github.com/cockroachdb/pebble/record\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n\t\"github.com/cockroachdb/pebble/wal\"\n\t\"github.com/cockroachdb/redact\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n)\n\n// CacheMetrics holds metrics for the block and file cache.\ntype CacheMetrics = cache.Metrics\n\n// FilterMetrics holds metrics for the filter policy\ntype FilterMetrics = sstable.FilterMetrics\n\n// ThroughputMetric is a cumulative throughput metric. See the detailed\n// comment in base.\ntype ThroughputMetric = base.ThroughputMetric\n\n// SecondaryCacheMetrics holds metrics for the persistent secondary cache\n// that caches commonly accessed blocks from blob storage on a local\n// file system.\ntype SecondaryCacheMetrics = sharedcache.Metrics\n\n// LevelMetrics holds per-level metrics such as the number of files and total\n// size of the files, and compaction related metrics.\ntype LevelMetrics struct {\n\t// The number of sublevels within the level. The sublevel count corresponds\n\t// to the read amplification for the level. An empty level will have a\n\t// sublevel count of 0, implying no read amplification. Only L0 will have\n\t// a sublevel count other than 0 or 1.\n\tSublevels int32\n\t// The total number of files in the level.\n\tNumFiles int64\n\t// The total number of virtual sstables in the level.\n\tNumVirtualFiles uint64\n\t// The total size in bytes of the files in the level.\n\tSize int64\n\t// The total size of the virtual sstables in the level.\n\tVirtualSize uint64\n\t// The level's compaction score. This is the compensatedScoreRatio in the\n\t// candidateLevelInfo.\n\tScore float64\n\t// The number of incoming bytes from other levels read during\n\t// compactions. This excludes bytes moved and bytes ingested. For L0 this is\n\t// the bytes written to the WAL.\n\tBytesIn uint64\n\t// The number of bytes ingested. The sibling metric for tables is\n\t// TablesIngested.\n\tBytesIngested uint64\n\t// The number of bytes moved into the level by a \"move\" compaction. The\n\t// sibling metric for tables is TablesMoved.\n\tBytesMoved uint64\n\t// The number of bytes read for compactions at the level. This includes bytes\n\t// read from other levels (BytesIn), as well as bytes read for the level.\n\tBytesRead uint64\n\t// The number of bytes written during compactions. The sibling\n\t// metric for tables is TablesCompacted. This metric may be summed\n\t// with BytesFlushed to compute the total bytes written for the level.\n\tBytesCompacted uint64\n\t// The number of bytes written during flushes. The sibling\n\t// metrics for tables is TablesFlushed. This metric is always\n\t// zero for all levels other than L0.\n\tBytesFlushed uint64\n\t// The number of sstables compacted to this level.\n\tTablesCompacted uint64\n\t// The number of sstables flushed to this level.\n\tTablesFlushed uint64\n\t// The number of sstables ingested into the level.\n\tTablesIngested uint64\n\t// The number of sstables moved to this level by a \"move\" compaction.\n\tTablesMoved uint64\n\t// The number of sstables deleted in a level by a delete-only compaction.\n\tTablesDeleted uint64\n\t// The number of sstables excised in a level by a delete-only compaction.\n\tTablesExcised uint64\n\n\tMultiLevel struct {\n\t\t// BytesInTop are the total bytes in a multilevel compaction coming from the top level.\n\t\tBytesInTop uint64\n\n\t\t// BytesIn, exclusively for multiLevel compactions.\n\t\tBytesIn uint64\n\n\t\t// BytesRead, exclusively for multilevel compactions.\n\t\tBytesRead uint64\n\t}\n\n\t// Additional contains misc additional metrics that are not always printed.\n\tAdditional struct {\n\t\t// The sum of Properties.ValueBlocksSize for all the sstables in this\n\t\t// level. Printed by LevelMetrics.format iff there is at least one level\n\t\t// with a non-zero value.\n\t\tValueBlocksSize uint64\n\t\t// Cumulative metrics about bytes written to data blocks and value blocks,\n\t\t// via compactions (except move compactions) or flushes. Not printed by\n\t\t// LevelMetrics.format, but are available to sophisticated clients.\n\t\tBytesWrittenDataBlocks  uint64\n\t\tBytesWrittenValueBlocks uint64\n\t}\n}\n\n// Add updates the counter metrics for the level.\nfunc (m *LevelMetrics) Add(u *LevelMetrics) {\n\tm.NumFiles += u.NumFiles\n\tm.NumVirtualFiles += u.NumVirtualFiles\n\tm.VirtualSize += u.VirtualSize\n\tm.Size += u.Size\n\tm.BytesIn += u.BytesIn\n\tm.BytesIngested += u.BytesIngested\n\tm.BytesMoved += u.BytesMoved\n\tm.BytesRead += u.BytesRead\n\tm.BytesCompacted += u.BytesCompacted\n\tm.BytesFlushed += u.BytesFlushed\n\tm.TablesCompacted += u.TablesCompacted\n\tm.TablesFlushed += u.TablesFlushed\n\tm.TablesIngested += u.TablesIngested\n\tm.TablesMoved += u.TablesMoved\n\tm.MultiLevel.BytesInTop += u.MultiLevel.BytesInTop\n\tm.MultiLevel.BytesRead += u.MultiLevel.BytesRead\n\tm.MultiLevel.BytesIn += u.MultiLevel.BytesIn\n\tm.Additional.BytesWrittenDataBlocks += u.Additional.BytesWrittenDataBlocks\n\tm.Additional.BytesWrittenValueBlocks += u.Additional.BytesWrittenValueBlocks\n\tm.Additional.ValueBlocksSize += u.Additional.ValueBlocksSize\n}\n\n// WriteAmp computes the write amplification for compactions at this\n// level. Computed as (BytesFlushed + BytesCompacted) / BytesIn.\nfunc (m *LevelMetrics) WriteAmp() float64 {\n\tif m.BytesIn == 0 {\n\t\treturn 0\n\t}\n\treturn float64(m.BytesFlushed+m.BytesCompacted) / float64(m.BytesIn)\n}\n\nvar categoryCompaction = sstable.RegisterCategory(\"pebble-compaction\", sstable.NonLatencySensitiveQoSLevel)\nvar categoryIngest = sstable.RegisterCategory(\"pebble-ingest\", sstable.LatencySensitiveQoSLevel)\nvar categoryGet = sstable.RegisterCategory(\"pebble-get\", sstable.LatencySensitiveQoSLevel)\n\n// Metrics holds metrics for various subsystems of the DB such as the Cache,\n// Compactions, WAL, and per-Level metrics.\n//\n// TODO(peter): The testing of these metrics is relatively weak. There should\n// be testing that performs various operations on a DB and verifies that the\n// metrics reflect those operations.\ntype Metrics struct {\n\tBlockCache CacheMetrics\n\n\tCompact struct {\n\t\t// The total number of compactions, and per-compaction type counts.\n\t\tCount                 int64\n\t\tDefaultCount          int64\n\t\tDeleteOnlyCount       int64\n\t\tElisionOnlyCount      int64\n\t\tCopyCount             int64\n\t\tMoveCount             int64\n\t\tReadCount             int64\n\t\tTombstoneDensityCount int64\n\t\tRewriteCount          int64\n\t\tMultiLevelCount       int64\n\t\tCounterLevelCount     int64\n\t\t// An estimate of the number of bytes that need to be compacted for the LSM\n\t\t// to reach a stable state.\n\t\tEstimatedDebt uint64\n\t\t// Number of bytes present in sstables being written by in-progress\n\t\t// compactions. This value will be zero if there are no in-progress\n\t\t// compactions.\n\t\tInProgressBytes int64\n\t\t// Number of compactions that are in-progress.\n\t\tNumInProgress int64\n\t\t// Number of compactions that were cancelled.\n\t\tCancelledCount int64\n\t\t// CancelledBytes the number of bytes written by compactions that were\n\t\t// cancelled.\n\t\tCancelledBytes int64\n\t\t// MarkedFiles is a count of files that are marked for\n\t\t// compaction. Such files are compacted in a rewrite compaction\n\t\t// when no other compactions are picked.\n\t\tMarkedFiles int\n\t\t// Duration records the cumulative duration of all compactions since the\n\t\t// database was opened.\n\t\tDuration time.Duration\n\t}\n\n\tIngest struct {\n\t\t// The total number of ingestions\n\t\tCount uint64\n\t}\n\n\tFlush struct {\n\t\t// The total number of flushes.\n\t\tCount           int64\n\t\tWriteThroughput ThroughputMetric\n\t\t// Number of flushes that are in-progress. In the current implementation\n\t\t// this will always be zero or one.\n\t\tNumInProgress int64\n\t\t// AsIngestCount is a monotonically increasing counter of flush operations\n\t\t// handling ingested tables.\n\t\tAsIngestCount uint64\n\t\t// AsIngestCount is a monotonically increasing counter of tables ingested as\n\t\t// flushables.\n\t\tAsIngestTableCount uint64\n\t\t// AsIngestBytes is a monotonically increasing counter of the bytes flushed\n\t\t// for flushables that originated as ingestion operations.\n\t\tAsIngestBytes uint64\n\t}\n\n\tFilter FilterMetrics\n\n\tLevels [numLevels]LevelMetrics\n\n\tMemTable struct {\n\t\t// The number of bytes allocated by memtables and large (flushable)\n\t\t// batches.\n\t\tSize uint64\n\t\t// The count of memtables.\n\t\tCount int64\n\t\t// The number of bytes present in zombie memtables which are no longer\n\t\t// referenced by the current DB state. An unbounded number of memtables\n\t\t// may be zombie if they're still in use by an iterator. One additional\n\t\t// memtable may be zombie if it's no longer in use and waiting to be\n\t\t// recycled.\n\t\tZombieSize uint64\n\t\t// The count of zombie memtables.\n\t\tZombieCount int64\n\t}\n\n\tKeys struct {\n\t\t// The approximate count of internal range key set keys in the database.\n\t\tRangeKeySetsCount uint64\n\t\t// The approximate count of internal tombstones (DEL, SINGLEDEL and\n\t\t// RANGEDEL key kinds) within the database.\n\t\tTombstoneCount uint64\n\t\t// A cumulative total number of missized DELSIZED keys encountered by\n\t\t// compactions since the database was opened.\n\t\tMissizedTombstonesCount uint64\n\t}\n\n\tSnapshots struct {\n\t\t// The number of currently open snapshots.\n\t\tCount int\n\t\t// The sequence number of the earliest, currently open snapshot.\n\t\tEarliestSeqNum base.SeqNum\n\t\t// A running tally of keys written to sstables during flushes or\n\t\t// compactions that would've been elided if it weren't for open\n\t\t// snapshots.\n\t\tPinnedKeys uint64\n\t\t// A running cumulative sum of the size of keys and values written to\n\t\t// sstables during flushes or compactions that would've been elided if\n\t\t// it weren't for open snapshots.\n\t\tPinnedSize uint64\n\t}\n\n\tTable struct {\n\t\t// The number of bytes present in obsolete tables which are no longer\n\t\t// referenced by the current DB state or any open iterators.\n\t\tObsoleteSize uint64\n\t\t// The count of obsolete tables.\n\t\tObsoleteCount int64\n\t\t// The number of bytes present in zombie tables which are no longer\n\t\t// referenced by the current DB state but are still in use by an iterator.\n\t\tZombieSize uint64\n\t\t// The count of zombie tables.\n\t\tZombieCount int64\n\t\t// The count of sstables backing virtual tables.\n\t\tBackingTableCount uint64\n\t\t// The sum of the sizes of the BackingTableCount sstables that are backing virtual tables.\n\t\tBackingTableSize uint64\n\t\t// The number of sstables that are compressed with an unknown compression\n\t\t// algorithm.\n\t\tCompressedCountUnknown int64\n\t\t// The number of sstables that are compressed with the default compression\n\t\t// algorithm, snappy.\n\t\tCompressedCountSnappy int64\n\t\t// The number of sstables that are compressed with zstd.\n\t\tCompressedCountZstd int64\n\t\t// The number of sstables that are uncompressed.\n\t\tCompressedCountNone int64\n\n\t\t// Local file sizes.\n\t\tLocal struct {\n\t\t\t// LiveSize is the number of bytes in live tables.\n\t\t\tLiveSize uint64\n\t\t\t// ObsoleteSize is the number of bytes in obsolete tables.\n\t\t\tObsoleteSize uint64\n\t\t\t// ZombieSize is the number of bytes in zombie tables.\n\t\t\tZombieSize uint64\n\t\t}\n\t}\n\n\tFileCache CacheMetrics\n\n\t// Count of the number of open sstable iterators.\n\tTableIters int64\n\t// Uptime is the total time since this DB was opened.\n\tUptime time.Duration\n\n\tWAL struct {\n\t\t// Number of live WAL files.\n\t\tFiles int64\n\t\t// Number of obsolete WAL files.\n\t\tObsoleteFiles int64\n\t\t// Physical size of the obsolete WAL files.\n\t\tObsoletePhysicalSize uint64\n\t\t// Size of the live data in the WAL files. Note that with WAL file\n\t\t// recycling this is less than the actual on-disk size of the WAL files.\n\t\tSize uint64\n\t\t// Physical size of the WAL files on-disk. With WAL file recycling,\n\t\t// this is greater than the live data in WAL files.\n\t\t//\n\t\t// TODO(sumeer): it seems this does not include ObsoletePhysicalSize.\n\t\t// Should the comment be updated?\n\t\tPhysicalSize uint64\n\t\t// Number of logical bytes written to the WAL.\n\t\tBytesIn uint64\n\t\t// Number of bytes written to the WAL.\n\t\tBytesWritten uint64\n\t\t// Failover contains failover stats. Empty if failover is not enabled.\n\t\tFailover wal.FailoverStats\n\t}\n\n\tLogWriter struct {\n\t\tFsyncLatency prometheus.Histogram\n\t\trecord.LogWriterMetrics\n\t}\n\n\tCategoryStats []sstable.CategoryStatsAggregate\n\n\tSecondaryCacheMetrics SecondaryCacheMetrics\n\n\tprivate struct {\n\t\toptionsFileSize  uint64\n\t\tmanifestFileSize uint64\n\t}\n\n\tmanualMemory manual.Metrics\n}\n\nvar (\n\t// FsyncLatencyBuckets are prometheus histogram buckets suitable for a histogram\n\t// that records latencies for fsyncs.\n\tFsyncLatencyBuckets = append(\n\t\tprometheus.LinearBuckets(0.0, float64(time.Microsecond*100), 50),\n\t\tprometheus.ExponentialBucketsRange(float64(time.Millisecond*5), float64(10*time.Second), 50)...,\n\t)\n\n\t// SecondaryCacheIOBuckets exported to enable exporting from package pebble to\n\t// enable exporting metrics with below buckets in CRDB.\n\tSecondaryCacheIOBuckets = sharedcache.IOBuckets\n\t// SecondaryCacheChannelWriteBuckets exported to enable exporting from package\n\t// pebble to enable exporting metrics with below buckets in CRDB.\n\tSecondaryCacheChannelWriteBuckets = sharedcache.ChannelWriteBuckets\n)\n\n// DiskSpaceUsage returns the total disk space used by the database in bytes,\n// including live and obsolete files. This only includes local files, i.e.,\n// remote files (as known to objstorage.Provider) are not included.\nfunc (m *Metrics) DiskSpaceUsage() uint64 {\n\tvar usageBytes uint64\n\tusageBytes += m.WAL.PhysicalSize\n\tusageBytes += m.WAL.ObsoletePhysicalSize\n\tusageBytes += m.Table.Local.LiveSize\n\tusageBytes += m.Table.Local.ObsoleteSize\n\tusageBytes += m.Table.Local.ZombieSize\n\tusageBytes += m.private.optionsFileSize\n\tusageBytes += m.private.manifestFileSize\n\t// TODO(sumeer): InProgressBytes does not distinguish between local and\n\t// remote files. This causes a small error. Fix.\n\tusageBytes += uint64(m.Compact.InProgressBytes)\n\treturn usageBytes\n}\n\n// NumVirtual is the number of virtual sstables in the latest version\n// summed over every level in the lsm.\nfunc (m *Metrics) NumVirtual() uint64 {\n\tvar n uint64\n\tfor _, level := range m.Levels {\n\t\tn += level.NumVirtualFiles\n\t}\n\treturn n\n}\n\n// VirtualSize is the sum of the sizes of the virtual sstables in the\n// latest version. BackingTableSize - VirtualSize gives an estimate for\n// the space amplification caused by not compacting virtual sstables.\nfunc (m *Metrics) VirtualSize() uint64 {\n\tvar size uint64\n\tfor _, level := range m.Levels {\n\t\tsize += level.VirtualSize\n\t}\n\treturn size\n}\n\n// ReadAmp returns the current read amplification of the database.\n// It's computed as the number of sublevels in L0 + the number of non-empty\n// levels below L0.\nfunc (m *Metrics) ReadAmp() int {\n\tvar ramp int32\n\tfor _, l := range m.Levels {\n\t\tramp += l.Sublevels\n\t}\n\treturn int(ramp)\n}\n\n// Total returns the sum of the per-level metrics and WAL metrics.\nfunc (m *Metrics) Total() LevelMetrics {\n\tvar total LevelMetrics\n\tfor level := 0; level < numLevels; level++ {\n\t\tl := &m.Levels[level]\n\t\ttotal.Add(l)\n\t\ttotal.Sublevels += l.Sublevels\n\t}\n\t// Compute total bytes-in as the bytes written to the WAL + bytes ingested.\n\ttotal.BytesIn = m.WAL.BytesWritten + total.BytesIngested\n\t// Add the total bytes-in to the total bytes-flushed. This is to account for\n\t// the bytes written to the log and bytes written externally and then\n\t// ingested.\n\ttotal.BytesFlushed += total.BytesIn\n\treturn total\n}\n\n// String pretty-prints the metrics as below:\n//\n//\t      |                             |       |       |   ingested   |     moved    |    written   |       |    amp\n//\tlevel | tables  size val-bl vtables | score |   in  | tables  size | tables  size | tables  size |  read |   r   w\n//\t------+-----------------------------+-------+-------+--------------+--------------+--------------+-------+---------\n//\t    0 |   101   102B     0B       0 | 103.0 |  104B |   112   104B |   113   106B |   221   217B |  107B |   1  2.1\n//\t    1 |   201   202B     0B       0 | 203.0 |  204B |   212   204B |   213   206B |   421   417B |  207B |   2  2.0\n//\t    2 |   301   302B     0B       0 | 303.0 |  304B |   312   304B |   313   306B |   621   617B |  307B |   3  2.0\n//\t    3 |   401   402B     0B       0 | 403.0 |  404B |   412   404B |   413   406B |   821   817B |  407B |   4  2.0\n//\t    4 |   501   502B     0B       0 | 503.0 |  504B |   512   504B |   513   506B |  1.0K  1017B |  507B |   5  2.0\n//\t    5 |   601   602B     0B       0 | 603.0 |  604B |   612   604B |   613   606B |  1.2K  1.2KB |  607B |   6  2.0\n//\t    6 |   701   702B     0B       0 |     - |  704B |   712   704B |   713   706B |  1.4K  1.4KB |  707B |   7  2.0\n//\ttotal |  2.8K  2.7KB     0B       0 |     - | 2.8KB |  2.9K  2.8KB |  2.9K  2.8KB |  5.7K  8.4KB | 2.8KB |  28  3.0\n//\t-------------------------------------------------------------------------------------------------------------------\n//\tWAL: 22 files (24B)  in: 25B  written: 26B (4% overhead)\n//\tFlushes: 8\n//\tCompactions: 5  estimated debt: 6B  in progress: 2 (7B)\n//\tdefault: 27  delete: 28  elision: 29  move: 30  read: 31  rewrite: 32  multi-level: 33\n//\tMemTables: 12 (11B)  zombie: 14 (13B)\n//\tZombie tables: 16 (15B)\n//\tBacking tables: 0 (0B)\n//\tBlock cache: 2 entries (1B)  hit rate: 42.9%\n//\tTable cache: 18 entries (17B)  hit rate: 48.7%\n//\tSecondary cache: 40 entries (40B)  hit rate: 49.9%\n//\tSnapshots: 4  earliest seq num: 1024\n//\tTable iters: 21\n//\tFilter utility: 47.4%\n//\tIngestions: 27  as flushable: 36 (34B in 35 tables)\nfunc (m *Metrics) String() string {\n\treturn redact.StringWithoutMarkers(m)\n}\n\nvar _ redact.SafeFormatter = &Metrics{}\n\n// SafeFormat implements redact.SafeFormatter.\nfunc (m *Metrics) SafeFormat(w redact.SafePrinter, _ rune) {\n\t// NB: Pebble does not make any assumptions as to which Go primitive types\n\t// have been registered as safe with redact.RegisterSafeType and does not\n\t// register any types itself. Some of the calls to `redact.Safe`, etc are\n\t// superfluous in the context of CockroachDB, which registers all the Go\n\t// numeric types as safe.\n\n\t// TODO(jackson): There are a few places where we use redact.SafeValue\n\t// instead of redact.RedactableString. This is necessary because of a bug\n\t// whereby formatting a redact.RedactableString argument does not respect\n\t// width specifiers. When the issue is fixed, we can convert these to\n\t// RedactableStrings. https://github.com/cockroachdb/redact/issues/17\n\n\tmultiExists := m.Compact.MultiLevelCount > 0\n\tappendIfMulti := func(line redact.SafeString) {\n\t\tif multiExists {\n\t\t\tw.SafeString(line)\n\t\t}\n\t}\n\tnewline := func() {\n\t\tw.SafeString(\"\\n\")\n\t}\n\n\tw.SafeString(\"      |                             |       |       |   ingested   |     moved    |    written   |       |    amp\")\n\tappendIfMulti(\"   |     multilevel\")\n\tnewline()\n\tw.SafeString(\"level | tables  size val-bl vtables | score |   in  | tables  size | tables  size | tables  size |  read |   r   w\")\n\tappendIfMulti(\"  |    top   in  read\")\n\tnewline()\n\tw.SafeString(\"------+-----------------------------+-------+-------+--------------+--------------+--------------+-------+---------\")\n\tappendIfMulti(\"-+------------------\")\n\tnewline()\n\n\t// formatRow prints out a row of the table.\n\tformatRow := func(m *LevelMetrics, score float64) {\n\t\tscoreStr := \"-\"\n\t\tif !math.IsNaN(score) {\n\t\t\t// Try to keep the string no longer than 5 characters.\n\t\t\tswitch {\n\t\t\tcase score < 99.995:\n\t\t\t\tscoreStr = fmt.Sprintf(\"%.2f\", score)\n\t\t\tcase score < 999.95:\n\t\t\t\tscoreStr = fmt.Sprintf(\"%.1f\", score)\n\t\t\tdefault:\n\t\t\t\tscoreStr = fmt.Sprintf(\"%.0f\", score)\n\t\t\t}\n\t\t}\n\t\tvar wampStr string\n\t\tif wamp := m.WriteAmp(); wamp > 99.5 {\n\t\t\twampStr = fmt.Sprintf(\"%.0f\", wamp)\n\t\t} else {\n\t\t\twampStr = fmt.Sprintf(\"%.1f\", wamp)\n\t\t}\n\n\t\tw.Printf(\"| %5s %6s %6s %7s | %5s | %5s | %5s %6s | %5s %6s | %5s %6s | %5s | %3d %4s\",\n\t\t\thumanize.Count.Int64(m.NumFiles),\n\t\t\thumanize.Bytes.Int64(m.Size),\n\t\t\thumanize.Bytes.Uint64(m.Additional.ValueBlocksSize),\n\t\t\thumanize.Count.Uint64(m.NumVirtualFiles),\n\t\t\tredact.Safe(scoreStr),\n\t\t\thumanize.Bytes.Uint64(m.BytesIn),\n\t\t\thumanize.Count.Uint64(m.TablesIngested),\n\t\t\thumanize.Bytes.Uint64(m.BytesIngested),\n\t\t\thumanize.Count.Uint64(m.TablesMoved),\n\t\t\thumanize.Bytes.Uint64(m.BytesMoved),\n\t\t\thumanize.Count.Uint64(m.TablesFlushed+m.TablesCompacted),\n\t\t\thumanize.Bytes.Uint64(m.BytesFlushed+m.BytesCompacted),\n\t\t\thumanize.Bytes.Uint64(m.BytesRead),\n\t\t\tredact.Safe(m.Sublevels),\n\t\t\tredact.Safe(wampStr))\n\n\t\tif multiExists {\n\t\t\tw.Printf(\" | %5s %5s %5s\",\n\t\t\t\thumanize.Bytes.Uint64(m.MultiLevel.BytesInTop),\n\t\t\t\thumanize.Bytes.Uint64(m.MultiLevel.BytesIn),\n\t\t\t\thumanize.Bytes.Uint64(m.MultiLevel.BytesRead))\n\t\t}\n\t\tnewline()\n\t}\n\n\tvar total LevelMetrics\n\tfor level := 0; level < numLevels; level++ {\n\t\tl := &m.Levels[level]\n\t\tw.Printf(\"%5d \", redact.Safe(level))\n\n\t\t// Format the score.\n\t\tscore := math.NaN()\n\t\tif level < numLevels-1 {\n\t\t\tscore = l.Score\n\t\t}\n\t\tformatRow(l, score)\n\t\ttotal.Add(l)\n\t\ttotal.Sublevels += l.Sublevels\n\t}\n\t// Compute total bytes-in as the bytes written to the WAL + bytes ingested.\n\ttotal.BytesIn = m.WAL.BytesWritten + total.BytesIngested\n\t// Add the total bytes-in to the total bytes-flushed. This is to account for\n\t// the bytes written to the log and bytes written externally and then\n\t// ingested.\n\ttotal.BytesFlushed += total.BytesIn\n\tw.SafeString(\"total \")\n\tformatRow(&total, math.NaN())\n\n\tw.SafeString(\"-------------------------------------------------------------------------------------------------------------------\")\n\tappendIfMulti(\"--------------------\")\n\tnewline()\n\tw.Printf(\"WAL: %d files (%s)  in: %s  written: %s (%.0f%% overhead)\",\n\t\tredact.Safe(m.WAL.Files),\n\t\thumanize.Bytes.Uint64(m.WAL.Size),\n\t\thumanize.Bytes.Uint64(m.WAL.BytesIn),\n\t\thumanize.Bytes.Uint64(m.WAL.BytesWritten),\n\t\tredact.Safe(percent(int64(m.WAL.BytesWritten)-int64(m.WAL.BytesIn), int64(m.WAL.BytesIn))))\n\tfailoverStats := m.WAL.Failover\n\tfailoverStats.FailoverWriteAndSyncLatency = nil\n\tif failoverStats == (wal.FailoverStats{}) {\n\t\tw.Printf(\"\\n\")\n\t} else {\n\t\tw.Printf(\" failover: (switches: %d, primary: %s, secondary: %s)\\n\", m.WAL.Failover.DirSwitchCount,\n\t\t\tm.WAL.Failover.PrimaryWriteDuration.String(), m.WAL.Failover.SecondaryWriteDuration.String())\n\t}\n\n\tw.Printf(\"Flushes: %d\\n\", redact.Safe(m.Flush.Count))\n\n\tw.Printf(\"Compactions: %d  estimated debt: %s  in progress: %d (%s)\\n\",\n\t\tredact.Safe(m.Compact.Count),\n\t\thumanize.Bytes.Uint64(m.Compact.EstimatedDebt),\n\t\tredact.Safe(m.Compact.NumInProgress),\n\t\thumanize.Bytes.Int64(m.Compact.InProgressBytes))\n\n\tw.Printf(\"             default: %d  delete: %d  elision: %d  move: %d  read: %d  tombstone-density: %d  rewrite: %d  copy: %d  multi-level: %d\\n\",\n\t\tredact.Safe(m.Compact.DefaultCount),\n\t\tredact.Safe(m.Compact.DeleteOnlyCount),\n\t\tredact.Safe(m.Compact.ElisionOnlyCount),\n\t\tredact.Safe(m.Compact.MoveCount),\n\t\tredact.Safe(m.Compact.ReadCount),\n\t\tredact.Safe(m.Compact.TombstoneDensityCount),\n\t\tredact.Safe(m.Compact.RewriteCount),\n\t\tredact.Safe(m.Compact.CopyCount),\n\t\tredact.Safe(m.Compact.MultiLevelCount))\n\n\tw.Printf(\"MemTables: %d (%s)  zombie: %d (%s)\\n\",\n\t\tredact.Safe(m.MemTable.Count),\n\t\thumanize.Bytes.Uint64(m.MemTable.Size),\n\t\tredact.Safe(m.MemTable.ZombieCount),\n\t\thumanize.Bytes.Uint64(m.MemTable.ZombieSize))\n\n\tw.Printf(\"Zombie tables: %d (%s, local: %s)\\n\",\n\t\tredact.Safe(m.Table.ZombieCount),\n\t\thumanize.Bytes.Uint64(m.Table.ZombieSize),\n\t\thumanize.Bytes.Uint64(m.Table.Local.ZombieSize))\n\n\tw.Printf(\"Backing tables: %d (%s)\\n\",\n\t\tredact.Safe(m.Table.BackingTableCount),\n\t\thumanize.Bytes.Uint64(m.Table.BackingTableSize))\n\tw.Printf(\"Virtual tables: %d (%s)\\n\",\n\t\tredact.Safe(m.NumVirtual()),\n\t\thumanize.Bytes.Uint64(m.VirtualSize()))\n\tw.Printf(\"Local tables size: %s\\n\", humanize.Bytes.Uint64(m.Table.Local.LiveSize))\n\tw.SafeString(\"Compression types:\")\n\tif count := m.Table.CompressedCountSnappy; count > 0 {\n\t\tw.Printf(\" snappy: %d\", redact.Safe(count))\n\t}\n\tif count := m.Table.CompressedCountZstd; count > 0 {\n\t\tw.Printf(\" zstd: %d\", redact.Safe(count))\n\t}\n\tif count := m.Table.CompressedCountNone; count > 0 {\n\t\tw.Printf(\" none: %d\", redact.Safe(count))\n\t}\n\tif count := m.Table.CompressedCountUnknown; count > 0 {\n\t\tw.Printf(\" unknown: %d\", redact.Safe(count))\n\t}\n\tw.Print(\"\\n\")\n\n\tformatCacheMetrics := func(m *CacheMetrics, name redact.SafeString) {\n\t\tw.Printf(\"%s: %s entries (%s)  hit rate: %.1f%%\\n\",\n\t\t\tname,\n\t\t\thumanize.Count.Int64(m.Count),\n\t\t\thumanize.Bytes.Int64(m.Size),\n\t\t\tredact.Safe(hitRate(m.Hits, m.Misses)))\n\t}\n\tformatCacheMetrics(&m.BlockCache, \"Block cache\")\n\tformatCacheMetrics(&m.FileCache, \"Table cache\")\n\n\tformatSharedCacheMetrics := func(w redact.SafePrinter, m *SecondaryCacheMetrics, name redact.SafeString) {\n\t\tw.Printf(\"%s: %s entries (%s)  hit rate: %.1f%%\\n\",\n\t\t\tname,\n\t\t\thumanize.Count.Int64(m.Count),\n\t\t\thumanize.Bytes.Int64(m.Size),\n\t\t\tredact.Safe(hitRate(m.ReadsWithFullHit, m.ReadsWithPartialHit+m.ReadsWithNoHit)))\n\t}\n\tif m.SecondaryCacheMetrics.Size > 0 || m.SecondaryCacheMetrics.ReadsWithFullHit > 0 {\n\t\tformatSharedCacheMetrics(w, &m.SecondaryCacheMetrics, \"Secondary cache\")\n\t}\n\n\tw.Printf(\"Snapshots: %d  earliest seq num: %d\\n\",\n\t\tredact.Safe(m.Snapshots.Count),\n\t\tredact.Safe(m.Snapshots.EarliestSeqNum))\n\n\tw.Printf(\"Table iters: %d\\n\", redact.Safe(m.TableIters))\n\tw.Printf(\"Filter utility: %.1f%%\\n\", redact.Safe(hitRate(m.Filter.Hits, m.Filter.Misses)))\n\tw.Printf(\"Ingestions: %d  as flushable: %d (%s in %d tables)\\n\",\n\t\tredact.Safe(m.Ingest.Count),\n\t\tredact.Safe(m.Flush.AsIngestCount),\n\t\thumanize.Bytes.Uint64(m.Flush.AsIngestBytes),\n\t\tredact.Safe(m.Flush.AsIngestTableCount))\n\n\tvar inUseTotal uint64\n\tfor i := range m.manualMemory {\n\t\tinUseTotal += m.manualMemory[i].InUseBytes\n\t}\n\tinUse := func(purpose manual.Purpose) uint64 {\n\t\treturn m.manualMemory[purpose].InUseBytes\n\t}\n\tw.Printf(\"Cgo memory usage: %s  block cache: %s (data: %s, maps: %s, entries: %s)  memtables: %s\\n\",\n\t\thumanize.Bytes.Uint64(inUseTotal),\n\t\thumanize.Bytes.Uint64(inUse(manual.BlockCacheData)+inUse(manual.BlockCacheMap)+inUse(manual.BlockCacheEntry)),\n\t\thumanize.Bytes.Uint64(inUse(manual.BlockCacheData)),\n\t\thumanize.Bytes.Uint64(inUse(manual.BlockCacheMap)),\n\t\thumanize.Bytes.Uint64(inUse(manual.BlockCacheEntry)),\n\t\thumanize.Bytes.Uint64(inUse(manual.MemTable)),\n\t)\n}\n\nfunc hitRate(hits, misses int64) float64 {\n\treturn percent(hits, hits+misses)\n}\n\nfunc percent(numerator, denominator int64) float64 {\n\tif denominator == 0 {\n\t\treturn 0\n\t}\n\treturn 100 * float64(numerator) / float64(denominator)\n}\n\n// StringForTests is identical to m.String() on 64-bit platforms. It is used to\n// provide a platform-independent result for tests.\nfunc (m *Metrics) StringForTests() string {\n\tmCopy := *m\n\tif math.MaxInt == math.MaxInt32 {\n\t\t// This is the difference in Sizeof(sstable.Reader{})) between 64 and 32 bit\n\t\t// platforms.\n\t\tconst tableCacheSizeAdjustment = 212\n\t\tmCopy.FileCache.Size += mCopy.FileCache.Count * tableCacheSizeAdjustment\n\t}\n\t// Don't show cgo memory statistics as they can vary based on architecture,\n\t// invariants tag, etc.\n\tmCopy.manualMemory = manual.Metrics{}\n\treturn redact.StringWithoutMarkers(&mCopy)\n}\n"
        },
        {
          "name": "metrics_test.go",
          "type": "blob",
          "size": 14.1494140625,
          "content": "// Copyright 2019 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"math/rand/v2\"\n\t\"runtime\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/datadriven\"\n\t\"github.com/cockroachdb/pebble/internal/cache\"\n\t\"github.com/cockroachdb/pebble/internal/humanize\"\n\t\"github.com/cockroachdb/pebble/internal/manual\"\n\t\"github.com/cockroachdb/pebble/internal/testkeys\"\n\t\"github.com/cockroachdb/pebble/objstorage/remote\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/cockroachdb/pebble/vfs/errorfs\"\n\t\"github.com/cockroachdb/redact\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc exampleMetrics() Metrics {\n\tvar m Metrics\n\tm.BlockCache.Size = 1\n\tm.BlockCache.Count = 2\n\tm.BlockCache.Hits = 3\n\tm.BlockCache.Misses = 4\n\tm.Compact.Count = 5\n\tm.Compact.DefaultCount = 27\n\tm.Compact.DeleteOnlyCount = 28\n\tm.Compact.ElisionOnlyCount = 29\n\tm.Compact.MoveCount = 30\n\tm.Compact.ReadCount = 31\n\tm.Compact.TombstoneDensityCount = 16\n\tm.Compact.RewriteCount = 32\n\tm.Compact.CopyCount = 33\n\tm.Compact.MultiLevelCount = 34\n\tm.Compact.EstimatedDebt = 6\n\tm.Compact.InProgressBytes = 7\n\tm.Compact.NumInProgress = 2\n\tm.Flush.Count = 8\n\tm.Flush.AsIngestBytes = 34\n\tm.Flush.AsIngestTableCount = 35\n\tm.Flush.AsIngestCount = 36\n\tm.Filter.Hits = 9\n\tm.Filter.Misses = 10\n\tm.MemTable.Size = 11\n\tm.MemTable.Count = 12\n\tm.MemTable.ZombieSize = 13\n\tm.MemTable.ZombieCount = 14\n\tm.Snapshots.Count = 4\n\tm.Snapshots.EarliestSeqNum = 1024\n\tm.Table.ZombieSize = 15\n\tm.Table.BackingTableCount = 1\n\tm.Table.BackingTableSize = 2 << 20\n\tm.Table.ZombieCount = 16\n\tm.FileCache.Size = 17\n\tm.FileCache.Count = 18\n\tm.FileCache.Hits = 19\n\tm.FileCache.Misses = 20\n\tm.TableIters = 21\n\tm.WAL.Files = 22\n\tm.WAL.ObsoleteFiles = 23\n\tm.WAL.Size = 24\n\tm.WAL.BytesIn = 25\n\tm.WAL.BytesWritten = 26\n\tm.Ingest.Count = 27\n\tm.Table.Local.LiveSize = 28\n\tm.Table.Local.ObsoleteSize = 29\n\tm.Table.Local.ZombieSize = 30\n\n\tfor i := range m.Levels {\n\t\tl := &m.Levels[i]\n\t\tbase := uint64((i + 1) * 100)\n\t\tl.Sublevels = int32(i + 1)\n\t\tl.NumFiles = int64(base) + 1\n\t\tl.NumVirtualFiles = uint64(base) + 1\n\t\tl.VirtualSize = base + 3\n\t\tl.Size = int64(base) + 2\n\t\tl.Score = float64(base) + 3\n\t\tl.BytesIn = base + 4\n\t\tl.BytesIngested = base + 4\n\t\tl.BytesMoved = base + 6\n\t\tl.BytesRead = base + 7\n\t\tl.BytesCompacted = base + 8\n\t\tl.BytesFlushed = base + 9\n\t\tl.TablesCompacted = base + 10\n\t\tl.TablesFlushed = base + 11\n\t\tl.TablesIngested = base + 12\n\t\tl.TablesMoved = base + 13\n\t\tl.MultiLevel.BytesInTop = base + 4\n\t\tl.MultiLevel.BytesIn = base + 4\n\t\tl.MultiLevel.BytesRead = base + 4\n\t}\n\tfor i := range m.manualMemory {\n\t\tm.manualMemory[i].InUseBytes = uint64((i + 1) * 1024)\n\t}\n\treturn m\n}\n\nfunc init() {\n\t// Register some categories for the purposes of the test.\n\tsstable.RegisterCategory(\"a\", sstable.NonLatencySensitiveQoSLevel)\n\tsstable.RegisterCategory(\"b\", sstable.LatencySensitiveQoSLevel)\n\tsstable.RegisterCategory(\"c\", sstable.NonLatencySensitiveQoSLevel)\n}\n\nfunc TestMetrics(t *testing.T) {\n\tif runtime.GOARCH == \"386\" {\n\t\tt.Skip(\"skipped on 32-bit due to slightly varied output\")\n\t}\n\tdefer sstable.DeterministicReadBlockDurationForTesting()()\n\n\tvar d *DB\n\tvar iters map[string]*Iterator\n\tvar closeFunc func()\n\tvar memFS *vfs.MemFS\n\tvar remoteStorage remote.Storage\n\tdefer func() {\n\t\tif closeFunc != nil {\n\t\t\tcloseFunc()\n\t\t}\n\t}()\n\tinit := func(t *testing.T, createOnSharedLower bool, reopen bool) {\n\t\tif closeFunc != nil {\n\t\t\tcloseFunc()\n\t\t}\n\t\tif !reopen {\n\t\t\tmemFS = vfs.NewMem()\n\t\t\tremoteStorage = remote.NewInMem()\n\t\t}\n\t\tc := cache.New(cacheDefaultSize)\n\t\tdefer c.Unref()\n\t\topts := &Options{\n\t\t\tCache:                 c,\n\t\t\tComparer:              testkeys.Comparer,\n\t\t\tFormatMajorVersion:    FormatNewest,\n\t\t\tFS:                    memFS,\n\t\t\tL0CompactionThreshold: 8,\n\t\t\t// Large value for determinism.\n\t\t\tMaxOpenFiles: 10000,\n\t\t}\n\t\topts.Experimental.EnableValueBlocks = func() bool { return true }\n\t\topts.Levels = append(opts.Levels, LevelOptions{TargetFileSize: 50})\n\n\t\t// Prevent foreground flushes and compactions from triggering asynchronous\n\t\t// follow-up compactions. This avoids asynchronously-scheduled work from\n\t\t// interfering with the expected metrics output and reduces test flakiness.\n\t\topts.DisableAutomaticCompactions = true\n\n\t\t// Increase the threshold for memtable stalls to allow for more flushable\n\t\t// ingests.\n\t\topts.MemTableStopWritesThreshold = 4\n\n\t\topts.Experimental.RemoteStorage = remote.MakeSimpleFactory(map[remote.Locator]remote.Storage{\n\t\t\t\"\": remoteStorage,\n\t\t})\n\t\tif createOnSharedLower {\n\t\t\topts.Experimental.CreateOnShared = remote.CreateOnSharedLower\n\t\t} else {\n\t\t\topts.Experimental.CreateOnShared = remote.CreateOnSharedNone\n\t\t}\n\t\tvar err error\n\t\td, err = Open(\"\", opts)\n\t\trequire.NoError(t, err)\n\t\tif createOnSharedLower {\n\t\t\trequire.NoError(t, d.SetCreatorID(1))\n\t\t}\n\t\tif reopen {\n\t\t\t// Stats population is eventually consistent, and happens in the\n\t\t\t// background when a DB is re-opened. To avoid races, wait synchronously\n\t\t\t// for all tables to have their stats fully populated, which requires\n\t\t\t// opening each SST.\n\t\t\td.mu.Lock()\n\t\t\tfor !d.mu.tableStats.loadedInitial {\n\t\t\t\td.mu.tableStats.cond.Wait()\n\t\t\t}\n\t\t\td.mu.Unlock()\n\t\t}\n\t\titers = make(map[string]*Iterator)\n\t\tcloseFunc = func() {\n\t\t\tfor _, i := range iters {\n\t\t\t\trequire.NoError(t, i.Close())\n\t\t\t}\n\t\t\trequire.NoError(t, d.Close())\n\t\t}\n\t}\n\tdatadriven.RunTest(t, \"testdata/metrics\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"init\":\n\t\t\tcreateOnSharedLower := false\n\t\t\tif td.HasArg(\"shared-lower\") {\n\t\t\t\tcreateOnSharedLower = true\n\t\t\t}\n\t\t\treopen := false\n\t\t\tif td.HasArg(\"reopen\") {\n\t\t\t\treopen = true\n\t\t\t}\n\t\t\tinit(t, createOnSharedLower, reopen)\n\t\t\treturn \"\"\n\n\t\tcase \"example\":\n\t\t\tm := exampleMetrics()\n\t\t\tres := m.String()\n\n\t\t\t// Nothing in the metrics should be redacted.\n\t\t\tredacted := string(redact.Sprintf(\"%s\", &m).Redact())\n\t\t\tif redacted != res {\n\t\t\t\ttd.Fatalf(t, \"redacted metrics don't match\\nunredacted:\\n%s\\nredacted:%s\\n\", res, redacted)\n\t\t\t}\n\t\t\treturn res\n\n\t\tcase \"batch\":\n\t\t\tb := d.NewBatch()\n\t\t\tif err := runBatchDefineCmd(td, b); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tb.Commit(nil)\n\t\t\treturn \"\"\n\n\t\tcase \"build\":\n\t\t\tif err := runBuildCmd(td, d, d.opts.FS); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"compact\":\n\t\t\tif err := runCompactCmd(td, d); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\n\t\t\td.mu.Lock()\n\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\td.mu.Unlock()\n\t\t\treturn s\n\n\t\tcase \"delay-flush\":\n\t\t\td.mu.Lock()\n\t\t\tdefer d.mu.Unlock()\n\t\t\tswitch td.Input {\n\t\t\tcase \"enable\":\n\t\t\t\td.mu.compact.flushing = true\n\t\t\tcase \"disable\":\n\t\t\t\td.mu.compact.flushing = false\n\t\t\tdefault:\n\t\t\t\treturn fmt.Sprintf(\"unknown directive %q (expected 'enable'/'disable')\", td.Input)\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"flush\":\n\t\t\tif err := d.Flush(); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\n\t\t\td.mu.Lock()\n\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\td.mu.Unlock()\n\t\t\treturn s\n\n\t\tcase \"ingest\":\n\t\t\tif err := runIngestCmd(td, d, d.opts.FS); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"lsm\":\n\t\t\td.mu.Lock()\n\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\td.mu.Unlock()\n\t\t\treturn s\n\n\t\tcase \"ingest-and-excise\":\n\t\t\tif err := runIngestAndExciseCmd(td, d, d.opts.FS); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"iter-close\":\n\t\t\tif len(td.CmdArgs) != 1 {\n\t\t\t\treturn \"iter-close <name>\"\n\t\t\t}\n\t\t\tname := td.CmdArgs[0].String()\n\t\t\tif iter := iters[name]; iter != nil {\n\t\t\t\tif err := iter.Close(); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\tdelete(iters, name)\n\t\t\t} else {\n\t\t\t\treturn fmt.Sprintf(\"%s: not found\", name)\n\t\t\t}\n\n\t\t\t// The deletion of obsolete files happens asynchronously when an iterator\n\t\t\t// is closed. Wait for the obsolete tables to be deleted.\n\t\t\td.cleanupManager.Wait()\n\t\t\treturn \"\"\n\n\t\tcase \"iter-new\":\n\t\t\tif len(td.CmdArgs) < 1 {\n\t\t\t\treturn \"iter-new <name>\"\n\t\t\t}\n\t\t\tname := td.CmdArgs[0].String()\n\t\t\tif iter := iters[name]; iter != nil {\n\t\t\t\tif err := iter.Close(); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t}\n\t\t\tcategory := sstable.CategoryUnknown\n\t\t\tif td.HasArg(\"category\") {\n\t\t\t\tvar s string\n\t\t\t\ttd.ScanArgs(t, \"category\", &s)\n\t\t\t\tcategory = sstable.StringToCategoryForTesting(s)\n\t\t\t}\n\t\t\titer, _ := d.NewIter(&IterOptions{Category: category})\n\t\t\t// Some iterators (eg. levelIter) do not instantiate the underlying\n\t\t\t// iterator until the first positioning call. Position the iterator\n\t\t\t// so that levelIters will have loaded an sstable.\n\t\t\titer.First()\n\t\t\titers[name] = iter\n\t\t\treturn \"\"\n\n\t\tcase \"metrics\":\n\t\t\t// The asynchronous loading of table stats can change metrics, so\n\t\t\t// wait for all the tables' stats to be loaded.\n\t\t\td.mu.Lock()\n\t\t\td.waitTableStats()\n\t\t\td.mu.Unlock()\n\n\t\t\tm := d.Metrics()\n\t\t\t// Don't show memory usage as that can depend on architecture, invariants\n\t\t\t// tag, etc.\n\t\t\tm.manualMemory = manual.Metrics{}\n\t\t\t// Some subset of cases show non-determinism in cache hits/misses.\n\t\t\tif td.HasArg(\"zero-cache-hits-misses\") {\n\t\t\t\t// Avoid non-determinism.\n\t\t\t\tm.FileCache = cache.Metrics{}\n\t\t\t\tm.BlockCache = cache.Metrics{}\n\t\t\t\t// Empirically, the unknown stats are also non-deterministic.\n\t\t\t\tif len(m.CategoryStats) > 0 && m.CategoryStats[0].Category == sstable.CategoryUnknown {\n\t\t\t\t\tm.CategoryStats[0].CategoryStats = sstable.CategoryStats{}\n\t\t\t\t}\n\t\t\t}\n\t\t\tvar buf strings.Builder\n\t\t\tfmt.Fprintf(&buf, \"%s\", m.StringForTests())\n\t\t\tif len(m.CategoryStats) > 0 {\n\t\t\t\tfmt.Fprintf(&buf, \"Iter category stats:\\n\")\n\t\t\t\tfor _, stats := range m.CategoryStats {\n\t\t\t\t\tfmt.Fprintf(&buf, \"%20s, %11s: %+v\\n\", stats.Category,\n\t\t\t\t\t\tredact.StringWithoutMarkers(stats.Category.QoSLevel()), stats.CategoryStats)\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn buf.String()\n\n\t\tcase \"metrics-value\":\n\t\t\t// metrics-value confirms the value of a given metric. Note that there\n\t\t\t// are some metrics which aren't deterministic and behave differently\n\t\t\t// for invariant/non-invariant builds. An example of this is cache\n\t\t\t// hit rates. Under invariant builds, the excising code will try\n\t\t\t// to create iterators and confirm that the virtual sstable bounds\n\t\t\t// are accurate. Reads on these iterators will change the cache hit\n\t\t\t// rates.\n\t\t\tlines := strings.Split(td.Input, \"\\n\")\n\t\t\tm := d.Metrics()\n\t\t\t// TODO(bananabrick): Use reflection to pull the values associated\n\t\t\t// with the metrics fields.\n\t\t\tvar buf bytes.Buffer\n\t\t\tfor i := range lines {\n\t\t\t\tline := lines[i]\n\t\t\t\tif line == \"num-backing\" {\n\t\t\t\t\tbuf.WriteString(fmt.Sprintf(\"%d\\n\", m.Table.BackingTableCount))\n\t\t\t\t} else if line == \"backing-size\" {\n\t\t\t\t\tbuf.WriteString(fmt.Sprintf(\"%s\\n\", humanize.Bytes.Uint64(m.Table.BackingTableSize)))\n\t\t\t\t} else if line == \"virtual-size\" {\n\t\t\t\t\tbuf.WriteString(fmt.Sprintf(\"%s\\n\", humanize.Bytes.Uint64(m.VirtualSize())))\n\t\t\t\t} else if strings.HasPrefix(line, \"num-virtual\") {\n\t\t\t\t\tsplits := strings.Split(line, \" \")\n\t\t\t\t\tif len(splits) == 1 {\n\t\t\t\t\t\tbuf.WriteString(fmt.Sprintf(\"%d\\n\", m.NumVirtual()))\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\t// Level is specified.\n\t\t\t\t\tl, err := strconv.Atoi(splits[1])\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tpanic(err)\n\t\t\t\t\t}\n\t\t\t\t\tif l >= numLevels {\n\t\t\t\t\t\tpanic(fmt.Sprintf(\"invalid level %d\", l))\n\t\t\t\t\t}\n\t\t\t\t\tbuf.WriteString(fmt.Sprintf(\"%d\\n\", m.Levels[l].NumVirtualFiles))\n\t\t\t\t} else {\n\t\t\t\t\tpanic(fmt.Sprintf(\"invalid field: %s\", line))\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn buf.String()\n\n\t\tcase \"disk-usage\":\n\t\t\treturn humanize.Bytes.Uint64(d.Metrics().DiskSpaceUsage()).String()\n\n\t\tcase \"additional-metrics\":\n\t\t\t// The asynchronous loading of table stats can change metrics, so\n\t\t\t// wait for all the tables' stats to be loaded.\n\t\t\td.mu.Lock()\n\t\t\td.waitTableStats()\n\t\t\td.mu.Unlock()\n\n\t\t\tm := d.Metrics()\n\t\t\tvar b strings.Builder\n\t\t\tfmt.Fprintf(&b, \"block bytes written:\\n\")\n\t\t\tfmt.Fprintf(&b, \" __level___data-block__value-block\\n\")\n\t\t\tfor i := range m.Levels {\n\t\t\t\tfmt.Fprintf(&b, \"%7d \", i)\n\t\t\t\tfmt.Fprintf(&b, \"%12s %12s\\n\",\n\t\t\t\t\thumanize.Bytes.Uint64(m.Levels[i].Additional.BytesWrittenDataBlocks),\n\t\t\t\t\thumanize.Bytes.Uint64(m.Levels[i].Additional.BytesWrittenValueBlocks))\n\t\t\t}\n\t\t\treturn b.String()\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc TestMetricsWAmpDisableWAL(t *testing.T) {\n\td, err := Open(\"\", &Options{FS: vfs.NewMem(), DisableWAL: true})\n\trequire.NoError(t, err)\n\tks := testkeys.Alpha(2)\n\two := WriteOptions{Sync: false}\n\tfor i := 0; i < 5; i++ {\n\t\tv := []byte(strconv.Itoa(i))\n\t\tfor j := int64(0); j < ks.Count(); j++ {\n\t\t\trequire.NoError(t, d.Set(testkeys.Key(ks, j), v, &wo))\n\t\t}\n\t\trequire.NoError(t, d.Flush())\n\t\trequire.NoError(t, d.Compact([]byte(\"a\"), []byte(\"z\"), false /* parallelize */))\n\t}\n\tm := d.Metrics()\n\ttot := m.Total()\n\trequire.Greater(t, tot.WriteAmp(), 1.0)\n\trequire.NoError(t, d.Close())\n}\n\n// TestMetricsWALBytesWrittenMonotonicity tests that the\n// Metrics.WAL.BytesWritten metric is always nondecreasing.\n// It's a regression test for issue #3505.\nfunc TestMetricsWALBytesWrittenMonotonicity(t *testing.T) {\n\tfs := errorfs.Wrap(vfs.NewMem(), errorfs.RandomLatency(\n\t\tnil, 100*time.Microsecond, time.Now().UnixNano(), 0 /* no limit */))\n\td, err := Open(\"\", &Options{\n\t\tFS: fs,\n\t\t// Use a tiny memtable size so that we get frequent flushes. While a\n\t\t// flush is in-progress or completing, the WAL bytes written should\n\t\t// remain nondecreasing.\n\t\tMemTableSize: 1 << 20, /* 20 KiB */\n\t})\n\trequire.NoError(t, err)\n\n\tstopCh := make(chan struct{})\n\n\tks := testkeys.Alpha(3)\n\tvar wg sync.WaitGroup\n\tconst concurrentWriters = 5\n\twg.Add(concurrentWriters)\n\tfor w := 0; w < concurrentWriters; w++ {\n\t\tgo func() {\n\t\t\tdefer wg.Done()\n\t\t\tdata := make([]byte, 1<<10) // 1 KiB\n\t\t\trng := rand.New(rand.NewPCG(0, uint64(time.Now().UnixNano())))\n\t\t\tfor i := range data {\n\t\t\t\tdata[i] = byte(rng.Uint32())\n\t\t\t}\n\n\t\t\tbuf := make([]byte, ks.MaxLen())\n\t\t\tfor i := 0; ; i++ {\n\t\t\t\tselect {\n\t\t\t\tcase <-stopCh:\n\t\t\t\t\treturn\n\t\t\t\tdefault:\n\t\t\t\t}\n\t\t\t\tn := testkeys.WriteKey(buf, ks, int64(i)%ks.Count())\n\t\t\t\trequire.NoError(t, d.Set(buf[:n], data, NoSync))\n\t\t\t}\n\t\t}()\n\t}\n\n\tfunc() {\n\t\tdefer func() { close(stopCh) }()\n\t\tabort := time.After(time.Second)\n\t\tvar prevWALBytesWritten uint64\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-abort:\n\t\t\t\treturn\n\t\t\tdefault:\n\t\t\t}\n\n\t\t\tm := d.Metrics()\n\t\t\tif m.WAL.BytesWritten < prevWALBytesWritten {\n\t\t\t\tt.Fatalf(\"WAL bytes written decreased: %d -> %d\", prevWALBytesWritten, m.WAL.BytesWritten)\n\t\t\t}\n\t\t\tprevWALBytesWritten = m.WAL.BytesWritten\n\t\t}\n\t}()\n\twg.Wait()\n}\n"
        },
        {
          "name": "objstorage",
          "type": "tree",
          "content": null
        },
        {
          "name": "obsolete_files.go",
          "type": "blob",
          "size": 18.234375,
          "content": "// Copyright 2019 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"cmp\"\n\t\"context\"\n\t\"runtime/pprof\"\n\t\"slices\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/crlib/crtime\"\n\t\"github.com/cockroachdb/errors/oserror\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/objstorage\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/cockroachdb/pebble/wal\"\n\t\"github.com/cockroachdb/tokenbucket\"\n)\n\n// Cleaner exports the base.Cleaner type.\ntype Cleaner = base.Cleaner\n\n// DeleteCleaner exports the base.DeleteCleaner type.\ntype DeleteCleaner = base.DeleteCleaner\n\n// ArchiveCleaner exports the base.ArchiveCleaner type.\ntype ArchiveCleaner = base.ArchiveCleaner\n\ntype cleanupManager struct {\n\topts            *Options\n\tobjProvider     objstorage.Provider\n\tonTableDeleteFn func(fileSize uint64, isLocal bool)\n\tdeletePacer     *deletionPacer\n\n\t// jobsCh is used as the cleanup job queue.\n\tjobsCh chan *cleanupJob\n\t// waitGroup is used to wait for the background goroutine to exit.\n\twaitGroup sync.WaitGroup\n\n\tmu struct {\n\t\tsync.Mutex\n\t\t// totalJobs is the total number of enqueued jobs (completed or in progress).\n\t\ttotalJobs              int\n\t\tcompletedJobs          int\n\t\tcompletedJobsCond      sync.Cond\n\t\tjobsQueueWarningIssued bool\n\t}\n}\n\n// We can queue this many jobs before we have to block EnqueueJob.\nconst jobsQueueDepth = 1000\n\n// deletableFile is used for non log files.\ntype deletableFile struct {\n\tdir      string\n\tfileNum  base.DiskFileNum\n\tfileSize uint64\n\tisLocal  bool\n}\n\n// obsoleteFile holds information about a file that needs to be deleted soon.\ntype obsoleteFile struct {\n\tfileType fileType\n\t// nonLogFile is populated when fileType != fileTypeLog.\n\tnonLogFile deletableFile\n\t// logFile is populated when fileType == fileTypeLog.\n\tlogFile wal.DeletableLog\n}\n\ntype cleanupJob struct {\n\tjobID         JobID\n\tobsoleteFiles []obsoleteFile\n}\n\n// openCleanupManager creates a cleanupManager and starts its background goroutine.\n// The cleanupManager must be Close()d.\nfunc openCleanupManager(\n\topts *Options,\n\tobjProvider objstorage.Provider,\n\tonTableDeleteFn func(fileSize uint64, isLocal bool),\n\tgetDeletePacerInfo func() deletionPacerInfo,\n) *cleanupManager {\n\tcm := &cleanupManager{\n\t\topts:            opts,\n\t\tobjProvider:     objProvider,\n\t\tonTableDeleteFn: onTableDeleteFn,\n\t\tdeletePacer:     newDeletionPacer(crtime.NowMono(), int64(opts.TargetByteDeletionRate), getDeletePacerInfo),\n\t\tjobsCh:          make(chan *cleanupJob, jobsQueueDepth),\n\t}\n\tcm.mu.completedJobsCond.L = &cm.mu.Mutex\n\tcm.waitGroup.Add(1)\n\n\tgo func() {\n\t\tpprof.Do(context.Background(), gcLabels, func(context.Context) {\n\t\t\tcm.mainLoop()\n\t\t})\n\t}()\n\n\treturn cm\n}\n\n// Close stops the background goroutine, waiting until all queued jobs are completed.\n// Delete pacing is disabled for the remaining jobs.\nfunc (cm *cleanupManager) Close() {\n\tclose(cm.jobsCh)\n\tcm.waitGroup.Wait()\n}\n\n// EnqueueJob adds a cleanup job to the manager's queue.\nfunc (cm *cleanupManager) EnqueueJob(jobID JobID, obsoleteFiles []obsoleteFile) {\n\tjob := &cleanupJob{\n\t\tjobID:         jobID,\n\t\tobsoleteFiles: obsoleteFiles,\n\t}\n\n\t// Report deleted bytes to the pacer, which can use this data to potentially\n\t// increase the deletion rate to keep up. We want to do this at enqueue time\n\t// rather than when we get to the job, otherwise the reported bytes will be\n\t// subject to the throttling rate which defeats the purpose.\n\tvar pacingBytes uint64\n\tfor _, of := range obsoleteFiles {\n\t\tif cm.needsPacing(of.fileType, of.nonLogFile.fileNum) {\n\t\t\tpacingBytes += of.nonLogFile.fileSize\n\t\t}\n\t}\n\tif pacingBytes > 0 {\n\t\tcm.deletePacer.ReportDeletion(crtime.NowMono(), pacingBytes)\n\t}\n\n\tcm.mu.Lock()\n\tcm.mu.totalJobs++\n\tcm.maybeLogLocked()\n\tcm.mu.Unlock()\n\n\tcm.jobsCh <- job\n}\n\n// Wait until the completion of all jobs that were already queued.\n//\n// Does not wait for jobs that are enqueued during the call.\n//\n// Note that DB.mu should not be held while calling this method; the background\n// goroutine needs to acquire DB.mu to update deleted table metrics.\nfunc (cm *cleanupManager) Wait() {\n\tcm.mu.Lock()\n\tdefer cm.mu.Unlock()\n\tn := cm.mu.totalJobs\n\tfor cm.mu.completedJobs < n {\n\t\tcm.mu.completedJobsCond.Wait()\n\t}\n}\n\n// mainLoop runs the manager's background goroutine.\nfunc (cm *cleanupManager) mainLoop() {\n\tdefer cm.waitGroup.Done()\n\n\tvar tb tokenbucket.TokenBucket\n\t// Use a token bucket with 1 token / second refill rate and 1 token burst.\n\ttb.Init(1.0, 1.0)\n\tfor job := range cm.jobsCh {\n\t\tfor _, of := range job.obsoleteFiles {\n\t\t\tswitch of.fileType {\n\t\t\tcase fileTypeTable:\n\t\t\t\tcm.maybePace(&tb, of.fileType, of.nonLogFile.fileNum, of.nonLogFile.fileSize)\n\t\t\t\tcm.onTableDeleteFn(of.nonLogFile.fileSize, of.nonLogFile.isLocal)\n\t\t\t\tcm.deleteObsoleteObject(fileTypeTable, job.jobID, of.nonLogFile.fileNum)\n\t\t\tcase fileTypeLog:\n\t\t\t\tcm.deleteObsoleteFile(of.logFile.FS, fileTypeLog, job.jobID, of.logFile.Path,\n\t\t\t\t\tbase.DiskFileNum(of.logFile.NumWAL), of.logFile.ApproxFileSize)\n\t\t\tdefault:\n\t\t\t\tpath := base.MakeFilepath(cm.opts.FS, of.nonLogFile.dir, of.fileType, of.nonLogFile.fileNum)\n\t\t\t\tcm.deleteObsoleteFile(\n\t\t\t\t\tcm.opts.FS, of.fileType, job.jobID, path, of.nonLogFile.fileNum, of.nonLogFile.fileSize)\n\t\t\t}\n\t\t}\n\t\tcm.mu.Lock()\n\t\tcm.mu.completedJobs++\n\t\tcm.mu.completedJobsCond.Broadcast()\n\t\tcm.maybeLogLocked()\n\t\tcm.mu.Unlock()\n\t}\n}\n\n// fileNumIfSST is read iff fileType is fileTypeTable.\nfunc (cm *cleanupManager) needsPacing(fileType base.FileType, fileNumIfSST base.DiskFileNum) bool {\n\tif fileType != fileTypeTable {\n\t\treturn false\n\t}\n\tmeta, err := cm.objProvider.Lookup(fileType, fileNumIfSST)\n\tif err != nil {\n\t\t// The object was already removed from the provider; we won't actually\n\t\t// delete anything, so we don't need to pace.\n\t\treturn false\n\t}\n\t// Don't throttle deletion of remote objects.\n\treturn !meta.IsRemote()\n}\n\n// maybePace sleeps before deleting an object if appropriate. It is always\n// called from the background goroutine.\nfunc (cm *cleanupManager) maybePace(\n\ttb *tokenbucket.TokenBucket, fileType base.FileType, fileNum base.DiskFileNum, fileSize uint64,\n) {\n\tif !cm.needsPacing(fileType, fileNum) {\n\t\treturn\n\t}\n\n\ttokens := cm.deletePacer.PacingDelay(crtime.NowMono(), fileSize)\n\tif tokens == 0.0 {\n\t\t// The token bucket might be in debt; it could make us wait even for 0\n\t\t// tokens. We don't want that if the pacer decided throttling should be\n\t\t// disabled.\n\t\treturn\n\t}\n\t// Wait for tokens. We use a token bucket instead of sleeping outright because\n\t// the token bucket accumulates up to one second of unused tokens.\n\tfor {\n\t\tok, d := tb.TryToFulfill(tokenbucket.Tokens(tokens))\n\t\tif ok {\n\t\t\tbreak\n\t\t}\n\t\ttime.Sleep(d)\n\t}\n}\n\n// deleteObsoleteFile deletes a (non-object) file that is no longer needed.\nfunc (cm *cleanupManager) deleteObsoleteFile(\n\tfs vfs.FS, fileType fileType, jobID JobID, path string, fileNum base.DiskFileNum, fileSize uint64,\n) {\n\t// TODO(peter): need to handle this error, probably by re-adding the\n\t// file that couldn't be deleted to one of the obsolete slices map.\n\terr := cm.opts.Cleaner.Clean(fs, fileType, path)\n\tif oserror.IsNotExist(err) {\n\t\treturn\n\t}\n\n\tswitch fileType {\n\tcase fileTypeLog:\n\t\tcm.opts.EventListener.WALDeleted(WALDeleteInfo{\n\t\t\tJobID:   int(jobID),\n\t\t\tPath:    path,\n\t\t\tFileNum: fileNum,\n\t\t\tErr:     err,\n\t\t})\n\tcase fileTypeManifest:\n\t\tcm.opts.EventListener.ManifestDeleted(ManifestDeleteInfo{\n\t\t\tJobID:   int(jobID),\n\t\t\tPath:    path,\n\t\t\tFileNum: fileNum,\n\t\t\tErr:     err,\n\t\t})\n\tcase fileTypeTable:\n\t\tpanic(\"invalid deletion of object file\")\n\t}\n}\n\nfunc (cm *cleanupManager) deleteObsoleteObject(\n\tfileType fileType, jobID JobID, fileNum base.DiskFileNum,\n) {\n\tif fileType != fileTypeTable {\n\t\tpanic(\"not an object\")\n\t}\n\n\tvar path string\n\tmeta, err := cm.objProvider.Lookup(fileType, fileNum)\n\tif err != nil {\n\t\tpath = \"<nil>\"\n\t} else {\n\t\tpath = cm.objProvider.Path(meta)\n\t\terr = cm.objProvider.Remove(fileType, fileNum)\n\t}\n\tif cm.objProvider.IsNotExistError(err) {\n\t\treturn\n\t}\n\n\tswitch fileType {\n\tcase fileTypeTable:\n\t\tcm.opts.EventListener.TableDeleted(TableDeleteInfo{\n\t\t\tJobID:   int(jobID),\n\t\t\tPath:    path,\n\t\t\tFileNum: fileNum,\n\t\t\tErr:     err,\n\t\t})\n\t}\n}\n\n// maybeLogLocked issues a log if the job queue gets 75% full and issues a log\n// when the job queue gets back to less than 10% full.\n//\n// Must be called with cm.mu locked.\nfunc (cm *cleanupManager) maybeLogLocked() {\n\tconst highThreshold = jobsQueueDepth * 3 / 4\n\tconst lowThreshold = jobsQueueDepth / 10\n\n\tjobsInQueue := cm.mu.totalJobs - cm.mu.completedJobs\n\n\tif !cm.mu.jobsQueueWarningIssued && jobsInQueue > highThreshold {\n\t\tcm.mu.jobsQueueWarningIssued = true\n\t\tcm.opts.Logger.Infof(\"cleanup falling behind; job queue has over %d jobs\", highThreshold)\n\t}\n\n\tif cm.mu.jobsQueueWarningIssued && jobsInQueue < lowThreshold {\n\t\tcm.mu.jobsQueueWarningIssued = false\n\t\tcm.opts.Logger.Infof(\"cleanup back to normal; job queue has under %d jobs\", lowThreshold)\n\t}\n}\n\nfunc (d *DB) getDeletionPacerInfo() deletionPacerInfo {\n\tvar pacerInfo deletionPacerInfo\n\t// Call GetDiskUsage after every file deletion. This may seem inefficient,\n\t// but in practice this was observed to take constant time, regardless of\n\t// volume size used, at least on linux with ext4 and zfs. All invocations\n\t// take 10 microseconds or less.\n\tpacerInfo.freeBytes = d.calculateDiskAvailableBytes()\n\td.mu.Lock()\n\tpacerInfo.obsoleteBytes = d.mu.versions.metrics.Table.ObsoleteSize\n\tpacerInfo.liveBytes = uint64(d.mu.versions.metrics.Total().Size)\n\td.mu.Unlock()\n\treturn pacerInfo\n}\n\n// onObsoleteTableDelete is called to update metrics when an sstable is deleted.\nfunc (d *DB) onObsoleteTableDelete(fileSize uint64, isLocal bool) {\n\td.mu.Lock()\n\td.mu.versions.metrics.Table.ObsoleteCount--\n\td.mu.versions.metrics.Table.ObsoleteSize -= fileSize\n\tif isLocal {\n\t\td.mu.versions.metrics.Table.Local.ObsoleteSize -= fileSize\n\t}\n\td.mu.Unlock()\n}\n\n// scanObsoleteFiles scans the filesystem for files that are no longer needed\n// and adds those to the internal lists of obsolete files. Note that the files\n// are not actually deleted by this method. A subsequent call to\n// deleteObsoleteFiles must be performed. Must be not be called concurrently\n// with compactions and flushes. db.mu must be held when calling this function.\nfunc (d *DB) scanObsoleteFiles(list []string, flushableIngests []*ingestedFlushable) {\n\t// Disable automatic compactions temporarily to avoid concurrent compactions /\n\t// flushes from interfering. The original value is restored on completion.\n\tdisabledPrev := d.opts.DisableAutomaticCompactions\n\tdefer func() {\n\t\td.opts.DisableAutomaticCompactions = disabledPrev\n\t}()\n\td.opts.DisableAutomaticCompactions = true\n\n\t// Wait for any ongoing compaction to complete before continuing.\n\tfor d.mu.compact.compactingCount > 0 || d.mu.compact.downloadingCount > 0 || d.mu.compact.flushing {\n\t\td.mu.compact.cond.Wait()\n\t}\n\n\tliveFileNums := make(map[base.DiskFileNum]struct{})\n\td.mu.versions.addLiveFileNums(liveFileNums)\n\t// Protect against files which are only referred to by the ingestedFlushable\n\t// from being deleted. These are added to the flushable queue on WAL replay\n\t// and handle their own obsoletion/deletion. We exclude them from this obsolete\n\t// file scan to avoid double-deleting these files.\n\tfor _, f := range flushableIngests {\n\t\tfor _, file := range f.files {\n\t\t\tliveFileNums[file.FileBacking.DiskFileNum] = struct{}{}\n\t\t}\n\t}\n\n\tmanifestFileNum := d.mu.versions.manifestFileNum\n\n\tvar obsoleteTables []tableInfo\n\tvar obsoleteManifests []fileInfo\n\tvar obsoleteOptions []fileInfo\n\n\tfor _, filename := range list {\n\t\tfileType, diskFileNum, ok := base.ParseFilename(d.opts.FS, filename)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tswitch fileType {\n\t\tcase fileTypeManifest:\n\t\t\tif diskFileNum >= manifestFileNum {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfi := fileInfo{FileNum: diskFileNum}\n\t\t\tif stat, err := d.opts.FS.Stat(filename); err == nil {\n\t\t\t\tfi.FileSize = uint64(stat.Size())\n\t\t\t}\n\t\t\tobsoleteManifests = append(obsoleteManifests, fi)\n\t\tcase fileTypeOptions:\n\t\t\tif diskFileNum >= d.optionsFileNum {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfi := fileInfo{FileNum: diskFileNum}\n\t\t\tif stat, err := d.opts.FS.Stat(filename); err == nil {\n\t\t\t\tfi.FileSize = uint64(stat.Size())\n\t\t\t}\n\t\t\tobsoleteOptions = append(obsoleteOptions, fi)\n\t\tcase fileTypeTable:\n\t\t\t// Objects are handled through the objstorage provider below.\n\t\tdefault:\n\t\t\t// Don't delete files we don't know about.\n\t\t}\n\t}\n\n\tobjects := d.objProvider.List()\n\tfor _, obj := range objects {\n\t\tswitch obj.FileType {\n\t\tcase fileTypeTable:\n\t\t\tif _, ok := liveFileNums[obj.DiskFileNum]; ok {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfileInfo := fileInfo{\n\t\t\t\tFileNum: obj.DiskFileNum,\n\t\t\t}\n\t\t\tif size, err := d.objProvider.Size(obj); err == nil {\n\t\t\t\tfileInfo.FileSize = uint64(size)\n\t\t\t}\n\t\t\tobsoleteTables = append(obsoleteTables, tableInfo{\n\t\t\t\tfileInfo: fileInfo,\n\t\t\t\tisLocal:  !obj.IsRemote(),\n\t\t\t})\n\n\t\tdefault:\n\t\t\t// Ignore object types we don't know about.\n\t\t}\n\t}\n\n\td.mu.versions.obsoleteTables = mergeTableInfos(d.mu.versions.obsoleteTables, obsoleteTables)\n\td.mu.versions.updateObsoleteTableMetricsLocked()\n\td.mu.versions.obsoleteManifests = merge(d.mu.versions.obsoleteManifests, obsoleteManifests)\n\td.mu.versions.obsoleteOptions = merge(d.mu.versions.obsoleteOptions, obsoleteOptions)\n}\n\n// disableFileDeletions disables file deletions and then waits for any\n// in-progress deletion to finish. The caller is required to call\n// enableFileDeletions in order to enable file deletions again. It is ok for\n// multiple callers to disable file deletions simultaneously, though they must\n// all invoke enableFileDeletions in order for file deletions to be re-enabled\n// (there is an internal reference count on file deletion disablement).\n//\n// d.mu must be held when calling this method.\nfunc (d *DB) disableFileDeletions() {\n\td.mu.disableFileDeletions++\n\td.mu.Unlock()\n\tdefer d.mu.Lock()\n\td.cleanupManager.Wait()\n}\n\n// enableFileDeletions enables previously disabled file deletions. A cleanup job\n// is queued if necessary.\n//\n// d.mu must be held when calling this method.\nfunc (d *DB) enableFileDeletions() {\n\tif d.mu.disableFileDeletions <= 0 {\n\t\tpanic(\"pebble: file deletion disablement invariant violated\")\n\t}\n\td.mu.disableFileDeletions--\n\tif d.mu.disableFileDeletions > 0 {\n\t\treturn\n\t}\n\td.deleteObsoleteFiles(d.newJobIDLocked())\n}\n\ntype fileInfo = base.FileInfo\n\n// deleteObsoleteFiles enqueues a cleanup job to the cleanup manager, if necessary.\n//\n// d.mu must be held when calling this. The function will release and re-aquire the mutex.\n//\n// Does nothing if file deletions are disabled (see disableFileDeletions). A\n// cleanup job will be scheduled when file deletions are re-enabled.\nfunc (d *DB) deleteObsoleteFiles(jobID JobID) {\n\tif d.mu.disableFileDeletions > 0 {\n\t\treturn\n\t}\n\t_, noRecycle := d.opts.Cleaner.(base.NeedsFileContents)\n\n\t// NB: d.mu.versions.minUnflushedLogNum is the log number of the earliest\n\t// log that has not had its contents flushed to an sstable.\n\tobsoleteLogs, err := d.mu.log.manager.Obsolete(wal.NumWAL(d.mu.versions.minUnflushedLogNum), noRecycle)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\tobsoleteTables := append([]tableInfo(nil), d.mu.versions.obsoleteTables...)\n\td.mu.versions.obsoleteTables = nil\n\n\tfor _, tbl := range obsoleteTables {\n\t\tdelete(d.mu.versions.zombieTables, tbl.FileNum)\n\t}\n\n\t// Sort the manifests cause we want to delete some contiguous prefix\n\t// of the older manifests.\n\tslices.SortFunc(d.mu.versions.obsoleteManifests, func(a, b fileInfo) int {\n\t\treturn cmp.Compare(a.FileNum, b.FileNum)\n\t})\n\n\tvar obsoleteManifests []fileInfo\n\tmanifestsToDelete := len(d.mu.versions.obsoleteManifests) - d.opts.NumPrevManifest\n\tif manifestsToDelete > 0 {\n\t\tobsoleteManifests = d.mu.versions.obsoleteManifests[:manifestsToDelete]\n\t\td.mu.versions.obsoleteManifests = d.mu.versions.obsoleteManifests[manifestsToDelete:]\n\t\tif len(d.mu.versions.obsoleteManifests) == 0 {\n\t\t\td.mu.versions.obsoleteManifests = nil\n\t\t}\n\t}\n\n\tobsoleteOptions := d.mu.versions.obsoleteOptions\n\td.mu.versions.obsoleteOptions = nil\n\n\t// Release d.mu while preparing the cleanup job and possibly waiting.\n\t// Note the unusual order: Unlock and then Lock.\n\td.mu.Unlock()\n\tdefer d.mu.Lock()\n\n\tfilesToDelete := make([]obsoleteFile, 0, len(obsoleteLogs)+len(obsoleteTables)+len(obsoleteManifests)+len(obsoleteOptions))\n\tfor _, f := range obsoleteLogs {\n\t\tfilesToDelete = append(filesToDelete, obsoleteFile{fileType: fileTypeLog, logFile: f})\n\t}\n\t// We sort to make the order of deletions deterministic, which is nice for\n\t// tests.\n\tslices.SortFunc(obsoleteTables, func(a, b tableInfo) int {\n\t\treturn cmp.Compare(a.FileNum, b.FileNum)\n\t})\n\tfor _, f := range obsoleteTables {\n\t\td.fileCache.evict(f.FileNum)\n\t\tfilesToDelete = append(filesToDelete, obsoleteFile{\n\t\t\tfileType: fileTypeTable,\n\t\t\tnonLogFile: deletableFile{\n\t\t\t\tdir:      d.dirname,\n\t\t\t\tfileNum:  f.FileNum,\n\t\t\t\tfileSize: f.FileSize,\n\t\t\t\tisLocal:  f.isLocal,\n\t\t\t},\n\t\t})\n\t}\n\tfiles := [2]struct {\n\t\tfileType fileType\n\t\tobsolete []fileInfo\n\t}{\n\t\t{fileTypeManifest, obsoleteManifests},\n\t\t{fileTypeOptions, obsoleteOptions},\n\t}\n\tfor _, f := range files {\n\t\t// We sort to make the order of deletions deterministic, which is nice for\n\t\t// tests.\n\t\tslices.SortFunc(f.obsolete, func(a, b fileInfo) int {\n\t\t\treturn cmp.Compare(a.FileNum, b.FileNum)\n\t\t})\n\t\tfor _, fi := range f.obsolete {\n\t\t\tdir := d.dirname\n\t\t\tfilesToDelete = append(filesToDelete, obsoleteFile{\n\t\t\t\tfileType: f.fileType,\n\t\t\t\tnonLogFile: deletableFile{\n\t\t\t\t\tdir:      dir,\n\t\t\t\t\tfileNum:  fi.FileNum,\n\t\t\t\t\tfileSize: fi.FileSize,\n\t\t\t\t\tisLocal:  true,\n\t\t\t\t},\n\t\t\t})\n\t\t}\n\t}\n\tif len(filesToDelete) > 0 {\n\t\td.cleanupManager.EnqueueJob(jobID, filesToDelete)\n\t}\n\tif d.opts.private.testingAlwaysWaitForCleanup {\n\t\td.cleanupManager.Wait()\n\t}\n}\n\nfunc (d *DB) maybeScheduleObsoleteTableDeletion() {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\td.maybeScheduleObsoleteTableDeletionLocked()\n}\n\nfunc (d *DB) maybeScheduleObsoleteTableDeletionLocked() {\n\tif len(d.mu.versions.obsoleteTables) > 0 {\n\t\td.deleteObsoleteFiles(d.newJobIDLocked())\n\t}\n}\n\nfunc merge(a, b []fileInfo) []fileInfo {\n\tif len(b) == 0 {\n\t\treturn a\n\t}\n\n\ta = append(a, b...)\n\tslices.SortFunc(a, func(a, b fileInfo) int {\n\t\treturn cmp.Compare(a.FileNum, b.FileNum)\n\t})\n\treturn slices.CompactFunc(a, func(a, b fileInfo) bool {\n\t\treturn a.FileNum == b.FileNum\n\t})\n}\n\nfunc mergeTableInfos(a, b []tableInfo) []tableInfo {\n\tif len(b) == 0 {\n\t\treturn a\n\t}\n\n\ta = append(a, b...)\n\tslices.SortFunc(a, func(a, b tableInfo) int {\n\t\treturn cmp.Compare(a.FileNum, b.FileNum)\n\t})\n\treturn slices.CompactFunc(a, func(a, b tableInfo) bool {\n\t\treturn a.FileNum == b.FileNum\n\t})\n}\n"
        },
        {
          "name": "obsolete_files_test.go",
          "type": "blob",
          "size": 3.2021484375,
          "content": "// Copyright 2019 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"fmt\"\n\t\"sort\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/cockroachdb/datadriven\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestCleaner(t *testing.T) {\n\tdbs := make(map[string]*DB)\n\tdefer func() {\n\t\tfor _, db := range dbs {\n\t\t\trequire.NoError(t, db.Close())\n\t\t}\n\t}()\n\n\tmem := vfs.NewMem()\n\tvar memLog base.InMemLogger\n\tfs := vfs.WithLogging(mem, memLog.Infof)\n\tdatadriven.RunTest(t, \"testdata/cleaner\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tmemLog.Reset()\n\t\tswitch td.Cmd {\n\t\tcase \"batch\":\n\t\t\tif len(td.CmdArgs) != 1 {\n\t\t\t\treturn \"batch <db>\"\n\t\t\t}\n\t\t\td := dbs[td.CmdArgs[0].String()]\n\t\t\tb := d.NewBatch()\n\t\t\tif err := runBatchDefineCmd(td, b); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tif err := b.Commit(Sync); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn memLog.String()\n\n\t\tcase \"compact\":\n\t\t\tif len(td.CmdArgs) != 1 {\n\t\t\t\treturn \"compact <db>\"\n\t\t\t}\n\t\t\td := dbs[td.CmdArgs[0].String()]\n\t\t\tif err := d.Compact(nil, []byte(\"\\xff\"), false); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn memLog.String()\n\n\t\tcase \"flush\":\n\t\t\tif len(td.CmdArgs) != 1 {\n\t\t\t\treturn \"flush <db>\"\n\t\t\t}\n\t\t\td := dbs[td.CmdArgs[0].String()]\n\t\t\tif err := d.Flush(); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn memLog.String()\n\n\t\tcase \"close\":\n\t\t\tif len(td.CmdArgs) != 1 {\n\t\t\t\treturn \"close <db>\"\n\t\t\t}\n\t\t\tdbDir := td.CmdArgs[0].String()\n\t\t\td := dbs[dbDir]\n\t\t\tif err := d.Close(); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tdelete(dbs, dbDir)\n\t\t\treturn memLog.String()\n\n\t\tcase \"list\":\n\t\t\tif len(td.CmdArgs) != 1 {\n\t\t\t\treturn \"list <dir>\"\n\t\t\t}\n\t\t\tpaths, err := mem.List(td.CmdArgs[0].String())\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tsort.Strings(paths)\n\t\t\treturn fmt.Sprintf(\"%s\\n\", strings.Join(paths, \"\\n\"))\n\n\t\tcase \"open\":\n\t\t\tif len(td.CmdArgs) < 1 || len(td.CmdArgs) > 3 {\n\t\t\t\treturn \"open <dir> [archive] [readonly]\"\n\t\t\t}\n\t\t\tdir := td.CmdArgs[0].String()\n\t\t\topts := (&Options{\n\t\t\t\tFS:     fs,\n\t\t\t\tWALDir: dir + \"_wal\",\n\t\t\t\tLogger: testLogger{t},\n\t\t\t}).WithFSDefaults()\n\n\t\t\tfor i := 1; i < len(td.CmdArgs); i++ {\n\t\t\t\tswitch td.CmdArgs[i].String() {\n\t\t\t\tcase \"readonly\":\n\t\t\t\t\topts.ReadOnly = true\n\t\t\t\tcase \"archive\":\n\t\t\t\t\topts.Cleaner = ArchiveCleaner{}\n\t\t\t\tdefault:\n\t\t\t\t\treturn \"open <dir> [archive] [readonly]\"\n\t\t\t\t}\n\t\t\t}\n\t\t\t// Asynchronous table stats retrieval makes the output flaky.\n\t\t\topts.DisableTableStats = true\n\t\t\topts.private.testingAlwaysWaitForCleanup = true\n\t\t\td, err := Open(dir, opts)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\td.TestOnlyWaitForCleaning()\n\t\t\tdbs[dir] = d\n\t\t\treturn memLog.String()\n\n\t\tcase \"create-bogus-file\":\n\t\t\tif len(td.CmdArgs) != 1 {\n\t\t\t\treturn \"create-bogus-file <db/file>\"\n\t\t\t}\n\t\t\tdst, err := fs.Create(td.CmdArgs[0].String(), vfs.WriteCategoryUnspecified)\n\t\t\trequire.NoError(t, err)\n\t\t\t_, err = dst.Write([]byte(\"bogus data\"))\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.NoError(t, dst.Sync())\n\t\t\trequire.NoError(t, dst.Close())\n\t\t\treturn memLog.String()\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t}\n\t})\n}\n"
        },
        {
          "name": "open.go",
          "type": "blob",
          "size": 40.96484375,
          "content": "// Copyright 2012 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\t\"os\"\n\t\"slices\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/crlib/crtime\"\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/errors/oserror\"\n\t\"github.com/cockroachdb/pebble/batchrepr\"\n\t\"github.com/cockroachdb/pebble/internal/arenaskl\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/cache\"\n\t\"github.com/cockroachdb/pebble/internal/constants\"\n\t\"github.com/cockroachdb/pebble/internal/invariants\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n\t\"github.com/cockroachdb/pebble/internal/manual\"\n\t\"github.com/cockroachdb/pebble/objstorage\"\n\t\"github.com/cockroachdb/pebble/objstorage/objstorageprovider\"\n\t\"github.com/cockroachdb/pebble/objstorage/remote\"\n\t\"github.com/cockroachdb/pebble/record\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/cockroachdb/pebble/wal\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n)\n\nconst (\n\tinitialMemTableSize = 256 << 10 // 256 KB\n\n\t// The max batch size is limited by the uint32 offsets stored in\n\t// internal/batchskl.node, DeferredBatchOp, and flushableBatchEntry.\n\t//\n\t// We limit the size to MaxUint32 (just short of 4GB) so that the exclusive\n\t// end of an allocation fits in uint32.\n\t//\n\t// On 32-bit systems, slices are naturally limited to MaxInt (just short of\n\t// 2GB).\n\tmaxBatchSize = constants.MaxUint32OrInt\n\n\t// The max memtable size is limited by the uint32 offsets stored in\n\t// internal/arenaskl.node, DeferredBatchOp, and flushableBatchEntry.\n\t//\n\t// We limit the size to MaxUint32 (just short of 4GB) so that the exclusive\n\t// end of an allocation fits in uint32.\n\t//\n\t// On 32-bit systems, slices are naturally limited to MaxInt (just short of\n\t// 2GB).\n\tmaxMemTableSize = constants.MaxUint32OrInt\n)\n\n// FileCacheSize can be used to determine the file\n// cache size for a single db, given the maximum open\n// files which can be used by a file cache which is\n// only used by a single db.\nfunc FileCacheSize(maxOpenFiles int) int {\n\tfileCacheSize := maxOpenFiles - numNonFileCacheFiles\n\tif fileCacheSize < minFileCacheSize {\n\t\tfileCacheSize = minFileCacheSize\n\t}\n\treturn fileCacheSize\n}\n\n// Open opens a DB whose files live in the given directory.\nfunc Open(dirname string, opts *Options) (db *DB, err error) {\n\t// Make a copy of the options so that we don't mutate the passed in options.\n\topts = opts.Clone()\n\topts = opts.EnsureDefaults()\n\tif err := opts.Validate(); err != nil {\n\t\treturn nil, err\n\t}\n\tif opts.LoggerAndTracer == nil {\n\t\topts.LoggerAndTracer = &base.LoggerWithNoopTracer{Logger: opts.Logger}\n\t} else {\n\t\topts.Logger = opts.LoggerAndTracer\n\t}\n\n\tif invariants.Sometimes(5) {\n\t\tassertComparer := base.MakeAssertComparer(*opts.Comparer)\n\t\topts.Comparer = &assertComparer\n\t}\n\n\t// In all error cases, we return db = nil; this is used by various\n\t// deferred cleanups.\n\n\t// Open the database and WAL directories first.\n\twalDirname, dataDir, err := prepareAndOpenDirs(dirname, opts)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"error opening database at %q\", dirname)\n\t}\n\tdefer func() {\n\t\tif db == nil {\n\t\t\tdataDir.Close()\n\t\t}\n\t}()\n\n\t// Lock the database directory.\n\tvar fileLock *Lock\n\tif opts.Lock != nil {\n\t\t// The caller already acquired the database lock. Ensure that the\n\t\t// directory matches.\n\t\tif err := opts.Lock.pathMatches(dirname); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif err := opts.Lock.refForOpen(); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tfileLock = opts.Lock\n\t} else {\n\t\tfileLock, err = LockDirectory(dirname, opts.FS)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\tdefer func() {\n\t\tif db == nil {\n\t\t\tfileLock.Close()\n\t\t}\n\t}()\n\n\t// List the directory contents. This also happens to include WAL log files, if\n\t// they are in the same dir, but we will ignore those below. The provider is\n\t// also given this list, but it ignores non sstable files.\n\tls, err := opts.FS.List(dirname)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Establish the format major version.\n\tformatVersion, formatVersionMarker, err := lookupFormatMajorVersion(opts.FS, dirname, ls)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer func() {\n\t\tif db == nil {\n\t\t\tformatVersionMarker.Close()\n\t\t}\n\t}()\n\n\tnoFormatVersionMarker := formatVersion == FormatDefault\n\tif noFormatVersionMarker {\n\t\t// We will initialize the store at the minimum possible format, then upgrade\n\t\t// the format to the desired one. This helps test the format upgrade code.\n\t\tformatVersion = FormatMinSupported\n\t\tif opts.Experimental.CreateOnShared != remote.CreateOnSharedNone {\n\t\t\tformatVersion = FormatMinForSharedObjects\n\t\t}\n\t\t// There is no format version marker file. There are three cases:\n\t\t//  - we are trying to open an existing store that was created at\n\t\t//    FormatMostCompatible (the only one without a version marker file)\n\t\t//  - we are creating a new store;\n\t\t//  - we are retrying a failed creation.\n\t\t//\n\t\t// To error in the first case, we set ErrorIfNotPristine.\n\t\topts.ErrorIfNotPristine = true\n\t\tdefer func() {\n\t\t\tif err != nil && errors.Is(err, ErrDBNotPristine) {\n\t\t\t\t// We must be trying to open an existing store at FormatMostCompatible.\n\t\t\t\t// Correct the error in this case -we\n\t\t\t\terr = errors.Newf(\n\t\t\t\t\t\"pebble: database %q written in format major version 1 which is no longer supported\",\n\t\t\t\t\tdirname)\n\t\t\t}\n\t\t}()\n\t} else {\n\t\tif opts.Experimental.CreateOnShared != remote.CreateOnSharedNone && formatVersion < FormatMinForSharedObjects {\n\t\t\treturn nil, errors.Newf(\n\t\t\t\t\"pebble: database %q configured with shared objects but written in too old format major version %d\",\n\t\t\t\tdirname, formatVersion)\n\t\t}\n\t}\n\n\t// Find the currently active manifest, if there is one.\n\tmanifestMarker, manifestFileNum, manifestExists, err := findCurrentManifest(opts.FS, dirname, ls)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"pebble: database %q\", dirname)\n\t}\n\tdefer func() {\n\t\tif db == nil {\n\t\t\tmanifestMarker.Close()\n\t\t}\n\t}()\n\n\t// Atomic markers may leave behind obsolete files if there's a crash\n\t// mid-update. Clean these up if we're not in read-only mode.\n\tif !opts.ReadOnly {\n\t\tif err := formatVersionMarker.RemoveObsolete(); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif err := manifestMarker.RemoveObsolete(); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tif opts.Cache == nil {\n\t\topts.Cache = cache.New(cacheDefaultSize)\n\t} else {\n\t\topts.Cache.Ref()\n\t}\n\n\td := &DB{\n\t\tcacheID:             opts.Cache.NewID(),\n\t\tdirname:             dirname,\n\t\topts:                opts,\n\t\tcmp:                 opts.Comparer.Compare,\n\t\tequal:               opts.Comparer.Equal,\n\t\tmerge:               opts.Merger.Merge,\n\t\tsplit:               opts.Comparer.Split,\n\t\tabbreviatedKey:      opts.Comparer.AbbreviatedKey,\n\t\tlargeBatchThreshold: (opts.MemTableSize - uint64(memTableEmptySize)) / 2,\n\t\tfileLock:            fileLock,\n\t\tdataDir:             dataDir,\n\t\tclosed:              new(atomic.Value),\n\t\tclosedCh:            make(chan struct{}),\n\t}\n\td.mu.versions = &versionSet{}\n\td.diskAvailBytes.Store(math.MaxUint64)\n\n\tdefer func() {\n\t\t// If an error or panic occurs during open, attempt to release the manually\n\t\t// allocated memory resources. Note that rather than look for an error, we\n\t\t// look for the return of a nil DB pointer.\n\t\tif r := recover(); db == nil {\n\t\t\t// If there's an unused, recycled memtable, we need to release its memory.\n\t\t\tif obsoleteMemTable := d.memTableRecycle.Swap(nil); obsoleteMemTable != nil {\n\t\t\t\td.freeMemTable(obsoleteMemTable)\n\t\t\t}\n\n\t\t\t// Release our references to the Cache. Note that both the DB, and\n\t\t\t// fileCache have a reference. When we release the reference to\n\t\t\t// the fileCache, and if there are no other references to\n\t\t\t// the fileCache, then the fileCache will also release its\n\t\t\t// reference to the cache.\n\t\t\topts.Cache.Unref()\n\n\t\t\tif d.fileCache != nil {\n\t\t\t\t_ = d.fileCache.close()\n\t\t\t}\n\n\t\t\tfor _, mem := range d.mu.mem.queue {\n\t\t\t\tswitch t := mem.flushable.(type) {\n\t\t\t\tcase *memTable:\n\t\t\t\t\tmanual.Free(manual.MemTable, t.arenaBuf)\n\t\t\t\t\tt.arenaBuf = manual.Buf{}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif d.cleanupManager != nil {\n\t\t\t\td.cleanupManager.Close()\n\t\t\t}\n\t\t\tif d.objProvider != nil {\n\t\t\t\td.objProvider.Close()\n\t\t\t}\n\t\t\tif r != nil {\n\t\t\t\tpanic(r)\n\t\t\t}\n\t\t}\n\t}()\n\n\td.commit = newCommitPipeline(commitEnv{\n\t\tlogSeqNum:     &d.mu.versions.logSeqNum,\n\t\tvisibleSeqNum: &d.mu.versions.visibleSeqNum,\n\t\tapply:         d.commitApply,\n\t\twrite:         d.commitWrite,\n\t})\n\td.mu.nextJobID = 1\n\td.mu.mem.nextSize = opts.MemTableSize\n\tif d.mu.mem.nextSize > initialMemTableSize {\n\t\td.mu.mem.nextSize = initialMemTableSize\n\t}\n\td.mu.compact.cond.L = &d.mu.Mutex\n\td.mu.compact.inProgress = make(map[*compaction]struct{})\n\td.mu.compact.noOngoingFlushStartTime = crtime.NowMono()\n\td.mu.snapshots.init()\n\t// logSeqNum is the next sequence number that will be assigned.\n\t// Start assigning sequence numbers from base.SeqNumStart to leave\n\t// room for reserved sequence numbers (see comments around\n\t// SeqNumStart).\n\td.mu.versions.logSeqNum.Store(base.SeqNumStart)\n\td.mu.formatVers.vers.Store(uint64(formatVersion))\n\td.mu.formatVers.marker = formatVersionMarker\n\n\td.timeNow = time.Now\n\td.openedAt = d.timeNow()\n\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\n\tjobID := d.newJobIDLocked()\n\n\tproviderSettings := objstorageprovider.Settings{\n\t\tLogger:              opts.Logger,\n\t\tFS:                  opts.FS,\n\t\tFSDirName:           dirname,\n\t\tFSDirInitialListing: ls,\n\t\tFSCleaner:           opts.Cleaner,\n\t\tNoSyncOnClose:       opts.NoSyncOnClose,\n\t\tBytesPerSync:        opts.BytesPerSync,\n\t}\n\tproviderSettings.Local.ReadaheadConfig = opts.Local.ReadaheadConfig\n\tproviderSettings.Remote.StorageFactory = opts.Experimental.RemoteStorage\n\tproviderSettings.Remote.CreateOnShared = opts.Experimental.CreateOnShared\n\tproviderSettings.Remote.CreateOnSharedLocator = opts.Experimental.CreateOnSharedLocator\n\tproviderSettings.Remote.CacheSizeBytes = opts.Experimental.SecondaryCacheSizeBytes\n\n\td.objProvider, err = objstorageprovider.Open(providerSettings)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif !manifestExists {\n\t\t// DB does not exist.\n\t\tif d.opts.ErrorIfNotExists || d.opts.ReadOnly {\n\t\t\treturn nil, errors.Wrapf(ErrDBDoesNotExist, \"dirname=%q\", dirname)\n\t\t}\n\n\t\t// Create the DB.\n\t\tif err := d.mu.versions.create(\n\t\t\tjobID, dirname, d.objProvider, opts, manifestMarker, d.FormatMajorVersion, &d.mu.Mutex); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t} else {\n\t\tif opts.ErrorIfExists {\n\t\t\treturn nil, errors.Wrapf(ErrDBAlreadyExists, \"dirname=%q\", dirname)\n\t\t}\n\t\t// Load the version set.\n\t\tif err := d.mu.versions.load(\n\t\t\tdirname, d.objProvider, opts, manifestFileNum, manifestMarker, d.FormatMajorVersion, &d.mu.Mutex); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif opts.ErrorIfNotPristine {\n\t\t\tliveFileNums := make(map[base.DiskFileNum]struct{})\n\t\t\td.mu.versions.addLiveFileNums(liveFileNums)\n\t\t\tif len(liveFileNums) != 0 {\n\t\t\t\treturn nil, errors.Wrapf(ErrDBNotPristine, \"dirname=%q\", dirname)\n\t\t\t}\n\t\t}\n\t}\n\n\t// In read-only mode, we replay directly into the mutable memtable but never\n\t// flush it. We need to delay creation of the memtable until we know the\n\t// sequence number of the first batch that will be inserted.\n\tif !d.opts.ReadOnly {\n\t\tvar entry *flushableEntry\n\t\td.mu.mem.mutable, entry = d.newMemTable(0 /* logNum */, d.mu.versions.logSeqNum.Load(), 0 /* minSize */)\n\t\td.mu.mem.queue = append(d.mu.mem.queue, entry)\n\t}\n\n\td.mu.log.metrics.fsyncLatency = prometheus.NewHistogram(prometheus.HistogramOpts{\n\t\tBuckets: FsyncLatencyBuckets,\n\t})\n\twalOpts := wal.Options{\n\t\tPrimary:              wal.Dir{FS: opts.FS, Dirname: walDirname},\n\t\tSecondary:            wal.Dir{},\n\t\tMinUnflushedWALNum:   wal.NumWAL(d.mu.versions.minUnflushedLogNum),\n\t\tMaxNumRecyclableLogs: opts.MemTableStopWritesThreshold + 1,\n\t\tNoSyncOnClose:        opts.NoSyncOnClose,\n\t\tBytesPerSync:         opts.WALBytesPerSync,\n\t\tPreallocateSize:      d.walPreallocateSize,\n\t\tMinSyncInterval:      opts.WALMinSyncInterval,\n\t\tFsyncLatency:         d.mu.log.metrics.fsyncLatency,\n\t\tQueueSemChan:         d.commit.logSyncQSem,\n\t\tLogger:               opts.Logger,\n\t\tEventListener:        walEventListenerAdaptor{l: opts.EventListener},\n\t}\n\tif opts.WALFailover != nil {\n\t\twalOpts.Secondary = opts.WALFailover.Secondary\n\t\twalOpts.FailoverOptions = opts.WALFailover.FailoverOptions\n\t\twalOpts.FailoverWriteAndSyncLatency = prometheus.NewHistogram(prometheus.HistogramOpts{\n\t\t\tBuckets: FsyncLatencyBuckets,\n\t\t})\n\t}\n\twalDirs := append(walOpts.Dirs(), opts.WALRecoveryDirs...)\n\twals, err := wal.Scan(walDirs...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\twalManager, err := wal.Init(walOpts, wals)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer func() {\n\t\tif db == nil {\n\t\t\twalManager.Close()\n\t\t}\n\t}()\n\n\td.mu.log.manager = walManager\n\n\td.cleanupManager = openCleanupManager(opts, d.objProvider, d.onObsoleteTableDelete, d.getDeletionPacerInfo)\n\n\tif manifestExists && !opts.DisableConsistencyCheck {\n\t\tcurVersion := d.mu.versions.currentVersion()\n\t\tif err := checkConsistency(curVersion, dirname, d.objProvider); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tfileCacheSize := FileCacheSize(opts.MaxOpenFiles)\n\td.fileCache = newFileCacheContainer(\n\t\topts.FileCache, d.cacheID, d.objProvider, d.opts, fileCacheSize,\n\t\t&sstable.CategoryStatsCollector{})\n\td.newIters = d.fileCache.newIters\n\td.tableNewRangeKeyIter = tableNewRangeKeyIter(d.newIters)\n\n\td.mu.annotators.totalSize = d.makeFileSizeAnnotator(func(f *manifest.FileMetadata) bool {\n\t\treturn true\n\t})\n\td.mu.annotators.remoteSize = d.makeFileSizeAnnotator(func(f *manifest.FileMetadata) bool {\n\t\tmeta, err := d.objProvider.Lookup(fileTypeTable, f.FileBacking.DiskFileNum)\n\t\tif err != nil {\n\t\t\treturn false\n\t\t}\n\t\treturn meta.IsRemote()\n\t})\n\td.mu.annotators.externalSize = d.makeFileSizeAnnotator(func(f *manifest.FileMetadata) bool {\n\t\tmeta, err := d.objProvider.Lookup(fileTypeTable, f.FileBacking.DiskFileNum)\n\t\tif err != nil {\n\t\t\treturn false\n\t\t}\n\t\treturn meta.IsRemote() && meta.Remote.CleanupMethod == objstorage.SharedNoCleanup\n\t})\n\n\tvar previousOptionsFileNum base.DiskFileNum\n\tvar previousOptionsFilename string\n\tfor _, filename := range ls {\n\t\tft, fn, ok := base.ParseFilename(opts.FS, filename)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Don't reuse any obsolete file numbers to avoid modifying an\n\t\t// ingested sstable's original external file.\n\t\td.mu.versions.markFileNumUsed(fn)\n\n\t\tswitch ft {\n\t\tcase fileTypeLog:\n\t\t\t// Ignore.\n\t\tcase fileTypeOptions:\n\t\t\tif previousOptionsFileNum < fn {\n\t\t\t\tpreviousOptionsFileNum = fn\n\t\t\t\tpreviousOptionsFilename = filename\n\t\t\t}\n\t\tcase fileTypeTemp, fileTypeOldTemp:\n\t\t\tif !d.opts.ReadOnly {\n\t\t\t\t// Some codepaths write to a temporary file and then\n\t\t\t\t// rename it to its final location when complete.  A\n\t\t\t\t// temp file is leftover if a process exits before the\n\t\t\t\t// rename.  Remove it.\n\t\t\t\terr := opts.FS.Remove(opts.FS.PathJoin(dirname, filename))\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif n := len(wals); n > 0 {\n\t\t// Don't reuse any obsolete file numbers to avoid modifying an\n\t\t// ingested sstable's original external file.\n\t\td.mu.versions.markFileNumUsed(base.DiskFileNum(wals[n-1].Num))\n\t}\n\n\t// Ratchet d.mu.versions.nextFileNum ahead of all known objects in the\n\t// objProvider. This avoids FileNum collisions with obsolete sstables.\n\tobjects := d.objProvider.List()\n\tfor _, obj := range objects {\n\t\td.mu.versions.markFileNumUsed(obj.DiskFileNum)\n\t}\n\n\t// Validate the most-recent OPTIONS file, if there is one.\n\tif previousOptionsFilename != \"\" {\n\t\tpath := opts.FS.PathJoin(dirname, previousOptionsFilename)\n\t\tpreviousOptions, err := readOptionsFile(opts, path)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif err := opts.CheckCompatibility(previousOptions); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\t// Replay any newer log files than the ones named in the manifest.\n\tvar replayWALs wal.Logs\n\tfor i, w := range wals {\n\t\tif base.DiskFileNum(w.Num) >= d.mu.versions.minUnflushedLogNum {\n\t\t\treplayWALs = wals[i:]\n\t\t\tbreak\n\t\t}\n\t}\n\tvar flushableIngests []*ingestedFlushable\n\tfor i, lf := range replayWALs {\n\t\t// WALs other than the last one would have been closed cleanly.\n\t\t//\n\t\t// Note: we used to never require strict WAL tails when reading from older\n\t\t// versions: RocksDB 6.2.1 and the version of Pebble included in CockroachDB\n\t\t// 20.1 do not guarantee that closed WALs end cleanly. But the earliest\n\t\t// compatible Pebble format is newer and guarantees a clean EOF.\n\t\tstrictWALTail := i < len(replayWALs)-1\n\t\tfi, maxSeqNum, err := d.replayWAL(jobID, lf, strictWALTail)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif len(fi) > 0 {\n\t\t\tflushableIngests = append(flushableIngests, fi...)\n\t\t}\n\t\tif d.mu.versions.logSeqNum.Load() < maxSeqNum {\n\t\t\td.mu.versions.logSeqNum.Store(maxSeqNum)\n\t\t}\n\t}\n\tif d.mu.mem.mutable == nil {\n\t\t// Recreate the mutable memtable if replayWAL got rid of it.\n\t\tvar entry *flushableEntry\n\t\td.mu.mem.mutable, entry = d.newMemTable(d.mu.versions.getNextDiskFileNum(), d.mu.versions.logSeqNum.Load(), 0 /* minSize */)\n\t\td.mu.mem.queue = append(d.mu.mem.queue, entry)\n\t}\n\td.mu.versions.visibleSeqNum.Store(d.mu.versions.logSeqNum.Load())\n\n\tif !d.opts.ReadOnly {\n\t\td.maybeScheduleFlush()\n\t\tfor d.mu.compact.flushing {\n\t\t\td.mu.compact.cond.Wait()\n\t\t}\n\n\t\t// Create an empty .log file for the mutable memtable.\n\t\tnewLogNum := d.mu.versions.getNextDiskFileNum()\n\t\td.mu.log.writer, err = d.mu.log.manager.Create(wal.NumWAL(newLogNum), int(jobID))\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\t// This isn't strictly necessary as we don't use the log number for\n\t\t// memtables being flushed, only for the next unflushed memtable.\n\t\td.mu.mem.queue[len(d.mu.mem.queue)-1].logNum = newLogNum\n\t}\n\td.updateReadStateLocked(d.opts.DebugCheck)\n\n\tif !d.opts.ReadOnly {\n\t\t// If the Options specify a format major version higher than the\n\t\t// loaded database's, upgrade it. If this is a new database, this\n\t\t// code path also performs an initial upgrade from the starting\n\t\t// implicit MinSupported version.\n\t\t//\n\t\t// We ratchet the version this far into Open so that migrations have a read\n\t\t// state available. Note that this also results in creating/updating the\n\t\t// format version marker file.\n\t\tif opts.FormatMajorVersion > d.FormatMajorVersion() {\n\t\t\tif err := d.ratchetFormatMajorVersionLocked(opts.FormatMajorVersion); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t} else if noFormatVersionMarker {\n\t\t\t// We are creating a new store. Create the format version marker file.\n\t\t\tif err := d.writeFormatVersionMarker(d.FormatMajorVersion()); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\n\t\t// Write the current options to disk.\n\t\td.optionsFileNum = d.mu.versions.getNextDiskFileNum()\n\t\ttmpPath := base.MakeFilepath(opts.FS, dirname, fileTypeTemp, d.optionsFileNum)\n\t\toptionsPath := base.MakeFilepath(opts.FS, dirname, fileTypeOptions, d.optionsFileNum)\n\n\t\t// Write them to a temporary file first, in case we crash before\n\t\t// we're done. A corrupt options file prevents opening the\n\t\t// database.\n\t\toptionsFile, err := opts.FS.Create(tmpPath, vfs.WriteCategoryUnspecified)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tserializedOpts := []byte(opts.String())\n\t\tif _, err := optionsFile.Write(serializedOpts); err != nil {\n\t\t\treturn nil, errors.CombineErrors(err, optionsFile.Close())\n\t\t}\n\t\td.optionsFileSize = uint64(len(serializedOpts))\n\t\tif err := optionsFile.Sync(); err != nil {\n\t\t\treturn nil, errors.CombineErrors(err, optionsFile.Close())\n\t\t}\n\t\tif err := optionsFile.Close(); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\t// Atomically rename to the OPTIONS-XXXXXX path. This rename is\n\t\t// guaranteed to be atomic because the destination path does not\n\t\t// exist.\n\t\tif err := opts.FS.Rename(tmpPath, optionsPath); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif err := d.dataDir.Sync(); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tif !d.opts.ReadOnly {\n\t\t// Get a fresh list of files, in case some of the earlier flushes/compactions\n\t\t// have deleted some files.\n\t\tls, err := opts.FS.List(dirname)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\td.scanObsoleteFiles(ls, flushableIngests)\n\t\td.deleteObsoleteFiles(jobID)\n\t}\n\t// Else, nothing is obsolete.\n\n\td.mu.tableStats.cond.L = &d.mu.Mutex\n\td.mu.tableValidation.cond.L = &d.mu.Mutex\n\tif !d.opts.ReadOnly {\n\t\td.maybeCollectTableStatsLocked()\n\t}\n\td.calculateDiskAvailableBytes()\n\n\td.maybeScheduleFlush()\n\td.maybeScheduleCompaction()\n\n\t// Note: this is a no-op if invariants are disabled or race is enabled.\n\t//\n\t// Setting a finalizer on *DB causes *DB to never be reclaimed and the\n\t// finalizer to never be run. The problem is due to this limitation of\n\t// finalizers mention in the SetFinalizer docs:\n\t//\n\t//   If a cyclic structure includes a block with a finalizer, that cycle is\n\t//   not guaranteed to be garbage collected and the finalizer is not\n\t//   guaranteed to run, because there is no ordering that respects the\n\t//   dependencies.\n\t//\n\t// DB has cycles with several of its internal structures: readState,\n\t// newIters, fileCache, versions, etc. Each of this individually cause a\n\t// cycle and prevent the finalizer from being run. But we can workaround this\n\t// finializer limitation by setting a finalizer on another object that is\n\t// tied to the lifetime of DB: the DB.closed atomic.Value.\n\tdPtr := fmt.Sprintf(\"%p\", d)\n\tinvariants.SetFinalizer(d.closed, func(obj interface{}) {\n\t\tv := obj.(*atomic.Value)\n\t\tif err := v.Load(); err == nil {\n\t\t\tfmt.Fprintf(os.Stderr, \"%s: unreferenced DB not closed\\n\", dPtr)\n\t\t\tos.Exit(1)\n\t\t}\n\t})\n\n\treturn d, nil\n}\n\n// prepareAndOpenDirs opens the directories for the store (and creates them if\n// necessary).\n//\n// Returns an error if ReadOnly is set and the directories don't exist.\nfunc prepareAndOpenDirs(\n\tdirname string, opts *Options,\n) (walDirname string, dataDir vfs.File, err error) {\n\twalDirname = opts.WALDir\n\tif opts.WALDir == \"\" {\n\t\twalDirname = dirname\n\t}\n\n\t// Create directories if needed.\n\tif !opts.ReadOnly {\n\t\tf, err := mkdirAllAndSyncParents(opts.FS, dirname)\n\t\tif err != nil {\n\t\t\treturn \"\", nil, err\n\t\t}\n\t\tf.Close()\n\t\tif walDirname != dirname {\n\t\t\tf, err := mkdirAllAndSyncParents(opts.FS, walDirname)\n\t\t\tif err != nil {\n\t\t\t\treturn \"\", nil, err\n\t\t\t}\n\t\t\tf.Close()\n\t\t}\n\t\tif opts.WALFailover != nil {\n\t\t\tsecondary := opts.WALFailover.Secondary\n\t\t\tf, err := mkdirAllAndSyncParents(secondary.FS, secondary.Dirname)\n\t\t\tif err != nil {\n\t\t\t\treturn \"\", nil, err\n\t\t\t}\n\t\t\tf.Close()\n\t\t}\n\t}\n\n\tdataDir, err = opts.FS.OpenDir(dirname)\n\tif err != nil {\n\t\tif opts.ReadOnly && oserror.IsNotExist(err) {\n\t\t\treturn \"\", nil, errors.Errorf(\"pebble: database %q does not exist\", dirname)\n\t\t}\n\t\treturn \"\", nil, err\n\t}\n\tif opts.ReadOnly && walDirname != dirname {\n\t\t// Check that the wal dir exists.\n\t\twalDir, err := opts.FS.OpenDir(walDirname)\n\t\tif err != nil {\n\t\t\tdataDir.Close()\n\t\t\treturn \"\", nil, err\n\t\t}\n\t\twalDir.Close()\n\t}\n\n\treturn walDirname, dataDir, nil\n}\n\n// GetVersion returns the engine version string from the latest options\n// file present in dir. Used to check what Pebble or RocksDB version was last\n// used to write to the database stored in this directory. An empty string is\n// returned if no valid OPTIONS file with a version key was found.\nfunc GetVersion(dir string, fs vfs.FS) (string, error) {\n\tls, err := fs.List(dir)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tvar version string\n\tlastOptionsSeen := base.DiskFileNum(0)\n\tfor _, filename := range ls {\n\t\tft, fn, ok := base.ParseFilename(fs, filename)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tswitch ft {\n\t\tcase fileTypeOptions:\n\t\t\t// If this file has a higher number than the last options file\n\t\t\t// processed, reset version. This is because rocksdb often\n\t\t\t// writes multiple options files without deleting previous ones.\n\t\t\t// Otherwise, skip parsing this options file.\n\t\t\tif fn > lastOptionsSeen {\n\t\t\t\tversion = \"\"\n\t\t\t\tlastOptionsSeen = fn\n\t\t\t} else {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tf, err := fs.Open(fs.PathJoin(dir, filename))\n\t\t\tif err != nil {\n\t\t\t\treturn \"\", err\n\t\t\t}\n\t\t\tdata, err := io.ReadAll(f)\n\t\t\tf.Close()\n\n\t\t\tif err != nil {\n\t\t\t\treturn \"\", err\n\t\t\t}\n\t\t\terr = parseOptions(string(data), parseOptionsFuncs{\n\t\t\t\tvisitKeyValue: func(i, j int, section, key, value string) error {\n\t\t\t\t\tswitch {\n\t\t\t\t\tcase section == \"Version\":\n\t\t\t\t\t\tswitch key {\n\t\t\t\t\t\tcase \"pebble_version\":\n\t\t\t\t\t\t\tversion = value\n\t\t\t\t\t\tcase \"rocksdb_version\":\n\t\t\t\t\t\t\tversion = fmt.Sprintf(\"rocksdb v%s\", value)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\treturn nil\n\t\t\t\t},\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\treturn \"\", err\n\t\t\t}\n\t\t}\n\t}\n\treturn version, nil\n}\n\nfunc (d *DB) replayIngestedFlushable(\n\tb *Batch, logNum base.DiskFileNum,\n) (entry *flushableEntry, err error) {\n\tbr := b.Reader()\n\tseqNum := b.SeqNum()\n\n\tfileNums := make([]base.DiskFileNum, 0, b.Count())\n\tvar exciseSpan KeyRange\n\taddFileNum := func(encodedFileNum []byte) {\n\t\tfileNum, n := binary.Uvarint(encodedFileNum)\n\t\tif n <= 0 {\n\t\t\tpanic(\"pebble: ingest sstable file num is invalid\")\n\t\t}\n\t\tfileNums = append(fileNums, base.DiskFileNum(fileNum))\n\t}\n\n\tfor i := 0; i < int(b.Count()); i++ {\n\t\tkind, key, val, ok, err := br.Next()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif kind != InternalKeyKindIngestSST && kind != InternalKeyKindExcise {\n\t\t\tpanic(\"pebble: invalid batch key kind\")\n\t\t}\n\t\tif !ok {\n\t\t\tpanic(\"pebble: invalid batch count\")\n\t\t}\n\t\tif kind == base.InternalKeyKindExcise {\n\t\t\tif exciseSpan.Valid() {\n\t\t\t\tpanic(\"pebble: multiple excise spans in a single batch\")\n\t\t\t}\n\t\t\texciseSpan.Start = slices.Clone(key)\n\t\t\texciseSpan.End = slices.Clone(val)\n\t\t\tcontinue\n\t\t}\n\t\taddFileNum(key)\n\t}\n\n\tif _, _, _, ok, err := br.Next(); err != nil {\n\t\treturn nil, err\n\t} else if ok {\n\t\tpanic(\"pebble: invalid number of entries in batch\")\n\t}\n\n\tmeta := make([]*fileMetadata, len(fileNums))\n\tvar lastRangeKey keyspan.Span\n\tfor i, n := range fileNums {\n\t\treadable, err := d.objProvider.OpenForReading(context.TODO(), fileTypeTable, n, objstorage.OpenOptions{MustExist: true})\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"pebble: error when opening flushable ingest files\")\n\t\t}\n\t\t// NB: ingestLoad1 will close readable.\n\t\tmeta[i], lastRangeKey, err = ingestLoad1(context.TODO(), d.opts, d.FormatMajorVersion(),\n\t\t\treadable, d.cacheID, base.PhysicalTableFileNum(n), disableRangeKeyChecks())\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"pebble: error when loading flushable ingest files\")\n\t\t}\n\t}\n\tif lastRangeKey.Valid() && d.opts.Comparer.Split.HasSuffix(lastRangeKey.End) {\n\t\treturn nil, errors.AssertionFailedf(\"pebble: last ingest sstable has suffixed range key end %s\",\n\t\t\td.opts.Comparer.FormatKey(lastRangeKey.End))\n\t}\n\n\tnumFiles := len(meta)\n\tif exciseSpan.Valid() {\n\t\tnumFiles++\n\t}\n\tif uint32(numFiles) != b.Count() {\n\t\tpanic(\"pebble: couldn't load all files in WAL entry\")\n\t}\n\n\treturn d.newIngestedFlushableEntry(meta, seqNum, logNum, exciseSpan)\n}\n\n// replayWAL replays the edits in the specified WAL. If the DB is in read\n// only mode, then the WALs are replayed into memtables and not flushed. If\n// the DB is not in read only mode, then the contents of the WAL are\n// guaranteed to be flushed when a flush is scheduled after this method is run.\n// Note that this flushing is very important for guaranteeing durability:\n// the application may have had a number of pending\n// fsyncs to the WAL before the process crashed, and those fsyncs may not have\n// happened but the corresponding data may now be readable from the WAL (while\n// sitting in write-back caches in the kernel or the storage device). By\n// reading the WAL (including the non-fsynced data) and then flushing all\n// these changes (flush does fsyncs), we are able to guarantee that the\n// initial state of the DB is durable.\n//\n// This method mutates d.mu.mem.queue and possibly d.mu.mem.mutable and replays\n// WALs into the flushable queue. Flushing of the queue is expected to be handled\n// by callers. A list of flushable ingests (but not memtables) replayed is returned.\n//\n// d.mu must be held when calling this, but the mutex may be dropped and\n// re-acquired during the course of this method.\nfunc (d *DB) replayWAL(\n\tjobID JobID, ll wal.LogicalLog, strictWALTail bool,\n) (flushableIngests []*ingestedFlushable, maxSeqNum base.SeqNum, err error) {\n\trr := ll.OpenForRead()\n\tdefer rr.Close()\n\tvar (\n\t\tb               Batch\n\t\tbuf             bytes.Buffer\n\t\tmem             *memTable\n\t\tentry           *flushableEntry\n\t\toffset          wal.Offset\n\t\tlastFlushOffset int64\n\t\tkeysReplayed    int64 // number of keys replayed\n\t\tbatchesReplayed int64 // number of batches replayed\n\t)\n\n\t// TODO(jackson): This function is interspersed with panics, in addition to\n\t// corruption error propagation. Audit them to ensure we're truly only\n\t// panicking where the error points to Pebble bug and not user or\n\t// hardware-induced corruption.\n\n\t// \"Flushes\" (ie. closes off) the current memtable, if not nil.\n\tflushMem := func() {\n\t\tif mem == nil {\n\t\t\treturn\n\t\t}\n\t\tmem.writerUnref()\n\t\tif d.mu.mem.mutable == mem {\n\t\t\td.mu.mem.mutable = nil\n\t\t}\n\t\tentry.flushForced = !d.opts.ReadOnly\n\t\tvar logSize uint64\n\t\tmergedOffset := offset.Physical + offset.PreviousFilesBytes\n\t\tif mergedOffset >= lastFlushOffset {\n\t\t\tlogSize = uint64(mergedOffset - lastFlushOffset)\n\t\t}\n\t\t// Else, this was the initial memtable in the read-only case which must have\n\t\t// been empty, but we need to flush it since we don't want to add to it later.\n\t\tlastFlushOffset = mergedOffset\n\t\tentry.logSize = logSize\n\t\tmem, entry = nil, nil\n\t}\n\n\tmem = d.mu.mem.mutable\n\tif mem != nil {\n\t\tentry = d.mu.mem.queue[len(d.mu.mem.queue)-1]\n\t\tif !d.opts.ReadOnly {\n\t\t\tflushMem()\n\t\t}\n\t}\n\n\t// Creates a new memtable if there is no current memtable.\n\tensureMem := func(seqNum base.SeqNum) {\n\t\tif mem != nil {\n\t\t\treturn\n\t\t}\n\t\tmem, entry = d.newMemTable(base.DiskFileNum(ll.Num), seqNum, 0 /* minSize */)\n\t\td.mu.mem.mutable = mem\n\t\td.mu.mem.queue = append(d.mu.mem.queue, entry)\n\t}\n\n\tdefer func() {\n\t\tif err != nil {\n\t\t\terr = errors.WithDetailf(err, \"replaying wal %d, offset %s\", ll.Num, offset)\n\t\t}\n\t}()\n\n\tfor {\n\t\tvar r io.Reader\n\t\tvar err error\n\t\tr, offset, err = rr.NextRecord()\n\t\tif err == nil {\n\t\t\t_, err = io.Copy(&buf, r)\n\t\t}\n\t\tif err != nil {\n\t\t\t// It is common to encounter a zeroed or invalid chunk due to WAL\n\t\t\t// preallocation and WAL recycling. We need to distinguish these\n\t\t\t// errors from EOF in order to recognize that the record was\n\t\t\t// truncated and to avoid replaying subsequent WALs, but want\n\t\t\t// to otherwise treat them like EOF.\n\t\t\tif err == io.EOF {\n\t\t\t\tbreak\n\t\t\t} else if record.IsInvalidRecord(err) && !strictWALTail {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\treturn nil, 0, errors.Wrap(err, \"pebble: error when replaying WAL\")\n\t\t}\n\n\t\tif buf.Len() < batchrepr.HeaderLen {\n\t\t\treturn nil, 0, base.CorruptionErrorf(\"pebble: corrupt wal %s (offset %s)\",\n\t\t\t\terrors.Safe(base.DiskFileNum(ll.Num)), offset)\n\t\t}\n\n\t\tif d.opts.ErrorIfNotPristine {\n\t\t\treturn nil, 0, errors.WithDetailf(ErrDBNotPristine, \"location: %q\", d.dirname)\n\t\t}\n\n\t\t// Specify Batch.db so that Batch.SetRepr will compute Batch.memTableSize\n\t\t// which is used below.\n\t\tb = Batch{}\n\t\tb.db = d\n\t\tb.SetRepr(buf.Bytes())\n\t\tseqNum := b.SeqNum()\n\t\tmaxSeqNum = seqNum + base.SeqNum(b.Count())\n\t\tkeysReplayed += int64(b.Count())\n\t\tbatchesReplayed++\n\t\t{\n\t\t\tbr := b.Reader()\n\t\t\tif kind, _, _, ok, err := br.Next(); err != nil {\n\t\t\t\treturn nil, 0, err\n\t\t\t} else if ok && (kind == InternalKeyKindIngestSST || kind == InternalKeyKindExcise) {\n\t\t\t\t// We're in the flushable ingests (+ possibly excises) case.\n\t\t\t\t//\n\t\t\t\t// Ingests require an up-to-date view of the LSM to determine the target\n\t\t\t\t// level of ingested sstables, and to accurately compute excises. Instead of\n\t\t\t\t// doing an ingest in this function, we just enqueue a flushable ingest\n\t\t\t\t// in the flushables queue and run a regular flush.\n\t\t\t\tflushMem()\n\t\t\t\t// mem is nil here.\n\t\t\t\tentry, err = d.replayIngestedFlushable(&b, base.DiskFileNum(ll.Num))\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, 0, err\n\t\t\t\t}\n\t\t\t\tfi := entry.flushable.(*ingestedFlushable)\n\t\t\t\tflushableIngests = append(flushableIngests, fi)\n\t\t\t\td.mu.mem.queue = append(d.mu.mem.queue, entry)\n\t\t\t\t// A flushable ingest is always followed by a WAL rotation.\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\n\t\tif b.memTableSize >= uint64(d.largeBatchThreshold) {\n\t\t\tflushMem()\n\t\t\t// Make a copy of the data slice since it is currently owned by buf and will\n\t\t\t// be reused in the next iteration.\n\t\t\tb.data = slices.Clone(b.data)\n\t\t\tb.flushable, err = newFlushableBatch(&b, d.opts.Comparer)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, 0, err\n\t\t\t}\n\t\t\tentry := d.newFlushableEntry(b.flushable, base.DiskFileNum(ll.Num), b.SeqNum())\n\t\t\t// Disable memory accounting by adding a reader ref that will never be\n\t\t\t// removed.\n\t\t\tentry.readerRefs.Add(1)\n\t\t\td.mu.mem.queue = append(d.mu.mem.queue, entry)\n\t\t} else {\n\t\t\tensureMem(seqNum)\n\t\t\tif err = mem.prepare(&b); err != nil && err != arenaskl.ErrArenaFull {\n\t\t\t\treturn nil, 0, err\n\t\t\t}\n\t\t\t// We loop since DB.newMemTable() slowly grows the size of allocated memtables, so the\n\t\t\t// batch may not initially fit, but will eventually fit (since it is smaller than\n\t\t\t// largeBatchThreshold).\n\t\t\tfor err == arenaskl.ErrArenaFull {\n\t\t\t\tflushMem()\n\t\t\t\tensureMem(seqNum)\n\t\t\t\terr = mem.prepare(&b)\n\t\t\t\tif err != nil && err != arenaskl.ErrArenaFull {\n\t\t\t\t\treturn nil, 0, err\n\t\t\t\t}\n\t\t\t}\n\t\t\tif err = mem.apply(&b, seqNum); err != nil {\n\t\t\t\treturn nil, 0, err\n\t\t\t}\n\t\t\tmem.writerUnref()\n\t\t}\n\t\tbuf.Reset()\n\t}\n\n\td.opts.Logger.Infof(\"[JOB %d] WAL %s stopped reading at offset: %s; replayed %d keys in %d batches\",\n\t\tjobID, ll.String(), offset, keysReplayed, batchesReplayed)\n\tif !d.opts.ReadOnly {\n\t\tflushMem()\n\t}\n\n\t// mem is nil here, if !ReadOnly.\n\treturn flushableIngests, maxSeqNum, err\n}\n\nfunc readOptionsFile(opts *Options, path string) (string, error) {\n\tf, err := opts.FS.Open(path)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tdefer f.Close()\n\n\tdata, err := io.ReadAll(f)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn string(data), nil\n}\n\n// DBDesc briefly describes high-level state about a database.\ntype DBDesc struct {\n\t// Exists is true if an existing database was found.\n\tExists bool\n\t// FormatMajorVersion indicates the database's current format\n\t// version.\n\tFormatMajorVersion FormatMajorVersion\n\t// ManifestFilename is the filename of the current active manifest,\n\t// if the database exists.\n\tManifestFilename string\n\t// OptionsFilename is the filename of the most recent OPTIONS file, if it\n\t// exists.\n\tOptionsFilename string\n}\n\n// String implements fmt.Stringer.\nfunc (d *DBDesc) String() string {\n\tif !d.Exists {\n\t\treturn \"uninitialized\"\n\t}\n\tvar buf bytes.Buffer\n\tfmt.Fprintf(&buf, \"initialized at format major version %s\\n\", d.FormatMajorVersion)\n\tfmt.Fprintf(&buf, \"manifest: %s\\n\", d.ManifestFilename)\n\tfmt.Fprintf(&buf, \"options: %s\", d.OptionsFilename)\n\treturn buf.String()\n}\n\n// Peek looks for an existing database in dirname on the provided FS. It\n// returns a brief description of the database. Peek is read-only and\n// does not open the database\nfunc Peek(dirname string, fs vfs.FS) (*DBDesc, error) {\n\tls, err := fs.List(dirname)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvers, versMarker, err := lookupFormatMajorVersion(fs, dirname, ls)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t// TODO(jackson): Immediately closing the marker is clunky. Add a\n\t// PeekMarker variant that avoids opening the directory.\n\tif err := versMarker.Close(); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Find the currently active manifest, if there is one.\n\tmanifestMarker, manifestFileNum, exists, err := findCurrentManifest(fs, dirname, ls)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t// TODO(jackson): Immediately closing the marker is clunky. Add a\n\t// PeekMarker variant that avoids opening the directory.\n\tif err := manifestMarker.Close(); err != nil {\n\t\treturn nil, err\n\t}\n\n\tdesc := &DBDesc{\n\t\tExists:             exists,\n\t\tFormatMajorVersion: vers,\n\t}\n\n\t// Find the OPTIONS file with the highest file number within the list of\n\t// directory entries.\n\tvar previousOptionsFileNum base.DiskFileNum\n\tfor _, filename := range ls {\n\t\tft, fn, ok := base.ParseFilename(fs, filename)\n\t\tif !ok || ft != fileTypeOptions || fn < previousOptionsFileNum {\n\t\t\tcontinue\n\t\t}\n\t\tpreviousOptionsFileNum = fn\n\t\tdesc.OptionsFilename = fs.PathJoin(dirname, filename)\n\t}\n\n\tif exists {\n\t\tdesc.ManifestFilename = base.MakeFilepath(fs, dirname, fileTypeManifest, manifestFileNum)\n\t}\n\treturn desc, nil\n}\n\n// LockDirectory acquires the database directory lock in the named directory,\n// preventing another process from opening the database. LockDirectory returns a\n// handle to the held lock that may be passed to Open through Options.Lock to\n// subsequently open the database, skipping lock acquistion during Open.\n//\n// LockDirectory may be used to expand the critical section protected by the\n// database lock to include setup before the call to Open.\nfunc LockDirectory(dirname string, fs vfs.FS) (*Lock, error) {\n\tfileLock, err := fs.Lock(base.MakeFilepath(fs, dirname, fileTypeLock, base.DiskFileNum(0)))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tl := &Lock{dirname: dirname, fileLock: fileLock}\n\tl.refs.Store(1)\n\tinvariants.SetFinalizer(l, func(obj interface{}) {\n\t\tif refs := obj.(*Lock).refs.Load(); refs > 0 {\n\t\t\tpanic(errors.AssertionFailedf(\"lock for %q finalized with %d refs\", dirname, refs))\n\t\t}\n\t})\n\treturn l, nil\n}\n\n// Lock represents a file lock on a directory. It may be passed to Open through\n// Options.Lock to elide lock aquisition during Open.\ntype Lock struct {\n\tdirname  string\n\tfileLock io.Closer\n\t// refs is a count of the number of handles on the lock. refs must be 0, 1\n\t// or 2.\n\t//\n\t// When acquired by the client and passed to Open, refs = 1 and the Open\n\t// call increments it to 2. When the database is closed, it's decremented to\n\t// 1. Finally when the original caller, calls Close on the Lock, it's\n\t// drecemented to zero and the underlying file lock is released.\n\t//\n\t// When Open acquires the file lock, refs remains at 1 until the database is\n\t// closed.\n\trefs atomic.Int32\n}\n\nfunc (l *Lock) refForOpen() error {\n\t// During Open, when a user passed in a lock, the reference count must be\n\t// exactly 1. If it's zero, the lock is no longer held and is invalid. If\n\t// it's 2, the lock is already in use by another database within the\n\t// process.\n\tif !l.refs.CompareAndSwap(1, 2) {\n\t\treturn errors.Errorf(\"pebble: unexpected Lock reference count; is the lock already in use?\")\n\t}\n\treturn nil\n}\n\n// Close releases the lock, permitting another process to lock and open the\n// database. Close must not be called until after a database using the Lock has\n// been closed.\nfunc (l *Lock) Close() error {\n\tif l.refs.Add(-1) > 0 {\n\t\treturn nil\n\t}\n\tdefer func() { l.fileLock = nil }()\n\treturn l.fileLock.Close()\n}\n\nfunc (l *Lock) pathMatches(dirname string) error {\n\tif dirname == l.dirname {\n\t\treturn nil\n\t}\n\t// Check for relative paths, symlinks, etc. This isn't ideal because we're\n\t// circumventing the vfs.FS interface here.\n\t//\n\t// TODO(jackson): We could add support for retrieving file inodes through Stat\n\t// calls in the VFS interface on platforms where it's available and use that\n\t// to differentiate.\n\tdirStat, err1 := os.Stat(dirname)\n\tlockDirStat, err2 := os.Stat(l.dirname)\n\tif err1 == nil && err2 == nil && os.SameFile(dirStat, lockDirStat) {\n\t\treturn nil\n\t}\n\treturn errors.Join(\n\t\terrors.Newf(\"pebble: opts.Lock acquired in %q not %q\", l.dirname, dirname),\n\t\terr1, err2)\n}\n\n// ErrDBDoesNotExist is generated when ErrorIfNotExists is set and the database\n// does not exist.\n//\n// Note that errors can be wrapped with more details; use errors.Is().\nvar ErrDBDoesNotExist = errors.New(\"pebble: database does not exist\")\n\n// ErrDBAlreadyExists is generated when ErrorIfExists is set and the database\n// already exists.\n//\n// Note that errors can be wrapped with more details; use errors.Is().\nvar ErrDBAlreadyExists = errors.New(\"pebble: database already exists\")\n\n// ErrDBNotPristine is generated when ErrorIfNotPristine is set and the database\n// already exists and is not pristine.\n//\n// Note that errors can be wrapped with more details; use errors.Is().\nvar ErrDBNotPristine = errors.New(\"pebble: database already exists and is not pristine\")\n\n// IsCorruptionError returns true if the given error indicates database\n// corruption.\nfunc IsCorruptionError(err error) bool {\n\treturn errors.Is(err, base.ErrCorruption)\n}\n\nfunc checkConsistency(v *manifest.Version, dirname string, objProvider objstorage.Provider) error {\n\tvar errs []error\n\tdedup := make(map[base.DiskFileNum]struct{})\n\tfor level, files := range v.Levels {\n\t\titer := files.Iter()\n\t\tfor f := iter.First(); f != nil; f = iter.Next() {\n\t\t\tbackingState := f.FileBacking\n\t\t\tif _, ok := dedup[backingState.DiskFileNum]; ok {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tdedup[backingState.DiskFileNum] = struct{}{}\n\t\t\tfileNum := backingState.DiskFileNum\n\t\t\tfileSize := backingState.Size\n\t\t\t// We skip over remote objects; those are instead checked asynchronously\n\t\t\t// by the table stats loading job.\n\t\t\tmeta, err := objProvider.Lookup(base.FileTypeTable, fileNum)\n\t\t\tvar size int64\n\t\t\tif err == nil {\n\t\t\t\tif meta.IsRemote() {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tsize, err = objProvider.Size(meta)\n\t\t\t}\n\t\t\tif err != nil {\n\t\t\t\terrs = append(errs, errors.Wrapf(err, \"L%d: %s\", errors.Safe(level), fileNum))\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tif size != int64(fileSize) {\n\t\t\t\terrs = append(errs, errors.Errorf(\n\t\t\t\t\t\"L%d: %s: object size mismatch (%s): %d (disk) != %d (MANIFEST)\",\n\t\t\t\t\terrors.Safe(level), fileNum, objProvider.Path(meta),\n\t\t\t\t\terrors.Safe(size), errors.Safe(fileSize)))\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t}\n\treturn errors.Join(errs...)\n}\n\ntype walEventListenerAdaptor struct {\n\tl *EventListener\n}\n\nfunc (l walEventListenerAdaptor) LogCreated(ci wal.CreateInfo) {\n\t// TODO(sumeer): extend WALCreateInfo for the failover case in case the path\n\t// is insufficient to infer whether primary or secondary.\n\twci := WALCreateInfo{\n\t\tJobID:           ci.JobID,\n\t\tPath:            ci.Path,\n\t\tFileNum:         base.DiskFileNum(ci.Num),\n\t\tRecycledFileNum: ci.RecycledFileNum,\n\t\tErr:             ci.Err,\n\t}\n\tl.l.WALCreated(wci)\n}\n"
        },
        {
          "name": "open_test.go",
          "type": "blob",
          "size": 48.3427734375,
          "content": "// Copyright 2012 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\trandv1 \"math/rand\"\n\t\"math/rand/v2\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"reflect\"\n\t\"runtime\"\n\t\"runtime/debug\"\n\t\"slices\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"syscall\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/datadriven\"\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/metamorphic\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/cache\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n\t\"github.com/cockroachdb/pebble/objstorage\"\n\t\"github.com/cockroachdb/pebble/objstorage/objstorageprovider\"\n\t\"github.com/cockroachdb/pebble/objstorage/remote\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/cockroachdb/pebble/vfs/atomicfs\"\n\t\"github.com/cockroachdb/pebble/vfs/errorfs\"\n\t\"github.com/cockroachdb/pebble/wal\"\n\t\"github.com/cockroachdb/redact\"\n\t\"github.com/ghemawat/stream\"\n\t\"github.com/kr/pretty\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestOpenSharedFileCache(t *testing.T) {\n\tc := cache.New(cacheDefaultSize)\n\ttc := NewFileCache(c, 16, 100)\n\tdefer tc.Unref()\n\tdefer c.Unref()\n\n\td0, err := Open(\"\", testingRandomized(t, &Options{\n\t\tFS:        vfs.NewMem(),\n\t\tCache:     c,\n\t\tFileCache: tc,\n\t}))\n\tif err != nil {\n\t\tt.Errorf(\"d0 Open: %s\", err.Error())\n\t}\n\tdefer d0.Close()\n\n\td1, err := Open(\"\", testingRandomized(t, &Options{\n\t\tFS:        vfs.NewMem(),\n\t\tCache:     c,\n\t\tFileCache: tc,\n\t}))\n\tif err != nil {\n\t\tt.Errorf(\"d1 Open: %s\", err.Error())\n\t}\n\tdefer d1.Close()\n\n\t// Make sure that the Open function is using the passed in file cache\n\t// when the FileCache option is set.\n\trequire.Equalf(\n\t\tt, d0.fileCache.fileCache, d1.fileCache.fileCache,\n\t\t\"expected fileCache for both d0 and d1 to be the same\",\n\t)\n}\n\nfunc TestErrorIfExists(t *testing.T) {\n\topts := testingRandomized(t, &Options{\n\t\tFS:            vfs.NewMem(),\n\t\tErrorIfExists: true,\n\t})\n\tdefer ensureFilesClosed(t, opts)()\n\n\td0, err := Open(\"\", opts)\n\trequire.NoError(t, err)\n\trequire.NoError(t, d0.Close())\n\n\tif _, err := Open(\"\", opts); !errors.Is(err, ErrDBAlreadyExists) {\n\t\tt.Fatalf(\"expected db-already-exists error, got %v\", err)\n\t}\n\n\topts.ErrorIfExists = false\n\td1, err := Open(\"\", opts)\n\trequire.NoError(t, err)\n\trequire.NoError(t, d1.Close())\n}\n\nfunc TestErrorIfNotExists(t *testing.T) {\n\topts := testingRandomized(t, &Options{\n\t\tFS:               vfs.NewMem(),\n\t\tErrorIfNotExists: true,\n\t})\n\tdefer ensureFilesClosed(t, opts)()\n\n\t_, err := Open(\"\", opts)\n\tif !errors.Is(err, ErrDBDoesNotExist) {\n\t\tt.Fatalf(\"expected db-does-not-exist error, got %v\", err)\n\t}\n\n\t// Create the DB and try again.\n\topts.ErrorIfNotExists = false\n\td0, err := Open(\"\", opts)\n\trequire.NoError(t, err)\n\trequire.NoError(t, d0.Close())\n\n\topts.ErrorIfNotExists = true\n\td1, err := Open(\"\", opts)\n\trequire.NoError(t, err)\n\trequire.NoError(t, d1.Close())\n}\n\nfunc TestErrorIfNotPristine(t *testing.T) {\n\topts := testingRandomized(t, &Options{\n\t\tFS:                 vfs.NewMem(),\n\t\tErrorIfNotPristine: true,\n\t})\n\tdefer ensureFilesClosed(t, opts)()\n\n\td0, err := Open(\"\", opts)\n\trequire.NoError(t, err)\n\trequire.NoError(t, d0.Close())\n\n\t// Store is pristine; ok to open.\n\td1, err := Open(\"\", opts)\n\trequire.NoError(t, err)\n\trequire.NoError(t, d1.Set([]byte(\"foo\"), []byte(\"bar\"), Sync))\n\trequire.NoError(t, d1.Close())\n\n\tif _, err := Open(\"\", opts); !errors.Is(err, ErrDBNotPristine) {\n\t\tt.Fatalf(\"expected db-not-pristine error, got %v\", err)\n\t}\n\n\t// Run compaction and make sure we're still not allowed to open.\n\topts.ErrorIfNotPristine = false\n\td2, err := Open(\"\", opts)\n\trequire.NoError(t, err)\n\trequire.NoError(t, d2.Compact([]byte(\"a\"), []byte(\"z\"), false /* parallelize */))\n\trequire.NoError(t, d2.Close())\n\n\topts.ErrorIfNotPristine = true\n\tif _, err := Open(\"\", opts); !errors.Is(err, ErrDBNotPristine) {\n\t\tt.Fatalf(\"expected db-already-exists error, got %v\", err)\n\t}\n}\n\nfunc TestOpen_WALFailover(t *testing.T) {\n\tfilesystems := map[string]vfs.FS{}\n\n\textractFSAndPath := func(cmdArg datadriven.CmdArg) (fs vfs.FS, dir string) {\n\t\tvar ok bool\n\t\tif fs, ok = filesystems[cmdArg.Vals[0]]; !ok {\n\t\t\tfs = vfs.NewMem()\n\t\t\tfilesystems[cmdArg.Vals[0]] = fs\n\t\t}\n\t\tdir = cmdArg.Vals[1]\n\t\treturn fs, dir\n\t}\n\n\tgetFSAndPath := func(td *datadriven.TestData, key string) (fs vfs.FS, dir string, ok bool) {\n\t\tcmdArg, ok := td.Arg(key)\n\t\tif !ok {\n\t\t\treturn nil, \"\", false\n\t\t}\n\t\tfs, dir = extractFSAndPath(cmdArg)\n\t\treturn fs, dir, ok\n\t}\n\n\tdatadriven.RunTest(t, \"testdata/open_wal_failover\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"grep-between\":\n\t\t\tfs, path, ok := getFSAndPath(td, \"path\")\n\t\t\tif !ok {\n\t\t\t\treturn \"no `dir` provided\"\n\t\t\t}\n\t\t\tvar start, end string\n\t\t\ttd.ScanArgs(t, \"start\", &start)\n\t\t\ttd.ScanArgs(t, \"end\", &end)\n\t\t\t// Read the entirety of the file into memory (rather than passing\n\t\t\t// f into stream.ReadLines) to avoid a data race between closing the\n\t\t\t// file and stream.ReadLines asynchronously reading the file.\n\t\t\tf, err := fs.Open(path)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tdata, err := io.ReadAll(f)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\trequire.NoError(t, f.Close())\n\n\t\t\tvar buf bytes.Buffer\n\t\t\tif err := stream.Run(stream.Sequence(\n\t\t\t\tstream.ReadLines(bytes.NewReader(data)),\n\t\t\t\tstreamFilterBetweenGrep(start, end),\n\t\t\t\tstream.WriteLines(&buf),\n\t\t\t)); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn buf.String()\n\t\tcase \"list\":\n\t\t\tfs, dir, ok := getFSAndPath(td, \"path\")\n\t\t\tif !ok {\n\t\t\t\treturn \"no `path` provided\"\n\t\t\t}\n\t\t\tls, err := fs.List(dir)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tslices.Sort(ls)\n\t\t\tvar buf bytes.Buffer\n\t\t\tfor _, f := range ls {\n\t\t\t\tfmt.Fprintf(&buf, \"  %s\\n\", f)\n\t\t\t}\n\t\t\treturn buf.String()\n\t\tcase \"open\":\n\t\t\tvar dataDir string\n\t\t\to := &Options{Logger: testLogger{t}}\n\t\t\tfor _, cmdArg := range td.CmdArgs {\n\t\t\t\tswitch cmdArg.Key {\n\t\t\t\tcase \"path\":\n\t\t\t\t\to.FS, dataDir = extractFSAndPath(cmdArg)\n\t\t\t\tcase \"secondary\":\n\t\t\t\t\tfs, dir := extractFSAndPath(cmdArg)\n\t\t\t\t\to.WALFailover = &WALFailoverOptions{\n\t\t\t\t\t\tSecondary: wal.Dir{FS: fs, Dirname: dir},\n\t\t\t\t\t}\n\t\t\t\tcase \"wal-recovery-dir\":\n\t\t\t\t\tfs, dir := extractFSAndPath(cmdArg)\n\t\t\t\t\to.WALRecoveryDirs = append(o.WALRecoveryDirs, wal.Dir{FS: fs, Dirname: dir})\n\t\t\t\tdefault:\n\t\t\t\t\treturn fmt.Sprintf(\"unrecognized cmdArg %q\", cmdArg.Key)\n\t\t\t\t}\n\t\t\t}\n\t\t\tif o.FS == nil {\n\t\t\t\treturn \"no path\"\n\t\t\t}\n\t\t\td, err := Open(dataDir, o)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\trequire.NoError(t, d.Close())\n\t\t\treturn \"ok\"\n\t\tcase \"stat\":\n\t\t\tfs, path, ok := getFSAndPath(td, \"path\")\n\t\t\tif !ok {\n\t\t\t\treturn \"no `path` provided\"\n\t\t\t}\n\t\t\tfinfo, err := fs.Stat(path)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn fmt.Sprintf(\"IsDir: %t\", finfo.IsDir())\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unrecognized command %q\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc TestOpenAlreadyLocked(t *testing.T) {\n\trunTest := func(t *testing.T, lockPath, dirname string, fs vfs.FS) {\n\t\topts := testingRandomized(t, &Options{FS: fs})\n\t\tvar err error\n\t\topts.Lock, err = LockDirectory(lockPath, fs)\n\t\trequire.NoError(t, err)\n\n\t\td, err := Open(dirname, opts)\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, d.Set([]byte(\"foo\"), []byte(\"bar\"), Sync))\n\n\t\t// Try to open the same database reusing the Options containing the same\n\t\t// Lock. It should error when it observes that it's already referenced.\n\t\t_, err = Open(dirname, opts)\n\t\trequire.Error(t, err)\n\n\t\t// Close the database.\n\t\trequire.NoError(t, d.Close())\n\n\t\t// Now Opening should succeed again.\n\t\td, err = Open(dirname, opts)\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, d.Close())\n\n\t\trequire.NoError(t, opts.Lock.Close())\n\t\t// There should be no more remaining references.\n\t\trequire.Equal(t, int32(0), opts.Lock.refs.Load())\n\t}\n\tt.Run(\"memfs\", func(t *testing.T) {\n\t\trunTest(t, \"\", \"\", vfs.NewMem())\n\t})\n\tt.Run(\"disk\", func(t *testing.T) {\n\t\tt.Run(\"absolute\", func(t *testing.T) {\n\t\t\tdir := t.TempDir()\n\t\t\trunTest(t, dir, dir, vfs.Default)\n\t\t})\n\t\tt.Run(\"relative\", func(t *testing.T) {\n\t\t\tdir := t.TempDir()\n\t\t\toriginal, err := os.Getwd()\n\t\t\trequire.NoError(t, err)\n\t\t\tdefer func() { require.NoError(t, os.Chdir(original)) }()\n\n\t\t\twd := filepath.Dir(dir)\n\t\t\trequire.NoError(t, os.Chdir(wd))\n\t\t\tlockPath, err := filepath.Rel(wd, dir)\n\t\t\trequire.NoError(t, err)\n\t\t\trunTest(t, lockPath, dir, vfs.Default)\n\t\t})\n\t})\n}\n\nfunc TestNewDBFilenames(t *testing.T) {\n\tversions := map[FormatMajorVersion][]string{\n\t\tinternalFormatNewest: {\n\t\t\t\"000002.log\",\n\t\t\t\"LOCK\",\n\t\t\t\"MANIFEST-000001\",\n\t\t\t\"OPTIONS-000003\",\n\t\t\t\"marker.format-version.000006.019\",\n\t\t\t\"marker.manifest.000001.MANIFEST-000001\",\n\t\t},\n\t}\n\n\tfor formatVers, want := range versions {\n\t\tt.Run(fmt.Sprintf(\"vers=%s\", formatVers), func(t *testing.T) {\n\t\t\tmem := vfs.NewMem()\n\t\t\tfooBar := mem.PathJoin(\"foo\", \"bar\")\n\t\t\td, err := Open(fooBar, &Options{\n\t\t\t\tFS:                 mem,\n\t\t\t\tFormatMajorVersion: formatVers,\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Open: %v\", err)\n\t\t\t}\n\t\t\tif err := d.Close(); err != nil {\n\t\t\t\tt.Fatalf(\"Close: %v\", err)\n\t\t\t}\n\t\t\tgot, err := mem.List(fooBar)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"List: %v\", err)\n\t\t\t}\n\t\t\tsort.Strings(got)\n\t\t\tif !reflect.DeepEqual(got, want) {\n\t\t\t\tt.Errorf(\"\\ngot  %v\\nwant %v\", got, want)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc testOpenCloseOpenClose(t *testing.T, fs vfs.FS, root string) {\n\topts := testingRandomized(t, &Options{FS: fs})\n\n\tfor _, startFromEmpty := range []bool{false, true} {\n\t\tfor _, walDirname := range []string{\"\", \"wal\"} {\n\t\t\tfor _, length := range []int{-1, 0, 1, 1000, 10000, 100000} {\n\t\t\t\tdirname := \"sharedDatabase\" + walDirname\n\t\t\t\tif startFromEmpty {\n\t\t\t\t\tdirname = \"startFromEmpty\" + walDirname + strconv.Itoa(length)\n\t\t\t\t}\n\t\t\t\tdirname = fs.PathJoin(root, dirname)\n\t\t\t\tif walDirname == \"\" {\n\t\t\t\t\topts.WALDir = \"\"\n\t\t\t\t} else {\n\t\t\t\t\topts.WALDir = fs.PathJoin(dirname, walDirname)\n\t\t\t\t}\n\n\t\t\t\tgot, xxx := []byte(nil), \"\"\n\t\t\t\tif length >= 0 {\n\t\t\t\t\txxx = strings.Repeat(\"x\", length)\n\t\t\t\t}\n\n\t\t\t\td0, err := Open(dirname, opts)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatalf(\"sfe=%t, length=%d: Open #0: %v\",\n\t\t\t\t\t\tstartFromEmpty, length, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif length >= 0 {\n\t\t\t\t\terr = d0.Set([]byte(\"key\"), []byte(xxx), nil)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tt.Errorf(\"sfe=%t, length=%d: Set: %v\",\n\t\t\t\t\t\t\tstartFromEmpty, length, err)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\terr = d0.Close()\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Errorf(\"sfe=%t, length=%d: Close #0: %v\",\n\t\t\t\t\t\tstartFromEmpty, length, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\td1, err := Open(dirname, opts)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Errorf(\"sfe=%t, length=%d: Open #1: %v\",\n\t\t\t\t\t\tstartFromEmpty, length, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif length >= 0 {\n\t\t\t\t\tvar closer io.Closer\n\t\t\t\t\tgot, closer, err = d1.Get([]byte(\"key\"))\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tt.Errorf(\"sfe=%t, length=%d: Get: %v\",\n\t\t\t\t\t\t\tstartFromEmpty, length, err)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tgot = append([]byte(nil), got...)\n\t\t\t\t\tcloser.Close()\n\t\t\t\t}\n\t\t\t\terr = d1.Close()\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Errorf(\"sfe=%t, length=%d: Close #1: %v\",\n\t\t\t\t\t\tstartFromEmpty, length, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tif length >= 0 && string(got) != xxx {\n\t\t\t\t\tt.Errorf(\"sfe=%t, length=%d: got value differs from set value\",\n\t\t\t\t\t\tstartFromEmpty, length)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\t{\n\t\t\t\t\tgot, err := opts.FS.List(dirname)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tt.Fatalf(\"List: %v\", err)\n\t\t\t\t\t}\n\t\t\t\t\tvar optionsCount int\n\t\t\t\t\tfor _, s := range got {\n\t\t\t\t\t\tif t, _, ok := base.ParseFilename(opts.FS, s); ok && t == fileTypeOptions {\n\t\t\t\t\t\t\toptionsCount++\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tif optionsCount != 1 {\n\t\t\t\t\t\tt.Fatalf(\"expected 1 OPTIONS file, but found %d\", optionsCount)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestOpenCloseOpenClose(t *testing.T) {\n\tfor _, fstype := range []string{\"disk\", \"mem\"} {\n\t\tt.Run(fstype, func(t *testing.T) {\n\t\t\tvar fs vfs.FS\n\t\t\tvar dir string\n\t\t\tswitch fstype {\n\t\t\tcase \"disk\":\n\t\t\t\tvar err error\n\t\t\t\tdir, err = os.MkdirTemp(\"\", \"open-close\")\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\tdefer func() {\n\t\t\t\t\t_ = os.RemoveAll(dir)\n\t\t\t\t}()\n\t\t\t\tfs = vfs.Default\n\t\t\tcase \"mem\":\n\t\t\t\tdir = \"\"\n\t\t\t\tfs = vfs.NewMem()\n\t\t\t}\n\t\t\ttestOpenCloseOpenClose(t, fs, dir)\n\t\t})\n\t}\n}\n\nfunc TestOpenOptionsCheck(t *testing.T) {\n\tmem := vfs.NewMem()\n\topts := &Options{FS: mem}\n\n\td, err := Open(\"\", opts)\n\trequire.NoError(t, err)\n\trequire.NoError(t, d.Close())\n\n\tfooCmp := *base.DefaultComparer\n\tfooCmp.Name = \"foo\"\n\n\topts = &Options{\n\t\tComparer: &fooCmp,\n\t\tFS:       mem,\n\t}\n\t_, err = Open(\"\", opts)\n\trequire.Regexp(t, `comparer name from file.*!=.*`, err)\n\n\topts = &Options{\n\t\tMerger: &Merger{Name: \"bar\"},\n\t\tFS:     mem,\n\t}\n\t_, err = Open(\"\", opts)\n\trequire.Regexp(t, `merger name from file.*!=.*`, err)\n}\n\nfunc TestOpenCrashWritingOptions(t *testing.T) {\n\tmemFS := vfs.NewMem()\n\n\td, err := Open(\"\", &Options{FS: memFS})\n\trequire.NoError(t, err)\n\trequire.NoError(t, d.Close())\n\n\t// Open the database again, this time with a mocked filesystem that\n\t// will only succeed in partially writing the OPTIONS file.\n\tfs := optionsTornWriteFS{FS: memFS}\n\t_, err = Open(\"\", &Options{FS: fs, Logger: testLogger{t}})\n\trequire.Error(t, err)\n\n\t// Re-opening the database must succeed.\n\td, err = Open(\"\", &Options{FS: memFS, Logger: testLogger{t}})\n\trequire.NoError(t, err)\n\trequire.NoError(t, d.Close())\n}\n\ntype optionsTornWriteFS struct {\n\tvfs.FS\n}\n\nfunc (fs optionsTornWriteFS) Create(name string, category vfs.DiskWriteCategory) (vfs.File, error) {\n\tfile, err := fs.FS.Create(name, category)\n\tif file != nil {\n\t\tfile = optionsTornWriteFile{File: file}\n\t}\n\treturn file, err\n}\n\ntype optionsTornWriteFile struct {\n\tvfs.File\n}\n\nfunc (f optionsTornWriteFile) Write(b []byte) (int, error) {\n\t// Look for the OPTIONS-XXXXXX file's `comparer=` field.\n\tcomparerKey := []byte(\"comparer=\")\n\ti := bytes.Index(b, comparerKey)\n\tif i == -1 {\n\t\treturn f.File.Write(b)\n\t}\n\t// Write only the contents through `comparer=` and return an error.\n\tn, _ := f.File.Write(b[:i+len(comparerKey)])\n\treturn n, syscall.EIO\n}\n\nfunc TestOpenReadOnly(t *testing.T) {\n\tmem := vfs.NewMem()\n\n\t{\n\t\t// Opening a non-existent DB in read-only mode should result in no mutable\n\t\t// filesystem operations.\n\t\tvar memLog base.InMemLogger\n\t\t_, err := Open(\"non-existent\", testingRandomized(t, &Options{\n\t\t\tFS:       vfs.WithLogging(mem, memLog.Infof),\n\t\t\tReadOnly: true,\n\t\t\tWALDir:   \"non-existent-waldir\",\n\t\t}))\n\t\tif err == nil {\n\t\t\tt.Fatalf(\"expected error, but found success\")\n\t\t}\n\t\tconst expected = `open-dir: non-existent`\n\t\tif trimmed := strings.TrimSpace(memLog.String()); expected != trimmed {\n\t\t\tt.Fatalf(\"expected %q, but found %q\", expected, trimmed)\n\t\t}\n\t}\n\n\t{\n\t\t// Opening a DB with a non-existent WAL dir in read-only mode should result\n\t\t// in no mutable filesystem operations other than the LOCK.\n\t\tvar memLog base.InMemLogger\n\t\t_, err := Open(\"\", testingRandomized(t, &Options{\n\t\t\tFS:       vfs.WithLogging(mem, memLog.Infof),\n\t\t\tReadOnly: true,\n\t\t\tWALDir:   \"non-existent-waldir\",\n\t\t}))\n\t\tif err == nil {\n\t\t\tt.Fatalf(\"expected error, but found success\")\n\t\t}\n\t\tconst expected = \"open-dir: \\nopen-dir: non-existent-waldir\\nclose:\"\n\t\tif trimmed := strings.TrimSpace(memLog.String()); expected != trimmed {\n\t\t\tt.Fatalf(\"expected %q, but found %q\", expected, trimmed)\n\t\t}\n\t}\n\n\tvar contents []string\n\t{\n\t\t// Create a new DB and populate it with a small amount of data.\n\t\td, err := Open(\"\", testingRandomized(t, &Options{\n\t\t\tFS: mem,\n\t\t}))\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, d.Set([]byte(\"test\"), nil, nil))\n\t\trequire.NoError(t, d.Close())\n\t\tcontents, err = mem.List(\"\")\n\t\trequire.NoError(t, err)\n\t\tsort.Strings(contents)\n\t}\n\n\t{\n\t\t// Re-open the DB read-only. The directory contents should be unchanged.\n\t\td, err := Open(\"\", testingRandomized(t, &Options{\n\t\t\tFS:       mem,\n\t\t\tReadOnly: true,\n\t\t}))\n\t\trequire.NoError(t, err)\n\n\t\t// Verify various write operations fail in read-only mode.\n\t\trequire.EqualValues(t, ErrReadOnly, d.Compact(nil, []byte(\"\\xff\"), false))\n\t\trequire.EqualValues(t, ErrReadOnly, d.Flush())\n\t\trequire.EqualValues(t, ErrReadOnly, func() error { _, err := d.AsyncFlush(); return err }())\n\n\t\trequire.EqualValues(t, ErrReadOnly, d.Delete(nil, nil))\n\t\trequire.EqualValues(t, ErrReadOnly, d.DeleteRange(nil, nil, nil))\n\t\trequire.EqualValues(t, ErrReadOnly, d.Ingest(context.Background(), nil))\n\t\trequire.EqualValues(t, ErrReadOnly, d.LogData(nil, nil))\n\t\trequire.EqualValues(t, ErrReadOnly, d.Merge(nil, nil, nil))\n\t\trequire.EqualValues(t, ErrReadOnly, d.Set(nil, nil, nil))\n\n\t\t// Verify we can still read in read-only mode.\n\t\trequire.NoError(t, func() error {\n\t\t\t_, closer, err := d.Get([]byte(\"test\"))\n\t\t\tif closer != nil {\n\t\t\t\tcloser.Close()\n\t\t\t}\n\t\t\treturn err\n\t\t}())\n\n\t\tcheckIter := func(iter *Iterator, err error) {\n\t\t\tt.Helper()\n\n\t\t\tvar keys []string\n\t\t\tfor valid := iter.First(); valid; valid = iter.Next() {\n\t\t\t\tkeys = append(keys, string(iter.Key()))\n\t\t\t}\n\t\t\trequire.NoError(t, iter.Close())\n\t\t\texpectedKeys := []string{\"test\"}\n\t\t\tif diff := pretty.Diff(keys, expectedKeys); diff != nil {\n\t\t\t\tt.Fatalf(\"%s\\n%s\", strings.Join(diff, \"\\n\"), keys)\n\t\t\t}\n\t\t}\n\n\t\tcheckIter(d.NewIter(nil))\n\n\t\tb := d.NewIndexedBatch()\n\t\tcheckIter(b.NewIter(nil))\n\t\trequire.EqualValues(t, ErrReadOnly, b.Commit(nil))\n\t\trequire.EqualValues(t, ErrReadOnly, d.Apply(b, nil))\n\n\t\ts := d.NewSnapshot()\n\t\tcheckIter(s.NewIter(nil))\n\t\trequire.NoError(t, s.Close())\n\n\t\trequire.NoError(t, d.Close())\n\n\t\tnewContents, err := mem.List(\"\")\n\t\trequire.NoError(t, err)\n\n\t\tsort.Strings(newContents)\n\t\tif diff := pretty.Diff(contents, newContents); diff != nil {\n\t\t\tt.Fatalf(\"%s\", strings.Join(diff, \"\\n\"))\n\t\t}\n\t}\n}\n\nfunc TestOpenWALReplay(t *testing.T) {\n\tlargeValue := []byte(strings.Repeat(\"a\", 100<<10))\n\thugeValue := []byte(strings.Repeat(\"b\", 10<<20))\n\tcheckIter := func(iter *Iterator, err error) {\n\t\tt.Helper()\n\n\t\tvar keys []string\n\t\tfor valid := iter.First(); valid; valid = iter.Next() {\n\t\t\tkeys = append(keys, string(iter.Key()))\n\t\t}\n\t\trequire.NoError(t, iter.Close())\n\t\texpectedKeys := []string{\"1\", \"2\", \"3\", \"4\", \"5\"}\n\t\tif diff := pretty.Diff(keys, expectedKeys); diff != nil {\n\t\t\tt.Fatalf(\"%s\\n%s\", strings.Join(diff, \"\\n\"), keys)\n\t\t}\n\t}\n\n\tfor _, readOnly := range []bool{false, true} {\n\t\tt.Run(fmt.Sprintf(\"read-only=%t\", readOnly), func(t *testing.T) {\n\t\t\t// Create a new DB and populate it with some data.\n\t\t\tconst dir = \"\"\n\t\t\tmem := vfs.NewMem()\n\t\t\td, err := Open(dir, testingRandomized(t, &Options{\n\t\t\t\tFS:           mem,\n\t\t\t\tMemTableSize: 32 << 20,\n\t\t\t}))\n\t\t\trequire.NoError(t, err)\n\t\t\t// All these values will fit in a single memtable, so on closing the db there\n\t\t\t// will be no sst and all the data is in a single WAL.\n\t\t\trequire.NoError(t, d.Set([]byte(\"1\"), largeValue, nil))\n\t\t\trequire.NoError(t, d.Set([]byte(\"2\"), largeValue, nil))\n\t\t\trequire.NoError(t, d.Set([]byte(\"3\"), largeValue, nil))\n\t\t\trequire.NoError(t, d.Set([]byte(\"4\"), hugeValue, nil))\n\t\t\trequire.NoError(t, d.Set([]byte(\"5\"), largeValue, nil))\n\t\t\tcheckIter(d.NewIter(nil))\n\t\t\trequire.NoError(t, d.Close())\n\t\t\tfiles, err := mem.List(dir)\n\t\t\trequire.NoError(t, err)\n\t\t\tsort.Strings(files)\n\t\t\tlogCount, sstCount := 0, 0\n\t\t\tfor _, fname := range files {\n\t\t\t\tif strings.HasSuffix(fname, \".sst\") {\n\t\t\t\t\tsstCount++\n\t\t\t\t}\n\t\t\t\tif strings.HasSuffix(fname, \".log\") {\n\t\t\t\t\tlogCount++\n\t\t\t\t}\n\t\t\t}\n\t\t\trequire.Equal(t, 0, sstCount)\n\t\t\t// The memtable size starts at 256KB and doubles up to 32MB. But,\n\t\t\t// we'll jump to a power of two large enough if a batch motivating a\n\t\t\t// rotation is larger than the next power of two. We expect 3 WALs\n\t\t\t// here.\n\t\t\trequire.Equal(t, 3, logCount)\n\n\t\t\t// Re-open the DB with a smaller memtable. Values for 1, 2 will fit in the first memtable;\n\t\t\t// value for 3 will go in the next memtable; value for 4 will be in a flushable batch\n\t\t\t// which will cause the previous memtable to be flushed; value for 5 will go in the next\n\t\t\t// memtable\n\t\t\td, err = Open(dir, testingRandomized(t, &Options{\n\t\t\t\tFS:           mem,\n\t\t\t\tMemTableSize: 300 << 10,\n\t\t\t\tReadOnly:     readOnly,\n\t\t\t}))\n\t\t\trequire.NoError(t, err)\n\n\t\t\tif readOnly {\n\t\t\t\tm := d.Metrics()\n\t\t\t\trequire.Equal(t, int64(logCount), m.WAL.ObsoleteFiles)\n\t\t\t\td.mu.Lock()\n\t\t\t\trequire.NotNil(t, d.mu.mem.mutable)\n\t\t\t\td.mu.Unlock()\n\t\t\t}\n\t\t\tcheckIter(d.NewIter(nil))\n\t\t\trequire.NoError(t, d.Close())\n\t\t})\n\t}\n}\n\n// Reproduction for https://github.com/cockroachdb/pebble/issues/2234.\nfunc TestWALReplaySequenceNumBug(t *testing.T) {\n\tmem := vfs.NewMem()\n\td, err := Open(\"\", testingRandomized(t, &Options{\n\t\tFS: mem,\n\t}))\n\trequire.NoError(t, err)\n\n\td.mu.Lock()\n\t// Disable any flushes.\n\td.mu.compact.flushing = true\n\td.mu.Unlock()\n\n\trequire.NoError(t, d.Set([]byte(\"1\"), nil, nil))\n\trequire.NoError(t, d.Set([]byte(\"2\"), nil, nil))\n\n\t// Write a large batch. This should go to a separate memtable.\n\tlargeValue := []byte(strings.Repeat(\"a\", int(d.largeBatchThreshold)))\n\trequire.NoError(t, d.Set([]byte(\"1\"), largeValue, nil))\n\n\t// This write should go the mutable memtable after the large batch in the\n\t// memtable queue.\n\td.Set([]byte(\"1\"), nil, nil)\n\n\td.mu.Lock()\n\td.mu.compact.flushing = false\n\td.mu.Unlock()\n\n\t// Make sure none of the flushables have been flushed.\n\trequire.Equal(t, 3, len(d.mu.mem.queue))\n\n\t// Close the db. This doesn't cause a flush of the memtables, so they'll\n\t// have to be replayed when the db is reopened.\n\trequire.NoError(t, d.Close())\n\n\tfiles, err := mem.List(\"\")\n\trequire.NoError(t, err)\n\tsort.Strings(files)\n\tsstCount := 0\n\tfor _, fname := range files {\n\t\tif strings.HasSuffix(fname, \".sst\") {\n\t\t\tsstCount++\n\t\t}\n\t}\n\trequire.Equal(t, 0, sstCount)\n\n\t// Reopen db in read only mode to force read only wal replay.\n\td, err = Open(\"\", &Options{\n\t\tFS:       mem,\n\t\tReadOnly: true,\n\t\tLogger:   testLogger{t},\n\t})\n\trequire.NoError(t, err)\n\tval, c, _ := d.Get([]byte(\"1\"))\n\trequire.Equal(t, []byte{}, val)\n\tc.Close()\n\trequire.NoError(t, d.Close())\n}\n\n// Similar to TestOpenWALReplay, except we test replay behavior after a\n// memtable has been flushed. We test all 3 reasons for flushing: forced, size,\n// and large-batch.\nfunc TestOpenWALReplay2(t *testing.T) {\n\tfor _, readOnly := range []bool{false, true} {\n\t\tt.Run(fmt.Sprintf(\"read-only=%t\", readOnly), func(t *testing.T) {\n\t\t\tfor _, reason := range []string{\"forced\", \"size\", \"large-batch\"} {\n\t\t\t\tt.Run(reason, func(t *testing.T) {\n\t\t\t\t\tmem := vfs.NewMem()\n\t\t\t\t\td, err := Open(\"\", testingRandomized(t, &Options{\n\t\t\t\t\t\tFS:           mem,\n\t\t\t\t\t\tMemTableSize: 256 << 10,\n\t\t\t\t\t}))\n\t\t\t\t\trequire.NoError(t, err)\n\n\t\t\t\t\tswitch reason {\n\t\t\t\t\tcase \"forced\":\n\t\t\t\t\t\trequire.NoError(t, d.Set([]byte(\"1\"), nil, nil))\n\t\t\t\t\t\trequire.NoError(t, d.Flush())\n\t\t\t\t\t\trequire.NoError(t, d.Set([]byte(\"2\"), nil, nil))\n\t\t\t\t\tcase \"size\":\n\t\t\t\t\t\tlargeValue := []byte(strings.Repeat(\"a\", 100<<10))\n\t\t\t\t\t\trequire.NoError(t, d.Set([]byte(\"1\"), largeValue, nil))\n\t\t\t\t\t\trequire.NoError(t, d.Set([]byte(\"2\"), largeValue, nil))\n\t\t\t\t\t\trequire.NoError(t, d.Set([]byte(\"3\"), largeValue, nil))\n\t\t\t\t\tcase \"large-batch\":\n\t\t\t\t\t\tlargeValue := []byte(strings.Repeat(\"a\", int(d.largeBatchThreshold)))\n\t\t\t\t\t\trequire.NoError(t, d.Set([]byte(\"1\"), nil, nil))\n\t\t\t\t\t\trequire.NoError(t, d.Set([]byte(\"2\"), largeValue, nil))\n\t\t\t\t\t\trequire.NoError(t, d.Set([]byte(\"3\"), nil, nil))\n\t\t\t\t\t}\n\t\t\t\t\trequire.NoError(t, d.Close())\n\n\t\t\t\t\tfiles, err := mem.List(\"\")\n\t\t\t\t\trequire.NoError(t, err)\n\t\t\t\t\tsort.Strings(files)\n\t\t\t\t\tsstCount := 0\n\t\t\t\t\tfor _, fname := range files {\n\t\t\t\t\t\tif strings.HasSuffix(fname, \".sst\") {\n\t\t\t\t\t\t\tsstCount++\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\trequire.Equal(t, 1, sstCount)\n\n\t\t\t\t\t// Re-open the DB with a smaller memtable. Values for 1, 2 will fit in the first memtable;\n\t\t\t\t\t// value for 3 will go in the next memtable; value for 4 will be in a flushable batch\n\t\t\t\t\t// which will cause the previous memtable to be flushed; value for 5 will go in the next\n\t\t\t\t\t// memtable\n\t\t\t\t\td, err = Open(\"\", testingRandomized(t, &Options{\n\t\t\t\t\t\tFS:           mem,\n\t\t\t\t\t\tMemTableSize: 300 << 10,\n\t\t\t\t\t\tReadOnly:     readOnly,\n\t\t\t\t\t}))\n\t\t\t\t\trequire.NoError(t, err)\n\t\t\t\t\trequire.NoError(t, d.Close())\n\t\t\t\t})\n\t\t\t}\n\t\t})\n\t}\n}\n\n// TestTwoWALReplayCorrupt tests WAL-replay behavior when the first of the two\n// WALs is corrupted with an sstable checksum error. Replay must stop at the\n// first WAL because otherwise we may violate point-in-time recovery\n// semantics. See #864.\nfunc TestTwoWALReplayCorrupt(t *testing.T) {\n\t// Use the real filesystem so that we can seek and overwrite WAL data\n\t// easily.\n\tdir, err := os.MkdirTemp(\"\", \"wal-replay\")\n\trequire.NoError(t, err)\n\tdefer os.RemoveAll(dir)\n\n\td, err := Open(dir, testingRandomized(t, &Options{\n\t\tMemTableStopWritesThreshold: 4,\n\t\tMemTableSize:                2048,\n\t}))\n\trequire.NoError(t, err)\n\td.mu.Lock()\n\td.mu.compact.flushing = true\n\td.mu.Unlock()\n\trequire.NoError(t, d.Set([]byte(\"1\"), []byte(strings.Repeat(\"a\", 1024)), nil))\n\trequire.NoError(t, d.Set([]byte(\"2\"), nil, nil))\n\td.mu.Lock()\n\td.mu.compact.flushing = false\n\td.mu.Unlock()\n\trequire.NoError(t, d.Close())\n\n\t// We should have two WALs.\n\tvar logs []string\n\tls, err := vfs.Default.List(dir)\n\trequire.NoError(t, err)\n\tfor _, name := range ls {\n\t\tif filepath.Ext(name) == \".log\" {\n\t\t\tlogs = append(logs, name)\n\t\t}\n\t}\n\tsort.Strings(logs)\n\tif len(logs) < 2 {\n\t\tt.Fatalf(\"expected at least two log files, found %d\", len(logs))\n\t}\n\n\t// Corrupt the (n-1)th WAL by zeroing four bytes, 100 bytes from the end\n\t// of the file.\n\tf, err := os.OpenFile(filepath.Join(dir, logs[len(logs)-2]), os.O_RDWR, os.ModePerm)\n\trequire.NoError(t, err)\n\toff, err := f.Seek(-100, 2)\n\trequire.NoError(t, err)\n\t_, err = f.Write([]byte{0, 0, 0, 0})\n\trequire.NoError(t, err)\n\trequire.NoError(t, f.Close())\n\tt.Logf(\"zeored four bytes in %s at offset %d\\n\", logs[len(logs)-2], off)\n\n\t// Re-opening the database should detect and report the corruption.\n\t_, err = Open(dir, nil)\n\trequire.Error(t, err, \"pebble: corruption\")\n}\n\n// TestCrashOpenCrashAfterWALCreation tests a database that exits\n// ungracefully, begins recovery, creates the new WAL but promptly exits\n// ungracefully again.\n//\n// This sequence has the potential to be problematic with the strict_wal_tail\n// behavior because the first crash's WAL has an unclean tail. By the time the\n// new WAL is created, the current manifest's MinUnflushedLogNum must be\n// higher than the previous WAL.\nfunc TestCrashOpenCrashAfterWALCreation(t *testing.T) {\n\tfs := vfs.NewCrashableMem()\n\n\tgetLogs := func() (logs []string) {\n\t\tls, err := fs.List(\"\")\n\t\trequire.NoError(t, err)\n\t\tfor _, name := range ls {\n\t\t\tif filepath.Ext(name) == \".log\" {\n\t\t\t\tlogs = append(logs, name)\n\t\t\t}\n\t\t}\n\t\treturn logs\n\t}\n\n\td, err := Open(\"\", testingRandomized(t, &Options{FS: fs}))\n\trequire.NoError(t, err)\n\trequire.NoError(t, d.Set([]byte(\"abc\"), nil, Sync))\n\n\t// simulate a crash. This will leave the WAL\n\t// without an EOF trailer. It won't be an 'unclean tail' yet since the\n\t// log file was not recycled, but we'll fix that down below.\n\tcrashFS := fs.CrashClone(vfs.CrashCloneCfg{UnsyncedDataPercent: 0})\n\trequire.NoError(t, d.Close())\n\n\tfs = crashFS\n\n\t// There should be one WAL.\n\tlogs := getLogs()\n\tif len(logs) != 1 {\n\t\tt.Fatalf(\"expected one log file, found %d\", len(logs))\n\t}\n\n\t// The one WAL file doesn't have an EOF trailer, but since it wasn't\n\t// recycled it won't have garbage at the end. Rewrite it so that it has\n\t// the same contents it currently has, followed by garbage.\n\t{\n\t\tf, err := fs.Open(logs[0])\n\t\trequire.NoError(t, err)\n\t\tb, err := io.ReadAll(f)\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, f.Close())\n\t\tf, err = fs.Create(logs[0], vfs.WriteCategoryUnspecified)\n\t\trequire.NoError(t, err)\n\t\t_, err = f.Write(b)\n\t\trequire.NoError(t, err)\n\t\t_, err = f.Write([]byte{0xde, 0xad, 0xbe, 0xef})\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, f.Sync())\n\t\trequire.NoError(t, f.Close())\n\t\tdir, err := fs.OpenDir(\"\")\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, dir.Sync())\n\t\trequire.NoError(t, dir.Close())\n\t}\n\n\t// Open the database again (with syncs respected again). Wrap the\n\t// filesystem with an errorfs that will turn off syncs after a new .log\n\t// file is created and after a subsequent directory sync occurs. This\n\t// simulates a crash after the new log file is created and synced.\n\tcrashFS = nil\n\tvar crashFSAtomic atomic.Pointer[vfs.MemFS]\n\t{\n\t\tvar walCreated, dirSynced atomic.Bool\n\t\td, err := Open(\"\", &Options{\n\t\t\tFS: errorfs.Wrap(fs, errorfs.InjectorFunc(func(op errorfs.Op) error {\n\t\t\t\tif dirSynced.Load() {\n\t\t\t\t\tcrashFSAtomic.CompareAndSwap(nil, fs.CrashClone(vfs.CrashCloneCfg{UnsyncedDataPercent: 0}))\n\t\t\t\t}\n\t\t\t\tif op.Kind == errorfs.OpCreate && filepath.Ext(op.Path) == \".log\" {\n\t\t\t\t\twalCreated.Store(true)\n\t\t\t\t}\n\t\t\t\t// Record when there's a sync of the data directory after the\n\t\t\t\t// WAL was created. The data directory will have an empty\n\t\t\t\t// path because that's what we passed into Open.\n\t\t\t\tif op.Kind == errorfs.OpFileSync && op.Path == \"\" && walCreated.Load() {\n\t\t\t\t\tdirSynced.Store(true)\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t})),\n\t\t\tLogger: testLogger{t},\n\t\t})\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, d.Close())\n\t}\n\n\trequire.NotNil(t, crashFSAtomic.Load())\n\tfs = crashFSAtomic.Load()\n\n\tnewLogs := getLogs()\n\tif n := len(newLogs); n > 2 || n < 1 {\n\t\tt.Fatalf(\"expected one or two logs, found %d\\n\", n)\n\t} else if n == 1 {\n\t\t// On rare occasions, we can race between the cleaner cleaning away the old log\n\t\t// and d.Close(). If we only see one log, confirm that it has a higher\n\t\t// lognum than the previous log.\n\t\torigLogNum, err := strconv.Atoi(strings.Split(logs[0], \".\")[0])\n\t\trequire.NoError(t, err)\n\t\tcurLogNum, err := strconv.Atoi(strings.Split(newLogs[0], \".\")[0])\n\t\trequire.NoError(t, err)\n\t\trequire.Greater(t, curLogNum, origLogNum)\n\t}\n\n\t// Finally, open the database with syncs enabled.\n\td, err = Open(\"\", testingRandomized(t, &Options{FS: fs}))\n\trequire.NoError(t, err)\n\trequire.NoError(t, d.Close())\n}\n\n// TestOpenWALReplayReadOnlySeqNums tests opening a database:\n//   - in read-only mode\n//   - with multiple unflushed log files that must replayed\n//   - a MANIFEST that sets the last sequence number to a number greater than\n//     the unflushed log files\n//\n// See cockroachdb/cockroach#48660.\nfunc TestOpenWALReplayReadOnlySeqNums(t *testing.T) {\n\tconst root = \"\"\n\tmem := vfs.NewMem()\n\n\tcopyFiles := func(srcDir, dstDir string) {\n\t\tfiles, err := mem.List(srcDir)\n\t\trequire.NoError(t, err)\n\t\tfor _, f := range files {\n\t\t\trequire.NoError(t, vfs.Copy(mem, mem.PathJoin(srcDir, f), mem.PathJoin(dstDir, f)))\n\t\t}\n\t}\n\n\t// Create a new database under `/original` with a couple sstables.\n\tdir := mem.PathJoin(root, \"original\")\n\td, err := Open(dir, testingRandomized(t, &Options{FS: mem}))\n\trequire.NoError(t, err)\n\trequire.NoError(t, d.Set([]byte(\"a\"), nil, nil))\n\trequire.NoError(t, d.Flush())\n\trequire.NoError(t, d.Set([]byte(\"a\"), nil, nil))\n\trequire.NoError(t, d.Flush())\n\n\t// Prevent flushes so that multiple unflushed log files build up.\n\td.mu.Lock()\n\td.mu.compact.flushing = true\n\td.mu.Unlock()\n\n\trequire.NoError(t, d.Set([]byte(\"b\"), nil, nil))\n\td.AsyncFlush()\n\trequire.NoError(t, d.Set([]byte(\"c\"), nil, nil))\n\td.AsyncFlush()\n\trequire.NoError(t, d.Set([]byte(\"e\"), nil, nil))\n\n\t// Manually compact some of the key space so that the latest `logSeqNum` is\n\t// written to the MANIFEST. This produces a MANIFEST where the `logSeqNum`\n\t// is greater than the sequence numbers contained in the\n\t// `minUnflushedLogNum` log file\n\trequire.NoError(t, d.Compact([]byte(\"a\"), []byte(\"a\\x00\"), false))\n\td.mu.Lock()\n\tfor d.mu.compact.compactingCount > 0 {\n\t\td.mu.compact.cond.Wait()\n\t}\n\td.mu.Unlock()\n\n\td.TestOnlyWaitForCleaning()\n\t// While the MANIFEST is still in this state, copy all the files in the\n\t// database to a new directory.\n\treplayDir := mem.PathJoin(root, \"replay\")\n\trequire.NoError(t, mem.MkdirAll(replayDir, os.ModePerm))\n\tcopyFiles(dir, replayDir)\n\n\td.mu.Lock()\n\td.mu.compact.flushing = false\n\td.mu.Unlock()\n\trequire.NoError(t, d.Close())\n\n\t// Open the copy of the database in read-only mode. Since we copied all\n\t// the files before the flushes were allowed to complete, there should be\n\t// multiple unflushed log files that need to replay. Since the manual\n\t// compaction completed, the `logSeqNum` read from the manifest should be\n\t// greater than the unflushed log files' sequence numbers.\n\td, err = Open(replayDir, testingRandomized(t, &Options{\n\t\tFS:       mem,\n\t\tReadOnly: true,\n\t}))\n\trequire.NoError(t, err)\n\trequire.NoError(t, d.Close())\n}\n\nfunc TestOpenWALReplayMemtableGrowth(t *testing.T) {\n\tmem := vfs.NewMem()\n\tconst memTableSize = 64 * 1024 * 1024\n\topts := &Options{\n\t\tMemTableSize: memTableSize,\n\t\tFS:           mem,\n\t}\n\topts.testingRandomized(t)\n\tfunc() {\n\t\tdb, err := Open(\"\", opts)\n\t\trequire.NoError(t, err)\n\t\tdefer db.Close()\n\t\tb := db.NewBatch()\n\t\tdefer b.Close()\n\t\tkey := make([]byte, 8)\n\t\tval := make([]byte, 16*1024*1024)\n\t\tb.Set(key, val, nil)\n\t\trequire.NoError(t, db.Apply(b, Sync))\n\t}()\n\tdb, err := Open(\"\", opts)\n\trequire.NoError(t, err)\n\tdb.Close()\n}\n\nfunc TestPeek(t *testing.T) {\n\t// The file paths are UNIX-oriented. To avoid duplicating the test fixtures\n\t// just for Windows, just skip the tests on Windows.\n\tif runtime.GOOS == \"windows\" {\n\t\tt.Skip()\n\t}\n\n\tdatadriven.RunTest(t, \"testdata/peek\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"peek\":\n\t\t\tdesc, err := Peek(td.CmdArgs[0].String(), vfs.Default)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Sprintf(\"err=%v\", err)\n\t\t\t}\n\t\t\treturn desc.String()\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unrecognized command %q\\n\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc TestGetVersion(t *testing.T) {\n\tmem := vfs.NewMem()\n\topts := &Options{\n\t\tFS: mem,\n\t}\n\topts.testingRandomized(t)\n\n\t// Case 1: No options file.\n\tversion, err := GetVersion(\"\", mem)\n\trequire.NoError(t, err)\n\trequire.Empty(t, version)\n\n\t// Case 2: Pebble created file.\n\tdb, err := Open(\"\", opts)\n\trequire.NoError(t, err)\n\trequire.NoError(t, db.Close())\n\tversion, err = GetVersion(\"\", mem)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"0.1\", version)\n\n\t// Case 3: Manually created OPTIONS file with a higher number.\n\thighestOptionsNum := base.DiskFileNum(0)\n\tls, err := mem.List(\"\")\n\trequire.NoError(t, err)\n\tfor _, filename := range ls {\n\t\tft, fn, ok := base.ParseFilename(mem, filename)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tswitch ft {\n\t\tcase fileTypeOptions:\n\t\t\tif fn > highestOptionsNum {\n\t\t\t\thighestOptionsNum = fn\n\t\t\t}\n\t\t}\n\t}\n\tf, _ := mem.Create(fmt.Sprintf(\"OPTIONS-%d\", highestOptionsNum+1), vfs.WriteCategoryUnspecified)\n\t_, err = f.Write([]byte(\"[Version]\\n  pebble_version=0.2\\n\"))\n\trequire.NoError(t, err)\n\terr = f.Close()\n\trequire.NoError(t, err)\n\tversion, err = GetVersion(\"\", mem)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"0.2\", version)\n\n\t// Case 4: Manually created OPTIONS file with a RocksDB number.\n\tf, _ = mem.Create(fmt.Sprintf(\"OPTIONS-%d\", highestOptionsNum+2), vfs.WriteCategoryUnspecified)\n\t_, err = f.Write([]byte(\"[Version]\\n  rocksdb_version=6.2.1\\n\"))\n\trequire.NoError(t, err)\n\terr = f.Close()\n\trequire.NoError(t, err)\n\tversion, err = GetVersion(\"\", mem)\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"rocksdb v6.2.1\", version)\n}\n\n// TestOpenNeverFlushed verifies that we can open a database that had an\n// ingestion but no other operations.\nfunc TestOpenNeverFlushed(t *testing.T) {\n\tmem := vfs.NewMem()\n\n\tsstFile, err := mem.Create(\"to-ingest.sst\", vfs.WriteCategoryUnspecified)\n\trequire.NoError(t, err)\n\n\twriterOpts := sstable.WriterOptions{}\n\tw := sstable.NewWriter(objstorageprovider.NewFileWritable(sstFile), writerOpts)\n\tfor _, key := range []string{\"a\", \"b\", \"c\", \"d\"} {\n\t\trequire.NoError(t, w.Set([]byte(key), []byte(\"val-\"+key)))\n\t}\n\trequire.NoError(t, w.Close())\n\n\topts := &Options{\n\t\tFS:     mem,\n\t\tLogger: testLogger{t},\n\t}\n\tdb, err := Open(\"\", opts)\n\trequire.NoError(t, err)\n\trequire.NoError(t, db.Ingest(context.Background(), []string{\"to-ingest.sst\"}))\n\trequire.NoError(t, db.Close())\n\n\tdb, err = Open(\"\", opts)\n\trequire.NoError(t, err)\n\n\tval, closer, err := db.Get([]byte(\"b\"))\n\trequire.NoError(t, err)\n\trequire.Equal(t, \"val-b\", string(val))\n\trequire.NoError(t, closer.Close())\n\n\trequire.NoError(t, db.Close())\n}\n\nfunc TestOpen_ErrorIfUnknownFormatVersion(t *testing.T) {\n\tfs := vfs.NewMem()\n\td, err := Open(\"\", &Options{\n\t\tFS:                 fs,\n\t\tFormatMajorVersion: FormatMinSupported,\n\t})\n\trequire.NoError(t, err)\n\trequire.NoError(t, d.Close())\n\n\t// Move the marker to a version that does not exist.\n\tm, _, err := atomicfs.LocateMarker(fs, \"\", formatVersionMarkerName)\n\trequire.NoError(t, err)\n\trequire.NoError(t, m.Move(\"999999\"))\n\trequire.NoError(t, m.Close())\n\n\t_, err = Open(\"\", &Options{\n\t\tFS:                 fs,\n\t\tFormatMajorVersion: FormatMinSupported,\n\t})\n\trequire.Error(t, err)\n\trequire.EqualError(t, err, `pebble: database \"\" written in unknown format major version 999999`)\n}\n\n// ensureFilesClosed updates the provided Options to wrap the filesystem. It\n// returns a closure that when invoked fails the test if any files opened by the\n// filesystem are not closed.\n//\n// This function is intended to be used in tests with defer.\n//\n//\topts := &Options{FS: vfs.NewMem()}\n//\tdefer ensureFilesClosed(t, opts)()\n//\t/* test code */\nfunc ensureFilesClosed(t *testing.T, o *Options) func() {\n\tfs := &closeTrackingFS{\n\t\tFS:    o.FS,\n\t\tfiles: map[*closeTrackingFile]struct{}{},\n\t}\n\to.FS = fs\n\treturn func() {\n\t\t// fs.files should be empty if all the files were closed.\n\t\tfor f := range fs.files {\n\t\t\tt.Errorf(\"An open file was never closed. Opened at:\\n%s\", f.stack)\n\t\t}\n\t}\n}\n\ntype closeTrackingFS struct {\n\tvfs.FS\n\tfiles map[*closeTrackingFile]struct{}\n}\n\nfunc (fs *closeTrackingFS) wrap(file vfs.File, err error) (vfs.File, error) {\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tf := &closeTrackingFile{\n\t\tFile:  file,\n\t\tfs:    fs,\n\t\tstack: debug.Stack(),\n\t}\n\tfs.files[f] = struct{}{}\n\treturn f, err\n}\n\nfunc (fs *closeTrackingFS) Create(name string, category vfs.DiskWriteCategory) (vfs.File, error) {\n\treturn fs.wrap(fs.FS.Create(name, category))\n}\n\nfunc (fs *closeTrackingFS) Open(name string, opts ...vfs.OpenOption) (vfs.File, error) {\n\treturn fs.wrap(fs.FS.Open(name))\n}\n\nfunc (fs *closeTrackingFS) OpenDir(name string) (vfs.File, error) {\n\treturn fs.wrap(fs.FS.OpenDir(name))\n}\n\nfunc (fs *closeTrackingFS) ReuseForWrite(\n\toldname, newname string, category vfs.DiskWriteCategory,\n) (vfs.File, error) {\n\treturn fs.wrap(fs.FS.ReuseForWrite(oldname, newname, category))\n}\n\ntype closeTrackingFile struct {\n\tvfs.File\n\tfs    *closeTrackingFS\n\tstack []byte\n}\n\nfunc (f *closeTrackingFile) Close() error {\n\tdelete(f.fs.files, f)\n\treturn f.File.Close()\n}\n\nfunc TestCheckConsistency(t *testing.T) {\n\tconst dir = \"./test\"\n\tmem := vfs.NewMem()\n\tmem.MkdirAll(dir, 0755)\n\n\tprovider, err := objstorageprovider.Open(objstorageprovider.DefaultSettings(mem, dir))\n\trequire.NoError(t, err)\n\tdefer provider.Close()\n\n\tparseMeta := func(s string) (*manifest.FileMetadata, error) {\n\t\tif len(s) == 0 {\n\t\t\treturn nil, nil\n\t\t}\n\t\tparts := strings.Split(s, \":\")\n\t\tif len(parts) != 2 {\n\t\t\treturn nil, errors.Errorf(\"malformed table spec: %q\", s)\n\t\t}\n\t\tfileNum, err := strconv.Atoi(strings.TrimSpace(parts[0]))\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tsize, err := strconv.Atoi(strings.TrimSpace(parts[1]))\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tm := &manifest.FileMetadata{\n\t\t\tFileNum: base.FileNum(fileNum),\n\t\t\tSize:    uint64(size),\n\t\t}\n\t\tm.InitPhysicalBacking()\n\t\treturn m, nil\n\t}\n\n\tdatadriven.RunTest(t, \"testdata/version_check_consistency\",\n\t\tfunc(t *testing.T, d *datadriven.TestData) string {\n\t\t\tswitch d.Cmd {\n\t\t\tcase \"check-consistency\":\n\t\t\t\tvar filesByLevel [manifest.NumLevels][]*manifest.FileMetadata\n\t\t\t\tvar files *[]*manifest.FileMetadata\n\n\t\t\t\tfor _, data := range strings.Split(d.Input, \"\\n\") {\n\t\t\t\t\tswitch data {\n\t\t\t\t\tcase \"L0\", \"L1\", \"L2\", \"L3\", \"L4\", \"L5\", \"L6\":\n\t\t\t\t\t\tlevel, err := strconv.Atoi(data[1:])\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t\t}\n\t\t\t\t\t\tfiles = &filesByLevel[level]\n\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tm, err := parseMeta(data)\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif m != nil {\n\t\t\t\t\t\t\t*files = append(*files, m)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tredactErr := false\n\t\t\t\tfor _, arg := range d.CmdArgs {\n\t\t\t\t\tswitch v := arg.String(); v {\n\t\t\t\t\tcase \"redact\":\n\t\t\t\t\t\tredactErr = true\n\t\t\t\t\tdefault:\n\t\t\t\t\t\treturn fmt.Sprintf(\"unknown argument: %q\", v)\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tv := manifest.NewVersion(base.DefaultComparer, 0, filesByLevel)\n\t\t\t\terr := checkConsistency(v, dir, provider)\n\t\t\t\tif err != nil {\n\t\t\t\t\tif redactErr {\n\t\t\t\t\t\tredacted := redact.Sprint(err).Redact()\n\t\t\t\t\t\treturn string(redacted)\n\t\t\t\t\t}\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\treturn \"OK\"\n\n\t\t\tcase \"build\":\n\t\t\t\tfor _, data := range strings.Split(d.Input, \"\\n\") {\n\t\t\t\t\tm, err := parseMeta(data)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t\tpath := base.MakeFilepath(mem, dir, base.FileTypeTable, m.FileBacking.DiskFileNum)\n\t\t\t\t\t_ = mem.Remove(path)\n\t\t\t\t\tf, err := mem.Create(path, vfs.WriteCategoryUnspecified)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t\t_, err = f.Write(make([]byte, m.Size))\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err.Error()\n\t\t\t\t\t}\n\t\t\t\t\tf.Close()\n\t\t\t\t}\n\t\t\t\treturn \"\"\n\n\t\t\tdefault:\n\t\t\t\treturn fmt.Sprintf(\"unknown command: %s\", d.Cmd)\n\t\t\t}\n\t\t})\n}\n\nfunc TestOpenRatchetsNextFileNum(t *testing.T) {\n\tmem := vfs.NewMem()\n\tmemShared := remote.NewInMem()\n\n\topts := &Options{FS: mem, Logger: testLogger{t}}\n\topts.Experimental.CreateOnShared = remote.CreateOnSharedAll\n\topts.Experimental.RemoteStorage = remote.MakeSimpleFactory(map[remote.Locator]remote.Storage{\n\t\t\"\": memShared,\n\t})\n\td, err := Open(\"\", opts)\n\trequire.NoError(t, err)\n\td.SetCreatorID(1)\n\n\trequire.NoError(t, d.Set([]byte(\"foo\"), []byte(\"value\"), nil))\n\trequire.NoError(t, d.Set([]byte(\"bar\"), []byte(\"value\"), nil))\n\trequire.NoError(t, d.Flush())\n\trequire.NoError(t, d.Compact([]byte(\"a\"), []byte(\"z\"), false))\n\n\t// Create a shared file with the newest file num and then close the db.\n\td.mu.Lock()\n\tnextFileNum := d.mu.versions.getNextDiskFileNum()\n\tw, _, err := d.objProvider.Create(context.TODO(), fileTypeTable, nextFileNum, objstorage.CreateOptions{PreferSharedStorage: true})\n\trequire.NoError(t, err)\n\trequire.NoError(t, w.Write([]byte(\"foobar\")))\n\trequire.NoError(t, w.Finish())\n\trequire.NoError(t, d.objProvider.Sync())\n\td.mu.Unlock()\n\n\t// Write one key and then close the db. This write will stay in the memtable,\n\t// forcing the reopen to do a compaction on open.\n\trequire.NoError(t, d.Set([]byte(\"foo1\"), []byte(\"value\"), nil))\n\trequire.NoError(t, d.Close())\n\n\t// Reopen db. Compactions should happen without error.\n\td, err = Open(\"\", opts)\n\trequire.NoError(t, err)\n\trequire.NoError(t, d.Set([]byte(\"foo2\"), []byte(\"value\"), nil))\n\trequire.NoError(t, d.Set([]byte(\"bar2\"), []byte(\"value\"), nil))\n\trequire.NoError(t, d.Flush())\n\trequire.NoError(t, d.Compact([]byte(\"a\"), []byte(\"z\"), false))\n\n}\n\nfunc TestMkdirAllAndSyncParents(t *testing.T) {\n\tif filepath.Separator != '/' {\n\t\tt.Skip(\"skipping due to path separator\")\n\t}\n\tpwd, err := os.Getwd()\n\trequire.NoError(t, err)\n\tdefer func() { os.Chdir(pwd) }()\n\n\tfilesystems := map[string]vfs.FS{}\n\trootPaths := map[string]string{}\n\tvar buf bytes.Buffer\n\tdatadriven.RunTest(t, \"testdata/mkdir_all_and_sync_parents\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tbuf.Reset()\n\t\tswitch td.Cmd {\n\t\tcase \"mkfs\":\n\t\t\tvar fsName string\n\t\t\ttd.ScanArgs(t, \"fs\", &fsName)\n\t\t\tif td.HasArg(\"memfs\") {\n\t\t\t\tfilesystems[fsName] = vfs.NewMem()\n\t\t\t\treturn \"new memfs\"\n\t\t\t}\n\t\t\tfilesystems[fsName] = vfs.Default\n\t\t\trootPaths[fsName] = t.TempDir()\n\t\t\treturn \"new default fs\"\n\t\tcase \"mkdir-all-and-sync-parents\":\n\t\t\tvar fsName, path string\n\t\t\ttd.ScanArgs(t, \"fs\", &fsName)\n\t\t\ttd.ScanArgs(t, \"path\", &path)\n\t\t\tif p, ok := rootPaths[fsName]; ok {\n\t\t\t\trequire.NoError(t, os.Chdir(p))\n\t\t\t}\n\t\t\tfs := vfs.WithLogging(filesystems[fsName], func(format string, args ...interface{}) {\n\t\t\t\tfmt.Fprintf(&buf, format+\"\\n\", args...)\n\t\t\t})\n\t\t\tf, err := mkdirAllAndSyncParents(fs, path)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\trequire.NoError(t, f.Close())\n\t\t\treturn buf.String()\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unrecognized command %q\", td.Cmd)\n\t\t}\n\t})\n}\n\n// TestWALFailoverRandomized is a randomzied test exercising recovery in the\n// presence of WAL failover. It repeatedly opens a database, writes a number of\n// batches concurrently and simulates a hard crash using vfs.NewCrashableMem. It\n// ensures that the resulting DB state opens successfully, and the contents of\n// the DB match the expectations based on the keys written.\n//\n// This test is partially a regression test for #3865.\nfunc TestWALFailoverRandomized(t *testing.T) {\n\tseed := time.Now().UnixNano()\n\tt.Logf(\"seed %d\", seed)\n\tmem := vfs.NewCrashableMem()\n\tmakeOptions := func(mem *vfs.MemFS) *Options {\n\t\tfailoverOpts := WALFailoverOptions{\n\t\t\tSecondary: wal.Dir{FS: mem, Dirname: \"secondary\"},\n\t\t\tFailoverOptions: wal.FailoverOptions{\n\t\t\t\tPrimaryDirProbeInterval:      time.Microsecond,\n\t\t\t\tHealthyProbeLatencyThreshold: 20 * time.Microsecond,\n\t\t\t\tHealthyInterval:              10 * time.Microsecond,\n\t\t\t\tUnhealthySamplingInterval:    time.Microsecond,\n\t\t\t\tUnhealthyOperationLatencyThreshold: func() (time.Duration, bool) {\n\t\t\t\t\treturn 10 * time.Microsecond, true\n\t\t\t\t},\n\t\t\t\tElevatedWriteStallThresholdLag: 50 * time.Microsecond,\n\t\t\t},\n\t\t}\n\n\t\tmean := time.Duration(rand.ExpFloat64() * float64(time.Microsecond))\n\t\tp := rand.Float64()\n\t\tt.Logf(\"Injecting mean %s of latency with p=%.3f\", mean, p)\n\t\tfs := errorfs.Wrap(mem, errorfs.RandomLatency(errorfs.Randomly(p, seed), mean, seed, time.Second))\n\t\treturn &Options{\n\t\t\tFS:                          fs,\n\t\t\tFormatMajorVersion:          internalFormatNewest,\n\t\t\tLogger:                      testLogger{t},\n\t\t\tMemTableSize:                128 << 10, // 128 KiB\n\t\t\tMemTableStopWritesThreshold: 4,\n\t\t\tWALFailover:                 &failoverOpts,\n\t\t}\n\t}\n\n\t// KV state tracking.\n\t//\n\t// This test uses all uint16 big-endian integers as a keyspace. Values are\n\t// randomly sized but always contain the key in the first two bytes. We\n\t// track the state of all KVs throughout the test (whether they're\n\t// definitely set, maybe set or definitely unset).\n\t//\n\t// Note that the test may wrap around to the beginning of the keyspace. This\n\t// may cause KVs left at kvMaybeSet to be written and be definitively set\n\t// the second time around.\n\ttype kvState int8\n\tconst (\n\t\tkvUnset    kvState = 0\n\t\tkvMaybeSet kvState = 1\n\t\tkvSet      kvState = 2\n\t)\n\tconst keyspaceSize = math.MaxUint16 + 1\n\tvar kvs struct {\n\t\tsync.Mutex\n\t\tstates   [keyspaceSize]kvState\n\t\tcount    uint64 // [0, math.MaxUint16]; INVARIANT: states[count:] all zeroes\n\t\tcrashing bool\n\t}\n\tsetIsCrashing := func(crashing bool) {\n\t\tkvs.Lock()\n\t\tdefer kvs.Unlock()\n\t\tkvs.crashing = crashing\n\t}\n\t// transitionState is called by goroutines responsible for committing\n\t// batches to the engine. Note that 'i' is the index of the KV before\n\t// wrapping around and needs to be modded by math.MaxUint16.\n\ttransitionState := func(i, count uint64, state kvState) {\n\t\tkvs.Lock()\n\t\tdefer kvs.Unlock()\n\t\tif kvs.crashing && state == kvSet {\n\t\t\t// We're racing with a CrashClone call and it's indeterminate\n\t\t\t// whether what we think we synced actually made the cut. Leave the\n\t\t\t// kvs at the kvMaybeSet.\n\t\t\tstate = kvMaybeSet\n\t\t}\n\t\tfor j := uint64(0); j < count; j++ {\n\t\t\tidx := (i + j) % keyspaceSize\n\t\t\tkvs.states[idx] = max(kvs.states[idx], state)\n\t\t}\n\t\tkvs.count = max(kvs.count, i+count, math.MaxUint16)\n\t}\n\t// validateState is called on recovery to ensure that engine state agrees\n\t// with the tracked KV state.\n\tvalidateState := func(d *DB) {\n\t\tit, err := d.NewIter(nil)\n\t\trequire.NoError(t, err)\n\t\tvalid := it.First()\n\t\tfor i := 0; i < int(kvs.count); i++ {\n\t\t\tvar kvIsSet bool\n\t\t\tif valid {\n\t\t\t\trequire.Len(t, it.Key(), 2)\n\t\t\t\trequire.Equal(t, it.Key(), it.Value()[:2])\n\t\t\t\tkvIsSet = binary.BigEndian.Uint16(it.Key()) == uint16(i)\n\t\t\t}\n\t\t\tif kvIsSet && kvs.states[i] == kvUnset {\n\t\t\t\tt.Fatalf(\"key %04x is set; state says it should be unset\", i)\n\t\t\t} else if !kvIsSet && kvs.states[i] == kvSet {\n\t\t\t\tt.Fatalf(\"key %04x is unset; state says it should be set\", i)\n\t\t\t}\n\t\t\tif kvIsSet {\n\t\t\t\tvalid = it.Next()\n\t\t\t}\n\t\t}\n\t\trequire.NoError(t, it.Close())\n\t}\n\n\td, err := Open(\"primary\", makeOptions(mem))\n\trequire.NoError(t, err)\n\trng := rand.New(rand.NewPCG(0, uint64(seed)))\n\tvar wg sync.WaitGroup\n\tvar n uint64\n\trandomOps := metamorphic.Weighted[func()]{\n\t\t{Weight: 1, Item: func() {\n\t\t\ttime.Sleep(time.Microsecond * time.Duration(rand.IntN(30)))\n\t\t\tt.Log(\"initiating hard crash\")\n\t\t\tsetIsCrashing(true)\n\t\t\t// Take a crash-consistent clone of the filesystem and use that going forward.\n\t\t\tmem = mem.CrashClone(vfs.CrashCloneCfg{UnsyncedDataPercent: 50, RNG: rng})\n\t\t\twg.Wait() // Wait for outstanding batch commits to finish.\n\t\t\t_ = d.Close()\n\t\t\td, err = Open(\"primary\", makeOptions(mem))\n\t\t\trequire.NoError(t, err)\n\t\t\tvalidateState(d)\n\t\t\tsetIsCrashing(false)\n\t\t}},\n\t\t{Weight: 20, Item: func() {\n\t\t\tcount := rng.IntN(14) + 1\n\t\t\tvar k [2]byte\n\t\t\tvar v [4096]byte\n\t\t\tb := d.NewBatch()\n\t\t\tfor i := 0; i < count; i++ {\n\t\t\t\tj := uint16((n + uint64(i)) % keyspaceSize)\n\t\t\t\tbinary.BigEndian.PutUint16(k[:], j)\n\t\t\t\tvn := max(rng.IntN(cap(v)), 2)\n\t\t\t\tbinary.BigEndian.PutUint16(v[:], j)\n\t\t\t\trequire.NoError(t, b.Set(k[:], v[:vn], nil))\n\t\t\t}\n\t\t\tmaybeSync := NoSync\n\t\t\tif rng.IntN(2) == 1 {\n\t\t\t\tmaybeSync = Sync\n\t\t\t}\n\t\t\twg.Add(1)\n\t\t\tgo func(n, count uint64) {\n\t\t\t\tdefer wg.Done()\n\t\t\t\ttransitionState(n, count, kvMaybeSet)\n\t\t\t\trequire.NoError(t, b.Commit(maybeSync))\n\t\t\t\tif maybeSync == Sync {\n\t\t\t\t\ttransitionState(n, count, kvSet)\n\t\t\t\t}\n\t\t\t}(n, uint64(count))\n\t\t\tn += uint64(count)\n\t\t}},\n\t}\n\tnextRandomOp := randomOps.RandomDeck(randv1.New(randv1.NewSource(rng.Int64())))\n\tfor o := 0; o < 1000; o++ {\n\t\tnextRandomOp()()\n\t}\n}\n"
        },
        {
          "name": "options.go",
          "type": "blob",
          "size": 87.1787109375,
          "content": "// Copyright 2011 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"io\"\n\t\"runtime\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\t\"unicode\"\n\n\t\"github.com/cockroachdb/crlib/fifo\"\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/cache\"\n\t\"github.com/cockroachdb/pebble/internal/humanize\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n\t\"github.com/cockroachdb/pebble/internal/testkeys\"\n\t\"github.com/cockroachdb/pebble/objstorage/objstorageprovider\"\n\t\"github.com/cockroachdb/pebble/objstorage/remote\"\n\t\"github.com/cockroachdb/pebble/rangekey\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n\t\"github.com/cockroachdb/pebble/sstable/block\"\n\t\"github.com/cockroachdb/pebble/sstable/colblk\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/cockroachdb/pebble/wal\"\n)\n\nconst (\n\tcacheDefaultSize       = 8 << 20 // 8 MB\n\tdefaultLevelMultiplier = 10\n)\n\n// Compression exports the base.Compression type.\ntype Compression = block.Compression\n\n// Exported Compression constants.\nconst (\n\tDefaultCompression = block.DefaultCompression\n\tNoCompression      = block.NoCompression\n\tSnappyCompression  = block.SnappyCompression\n\tZstdCompression    = block.ZstdCompression\n)\n\n// FilterType exports the base.FilterType type.\ntype FilterType = base.FilterType\n\n// Exported TableFilter constants.\nconst (\n\tTableFilter = base.TableFilter\n)\n\n// FilterWriter exports the base.FilterWriter type.\ntype FilterWriter = base.FilterWriter\n\n// FilterPolicy exports the base.FilterPolicy type.\ntype FilterPolicy = base.FilterPolicy\n\n// KeySchema exports the colblk.KeySchema type.\ntype KeySchema = colblk.KeySchema\n\n// BlockPropertyCollector exports the sstable.BlockPropertyCollector type.\ntype BlockPropertyCollector = sstable.BlockPropertyCollector\n\n// BlockPropertyFilter exports the sstable.BlockPropertyFilter type.\ntype BlockPropertyFilter = base.BlockPropertyFilter\n\n// ShortAttributeExtractor exports the base.ShortAttributeExtractor type.\ntype ShortAttributeExtractor = base.ShortAttributeExtractor\n\n// UserKeyPrefixBound exports the sstable.UserKeyPrefixBound type.\ntype UserKeyPrefixBound = sstable.UserKeyPrefixBound\n\n// CompactionLimiter exports the base.CompactionLimiter type.\ntype CompactionLimiter = base.CompactionLimiter\n\n// CompactionSlot exports the base.CompactionSlot type.\ntype CompactionSlot = base.CompactionSlot\n\n// IterKeyType configures which types of keys an iterator should surface.\ntype IterKeyType int8\n\nconst (\n\t// IterKeyTypePointsOnly configures an iterator to iterate over point keys\n\t// only.\n\tIterKeyTypePointsOnly IterKeyType = iota\n\t// IterKeyTypeRangesOnly configures an iterator to iterate over range keys\n\t// only.\n\tIterKeyTypeRangesOnly\n\t// IterKeyTypePointsAndRanges configures an iterator iterate over both point\n\t// keys and range keys simultaneously.\n\tIterKeyTypePointsAndRanges\n)\n\n// String implements fmt.Stringer.\nfunc (t IterKeyType) String() string {\n\tswitch t {\n\tcase IterKeyTypePointsOnly:\n\t\treturn \"points-only\"\n\tcase IterKeyTypeRangesOnly:\n\t\treturn \"ranges-only\"\n\tcase IterKeyTypePointsAndRanges:\n\t\treturn \"points-and-ranges\"\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"unknown key type %d\", t))\n\t}\n}\n\n// IterOptions hold the optional per-query parameters for NewIter.\n//\n// Like Options, a nil *IterOptions is valid and means to use the default\n// values.\ntype IterOptions struct {\n\t// LowerBound specifies the smallest key (inclusive) that the iterator will\n\t// return during iteration. If the iterator is seeked or iterated past this\n\t// boundary the iterator will return Valid()==false. Setting LowerBound\n\t// effectively truncates the key space visible to the iterator.\n\tLowerBound []byte\n\t// UpperBound specifies the largest key (exclusive) that the iterator will\n\t// return during iteration. If the iterator is seeked or iterated past this\n\t// boundary the iterator will return Valid()==false. Setting UpperBound\n\t// effectively truncates the key space visible to the iterator.\n\tUpperBound []byte\n\t// SkipPoint may be used to skip over point keys that don't match an\n\t// arbitrary predicate during iteration. If set, the Iterator invokes\n\t// SkipPoint for keys encountered. If SkipPoint returns true, the iterator\n\t// will skip the key without yielding it to the iterator operation in\n\t// progress.\n\t//\n\t// SkipPoint must be a pure function and always return the same result when\n\t// provided the same arguments. The iterator may call SkipPoint multiple\n\t// times for the same user key.\n\tSkipPoint func(userKey []byte) bool\n\t// PointKeyFilters can be used to avoid scanning tables and blocks in tables\n\t// when iterating over point keys. This slice represents an intersection\n\t// across all filters, i.e., all filters must indicate that the block is\n\t// relevant.\n\t//\n\t// Performance note: When len(PointKeyFilters) > 0, the caller should ensure\n\t// that cap(PointKeyFilters) is at least len(PointKeyFilters)+1. This helps\n\t// avoid allocations in Pebble internal code that mutates the slice.\n\tPointKeyFilters []BlockPropertyFilter\n\t// RangeKeyFilters can be usefd to avoid scanning tables and blocks in tables\n\t// when iterating over range keys. The same requirements that apply to\n\t// PointKeyFilters apply here too.\n\tRangeKeyFilters []BlockPropertyFilter\n\t// KeyTypes configures which types of keys to iterate over: point keys,\n\t// range keys, or both.\n\tKeyTypes IterKeyType\n\t// RangeKeyMasking can be used to enable automatic masking of point keys by\n\t// range keys. Range key masking is only supported during combined range key\n\t// and point key iteration mode (IterKeyTypePointsAndRanges).\n\tRangeKeyMasking RangeKeyMasking\n\n\t// OnlyReadGuaranteedDurable is an advanced option that is only supported by\n\t// the Reader implemented by DB. When set to true, only the guaranteed to be\n\t// durable state is visible in the iterator.\n\t// - This definition is made under the assumption that the FS implementation\n\t//   is providing a durability guarantee when data is synced.\n\t// - The visible state represents a consistent point in the history of the\n\t//   DB.\n\t// - The implementation is free to choose a conservative definition of what\n\t//   is guaranteed durable. For simplicity, the current implementation\n\t//   ignores memtables. A more sophisticated implementation could track the\n\t//   highest seqnum that is synced to the WAL and published and use that as\n\t//   the visible seqnum for an iterator. Note that the latter approach is\n\t//   not strictly better than the former since we can have DBs that are (a)\n\t//   synced more rarely than memtable flushes, (b) have no WAL. (a) is\n\t//   likely to be true in a future CockroachDB context where the DB\n\t//   containing the state machine may be rarely synced.\n\t// NB: this current implementation relies on the fact that memtables are\n\t// flushed in seqnum order, and any ingested sstables that happen to have a\n\t// lower seqnum than a non-flushed memtable don't have any overlapping keys.\n\t// This is the fundamental level invariant used in other code too, like when\n\t// merging iterators.\n\t//\n\t// Semantically, using this option provides the caller a \"snapshot\" as of\n\t// the time the most recent memtable was flushed. An alternate interface\n\t// would be to add a NewSnapshot variant. Creating a snapshot is heavier\n\t// weight than creating an iterator, so we have opted to support this\n\t// iterator option.\n\tOnlyReadGuaranteedDurable bool\n\t// UseL6Filters allows the caller to opt into reading filter blocks for L6\n\t// sstables. Helpful if a lot of SeekPrefixGEs are expected in quick\n\t// succession, that are also likely to not yield a single key. Filter blocks in\n\t// L6 can be relatively large, often larger than data blocks, so the benefit of\n\t// loading them in the cache is minimized if the probability of the key\n\t// existing is not low or if we just expect a one-time Seek (where loading the\n\t// data block directly is better).\n\tUseL6Filters bool\n\t// Category is used for categorized iterator stats. This should not be\n\t// changed by calling SetOptions.\n\tCategory sstable.Category\n\n\tDebugRangeKeyStack bool\n\n\t// Internal options.\n\n\tlogger Logger\n\t// Layer corresponding to this file. Only passed in if constructed by a\n\t// levelIter.\n\tlayer manifest.Layer\n\t// disableLazyCombinedIteration is an internal testing option.\n\tdisableLazyCombinedIteration bool\n\t// snapshotForHideObsoletePoints is specified for/by levelIter when opening\n\t// files and is used to decide whether to hide obsolete points. A value of 0\n\t// implies obsolete points should not be hidden.\n\tsnapshotForHideObsoletePoints base.SeqNum\n\n\t// NB: If adding new Options, you must account for them in iterator\n\t// construction and Iterator.SetOptions.\n}\n\n// GetLowerBound returns the LowerBound or nil if the receiver is nil.\nfunc (o *IterOptions) GetLowerBound() []byte {\n\tif o == nil {\n\t\treturn nil\n\t}\n\treturn o.LowerBound\n}\n\n// GetUpperBound returns the UpperBound or nil if the receiver is nil.\nfunc (o *IterOptions) GetUpperBound() []byte {\n\tif o == nil {\n\t\treturn nil\n\t}\n\treturn o.UpperBound\n}\n\nfunc (o *IterOptions) pointKeys() bool {\n\tif o == nil {\n\t\treturn true\n\t}\n\treturn o.KeyTypes == IterKeyTypePointsOnly || o.KeyTypes == IterKeyTypePointsAndRanges\n}\n\nfunc (o *IterOptions) rangeKeys() bool {\n\tif o == nil {\n\t\treturn false\n\t}\n\treturn o.KeyTypes == IterKeyTypeRangesOnly || o.KeyTypes == IterKeyTypePointsAndRanges\n}\n\nfunc (o *IterOptions) getLogger() Logger {\n\tif o == nil || o.logger == nil {\n\t\treturn DefaultLogger\n\t}\n\treturn o.logger\n}\n\n// SpanIterOptions creates a SpanIterOptions from this IterOptions.\nfunc (o *IterOptions) SpanIterOptions() keyspan.SpanIterOptions {\n\tif o == nil {\n\t\treturn keyspan.SpanIterOptions{}\n\t}\n\treturn keyspan.SpanIterOptions{\n\t\tRangeKeyFilters: o.RangeKeyFilters,\n\t}\n}\n\n// scanInternalOptions is similar to IterOptions, meant for use with\n// scanInternalIterator.\ntype scanInternalOptions struct {\n\tIterOptions\n\n\tcategory sstable.Category\n\n\tvisitPointKey     func(key *InternalKey, value LazyValue, iterInfo IteratorLevel) error\n\tvisitRangeDel     func(start, end []byte, seqNum SeqNum) error\n\tvisitRangeKey     func(start, end []byte, keys []rangekey.Key) error\n\tvisitSharedFile   func(sst *SharedSSTMeta) error\n\tvisitExternalFile func(sst *ExternalFile) error\n\n\t// includeObsoleteKeys specifies whether keys shadowed by newer internal keys\n\t// are exposed. If false, only one internal key per user key is exposed.\n\tincludeObsoleteKeys bool\n\n\t// rateLimitFunc is used to limit the amount of bytes read per second.\n\trateLimitFunc func(key *InternalKey, value LazyValue) error\n}\n\n// RangeKeyMasking configures automatic hiding of point keys by range keys. A\n// non-nil Suffix enables range-key masking. When enabled, range keys with\n// suffixes ≥ Suffix behave as masks. All point keys that are contained within a\n// masking range key's bounds and have suffixes greater than the range key's\n// suffix are automatically skipped.\n//\n// Specifically, when configured with a RangeKeyMasking.Suffix _s_, and there\n// exists a range key with suffix _r_ covering a point key with suffix _p_, and\n//\n//\t_s_ ≤ _r_ < _p_\n//\n// then the point key is elided.\n//\n// Range-key masking may only be used when iterating over both point keys and\n// range keys with IterKeyTypePointsAndRanges.\ntype RangeKeyMasking struct {\n\t// Suffix configures which range keys may mask point keys. Only range keys\n\t// that are defined at suffixes greater than or equal to Suffix will mask\n\t// point keys.\n\tSuffix []byte\n\t// Filter is an optional field that may be used to improve performance of\n\t// range-key masking through a block-property filter defined over key\n\t// suffixes. If non-nil, Filter is called by Pebble to construct a\n\t// block-property filter mask at iterator creation. The filter is used to\n\t// skip whole point-key blocks containing point keys with suffixes greater\n\t// than a covering range-key's suffix.\n\t//\n\t// To use this functionality, the caller must create and configure (through\n\t// Options.BlockPropertyCollectors) a block-property collector that records\n\t// the maxmimum suffix contained within a block. The caller then must write\n\t// and provide a BlockPropertyFilterMask implementation on that same\n\t// property. See the BlockPropertyFilterMask type for more information.\n\tFilter func() BlockPropertyFilterMask\n}\n\n// BlockPropertyFilterMask extends the BlockPropertyFilter interface for use\n// with range-key masking. Unlike an ordinary block property filter, a\n// BlockPropertyFilterMask's filtering criteria is allowed to change when Pebble\n// invokes its SetSuffix method.\n//\n// When a Pebble iterator steps into a range key's bounds and the range key has\n// a suffix greater than or equal to RangeKeyMasking.Suffix, the range key acts\n// as a mask. The masking range key hides all point keys that fall within the\n// range key's bounds and have suffixes > the range key's suffix. Without a\n// filter mask configured, Pebble performs this hiding by stepping through point\n// keys and comparing suffixes. If large numbers of point keys are masked, this\n// requires Pebble to load, iterate through and discard a large number of\n// sstable blocks containing masked point keys.\n//\n// If a block-property collector and a filter mask are configured, Pebble may\n// skip loading some point-key blocks altogether. If a block's keys are known to\n// all fall within the bounds of the masking range key and the block was\n// annotated by a block-property collector with the maximal suffix, Pebble can\n// ask the filter mask to compare the property to the current masking range\n// key's suffix. If the mask reports no intersection, the block may be skipped.\n//\n// If unsuffixed and suffixed keys are written to the database, care must be\n// taken to avoid unintentionally masking un-suffixed keys located in the same\n// block as suffixed keys. One solution is to interpret unsuffixed keys as\n// containing the maximal suffix value, ensuring that blocks containing\n// unsuffixed keys are always loaded.\ntype BlockPropertyFilterMask interface {\n\tBlockPropertyFilter\n\n\t// SetSuffix configures the mask with the suffix of a range key. The filter\n\t// should return false from Intersects whenever it's provided with a\n\t// property encoding a block's minimum suffix that's greater (according to\n\t// Compare) than the provided suffix.\n\tSetSuffix(suffix []byte) error\n}\n\n// WriteOptions hold the optional per-query parameters for Set and Delete\n// operations.\n//\n// Like Options, a nil *WriteOptions is valid and means to use the default\n// values.\ntype WriteOptions struct {\n\t// Sync is whether to sync writes through the OS buffer cache and down onto\n\t// the actual disk, if applicable. Setting Sync is required for durability of\n\t// individual write operations but can result in slower writes.\n\t//\n\t// If false, and the process or machine crashes, then a recent write may be\n\t// lost. This is due to the recently written data being buffered inside the\n\t// process running Pebble. This differs from the semantics of a write system\n\t// call in which the data is buffered in the OS buffer cache and would thus\n\t// survive a process crash.\n\t//\n\t// The default value is true.\n\tSync bool\n}\n\n// Sync specifies the default write options for writes which synchronize to\n// disk.\nvar Sync = &WriteOptions{Sync: true}\n\n// NoSync specifies the default write options for writes which do not\n// synchronize to disk.\nvar NoSync = &WriteOptions{Sync: false}\n\n// GetSync returns the Sync value or true if the receiver is nil.\nfunc (o *WriteOptions) GetSync() bool {\n\treturn o == nil || o.Sync\n}\n\n// LevelOptions holds the optional per-level parameters.\ntype LevelOptions struct {\n\t// BlockRestartInterval is the number of keys between restart points\n\t// for delta encoding of keys.\n\t//\n\t// The default value is 16.\n\tBlockRestartInterval int\n\n\t// BlockSize is the target uncompressed size in bytes of each table block.\n\t//\n\t// The default value is 4096.\n\tBlockSize int\n\n\t// BlockSizeThreshold finishes a block if the block size is larger than the\n\t// specified percentage of the target block size and adding the next entry\n\t// would cause the block to be larger than the target block size.\n\t//\n\t// The default value is 90\n\tBlockSizeThreshold int\n\n\t// Compression defines the per-block compression to use.\n\t//\n\t// The default value (DefaultCompression) uses snappy compression.\n\tCompression func() Compression\n\n\t// FilterPolicy defines a filter algorithm (such as a Bloom filter) that can\n\t// reduce disk reads for Get calls.\n\t//\n\t// One such implementation is bloom.FilterPolicy(10) from the pebble/bloom\n\t// package.\n\t//\n\t// The default value means to use no filter.\n\tFilterPolicy FilterPolicy\n\n\t// FilterType defines whether an existing filter policy is applied at a\n\t// block-level or table-level. Block-level filters use less memory to create,\n\t// but are slower to access as a check for the key in the index must first be\n\t// performed to locate the filter block. A table-level filter will require\n\t// memory proportional to the number of keys in an sstable to create, but\n\t// avoids the index lookup when determining if a key is present. Table-level\n\t// filters should be preferred except under constrained memory situations.\n\tFilterType FilterType\n\n\t// IndexBlockSize is the target uncompressed size in bytes of each index\n\t// block. When the index block size is larger than this target, two-level\n\t// indexes are automatically enabled. Setting this option to a large value\n\t// (such as math.MaxInt32) disables the automatic creation of two-level\n\t// indexes.\n\t//\n\t// The default value is the value of BlockSize.\n\tIndexBlockSize int\n\n\t// The target file size for the level.\n\tTargetFileSize int64\n}\n\n// EnsureDefaults ensures that the default values for all of the options have\n// been initialized. It is valid to call EnsureDefaults on a nil receiver. A\n// non-nil result will always be returned.\nfunc (o *LevelOptions) EnsureDefaults() *LevelOptions {\n\tif o == nil {\n\t\to = &LevelOptions{}\n\t}\n\tif o.BlockRestartInterval <= 0 {\n\t\to.BlockRestartInterval = base.DefaultBlockRestartInterval\n\t}\n\tif o.BlockSize <= 0 {\n\t\to.BlockSize = base.DefaultBlockSize\n\t} else if o.BlockSize > sstable.MaximumBlockSize {\n\t\tpanic(errors.Errorf(\"BlockSize %d exceeds MaximumBlockSize\", o.BlockSize))\n\t}\n\tif o.BlockSizeThreshold <= 0 {\n\t\to.BlockSizeThreshold = base.DefaultBlockSizeThreshold\n\t}\n\tif o.Compression == nil {\n\t\to.Compression = func() Compression { return DefaultCompression }\n\t}\n\tif o.IndexBlockSize <= 0 {\n\t\to.IndexBlockSize = o.BlockSize\n\t}\n\tif o.TargetFileSize <= 0 {\n\t\to.TargetFileSize = 2 << 20 // 2 MB\n\t}\n\treturn o\n}\n\n// Options holds the optional parameters for configuring pebble. These options\n// apply to the DB at large; per-query options are defined by the IterOptions\n// and WriteOptions types.\ntype Options struct {\n\t// Sync sstables periodically in order to smooth out writes to disk. This\n\t// option does not provide any persistency guarantee, but is used to avoid\n\t// latency spikes if the OS automatically decides to write out a large chunk\n\t// of dirty filesystem buffers. This option only controls SSTable syncs; WAL\n\t// syncs are controlled by WALBytesPerSync.\n\t//\n\t// The default value is 512KB.\n\tBytesPerSync int\n\n\t// Cache is used to cache uncompressed blocks from sstables.\n\t//\n\t// The default cache size is 8 MB.\n\tCache *cache.Cache\n\n\t// LoadBlockSema, if set, is used to limit the number of blocks that can be\n\t// loaded (i.e. read from the filesystem) in parallel. Each load acquires one\n\t// unit from the semaphore for the duration of the read.\n\tLoadBlockSema *fifo.Semaphore\n\n\t// Cleaner cleans obsolete files.\n\t//\n\t// The default cleaner uses the DeleteCleaner.\n\tCleaner Cleaner\n\n\t// Local contains option that pertain to files stored on the local filesystem.\n\tLocal struct {\n\t\t// ReadaheadConfig is used to retrieve the current readahead mode; it is\n\t\t// consulted whenever a read handle is initialized.\n\t\tReadaheadConfig *ReadaheadConfig\n\n\t\t// TODO(radu): move BytesPerSync, LoadBlockSema, Cleaner here.\n\t}\n\n\t// Comparer defines a total ordering over the space of []byte keys: a 'less\n\t// than' relationship. The same comparison algorithm must be used for reads\n\t// and writes over the lifetime of the DB.\n\t//\n\t// The default value uses the same ordering as bytes.Compare.\n\tComparer *Comparer\n\n\t// DebugCheck is invoked, if non-nil, whenever a new version is being\n\t// installed. Typically, this is set to pebble.DebugCheckLevels in tests\n\t// or tools only, to check invariants over all the data in the database.\n\tDebugCheck func(*DB) error\n\n\t// Disable the write-ahead log (WAL). Disabling the write-ahead log prohibits\n\t// crash recovery, but can improve performance if crash recovery is not\n\t// needed (e.g. when only temporary state is being stored in the database).\n\t//\n\t// TODO(peter): untested\n\tDisableWAL bool\n\n\t// ErrorIfExists causes an error on Open if the database already exists.\n\t// The error can be checked with errors.Is(err, ErrDBAlreadyExists).\n\t//\n\t// The default value is false.\n\tErrorIfExists bool\n\n\t// ErrorIfNotExists causes an error on Open if the database does not already\n\t// exist. The error can be checked with errors.Is(err, ErrDBDoesNotExist).\n\t//\n\t// The default value is false which will cause a database to be created if it\n\t// does not already exist.\n\tErrorIfNotExists bool\n\n\t// ErrorIfNotPristine causes an error on Open if the database already exists\n\t// and any operations have been performed on the database. The error can be\n\t// checked with errors.Is(err, ErrDBNotPristine).\n\t//\n\t// Note that a database that contained keys that were all subsequently deleted\n\t// may or may not trigger the error. Currently, we check if there are any live\n\t// SSTs or log records to replay.\n\tErrorIfNotPristine bool\n\n\t// EventListener provides hooks to listening to significant DB events such as\n\t// flushes, compactions, and table deletion.\n\tEventListener *EventListener\n\n\t// Experimental contains experimental options which are off by default.\n\t// These options are temporary and will eventually either be deleted, moved\n\t// out of the experimental group, or made the non-adjustable default. These\n\t// options may change at any time, so do not rely on them.\n\tExperimental struct {\n\t\t// The threshold of L0 read-amplification at which compaction concurrency\n\t\t// is enabled (if CompactionDebtConcurrency was not already exceeded).\n\t\t// Every multiple of this value enables another concurrent\n\t\t// compaction up to MaxConcurrentCompactions.\n\t\tL0CompactionConcurrency int\n\n\t\t// CompactionDebtConcurrency controls the threshold of compaction debt\n\t\t// at which additional compaction concurrency slots are added. For every\n\t\t// multiple of this value in compaction debt bytes, an additional\n\t\t// concurrent compaction is added. This works \"on top\" of\n\t\t// L0CompactionConcurrency, so the higher of the count of compaction\n\t\t// concurrency slots as determined by the two options is chosen.\n\t\tCompactionDebtConcurrency uint64\n\n\t\t// IngestSplit, if it returns true, allows for ingest-time splitting of\n\t\t// existing sstables into two virtual sstables to allow ingestion sstables to\n\t\t// slot into a lower level than they otherwise would have.\n\t\tIngestSplit func() bool\n\n\t\t// ReadCompactionRate controls the frequency of read triggered\n\t\t// compactions by adjusting `AllowedSeeks` in manifest.FileMetadata:\n\t\t//\n\t\t// AllowedSeeks = FileSize / ReadCompactionRate\n\t\t//\n\t\t// From LevelDB:\n\t\t// ```\n\t\t// We arrange to automatically compact this file after\n\t\t// a certain number of seeks. Let's assume:\n\t\t//   (1) One seek costs 10ms\n\t\t//   (2) Writing or reading 1MB costs 10ms (100MB/s)\n\t\t//   (3) A compaction of 1MB does 25MB of IO:\n\t\t//         1MB read from this level\n\t\t//         10-12MB read from next level (boundaries may be misaligned)\n\t\t//         10-12MB written to next level\n\t\t// This implies that 25 seeks cost the same as the compaction\n\t\t// of 1MB of data.  I.e., one seek costs approximately the\n\t\t// same as the compaction of 40KB of data.  We are a little\n\t\t// conservative and allow approximately one seek for every 16KB\n\t\t// of data before triggering a compaction.\n\t\t// ```\n\t\tReadCompactionRate int64\n\n\t\t// ReadSamplingMultiplier is a multiplier for the readSamplingPeriod in\n\t\t// iterator.maybeSampleRead() to control the frequency of read sampling\n\t\t// to trigger a read triggered compaction. A value of -1 prevents sampling\n\t\t// and disables read triggered compactions. The default is 1 << 4. which\n\t\t// gets multiplied with a constant of 1 << 16 to yield 1 << 20 (1MB).\n\t\tReadSamplingMultiplier int64\n\n\t\t// NumDeletionsThreshold defines the minimum number of point tombstones\n\t\t// that must be present in a single data block for that block to be\n\t\t// considered tombstone-dense for the purposes of triggering a\n\t\t// tombstone density compaction. Data blocks may also be considered\n\t\t// tombstone-dense if they meet the criteria defined by\n\t\t// DeletionSizeRatioThreshold below. Tombstone-dense blocks are identified\n\t\t// when sstables are written, and so this is effectively an option for\n\t\t// sstable writers. The default value is 100.\n\t\tNumDeletionsThreshold int\n\n\t\t// DeletionSizeRatioThreshold defines the minimum ratio of the size of\n\t\t// point tombstones to the size of a data block that must be reached\n\t\t// for that block to be considered tombstone-dense for the purposes of\n\t\t// triggering a tombstone density compaction. Data blocks may also be\n\t\t// considered tombstone-dense if they meet the criteria defined by\n\t\t// NumDeletionsThreshold above. Tombstone-dense blocks are identified\n\t\t// when sstables are written, and so this is effectively an option for\n\t\t// sstable writers. The default value is 0.5.\n\t\tDeletionSizeRatioThreshold float32\n\n\t\t// TombstoneDenseCompactionThreshold is the minimum percent of data\n\t\t// blocks in a table that must be tombstone-dense for that table to be\n\t\t// eligible for a tombstone density compaction. It should be defined as a\n\t\t// ratio out of 1. The default value is 0.10.\n\t\t//\n\t\t// If multiple tables are eligible for a tombstone density compaction, then\n\t\t// tables with a higher percent of tombstone-dense blocks are still\n\t\t// prioritized for compaction.\n\t\t//\n\t\t// A zero or negative value disables tombstone density compactions.\n\t\tTombstoneDenseCompactionThreshold float64\n\n\t\t// FileCacheShards is the number of shards per file cache.\n\t\t// Reducing the value can reduce the number of idle goroutines per DB\n\t\t// instance which can be useful in scenarios with a lot of DB instances\n\t\t// and a large number of CPUs, but doing so can lead to higher contention\n\t\t// in the file cache and reduced performance.\n\t\t//\n\t\t// The default value is the number of logical CPUs, which can be\n\t\t// limited by runtime.GOMAXPROCS.\n\t\tFileCacheShards int\n\n\t\t// KeyValidationFunc is a function to validate a user key in an SSTable.\n\t\t//\n\t\t// Currently, this function is used to validate the smallest and largest\n\t\t// keys in an SSTable undergoing compaction. In this case, returning an\n\t\t// error from the validation function will result in a panic at runtime,\n\t\t// given that there is rarely any way of recovering from malformed keys\n\t\t// present in compacted files. By default, validation is not performed.\n\t\t//\n\t\t// Additional use-cases may be added in the future.\n\t\t//\n\t\t// NOTE: callers should take care to not mutate the key being validated.\n\t\tKeyValidationFunc func(userKey []byte) error\n\n\t\t// ValidateOnIngest schedules validation of sstables after they have\n\t\t// been ingested.\n\t\t//\n\t\t// By default, this value is false.\n\t\tValidateOnIngest bool\n\n\t\t// LevelMultiplier configures the size multiplier used to determine the\n\t\t// desired size of each level of the LSM. Defaults to 10.\n\t\tLevelMultiplier int\n\n\t\t// MultiLevelCompactionHeuristic determines whether to add an additional\n\t\t// level to a conventional two level compaction. If nil, a multilevel\n\t\t// compaction will never get triggered.\n\t\tMultiLevelCompactionHeuristic MultiLevelHeuristic\n\n\t\t// MaxWriterConcurrency is used to indicate the maximum number of\n\t\t// compression workers the compression queue is allowed to use. If\n\t\t// MaxWriterConcurrency > 0, then the Writer will use parallelism, to\n\t\t// compress and write blocks to disk. Otherwise, the writer will\n\t\t// compress and write blocks to disk synchronously.\n\t\tMaxWriterConcurrency int\n\n\t\t// ForceWriterParallelism is used to force parallelism in the sstable\n\t\t// Writer for the metamorphic tests. Even with the MaxWriterConcurrency\n\t\t// option set, we only enable parallelism in the sstable Writer if there\n\t\t// is enough CPU available, and this option bypasses that.\n\t\tForceWriterParallelism bool\n\n\t\t// CPUWorkPermissionGranter should be set if Pebble should be given the\n\t\t// ability to optionally schedule additional CPU. See the documentation\n\t\t// for CPUWorkPermissionGranter for more details.\n\t\tCPUWorkPermissionGranter CPUWorkPermissionGranter\n\n\t\t// EnableColumnarBlocks is used to decide whether to enable writing\n\t\t// TableFormatPebblev5 sstables. This setting is only respected by\n\t\t// FormatColumnarBlocks. In lower format major versions, the\n\t\t// TableFormatPebblev5 format is prohibited. If EnableColumnarBlocks is\n\t\t// nil and the DB is at FormatColumnarBlocks, the DB defaults to not\n\t\t// writing columnar blocks.\n\t\tEnableColumnarBlocks func() bool\n\n\t\t// EnableValueBlocks is used to decide whether to enable writing\n\t\t// TableFormatPebblev3 sstables. This setting is only respected by a\n\t\t// specific subset of format major versions: FormatSSTableValueBlocks,\n\t\t// FormatFlushableIngest and FormatPrePebblev1MarkedCompacted. In lower\n\t\t// format major versions, value blocks are never enabled. In higher\n\t\t// format major versions, value blocks are always enabled.\n\t\tEnableValueBlocks func() bool\n\n\t\t// ShortAttributeExtractor is used iff EnableValueBlocks() returns true\n\t\t// (else ignored). If non-nil, a ShortAttribute can be extracted from the\n\t\t// value and stored with the key, when the value is stored elsewhere.\n\t\tShortAttributeExtractor ShortAttributeExtractor\n\n\t\t// RequiredInPlaceValueBound specifies an optional span of user key\n\t\t// prefixes that are not-MVCC, but have a suffix. For these the values\n\t\t// must be stored with the key, since the concept of \"older versions\" is\n\t\t// not defined. It is also useful for statically known exclusions to value\n\t\t// separation. In CockroachDB, this will be used for the lock table key\n\t\t// space that has non-empty suffixes, but those locks don't represent\n\t\t// actual MVCC versions (the suffix ordering is arbitrary). We will also\n\t\t// need to add support for dynamically configured exclusions (we want the\n\t\t// default to be to allow Pebble to decide whether to separate the value\n\t\t// or not, hence this is structured as exclusions), for example, for users\n\t\t// of CockroachDB to dynamically exclude certain tables.\n\t\t//\n\t\t// Any change in exclusion behavior takes effect only on future written\n\t\t// sstables, and does not start rewriting existing sstables.\n\t\t//\n\t\t// Even ignoring changes in this setting, exclusions are interpreted as a\n\t\t// guidance by Pebble, and not necessarily honored. Specifically, user\n\t\t// keys with multiple Pebble-versions *may* have the older versions stored\n\t\t// in value blocks.\n\t\tRequiredInPlaceValueBound UserKeyPrefixBound\n\n\t\t// DisableIngestAsFlushable disables lazy ingestion of sstables through\n\t\t// a WAL write and memtable rotation. Only effectual if the format\n\t\t// major version is at least `FormatFlushableIngest`.\n\t\tDisableIngestAsFlushable func() bool\n\n\t\t// RemoteStorage enables use of remote storage (e.g. S3) for storing\n\t\t// sstables. Setting this option enables use of CreateOnShared option and\n\t\t// allows ingestion of external files.\n\t\tRemoteStorage remote.StorageFactory\n\n\t\t// If CreateOnShared is non-zero, new sstables are created on remote storage\n\t\t// (using CreateOnSharedLocator and with the appropriate\n\t\t// CreateOnSharedStrategy). These sstables can be shared between different\n\t\t// Pebble instances; the lifecycle of such objects is managed by the\n\t\t// remote.Storage constructed by options.RemoteStorage.\n\t\t//\n\t\t// Can only be used when RemoteStorage is set (and recognizes\n\t\t// CreateOnSharedLocator).\n\t\tCreateOnShared        remote.CreateOnSharedStrategy\n\t\tCreateOnSharedLocator remote.Locator\n\n\t\t// CacheSizeBytesBytes is the size of the on-disk block cache for objects\n\t\t// on shared storage in bytes. If it is 0, no cache is used.\n\t\tSecondaryCacheSizeBytes int64\n\n\t\t// EnableDeleteOnlyCompactionExcises enables delete-only compactions to also\n\t\t// apply delete-only compaction hints on sstables that partially overlap\n\t\t// with it. This application happens through an excise, similar to\n\t\t// the excise phase of IngestAndExcise.\n\t\tEnableDeleteOnlyCompactionExcises func() bool\n\n\t\t// CompactionLimiter, if set, is used to limit concurrent compactions as well\n\t\t// as to pace compactions and flushing compactions already chosen. If nil,\n\t\t// no limiting or pacing happens other than that controlled by other options\n\t\t// like L0CompactionConcurrency and CompactionDebtConcurrency.\n\t\tCompactionLimiter CompactionLimiter\n\n\t\tUserKeyCategories UserKeyCategories\n\t}\n\n\t// Filters is a map from filter policy name to filter policy. It is used for\n\t// debugging tools which may be used on multiple databases configured with\n\t// different filter policies. It is not necessary to populate this filters\n\t// map during normal usage of a DB (it will be done automatically by\n\t// EnsureDefaults).\n\tFilters map[string]FilterPolicy\n\n\t// FlushDelayDeleteRange configures how long the database should wait before\n\t// forcing a flush of a memtable that contains a range deletion. Disk space\n\t// cannot be reclaimed until the range deletion is flushed. No automatic\n\t// flush occurs if zero.\n\tFlushDelayDeleteRange time.Duration\n\n\t// FlushDelayRangeKey configures how long the database should wait before\n\t// forcing a flush of a memtable that contains a range key. Range keys in\n\t// the memtable prevent lazy combined iteration, so it's desirable to flush\n\t// range keys promptly. No automatic flush occurs if zero.\n\tFlushDelayRangeKey time.Duration\n\n\t// FlushSplitBytes denotes the target number of bytes per sublevel in\n\t// each flush split interval (i.e. range between two flush split keys)\n\t// in L0 sstables. When set to zero, only a single sstable is generated\n\t// by each flush. When set to a non-zero value, flushes are split at\n\t// points to meet L0's TargetFileSize, any grandparent-related overlap\n\t// options, and at boundary keys of L0 flush split intervals (which are\n\t// targeted to contain around FlushSplitBytes bytes in each sublevel\n\t// between pairs of boundary keys). Splitting sstables during flush\n\t// allows increased compaction flexibility and concurrency when those\n\t// tables are compacted to lower levels.\n\tFlushSplitBytes int64\n\n\t// FormatMajorVersion sets the format of on-disk files. It is\n\t// recommended to set the format major version to an explicit\n\t// version, as the default may change over time.\n\t//\n\t// At Open if the existing database is formatted using a later\n\t// format major version that is known to this version of Pebble,\n\t// Pebble will continue to use the later format major version. If\n\t// the existing database's version is unknown, the caller may use\n\t// FormatMostCompatible and will be able to open the database\n\t// regardless of its actual version.\n\t//\n\t// If the existing database is formatted using a format major\n\t// version earlier than the one specified, Open will automatically\n\t// ratchet the database to the specified format major version.\n\tFormatMajorVersion FormatMajorVersion\n\n\t// FS provides the interface for persistent file storage.\n\t//\n\t// The default value uses the underlying operating system's file system.\n\tFS vfs.FS\n\n\t// KeySchema is the name of the key schema that should be used when writing\n\t// new sstables. There must be a key schema with this name defined in\n\t// KeySchemas. If not set, colblk.DefaultKeySchema is used to construct a\n\t// default key schema.\n\tKeySchema string\n\n\t// KeySchemas defines the set of known schemas of user keys. When columnar\n\t// blocks are in use (see FormatColumnarBlocks), the user may specify how a\n\t// key should be decomposed into columns. Each KeySchema must have a unique\n\t// name. The schema named by Options.KeySchema is used while writing\n\t// sstables during flushes and compactions.\n\t//\n\t// Multiple KeySchemas may be used over the lifetime of a database. Once a\n\t// KeySchema is used, it must be provided in KeySchemas in subsequent calls\n\t// to Open for perpetuity.\n\tKeySchemas sstable.KeySchemas\n\n\t// Lock, if set, must be a database lock acquired through LockDirectory for\n\t// the same directory passed to Open. If provided, Open will skip locking\n\t// the directory. Closing the database will not release the lock, and it's\n\t// the responsibility of the caller to release the lock after closing the\n\t// database.\n\t//\n\t// Open will enforce that the Lock passed locks the same directory passed to\n\t// Open. Concurrent calls to Open using the same Lock are detected and\n\t// prohibited.\n\tLock *Lock\n\n\t// The count of L0 files necessary to trigger an L0 compaction.\n\tL0CompactionFileThreshold int\n\n\t// The amount of L0 read-amplification necessary to trigger an L0 compaction.\n\tL0CompactionThreshold int\n\n\t// Hard limit on L0 read-amplification, computed as the number of L0\n\t// sublevels. Writes are stopped when this threshold is reached.\n\tL0StopWritesThreshold int\n\n\t// The maximum number of bytes for LBase. The base level is the level which\n\t// L0 is compacted into. The base level is determined dynamically based on\n\t// the existing data in the LSM. The maximum number of bytes for other levels\n\t// is computed dynamically based on the base level's maximum size. When the\n\t// maximum number of bytes for a level is exceeded, compaction is requested.\n\tLBaseMaxBytes int64\n\n\t// Per-level options. Options for at least one level must be specified. The\n\t// options for the last level are used for all subsequent levels.\n\tLevels []LevelOptions\n\n\t// LoggerAndTracer will be used, if non-nil, else Logger will be used and\n\t// tracing will be a noop.\n\n\t// Logger used to write log messages.\n\t//\n\t// The default logger uses the Go standard library log package.\n\tLogger Logger\n\t// LoggerAndTracer is used for writing log messages and traces.\n\tLoggerAndTracer LoggerAndTracer\n\n\t// MaxManifestFileSize is the maximum size the MANIFEST file is allowed to\n\t// become. When the MANIFEST exceeds this size it is rolled over and a new\n\t// MANIFEST is created.\n\tMaxManifestFileSize int64\n\n\t// MaxOpenFiles is a soft limit on the number of open files that can be\n\t// used by the DB.\n\t//\n\t// The default value is 1000.\n\tMaxOpenFiles int\n\n\t// The size of a MemTable in steady state. The actual MemTable size starts at\n\t// min(256KB, MemTableSize) and doubles for each subsequent MemTable up to\n\t// MemTableSize. This reduces the memory pressure caused by MemTables for\n\t// short lived (test) DB instances. Note that more than one MemTable can be\n\t// in existence since flushing a MemTable involves creating a new one and\n\t// writing the contents of the old one in the\n\t// background. MemTableStopWritesThreshold places a hard limit on the size of\n\t// the queued MemTables.\n\t//\n\t// The default value is 4MB.\n\tMemTableSize uint64\n\n\t// Hard limit on the number of queued of MemTables. Writes are stopped when\n\t// the sum of the queued memtable sizes exceeds:\n\t//   MemTableStopWritesThreshold * MemTableSize.\n\t//\n\t// This value should be at least 2 or writes will stop whenever a MemTable is\n\t// being flushed.\n\t//\n\t// The default value is 2.\n\tMemTableStopWritesThreshold int\n\n\t// Merger defines the associative merge operation to use for merging values\n\t// written with {Batch,DB}.Merge.\n\t//\n\t// The default merger concatenates values.\n\tMerger *Merger\n\n\t// MaxConcurrentCompactions specifies the maximum number of concurrent\n\t// compactions (not including download compactions).\n\t//\n\t// Concurrent compactions are performed:\n\t//  - when L0 read-amplification passes the L0CompactionConcurrency threshold;\n\t//  - for automatic background compactions;\n\t//  - when a manual compaction for a level is split and parallelized.\n\t//\n\t// MaxConcurrentCompactions() must be greater than 0.\n\t//\n\t// The default value is 1.\n\tMaxConcurrentCompactions func() int\n\n\t// MaxConcurrentDownloads specifies the maximum number of download\n\t// compactions. These are compactions that copy an external file to the local\n\t// store.\n\t//\n\t// This limit is independent of MaxConcurrentCompactions; at any point in\n\t// time, we may be running MaxConcurrentCompactions non-download compactions\n\t// and MaxConcurrentDownloads download compactions.\n\t//\n\t// MaxConcurrentDownloads() must be greater than 0.\n\t//\n\t// The default value is 1.\n\tMaxConcurrentDownloads func() int\n\n\t// DisableAutomaticCompactions dictates whether automatic compactions are\n\t// scheduled or not. The default is false (enabled). This option is only used\n\t// externally when running a manual compaction, and internally for tests.\n\tDisableAutomaticCompactions bool\n\n\t// DisableConsistencyCheck disables the consistency check that is performed on\n\t// open. Should only be used when a database cannot be opened normally (e.g.\n\t// some of the tables don't exist / aren't accessible).\n\tDisableConsistencyCheck bool\n\n\t// DisableTableStats dictates whether tables should be loaded asynchronously\n\t// to compute statistics that inform compaction heuristics. The collection\n\t// of table stats improves compaction of tombstones, reclaiming disk space\n\t// more quickly and in some cases reducing write amplification in the\n\t// presence of tombstones. Disabling table stats may be useful in tests\n\t// that require determinism as the asynchronicity of table stats collection\n\t// introduces significant nondeterminism.\n\tDisableTableStats bool\n\n\t// NoSyncOnClose decides whether the Pebble instance will enforce a\n\t// close-time synchronization (e.g., fdatasync() or sync_file_range())\n\t// on files it writes to. Setting this to true removes the guarantee for a\n\t// sync on close. Some implementations can still issue a non-blocking sync.\n\tNoSyncOnClose bool\n\n\t// NumPrevManifest is the number of non-current or older manifests which\n\t// we want to keep around for debugging purposes. By default, we're going\n\t// to keep one older manifest.\n\tNumPrevManifest int\n\n\t// ReadOnly indicates that the DB should be opened in read-only mode. Writes\n\t// to the DB will return an error, background compactions are disabled, and\n\t// the flush that normally occurs after replaying the WAL at startup is\n\t// disabled.\n\tReadOnly bool\n\n\t// FileCache is an initialized FileCache which should be set as an\n\t// option if the DB needs to be initialized with a pre-existing file cache.\n\t// If FileCache is nil, then a file cache which is unique to the DB instance\n\t// is created. FileCache can be shared between db instances by setting it here.\n\t// The FileCache set here must use the same underlying cache as Options.Cache\n\t// and pebble will panic otherwise.\n\tFileCache *FileCache\n\n\t// BlockPropertyCollectors is a list of BlockPropertyCollector creation\n\t// functions. A new BlockPropertyCollector is created for each sstable\n\t// built and lives for the lifetime of writing that table.\n\tBlockPropertyCollectors []func() BlockPropertyCollector\n\n\t// WALBytesPerSync sets the number of bytes to write to a WAL before calling\n\t// Sync on it in the background. Just like with BytesPerSync above, this\n\t// helps smooth out disk write latencies, and avoids cases where the OS\n\t// writes a lot of buffered data to disk at once. However, this is less\n\t// necessary with WALs, as many write operations already pass in\n\t// Sync = true.\n\t//\n\t// The default value is 0, i.e. no background syncing. This matches the\n\t// default behaviour in RocksDB.\n\tWALBytesPerSync int\n\n\t// WALDir specifies the directory to store write-ahead logs (WALs) in. If\n\t// empty (the default), WALs will be stored in the same directory as sstables\n\t// (i.e. the directory passed to pebble.Open).\n\tWALDir string\n\n\t// WALFailover may be set to configure Pebble to monitor writes to its\n\t// write-ahead log and failover to writing write-ahead log entries to a\n\t// secondary location (eg, a separate physical disk). WALFailover may be\n\t// used to improve write availability in the presence of transient disk\n\t// unavailability.\n\tWALFailover *WALFailoverOptions\n\n\t// WALRecoveryDirs is a list of additional directories that should be\n\t// scanned for the existence of additional write-ahead logs. WALRecoveryDirs\n\t// is expected to be used when starting Pebble with a new WALDir or a new\n\t// WALFailover configuration. The directories associated with the previous\n\t// configuration may still contain WALs that are required for recovery of\n\t// the current database state.\n\t//\n\t// If a previous WAL configuration may have stored WALs elsewhere but there\n\t// is not a corresponding entry in WALRecoveryDirs, Open will error.\n\tWALRecoveryDirs []wal.Dir\n\n\t// WALMinSyncInterval is the minimum duration between syncs of the WAL. If\n\t// WAL syncs are requested faster than this interval, they will be\n\t// artificially delayed. Introducing a small artificial delay (500us) between\n\t// WAL syncs can allow more operations to arrive and reduce IO operations\n\t// while having a minimal impact on throughput. This option is supplied as a\n\t// closure in order to allow the value to be changed dynamically. The default\n\t// value is 0.\n\t//\n\t// TODO(peter): rather than a closure, should there be another mechanism for\n\t// changing options dynamically?\n\tWALMinSyncInterval func() time.Duration\n\n\t// TargetByteDeletionRate is the rate (in bytes per second) at which sstable file\n\t// deletions are limited to (under normal circumstances).\n\t//\n\t// Deletion pacing is used to slow down deletions when compactions finish up\n\t// or readers close and newly-obsolete files need cleaning up. Deleting lots\n\t// of files at once can cause disk latency to go up on some SSDs, which this\n\t// functionality guards against.\n\t//\n\t// This value is only a best-effort target; the effective rate can be\n\t// higher if deletions are falling behind or disk space is running low.\n\t//\n\t// Setting this to 0 disables deletion pacing, which is also the default.\n\tTargetByteDeletionRate int\n\n\t// EnableSQLRowSpillMetrics specifies whether the Pebble instance will only be used\n\t// to temporarily persist data spilled to disk for row-oriented SQL query execution.\n\tEnableSQLRowSpillMetrics bool\n\n\t// AllocatorSizeClasses provides a sorted list containing the supported size\n\t// classes of the underlying memory allocator. This provides hints to the\n\t// sstable block writer's flushing policy to select block sizes that\n\t// preemptively reduce internal fragmentation when loaded into the block cache.\n\tAllocatorSizeClasses []int\n\n\t// private options are only used by internal tests or are used internally\n\t// for facilitating upgrade paths of unconfigurable functionality.\n\tprivate struct {\n\t\t// disableDeleteOnlyCompactions prevents the scheduling of delete-only\n\t\t// compactions that drop sstables wholy covered by range tombstones or\n\t\t// range key tombstones.\n\t\tdisableDeleteOnlyCompactions bool\n\n\t\t// disableElisionOnlyCompactions prevents the scheduling of elision-only\n\t\t// compactions that rewrite sstables in place in order to elide obsolete\n\t\t// keys.\n\t\tdisableElisionOnlyCompactions bool\n\n\t\t// disableLazyCombinedIteration is a private option used by the\n\t\t// metamorphic tests to test equivalence between lazy-combined iteration\n\t\t// and constructing the range-key iterator upfront. It's a private\n\t\t// option to avoid littering the public interface with options that we\n\t\t// do not want to allow users to actually configure.\n\t\tdisableLazyCombinedIteration bool\n\n\t\t// testingAlwaysWaitForCleanup is set by some tests to force waiting for\n\t\t// obsolete file deletion (to make events deterministic).\n\t\ttestingAlwaysWaitForCleanup bool\n\n\t\t// fsCloser holds a closer that should be invoked after a DB using these\n\t\t// Options is closed. This is used to automatically stop the\n\t\t// long-running goroutine associated with the disk-health-checking FS.\n\t\t// See the initialization of FS in EnsureDefaults. Note that care has\n\t\t// been taken to ensure that it is still safe to continue using the FS\n\t\t// after this closer has been invoked. However, if write operations\n\t\t// against the FS are made after the DB is closed, the FS may leak a\n\t\t// goroutine indefinitely.\n\t\tfsCloser io.Closer\n\t}\n}\n\n// WALFailoverOptions configures the WAL failover mechanics to use during\n// transient write unavailability on the primary WAL volume.\ntype WALFailoverOptions struct {\n\t// Secondary indicates the secondary directory and VFS to use in the event a\n\t// write to the primary WAL stalls.\n\tSecondary wal.Dir\n\t// FailoverOptions provides configuration of the thresholds and intervals\n\t// involved in WAL failover. If any of its fields are left unspecified,\n\t// reasonable defaults will be used.\n\twal.FailoverOptions\n}\n\n// ReadaheadConfig controls the use of read-ahead.\ntype ReadaheadConfig = objstorageprovider.ReadaheadConfig\n\n// JemallocSizeClasses exports sstable.JemallocSizeClasses.\nvar JemallocSizeClasses = sstable.JemallocSizeClasses\n\n// DebugCheckLevels calls CheckLevels on the provided database.\n// It may be set in the DebugCheck field of Options to check\n// level invariants whenever a new version is installed.\nfunc DebugCheckLevels(db *DB) error {\n\treturn db.CheckLevels(nil)\n}\n\n// EnsureDefaults ensures that the default values for all options are set if a\n// valid value was not already specified. Returns the new options.\nfunc (o *Options) EnsureDefaults() *Options {\n\tif o == nil {\n\t\to = &Options{}\n\t}\n\to.Comparer = o.Comparer.EnsureDefaults()\n\n\tif o.BytesPerSync <= 0 {\n\t\to.BytesPerSync = 512 << 10 // 512 KB\n\t}\n\tif o.Cleaner == nil {\n\t\to.Cleaner = DeleteCleaner{}\n\t}\n\n\tif o.Experimental.DisableIngestAsFlushable == nil {\n\t\to.Experimental.DisableIngestAsFlushable = func() bool { return false }\n\t}\n\tif o.Experimental.L0CompactionConcurrency <= 0 {\n\t\to.Experimental.L0CompactionConcurrency = 10\n\t}\n\tif o.Experimental.CompactionDebtConcurrency <= 0 {\n\t\to.Experimental.CompactionDebtConcurrency = 1 << 30 // 1 GB\n\t}\n\tif o.Experimental.CompactionLimiter == nil {\n\t\to.Experimental.CompactionLimiter = &base.DefaultCompactionLimiter{}\n\t}\n\tif o.Experimental.KeyValidationFunc == nil {\n\t\to.Experimental.KeyValidationFunc = func([]byte) error { return nil }\n\t}\n\tif o.KeySchema == \"\" && len(o.KeySchemas) == 0 {\n\t\tks := colblk.DefaultKeySchema(o.Comparer, 16 /* bundleSize */)\n\t\to.KeySchema = ks.Name\n\t\to.KeySchemas = sstable.MakeKeySchemas(&ks)\n\t}\n\tif o.L0CompactionThreshold <= 0 {\n\t\to.L0CompactionThreshold = 4\n\t}\n\tif o.L0CompactionFileThreshold <= 0 {\n\t\t// Some justification for the default of 500:\n\t\t// Why not smaller?:\n\t\t// - The default target file size for L0 is 2MB, so 500 files is <= 1GB\n\t\t//   of data. At observed compaction speeds of > 20MB/s, L0 can be\n\t\t//   cleared of all files in < 1min, so this backlog is not huge.\n\t\t// - 500 files is low overhead for instantiating L0 sublevels from\n\t\t//   scratch.\n\t\t// - Lower values were observed to cause excessive and inefficient\n\t\t//   compactions out of L0 in a TPCC import benchmark.\n\t\t// Why not larger?:\n\t\t// - More than 1min to compact everything out of L0.\n\t\t// - CockroachDB's admission control system uses a threshold of 1000\n\t\t//   files to start throttling writes to Pebble. Using 500 here gives\n\t\t//   us headroom between when Pebble should start compacting L0 and\n\t\t//   when the admission control threshold is reached.\n\t\t//\n\t\t// We can revisit this default in the future based on better\n\t\t// experimental understanding.\n\t\t//\n\t\t// TODO(jackson): Experiment with slightly lower thresholds [or higher\n\t\t// admission control thresholds] to see whether a higher L0 score at the\n\t\t// threshold (currently 2.0) is necessary for some workloads to avoid\n\t\t// starving L0 in favor of lower-level compactions.\n\t\to.L0CompactionFileThreshold = 500\n\t}\n\tif o.L0StopWritesThreshold <= 0 {\n\t\to.L0StopWritesThreshold = 12\n\t}\n\tif o.LBaseMaxBytes <= 0 {\n\t\to.LBaseMaxBytes = 64 << 20 // 64 MB\n\t}\n\tif o.Levels == nil {\n\t\to.Levels = make([]LevelOptions, 1)\n\t\tfor i := range o.Levels {\n\t\t\tif i > 0 {\n\t\t\t\tl := &o.Levels[i]\n\t\t\t\tif l.TargetFileSize <= 0 {\n\t\t\t\t\tl.TargetFileSize = o.Levels[i-1].TargetFileSize * 2\n\t\t\t\t}\n\t\t\t}\n\t\t\to.Levels[i].EnsureDefaults()\n\t\t}\n\t} else {\n\t\tfor i := range o.Levels {\n\t\t\to.Levels[i].EnsureDefaults()\n\t\t}\n\t}\n\tif o.Logger == nil {\n\t\to.Logger = DefaultLogger\n\t}\n\tif o.EventListener == nil {\n\t\to.EventListener = &EventListener{}\n\t}\n\to.EventListener.EnsureDefaults(o.Logger)\n\tif o.MaxManifestFileSize == 0 {\n\t\to.MaxManifestFileSize = 128 << 20 // 128 MB\n\t}\n\tif o.MaxOpenFiles == 0 {\n\t\to.MaxOpenFiles = 1000\n\t}\n\tif o.MemTableSize <= 0 {\n\t\to.MemTableSize = 4 << 20 // 4 MB\n\t}\n\tif o.MemTableStopWritesThreshold <= 0 {\n\t\to.MemTableStopWritesThreshold = 2\n\t}\n\tif o.Merger == nil {\n\t\to.Merger = DefaultMerger\n\t}\n\tif o.MaxConcurrentCompactions == nil {\n\t\to.MaxConcurrentCompactions = func() int { return 1 }\n\t}\n\tif o.MaxConcurrentDownloads == nil {\n\t\to.MaxConcurrentDownloads = func() int { return 1 }\n\t}\n\tif o.NumPrevManifest <= 0 {\n\t\to.NumPrevManifest = 1\n\t}\n\n\tif o.FormatMajorVersion == FormatDefault {\n\t\to.FormatMajorVersion = FormatMinSupported\n\t\tif o.Experimental.CreateOnShared != remote.CreateOnSharedNone {\n\t\t\to.FormatMajorVersion = FormatMinForSharedObjects\n\t\t}\n\t}\n\n\tif o.FS == nil {\n\t\to.WithFSDefaults()\n\t}\n\tif o.FlushSplitBytes <= 0 {\n\t\to.FlushSplitBytes = 2 * o.Levels[0].TargetFileSize\n\t}\n\tif o.WALFailover != nil {\n\t\to.WALFailover.FailoverOptions.EnsureDefaults()\n\t}\n\tif o.Experimental.LevelMultiplier <= 0 {\n\t\to.Experimental.LevelMultiplier = defaultLevelMultiplier\n\t}\n\tif o.Experimental.ReadCompactionRate == 0 {\n\t\to.Experimental.ReadCompactionRate = 16000\n\t}\n\tif o.Experimental.ReadSamplingMultiplier == 0 {\n\t\to.Experimental.ReadSamplingMultiplier = 1 << 4\n\t}\n\tif o.Experimental.NumDeletionsThreshold == 0 {\n\t\to.Experimental.NumDeletionsThreshold = sstable.DefaultNumDeletionsThreshold\n\t}\n\tif o.Experimental.DeletionSizeRatioThreshold == 0 {\n\t\to.Experimental.DeletionSizeRatioThreshold = sstable.DefaultDeletionSizeRatioThreshold\n\t}\n\tif o.Experimental.TombstoneDenseCompactionThreshold == 0 {\n\t\to.Experimental.TombstoneDenseCompactionThreshold = 0.10\n\t}\n\tif o.Experimental.FileCacheShards <= 0 {\n\t\to.Experimental.FileCacheShards = runtime.GOMAXPROCS(0)\n\t}\n\tif o.Experimental.CPUWorkPermissionGranter == nil {\n\t\to.Experimental.CPUWorkPermissionGranter = defaultCPUWorkGranter{}\n\t}\n\tif o.Experimental.MultiLevelCompactionHeuristic == nil {\n\t\to.Experimental.MultiLevelCompactionHeuristic = WriteAmpHeuristic{}\n\t}\n\n\to.initMaps()\n\treturn o\n}\n\n// WithFSDefaults configures the Options to wrap the configured filesystem with\n// the default virtual file system middleware, like disk-health checking.\nfunc (o *Options) WithFSDefaults() *Options {\n\tif o.FS == nil {\n\t\to.FS = vfs.Default\n\t}\n\to.FS, o.private.fsCloser = vfs.WithDiskHealthChecks(o.FS, 5*time.Second, nil,\n\t\tfunc(info vfs.DiskSlowInfo) {\n\t\t\to.EventListener.DiskSlow(info)\n\t\t})\n\treturn o\n}\n\n// AddEventListener adds the provided event listener to the Options, in addition\n// to any existing event listener.\nfunc (o *Options) AddEventListener(l EventListener) {\n\tif o.EventListener != nil {\n\t\tl = TeeEventListener(l, *o.EventListener)\n\t}\n\to.EventListener = &l\n}\n\n// initMaps initializes the Comparers, Filters, and Mergers maps.\nfunc (o *Options) initMaps() {\n\tfor i := range o.Levels {\n\t\tl := &o.Levels[i]\n\t\tif l.FilterPolicy != nil {\n\t\t\tif o.Filters == nil {\n\t\t\t\to.Filters = make(map[string]FilterPolicy)\n\t\t\t}\n\t\t\tname := l.FilterPolicy.Name()\n\t\t\tif _, ok := o.Filters[name]; !ok {\n\t\t\t\to.Filters[name] = l.FilterPolicy\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Level returns the LevelOptions for the specified level.\nfunc (o *Options) Level(level int) LevelOptions {\n\tif level < len(o.Levels) {\n\t\treturn o.Levels[level]\n\t}\n\tn := len(o.Levels) - 1\n\tl := o.Levels[n]\n\tfor i := n; i < level; i++ {\n\t\tl.TargetFileSize *= 2\n\t}\n\treturn l\n}\n\n// Clone creates a shallow-copy of the supplied options.\nfunc (o *Options) Clone() *Options {\n\tn := &Options{}\n\tif o != nil {\n\t\t*n = *o\n\t}\n\treturn n\n}\n\nfunc filterPolicyName(p FilterPolicy) string {\n\tif p == nil {\n\t\treturn \"none\"\n\t}\n\treturn p.Name()\n}\n\nfunc (o *Options) String() string {\n\tvar buf bytes.Buffer\n\n\tcacheSize := int64(cacheDefaultSize)\n\tif o.Cache != nil {\n\t\tcacheSize = o.Cache.MaxSize()\n\t}\n\n\tfmt.Fprintf(&buf, \"[Version]\\n\")\n\tfmt.Fprintf(&buf, \"  pebble_version=0.1\\n\")\n\tfmt.Fprintf(&buf, \"\\n\")\n\tfmt.Fprintf(&buf, \"[Options]\\n\")\n\tfmt.Fprintf(&buf, \"  bytes_per_sync=%d\\n\", o.BytesPerSync)\n\tfmt.Fprintf(&buf, \"  cache_size=%d\\n\", cacheSize)\n\tfmt.Fprintf(&buf, \"  cleaner=%s\\n\", o.Cleaner)\n\tfmt.Fprintf(&buf, \"  compaction_debt_concurrency=%d\\n\", o.Experimental.CompactionDebtConcurrency)\n\tfmt.Fprintf(&buf, \"  comparer=%s\\n\", o.Comparer.Name)\n\tfmt.Fprintf(&buf, \"  disable_wal=%t\\n\", o.DisableWAL)\n\tif o.Experimental.DisableIngestAsFlushable != nil && o.Experimental.DisableIngestAsFlushable() {\n\t\tfmt.Fprintf(&buf, \"  disable_ingest_as_flushable=%t\\n\", true)\n\t}\n\tif o.Experimental.EnableColumnarBlocks != nil && o.Experimental.EnableColumnarBlocks() {\n\t\tfmt.Fprintf(&buf, \"  enable_columnar_blocks=%t\\n\", true)\n\t}\n\tfmt.Fprintf(&buf, \"  flush_delay_delete_range=%s\\n\", o.FlushDelayDeleteRange)\n\tfmt.Fprintf(&buf, \"  flush_delay_range_key=%s\\n\", o.FlushDelayRangeKey)\n\tfmt.Fprintf(&buf, \"  flush_split_bytes=%d\\n\", o.FlushSplitBytes)\n\tfmt.Fprintf(&buf, \"  format_major_version=%d\\n\", o.FormatMajorVersion)\n\tfmt.Fprintf(&buf, \"  key_schema=%s\\n\", o.KeySchema)\n\tfmt.Fprintf(&buf, \"  l0_compaction_concurrency=%d\\n\", o.Experimental.L0CompactionConcurrency)\n\tfmt.Fprintf(&buf, \"  l0_compaction_file_threshold=%d\\n\", o.L0CompactionFileThreshold)\n\tfmt.Fprintf(&buf, \"  l0_compaction_threshold=%d\\n\", o.L0CompactionThreshold)\n\tfmt.Fprintf(&buf, \"  l0_stop_writes_threshold=%d\\n\", o.L0StopWritesThreshold)\n\tfmt.Fprintf(&buf, \"  lbase_max_bytes=%d\\n\", o.LBaseMaxBytes)\n\tif o.Experimental.LevelMultiplier != defaultLevelMultiplier {\n\t\tfmt.Fprintf(&buf, \"  level_multiplier=%d\\n\", o.Experimental.LevelMultiplier)\n\t}\n\tfmt.Fprintf(&buf, \"  max_concurrent_compactions=%d\\n\", o.MaxConcurrentCompactions())\n\tfmt.Fprintf(&buf, \"  max_concurrent_downloads=%d\\n\", o.MaxConcurrentDownloads())\n\tfmt.Fprintf(&buf, \"  max_manifest_file_size=%d\\n\", o.MaxManifestFileSize)\n\tfmt.Fprintf(&buf, \"  max_open_files=%d\\n\", o.MaxOpenFiles)\n\tfmt.Fprintf(&buf, \"  mem_table_size=%d\\n\", o.MemTableSize)\n\tfmt.Fprintf(&buf, \"  mem_table_stop_writes_threshold=%d\\n\", o.MemTableStopWritesThreshold)\n\tfmt.Fprintf(&buf, \"  min_deletion_rate=%d\\n\", o.TargetByteDeletionRate)\n\tfmt.Fprintf(&buf, \"  merger=%s\\n\", o.Merger.Name)\n\tif o.Experimental.MultiLevelCompactionHeuristic != nil {\n\t\tfmt.Fprintf(&buf, \"  multilevel_compaction_heuristic=%s\\n\", o.Experimental.MultiLevelCompactionHeuristic.String())\n\t}\n\tfmt.Fprintf(&buf, \"  read_compaction_rate=%d\\n\", o.Experimental.ReadCompactionRate)\n\tfmt.Fprintf(&buf, \"  read_sampling_multiplier=%d\\n\", o.Experimental.ReadSamplingMultiplier)\n\tfmt.Fprintf(&buf, \"  num_deletions_threshold=%d\\n\", o.Experimental.NumDeletionsThreshold)\n\tfmt.Fprintf(&buf, \"  deletion_size_ratio_threshold=%f\\n\", o.Experimental.DeletionSizeRatioThreshold)\n\tfmt.Fprintf(&buf, \"  tombstone_dense_compaction_threshold=%f\\n\", o.Experimental.TombstoneDenseCompactionThreshold)\n\t// We no longer care about strict_wal_tail, but set it to true in case an\n\t// older version reads the options.\n\tfmt.Fprintf(&buf, \"  strict_wal_tail=%t\\n\", true)\n\tfmt.Fprintf(&buf, \"  table_cache_shards=%d\\n\", o.Experimental.FileCacheShards)\n\tfmt.Fprintf(&buf, \"  validate_on_ingest=%t\\n\", o.Experimental.ValidateOnIngest)\n\tfmt.Fprintf(&buf, \"  wal_dir=%s\\n\", o.WALDir)\n\tfmt.Fprintf(&buf, \"  wal_bytes_per_sync=%d\\n\", o.WALBytesPerSync)\n\tfmt.Fprintf(&buf, \"  max_writer_concurrency=%d\\n\", o.Experimental.MaxWriterConcurrency)\n\tfmt.Fprintf(&buf, \"  force_writer_parallelism=%t\\n\", o.Experimental.ForceWriterParallelism)\n\tfmt.Fprintf(&buf, \"  secondary_cache_size_bytes=%d\\n\", o.Experimental.SecondaryCacheSizeBytes)\n\tfmt.Fprintf(&buf, \"  create_on_shared=%d\\n\", o.Experimental.CreateOnShared)\n\n\t// Private options.\n\t//\n\t// These options are only encoded if true, because we do not want them to\n\t// appear in production serialized Options files, since they're testing-only\n\t// options. They're only serialized when true, which still ensures that the\n\t// metamorphic tests may propagate them to subprocesses.\n\tif o.private.disableDeleteOnlyCompactions {\n\t\tfmt.Fprintln(&buf, \"  disable_delete_only_compactions=true\")\n\t}\n\tif o.private.disableElisionOnlyCompactions {\n\t\tfmt.Fprintln(&buf, \"  disable_elision_only_compactions=true\")\n\t}\n\tif o.private.disableLazyCombinedIteration {\n\t\tfmt.Fprintln(&buf, \"  disable_lazy_combined_iteration=true\")\n\t}\n\n\tif o.WALFailover != nil {\n\t\tunhealthyThreshold, _ := o.WALFailover.FailoverOptions.UnhealthyOperationLatencyThreshold()\n\t\tfmt.Fprintf(&buf, \"\\n\")\n\t\tfmt.Fprintf(&buf, \"[WAL Failover]\\n\")\n\t\tfmt.Fprintf(&buf, \"  secondary_dir=%s\\n\", o.WALFailover.Secondary.Dirname)\n\t\tfmt.Fprintf(&buf, \"  primary_dir_probe_interval=%s\\n\", o.WALFailover.FailoverOptions.PrimaryDirProbeInterval)\n\t\tfmt.Fprintf(&buf, \"  healthy_probe_latency_threshold=%s\\n\", o.WALFailover.FailoverOptions.HealthyProbeLatencyThreshold)\n\t\tfmt.Fprintf(&buf, \"  healthy_interval=%s\\n\", o.WALFailover.FailoverOptions.HealthyInterval)\n\t\tfmt.Fprintf(&buf, \"  unhealthy_sampling_interval=%s\\n\", o.WALFailover.FailoverOptions.UnhealthySamplingInterval)\n\t\tfmt.Fprintf(&buf, \"  unhealthy_operation_latency_threshold=%s\\n\", unhealthyThreshold)\n\t\tfmt.Fprintf(&buf, \"  elevated_write_stall_threshold_lag=%s\\n\", o.WALFailover.FailoverOptions.ElevatedWriteStallThresholdLag)\n\t}\n\n\tfor i := range o.Levels {\n\t\tl := &o.Levels[i]\n\t\tfmt.Fprintf(&buf, \"\\n\")\n\t\tfmt.Fprintf(&buf, \"[Level \\\"%d\\\"]\\n\", i)\n\t\tfmt.Fprintf(&buf, \"  block_restart_interval=%d\\n\", l.BlockRestartInterval)\n\t\tfmt.Fprintf(&buf, \"  block_size=%d\\n\", l.BlockSize)\n\t\tfmt.Fprintf(&buf, \"  block_size_threshold=%d\\n\", l.BlockSizeThreshold)\n\t\tfmt.Fprintf(&buf, \"  compression=%s\\n\", resolveDefaultCompression(l.Compression()))\n\t\tfmt.Fprintf(&buf, \"  filter_policy=%s\\n\", filterPolicyName(l.FilterPolicy))\n\t\tfmt.Fprintf(&buf, \"  filter_type=%s\\n\", l.FilterType)\n\t\tfmt.Fprintf(&buf, \"  index_block_size=%d\\n\", l.IndexBlockSize)\n\t\tfmt.Fprintf(&buf, \"  target_file_size=%d\\n\", l.TargetFileSize)\n\t}\n\n\treturn buf.String()\n}\n\ntype parseOptionsFuncs struct {\n\tvisitNewSection          func(i, j int, section string) error\n\tvisitKeyValue            func(i, j int, section, key, value string) error\n\tvisitCommentOrWhitespace func(i, j int, whitespace string) error\n}\n\n// parseOptions takes options serialized by Options.String() and parses them\n// into keys and values. It calls fns.visitNewSection for the beginning of each\n// new section, fns.visitKeyValue for each key-value pair, and\n// visitCommentOrWhitespace for comments and whitespace between key-value pairs.\nfunc parseOptions(s string, fns parseOptionsFuncs) error {\n\tvar section, mappedSection string\n\ti := 0\n\tfor i < len(s) {\n\t\trem := s[i:]\n\t\tj := strings.IndexByte(rem, '\\n')\n\t\tif j < 0 {\n\t\t\tj = len(rem)\n\t\t} else {\n\t\t\tj += 1 // Include the newline.\n\t\t}\n\t\tline := strings.TrimSpace(s[i : i+j])\n\t\tstartOff, endOff := i, i+j\n\t\ti += j\n\n\t\tif len(line) == 0 || line[0] == ';' || line[0] == '#' {\n\t\t\t// Skip blank lines and comments.\n\t\t\tif fns.visitCommentOrWhitespace != nil {\n\t\t\t\tif err := fns.visitCommentOrWhitespace(startOff, endOff, line); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tn := len(line)\n\t\tif line[0] == '[' && line[n-1] == ']' {\n\t\t\t// Parse section.\n\t\t\tsection = line[1 : n-1]\n\t\t\t// RocksDB uses a similar (INI-style) syntax for the OPTIONS file, but\n\t\t\t// different section names and keys. The \"CFOptions ...\" paths are the\n\t\t\t// RocksDB versions which we map to the Pebble paths.\n\t\t\tmappedSection = section\n\t\t\tif section == `CFOptions \"default\"` {\n\t\t\t\tmappedSection = \"Options\"\n\t\t\t}\n\t\t\tif fns.visitNewSection != nil {\n\t\t\t\tif err := fns.visitNewSection(startOff, endOff, mappedSection); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\tpos := strings.Index(line, \"=\")\n\t\tif pos < 0 {\n\t\t\tconst maxLen = 50\n\t\t\tif len(line) > maxLen {\n\t\t\t\tline = line[:maxLen-3] + \"...\"\n\t\t\t}\n\t\t\treturn base.CorruptionErrorf(\"invalid key=value syntax: %q\", errors.Safe(line))\n\t\t}\n\n\t\tkey := strings.TrimSpace(line[:pos])\n\t\tvalue := strings.TrimSpace(line[pos+1:])\n\n\t\tif section == `CFOptions \"default\"` {\n\t\t\tswitch key {\n\t\t\tcase \"comparator\":\n\t\t\t\tkey = \"comparer\"\n\t\t\tcase \"merge_operator\":\n\t\t\t\tkey = \"merger\"\n\t\t\t}\n\t\t}\n\t\tif fns.visitKeyValue != nil {\n\t\t\tif err := fns.visitKeyValue(startOff, endOff, mappedSection, key, value); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\n// ParseHooks contains callbacks to create options fields which can have\n// user-defined implementations.\ntype ParseHooks struct {\n\tNewCache        func(size int64) *Cache\n\tNewCleaner      func(name string) (Cleaner, error)\n\tNewComparer     func(name string) (*Comparer, error)\n\tNewFilterPolicy func(name string) (FilterPolicy, error)\n\tNewKeySchema    func(name string) (KeySchema, error)\n\tNewMerger       func(name string) (*Merger, error)\n\tSkipUnknown     func(name, value string) bool\n}\n\n// Parse parses the options from the specified string. Note that certain\n// options cannot be parsed into populated fields. For example, comparer and\n// merger.\nfunc (o *Options) Parse(s string, hooks *ParseHooks) error {\n\tvisitKeyValue := func(i, j int, section, key, value string) error {\n\t\t// WARNING: DO NOT remove entries from the switches below because doing so\n\t\t// causes a key previously written to the OPTIONS file to be considered unknown,\n\t\t// a backwards incompatible change. Instead, leave in support for parsing the\n\t\t// key but simply don't parse the value.\n\n\t\tparseComparer := func(name string) (*Comparer, error) {\n\t\t\tswitch name {\n\t\t\tcase DefaultComparer.Name:\n\t\t\t\treturn DefaultComparer, nil\n\t\t\tcase testkeys.Comparer.Name:\n\t\t\t\treturn testkeys.Comparer, nil\n\t\t\tdefault:\n\t\t\t\tif hooks != nil && hooks.NewComparer != nil {\n\t\t\t\t\treturn hooks.NewComparer(name)\n\t\t\t\t}\n\t\t\t\treturn nil, nil\n\t\t\t}\n\t\t}\n\n\t\tswitch {\n\t\tcase section == \"Version\":\n\t\t\tswitch key {\n\t\t\tcase \"pebble_version\":\n\t\t\tdefault:\n\t\t\t\tif hooks != nil && hooks.SkipUnknown != nil && hooks.SkipUnknown(section+\".\"+key, value) {\n\t\t\t\t\treturn nil\n\t\t\t\t}\n\t\t\t\treturn errors.Errorf(\"pebble: unknown option: %s.%s\",\n\t\t\t\t\terrors.Safe(section), errors.Safe(key))\n\t\t\t}\n\t\t\treturn nil\n\n\t\tcase section == \"Options\":\n\t\t\tvar err error\n\t\t\tswitch key {\n\t\t\tcase \"bytes_per_sync\":\n\t\t\t\to.BytesPerSync, err = strconv.Atoi(value)\n\t\t\tcase \"cache_size\":\n\t\t\t\tvar n int64\n\t\t\t\tn, err = strconv.ParseInt(value, 10, 64)\n\t\t\t\tif err == nil && hooks != nil && hooks.NewCache != nil {\n\t\t\t\t\tif o.Cache != nil {\n\t\t\t\t\t\to.Cache.Unref()\n\t\t\t\t\t}\n\t\t\t\t\to.Cache = hooks.NewCache(n)\n\t\t\t\t}\n\t\t\t\t// We avoid calling cache.New in parsing because it makes it\n\t\t\t\t// too easy to leak a cache.\n\t\t\tcase \"cleaner\":\n\t\t\t\tswitch value {\n\t\t\t\tcase \"archive\":\n\t\t\t\t\to.Cleaner = ArchiveCleaner{}\n\t\t\t\tcase \"delete\":\n\t\t\t\t\to.Cleaner = DeleteCleaner{}\n\t\t\t\tdefault:\n\t\t\t\t\tif hooks != nil && hooks.NewCleaner != nil {\n\t\t\t\t\t\to.Cleaner, err = hooks.NewCleaner(value)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\tcase \"comparer\":\n\t\t\t\tvar comparer *Comparer\n\t\t\t\tcomparer, err = parseComparer(value)\n\t\t\t\tif comparer != nil {\n\t\t\t\t\to.Comparer = comparer\n\t\t\t\t}\n\t\t\tcase \"compaction_debt_concurrency\":\n\t\t\t\to.Experimental.CompactionDebtConcurrency, err = strconv.ParseUint(value, 10, 64)\n\t\t\tcase \"delete_range_flush_delay\":\n\t\t\t\t// NB: This is a deprecated serialization of the\n\t\t\t\t// `flush_delay_delete_range`.\n\t\t\t\to.FlushDelayDeleteRange, err = time.ParseDuration(value)\n\t\t\tcase \"disable_delete_only_compactions\":\n\t\t\t\to.private.disableDeleteOnlyCompactions, err = strconv.ParseBool(value)\n\t\t\tcase \"disable_elision_only_compactions\":\n\t\t\t\to.private.disableElisionOnlyCompactions, err = strconv.ParseBool(value)\n\t\t\tcase \"disable_ingest_as_flushable\":\n\t\t\t\tvar v bool\n\t\t\t\tv, err = strconv.ParseBool(value)\n\t\t\t\tif err == nil {\n\t\t\t\t\to.Experimental.DisableIngestAsFlushable = func() bool { return v }\n\t\t\t\t}\n\t\t\tcase \"disable_lazy_combined_iteration\":\n\t\t\t\to.private.disableLazyCombinedIteration, err = strconv.ParseBool(value)\n\t\t\tcase \"disable_wal\":\n\t\t\t\to.DisableWAL, err = strconv.ParseBool(value)\n\t\t\tcase \"enable_columnar_blocks\":\n\t\t\t\tvar v bool\n\t\t\t\tif v, err = strconv.ParseBool(value); err == nil {\n\t\t\t\t\to.Experimental.EnableColumnarBlocks = func() bool { return v }\n\t\t\t\t}\n\t\t\tcase \"flush_delay_delete_range\":\n\t\t\t\to.FlushDelayDeleteRange, err = time.ParseDuration(value)\n\t\t\tcase \"flush_delay_range_key\":\n\t\t\t\to.FlushDelayRangeKey, err = time.ParseDuration(value)\n\t\t\tcase \"flush_split_bytes\":\n\t\t\t\to.FlushSplitBytes, err = strconv.ParseInt(value, 10, 64)\n\t\t\tcase \"format_major_version\":\n\t\t\t\t// NB: The version written here may be stale. Open does\n\t\t\t\t// not use the format major version encoded in the\n\t\t\t\t// OPTIONS file other than to validate that the encoded\n\t\t\t\t// version is valid right here.\n\t\t\t\tvar v uint64\n\t\t\t\tv, err = strconv.ParseUint(value, 10, 64)\n\t\t\t\tif vers := FormatMajorVersion(v); vers > internalFormatNewest || vers == FormatDefault {\n\t\t\t\t\terr = errors.Newf(\"unsupported format major version %d\", o.FormatMajorVersion)\n\t\t\t\t}\n\t\t\t\tif err == nil {\n\t\t\t\t\to.FormatMajorVersion = FormatMajorVersion(v)\n\t\t\t\t}\n\t\t\tcase \"key_schema\":\n\t\t\t\to.KeySchema = value\n\t\t\t\tif o.KeySchemas == nil {\n\t\t\t\t\to.KeySchemas = make(map[string]*KeySchema)\n\t\t\t\t}\n\t\t\t\tif _, ok := o.KeySchemas[o.KeySchema]; !ok {\n\t\t\t\t\tif strings.HasPrefix(value, \"DefaultKeySchema(\") && strings.HasSuffix(value, \")\") {\n\t\t\t\t\t\targsStr := strings.TrimSuffix(strings.TrimPrefix(value, \"DefaultKeySchema(\"), \")\")\n\t\t\t\t\t\targs := strings.FieldsFunc(argsStr, func(r rune) bool {\n\t\t\t\t\t\t\treturn unicode.IsSpace(r) || r == ','\n\t\t\t\t\t\t})\n\t\t\t\t\t\tvar comparer *base.Comparer\n\t\t\t\t\t\tvar bundleSize int\n\t\t\t\t\t\tcomparer, err = parseComparer(args[0])\n\t\t\t\t\t\tif err == nil {\n\t\t\t\t\t\t\tbundleSize, err = strconv.Atoi(args[1])\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif err == nil {\n\t\t\t\t\t\t\tschema := colblk.DefaultKeySchema(comparer, bundleSize)\n\t\t\t\t\t\t\to.KeySchema = schema.Name\n\t\t\t\t\t\t\to.KeySchemas[o.KeySchema] = &schema\n\t\t\t\t\t\t}\n\t\t\t\t\t} else if hooks != nil && hooks.NewKeySchema != nil {\n\t\t\t\t\t\tvar schema KeySchema\n\t\t\t\t\t\tschema, err = hooks.NewKeySchema(value)\n\t\t\t\t\t\tif err == nil {\n\t\t\t\t\t\t\to.KeySchemas[value] = &schema\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\tcase \"l0_compaction_concurrency\":\n\t\t\t\to.Experimental.L0CompactionConcurrency, err = strconv.Atoi(value)\n\t\t\tcase \"l0_compaction_file_threshold\":\n\t\t\t\to.L0CompactionFileThreshold, err = strconv.Atoi(value)\n\t\t\tcase \"l0_compaction_threshold\":\n\t\t\t\to.L0CompactionThreshold, err = strconv.Atoi(value)\n\t\t\tcase \"l0_stop_writes_threshold\":\n\t\t\t\to.L0StopWritesThreshold, err = strconv.Atoi(value)\n\t\t\tcase \"l0_sublevel_compactions\":\n\t\t\t\t// Do nothing; option existed in older versions of pebble.\n\t\t\tcase \"lbase_max_bytes\":\n\t\t\t\to.LBaseMaxBytes, err = strconv.ParseInt(value, 10, 64)\n\t\t\tcase \"level_multiplier\":\n\t\t\t\to.Experimental.LevelMultiplier, err = strconv.Atoi(value)\n\t\t\tcase \"max_concurrent_compactions\":\n\t\t\t\tvar concurrentCompactions int\n\t\t\t\tconcurrentCompactions, err = strconv.Atoi(value)\n\t\t\t\tif concurrentCompactions <= 0 {\n\t\t\t\t\terr = errors.New(\"max_concurrent_compactions cannot be <= 0\")\n\t\t\t\t} else {\n\t\t\t\t\to.MaxConcurrentCompactions = func() int { return concurrentCompactions }\n\t\t\t\t}\n\t\t\tcase \"max_concurrent_downloads\":\n\t\t\t\tvar concurrentDownloads int\n\t\t\t\tconcurrentDownloads, err = strconv.Atoi(value)\n\t\t\t\tif concurrentDownloads <= 0 {\n\t\t\t\t\terr = errors.New(\"max_concurrent_compactions cannot be <= 0\")\n\t\t\t\t} else {\n\t\t\t\t\to.MaxConcurrentDownloads = func() int { return concurrentDownloads }\n\t\t\t\t}\n\t\t\tcase \"max_manifest_file_size\":\n\t\t\t\to.MaxManifestFileSize, err = strconv.ParseInt(value, 10, 64)\n\t\t\tcase \"max_open_files\":\n\t\t\t\to.MaxOpenFiles, err = strconv.Atoi(value)\n\t\t\tcase \"mem_table_size\":\n\t\t\t\to.MemTableSize, err = strconv.ParseUint(value, 10, 64)\n\t\t\tcase \"mem_table_stop_writes_threshold\":\n\t\t\t\to.MemTableStopWritesThreshold, err = strconv.Atoi(value)\n\t\t\tcase \"min_compaction_rate\":\n\t\t\t\t// Do nothing; option existed in older versions of pebble, and\n\t\t\t\t// may be meaningful again eventually.\n\t\t\tcase \"min_deletion_rate\":\n\t\t\t\to.TargetByteDeletionRate, err = strconv.Atoi(value)\n\t\t\tcase \"min_flush_rate\":\n\t\t\t\t// Do nothing; option existed in older versions of pebble, and\n\t\t\t\t// may be meaningful again eventually.\n\t\t\tcase \"multilevel_compaction_heuristic\":\n\t\t\t\tswitch {\n\t\t\t\tcase value == \"none\":\n\t\t\t\t\to.Experimental.MultiLevelCompactionHeuristic = NoMultiLevel{}\n\t\t\t\tcase strings.HasPrefix(value, \"wamp\"):\n\t\t\t\t\tfields := strings.FieldsFunc(strings.TrimPrefix(value, \"wamp\"), func(r rune) bool {\n\t\t\t\t\t\treturn unicode.IsSpace(r) || r == ',' || r == '(' || r == ')'\n\t\t\t\t\t})\n\t\t\t\t\tif len(fields) != 2 {\n\t\t\t\t\t\terr = errors.Newf(\"require 2 arguments\")\n\t\t\t\t\t}\n\t\t\t\t\tvar h WriteAmpHeuristic\n\t\t\t\t\tif err == nil {\n\t\t\t\t\t\th.AddPropensity, err = strconv.ParseFloat(fields[0], 64)\n\t\t\t\t\t}\n\t\t\t\t\tif err == nil {\n\t\t\t\t\t\th.AllowL0, err = strconv.ParseBool(fields[1])\n\t\t\t\t\t}\n\t\t\t\t\tif err == nil {\n\t\t\t\t\t\to.Experimental.MultiLevelCompactionHeuristic = h\n\t\t\t\t\t} else {\n\t\t\t\t\t\terr = errors.Wrapf(err, \"unexpected wamp heuristic arguments: %s\", value)\n\t\t\t\t\t}\n\t\t\t\tdefault:\n\t\t\t\t\terr = errors.Newf(\"unrecognized multilevel compaction heuristic: %s\", value)\n\t\t\t\t}\n\t\t\tcase \"point_tombstone_weight\":\n\t\t\t\t// Do nothing; deprecated.\n\t\t\tcase \"strict_wal_tail\":\n\t\t\t\tvar strictWALTail bool\n\t\t\t\tstrictWALTail, err = strconv.ParseBool(value)\n\t\t\t\tif err == nil && !strictWALTail {\n\t\t\t\t\terr = errors.Newf(\"reading from versions with strict_wal_tail=false no longer supported\")\n\t\t\t\t}\n\t\t\tcase \"merger\":\n\t\t\t\tswitch value {\n\t\t\t\tcase \"nullptr\":\n\t\t\t\t\to.Merger = nil\n\t\t\t\tcase \"pebble.concatenate\":\n\t\t\t\t\to.Merger = DefaultMerger\n\t\t\t\tdefault:\n\t\t\t\t\tif hooks != nil && hooks.NewMerger != nil {\n\t\t\t\t\t\to.Merger, err = hooks.NewMerger(value)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\tcase \"read_compaction_rate\":\n\t\t\t\to.Experimental.ReadCompactionRate, err = strconv.ParseInt(value, 10, 64)\n\t\t\tcase \"read_sampling_multiplier\":\n\t\t\t\to.Experimental.ReadSamplingMultiplier, err = strconv.ParseInt(value, 10, 64)\n\t\t\tcase \"num_deletions_threshold\":\n\t\t\t\to.Experimental.NumDeletionsThreshold, err = strconv.Atoi(value)\n\t\t\tcase \"deletion_size_ratio_threshold\":\n\t\t\t\tval, parseErr := strconv.ParseFloat(value, 32)\n\t\t\t\to.Experimental.DeletionSizeRatioThreshold = float32(val)\n\t\t\t\terr = parseErr\n\t\t\tcase \"tombstone_dense_compaction_threshold\":\n\t\t\t\to.Experimental.TombstoneDenseCompactionThreshold, err = strconv.ParseFloat(value, 64)\n\t\t\tcase \"table_cache_shards\":\n\t\t\t\to.Experimental.FileCacheShards, err = strconv.Atoi(value)\n\t\t\tcase \"table_format\":\n\t\t\t\tswitch value {\n\t\t\t\tcase \"leveldb\":\n\t\t\t\tcase \"rocksdbv2\":\n\t\t\t\tdefault:\n\t\t\t\t\treturn errors.Errorf(\"pebble: unknown table format: %q\", errors.Safe(value))\n\t\t\t\t}\n\t\t\tcase \"table_property_collectors\":\n\t\t\t\t// No longer implemented; ignore.\n\t\t\tcase \"validate_on_ingest\":\n\t\t\t\to.Experimental.ValidateOnIngest, err = strconv.ParseBool(value)\n\t\t\tcase \"wal_dir\":\n\t\t\t\to.WALDir = value\n\t\t\tcase \"wal_bytes_per_sync\":\n\t\t\t\to.WALBytesPerSync, err = strconv.Atoi(value)\n\t\t\tcase \"max_writer_concurrency\":\n\t\t\t\to.Experimental.MaxWriterConcurrency, err = strconv.Atoi(value)\n\t\t\tcase \"force_writer_parallelism\":\n\t\t\t\to.Experimental.ForceWriterParallelism, err = strconv.ParseBool(value)\n\t\t\tcase \"secondary_cache_size_bytes\":\n\t\t\t\to.Experimental.SecondaryCacheSizeBytes, err = strconv.ParseInt(value, 10, 64)\n\t\t\tcase \"create_on_shared\":\n\t\t\t\tvar createOnSharedInt int64\n\t\t\t\tcreateOnSharedInt, err = strconv.ParseInt(value, 10, 64)\n\t\t\t\to.Experimental.CreateOnShared = remote.CreateOnSharedStrategy(createOnSharedInt)\n\t\t\tdefault:\n\t\t\t\tif hooks != nil && hooks.SkipUnknown != nil && hooks.SkipUnknown(section+\".\"+key, value) {\n\t\t\t\t\treturn nil\n\t\t\t\t}\n\t\t\t\treturn errors.Errorf(\"pebble: unknown option: %s.%s\",\n\t\t\t\t\terrors.Safe(section), errors.Safe(key))\n\t\t\t}\n\t\t\treturn err\n\n\t\tcase section == \"WAL Failover\":\n\t\t\tif o.WALFailover == nil {\n\t\t\t\to.WALFailover = new(WALFailoverOptions)\n\t\t\t}\n\t\t\tvar err error\n\t\t\tswitch key {\n\t\t\tcase \"secondary_dir\":\n\t\t\t\to.WALFailover.Secondary = wal.Dir{Dirname: value, FS: vfs.Default}\n\t\t\tcase \"primary_dir_probe_interval\":\n\t\t\t\to.WALFailover.PrimaryDirProbeInterval, err = time.ParseDuration(value)\n\t\t\tcase \"healthy_probe_latency_threshold\":\n\t\t\t\to.WALFailover.HealthyProbeLatencyThreshold, err = time.ParseDuration(value)\n\t\t\tcase \"healthy_interval\":\n\t\t\t\to.WALFailover.HealthyInterval, err = time.ParseDuration(value)\n\t\t\tcase \"unhealthy_sampling_interval\":\n\t\t\t\to.WALFailover.UnhealthySamplingInterval, err = time.ParseDuration(value)\n\t\t\tcase \"unhealthy_operation_latency_threshold\":\n\t\t\t\tvar threshold time.Duration\n\t\t\t\tthreshold, err = time.ParseDuration(value)\n\t\t\t\to.WALFailover.UnhealthyOperationLatencyThreshold = func() (time.Duration, bool) {\n\t\t\t\t\treturn threshold, true\n\t\t\t\t}\n\t\t\tcase \"elevated_write_stall_threshold_lag\":\n\t\t\t\to.WALFailover.ElevatedWriteStallThresholdLag, err = time.ParseDuration(value)\n\t\t\tdefault:\n\t\t\t\tif hooks != nil && hooks.SkipUnknown != nil && hooks.SkipUnknown(section+\".\"+key, value) {\n\t\t\t\t\treturn nil\n\t\t\t\t}\n\t\t\t\treturn errors.Errorf(\"pebble: unknown option: %s.%s\",\n\t\t\t\t\terrors.Safe(section), errors.Safe(key))\n\t\t\t}\n\t\t\treturn err\n\n\t\tcase strings.HasPrefix(section, \"Level \"):\n\t\t\tvar index int\n\t\t\tif n, err := fmt.Sscanf(section, `Level \"%d\"`, &index); err != nil {\n\t\t\t\treturn err\n\t\t\t} else if n != 1 {\n\t\t\t\tif hooks != nil && hooks.SkipUnknown != nil && hooks.SkipUnknown(section, value) {\n\t\t\t\t\treturn nil\n\t\t\t\t}\n\t\t\t\treturn errors.Errorf(\"pebble: unknown section: %q\", errors.Safe(section))\n\t\t\t}\n\n\t\t\tif len(o.Levels) <= index {\n\t\t\t\tnewLevels := make([]LevelOptions, index+1)\n\t\t\t\tcopy(newLevels, o.Levels)\n\t\t\t\to.Levels = newLevels\n\t\t\t}\n\t\t\tl := &o.Levels[index]\n\n\t\t\tvar err error\n\t\t\tswitch key {\n\t\t\tcase \"block_restart_interval\":\n\t\t\t\tl.BlockRestartInterval, err = strconv.Atoi(value)\n\t\t\tcase \"block_size\":\n\t\t\t\tl.BlockSize, err = strconv.Atoi(value)\n\t\t\tcase \"block_size_threshold\":\n\t\t\t\tl.BlockSizeThreshold, err = strconv.Atoi(value)\n\t\t\tcase \"compression\":\n\t\t\t\tswitch value {\n\t\t\t\tcase \"Default\":\n\t\t\t\t\tl.Compression = func() Compression { return DefaultCompression }\n\t\t\t\tcase \"NoCompression\":\n\t\t\t\t\tl.Compression = func() Compression { return NoCompression }\n\t\t\t\tcase \"Snappy\":\n\t\t\t\t\tl.Compression = func() Compression { return SnappyCompression }\n\t\t\t\tcase \"ZSTD\":\n\t\t\t\t\tl.Compression = func() Compression { return ZstdCompression }\n\t\t\t\tdefault:\n\t\t\t\t\treturn errors.Errorf(\"pebble: unknown compression: %q\", errors.Safe(value))\n\t\t\t\t}\n\t\t\tcase \"filter_policy\":\n\t\t\t\tif hooks != nil && hooks.NewFilterPolicy != nil {\n\t\t\t\t\tl.FilterPolicy, err = hooks.NewFilterPolicy(value)\n\t\t\t\t}\n\t\t\tcase \"filter_type\":\n\t\t\t\tswitch value {\n\t\t\t\tcase \"table\":\n\t\t\t\t\tl.FilterType = TableFilter\n\t\t\t\tdefault:\n\t\t\t\t\treturn errors.Errorf(\"pebble: unknown filter type: %q\", errors.Safe(value))\n\t\t\t\t}\n\t\t\tcase \"index_block_size\":\n\t\t\t\tl.IndexBlockSize, err = strconv.Atoi(value)\n\t\t\tcase \"target_file_size\":\n\t\t\t\tl.TargetFileSize, err = strconv.ParseInt(value, 10, 64)\n\t\t\tdefault:\n\t\t\t\tif hooks != nil && hooks.SkipUnknown != nil && hooks.SkipUnknown(section+\".\"+key, value) {\n\t\t\t\t\treturn nil\n\t\t\t\t}\n\t\t\t\treturn errors.Errorf(\"pebble: unknown option: %s.%s\", errors.Safe(section), errors.Safe(key))\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\t\tif hooks != nil && hooks.SkipUnknown != nil && hooks.SkipUnknown(section+\".\"+key, value) {\n\t\t\treturn nil\n\t\t}\n\t\treturn errors.Errorf(\"pebble: unknown section: %q\", errors.Safe(section))\n\t}\n\treturn parseOptions(s, parseOptionsFuncs{\n\t\tvisitKeyValue: visitKeyValue,\n\t})\n}\n\n// ErrMissingWALRecoveryDir is an error returned when a database is attempted to be\n// opened without supplying a Options.WALRecoveryDir entry for a directory that\n// may contain WALs required to recover a consistent database state.\ntype ErrMissingWALRecoveryDir struct {\n\tDir string\n}\n\n// Error implements error.\nfunc (e ErrMissingWALRecoveryDir) Error() string {\n\treturn fmt.Sprintf(\"directory %q may contain relevant WALs\", e.Dir)\n}\n\n// CheckCompatibility verifies the options are compatible with the previous options\n// serialized by Options.String(). For example, the Comparer and Merger must be\n// the same, or data will not be able to be properly read from the DB.\n//\n// This function only looks at specific keys and does not error out if the\n// options are newer and contain unknown keys.\nfunc (o *Options) CheckCompatibility(previousOptions string) error {\n\tvisitKeyValue := func(i, j int, section, key, value string) error {\n\t\tswitch section + \".\" + key {\n\t\tcase \"Options.comparer\":\n\t\t\tif value != o.Comparer.Name {\n\t\t\t\treturn errors.Errorf(\"pebble: comparer name from file %q != comparer name from options %q\",\n\t\t\t\t\terrors.Safe(value), errors.Safe(o.Comparer.Name))\n\t\t\t}\n\t\tcase \"Options.merger\":\n\t\t\t// RocksDB allows the merge operator to be unspecified, in which case it\n\t\t\t// shows up as \"nullptr\".\n\t\t\tif value != \"nullptr\" && value != o.Merger.Name {\n\t\t\t\treturn errors.Errorf(\"pebble: merger name from file %q != merger name from options %q\",\n\t\t\t\t\terrors.Safe(value), errors.Safe(o.Merger.Name))\n\t\t\t}\n\t\tcase \"Options.wal_dir\", \"WAL Failover.secondary_dir\":\n\t\t\tswitch {\n\t\t\tcase o.WALDir == value:\n\t\t\t\treturn nil\n\t\t\tcase o.WALFailover != nil && o.WALFailover.Secondary.Dirname == value:\n\t\t\t\treturn nil\n\t\t\tdefault:\n\t\t\t\tfor _, d := range o.WALRecoveryDirs {\n\t\t\t\t\tif d.Dirname == value {\n\t\t\t\t\t\treturn nil\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn ErrMissingWALRecoveryDir{Dir: value}\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}\n\treturn parseOptions(previousOptions, parseOptionsFuncs{visitKeyValue: visitKeyValue})\n}\n\n// Validate verifies that the options are mutually consistent. For example,\n// L0StopWritesThreshold must be >= L0CompactionThreshold, otherwise a write\n// stall would persist indefinitely.\nfunc (o *Options) Validate() error {\n\t// Note that we can presume Options.EnsureDefaults has been called, so there\n\t// is no need to check for zero values.\n\n\tvar buf strings.Builder\n\tif o.Experimental.L0CompactionConcurrency < 1 {\n\t\tfmt.Fprintf(&buf, \"L0CompactionConcurrency (%d) must be >= 1\\n\",\n\t\t\to.Experimental.L0CompactionConcurrency)\n\t}\n\tif o.L0StopWritesThreshold < o.L0CompactionThreshold {\n\t\tfmt.Fprintf(&buf, \"L0StopWritesThreshold (%d) must be >= L0CompactionThreshold (%d)\\n\",\n\t\t\to.L0StopWritesThreshold, o.L0CompactionThreshold)\n\t}\n\tif uint64(o.MemTableSize) >= maxMemTableSize {\n\t\tfmt.Fprintf(&buf, \"MemTableSize (%s) must be < %s\\n\",\n\t\t\thumanize.Bytes.Uint64(uint64(o.MemTableSize)), humanize.Bytes.Uint64(maxMemTableSize))\n\t}\n\tif o.MemTableStopWritesThreshold < 2 {\n\t\tfmt.Fprintf(&buf, \"MemTableStopWritesThreshold (%d) must be >= 2\\n\",\n\t\t\to.MemTableStopWritesThreshold)\n\t}\n\tif o.FormatMajorVersion < FormatMinSupported || o.FormatMajorVersion > internalFormatNewest {\n\t\tfmt.Fprintf(&buf, \"FormatMajorVersion (%d) must be between %d and %d\\n\",\n\t\t\to.FormatMajorVersion, FormatMinSupported, internalFormatNewest)\n\t}\n\tif o.Experimental.CreateOnShared != remote.CreateOnSharedNone && o.FormatMajorVersion < FormatMinForSharedObjects {\n\t\tfmt.Fprintf(&buf, \"FormatMajorVersion (%d) when CreateOnShared is set must be at least %d\\n\",\n\t\t\to.FormatMajorVersion, FormatMinForSharedObjects)\n\t}\n\tif o.FileCache != nil && o.Cache != o.FileCache.cache {\n\t\tfmt.Fprintf(&buf, \"underlying cache in the FileCache and the Cache dont match\\n\")\n\t}\n\tif len(o.KeySchemas) > 0 {\n\t\tif o.KeySchema == \"\" {\n\t\t\tfmt.Fprintf(&buf, \"KeySchemas is set but KeySchema is not\\n\")\n\t\t}\n\t\tif _, ok := o.KeySchemas[o.KeySchema]; !ok {\n\t\t\tfmt.Fprintf(&buf, \"KeySchema %q not found in KeySchemas\\n\", o.KeySchema)\n\t\t}\n\t}\n\tif buf.Len() == 0 {\n\t\treturn nil\n\t}\n\treturn errors.New(buf.String())\n}\n\n// MakeReaderOptions constructs sstable.ReaderOptions from the corresponding\n// options in the receiver.\nfunc (o *Options) MakeReaderOptions() sstable.ReaderOptions {\n\tvar readerOpts sstable.ReaderOptions\n\tif o != nil {\n\t\treaderOpts.Comparer = o.Comparer\n\t\treaderOpts.Filters = o.Filters\n\t\treaderOpts.KeySchemas = o.KeySchemas\n\t\treaderOpts.LoadBlockSema = o.LoadBlockSema\n\t\treaderOpts.LoggerAndTracer = o.LoggerAndTracer\n\t\treaderOpts.Merger = o.Merger\n\t}\n\treturn readerOpts\n}\n\n// MakeWriterOptions constructs sstable.WriterOptions for the specified level\n// from the corresponding options in the receiver.\nfunc (o *Options) MakeWriterOptions(level int, format sstable.TableFormat) sstable.WriterOptions {\n\tvar writerOpts sstable.WriterOptions\n\twriterOpts.TableFormat = format\n\tif o != nil {\n\t\twriterOpts.Comparer = o.Comparer\n\t\tif o.Merger != nil {\n\t\t\twriterOpts.MergerName = o.Merger.Name\n\t\t}\n\t\twriterOpts.BlockPropertyCollectors = o.BlockPropertyCollectors\n\t}\n\tif format >= sstable.TableFormatPebblev3 {\n\t\twriterOpts.ShortAttributeExtractor = o.Experimental.ShortAttributeExtractor\n\t\twriterOpts.RequiredInPlaceValueBound = o.Experimental.RequiredInPlaceValueBound\n\t\tif format >= sstable.TableFormatPebblev4 && level == numLevels-1 {\n\t\t\twriterOpts.WritingToLowestLevel = true\n\t\t}\n\t}\n\tlevelOpts := o.Level(level)\n\twriterOpts.BlockRestartInterval = levelOpts.BlockRestartInterval\n\twriterOpts.BlockSize = levelOpts.BlockSize\n\twriterOpts.BlockSizeThreshold = levelOpts.BlockSizeThreshold\n\twriterOpts.Compression = resolveDefaultCompression(levelOpts.Compression())\n\twriterOpts.FilterPolicy = levelOpts.FilterPolicy\n\twriterOpts.FilterType = levelOpts.FilterType\n\twriterOpts.IndexBlockSize = levelOpts.IndexBlockSize\n\twriterOpts.KeySchema = o.KeySchemas[o.KeySchema]\n\twriterOpts.AllocatorSizeClasses = o.AllocatorSizeClasses\n\twriterOpts.NumDeletionsThreshold = o.Experimental.NumDeletionsThreshold\n\twriterOpts.DeletionSizeRatioThreshold = o.Experimental.DeletionSizeRatioThreshold\n\treturn writerOpts\n}\n\nfunc resolveDefaultCompression(c Compression) Compression {\n\tif c <= DefaultCompression || c >= block.NCompression {\n\t\tc = SnappyCompression\n\t}\n\treturn c\n}\n\n// UserKeyCategories describes a partitioning of the user key space. Each\n// partition is a category with a name. The categories are used for informative\n// purposes only (like pprof labels). Pebble does not treat keys differently\n// based on the UserKeyCategories.\n//\n// The partitions are defined by their upper bounds. The last partition is\n// assumed to go until the end of keyspace; its UpperBound is ignored. The rest\n// of the partitions are ordered by their UpperBound.\ntype UserKeyCategories struct {\n\tcategories []UserKeyCategory\n\tcmp        base.Compare\n\t// rangeNames[i][j] contains the string referring to the categories in the\n\t// range [i, j], with j > i.\n\trangeNames [][]string\n}\n\n// UserKeyCategory describes a partition of the user key space.\n//\n// User keys >= the previous category's UpperBound and < this category's\n// UpperBound are part of this category.\ntype UserKeyCategory struct {\n\tName string\n\t// UpperBound is the exclusive upper bound of the category. All user keys >= the\n\t// previous category's UpperBound and < this UpperBound are part of this\n\t// category.\n\tUpperBound []byte\n}\n\n// MakeUserKeyCategories creates a UserKeyCategories object with the given\n// categories. The object is immutable and can be reused across different\n// stores.\nfunc MakeUserKeyCategories(cmp base.Compare, categories ...UserKeyCategory) UserKeyCategories {\n\tn := len(categories)\n\tif n == 0 {\n\t\treturn UserKeyCategories{}\n\t}\n\tif categories[n-1].UpperBound != nil {\n\t\tpanic(\"last category UpperBound must be nil\")\n\t}\n\t// Verify that the partitions are ordered as expected.\n\tfor i := 1; i < n-1; i++ {\n\t\tif cmp(categories[i-1].UpperBound, categories[i].UpperBound) >= 0 {\n\t\t\tpanic(\"invalid UserKeyCategories: key prefixes must be sorted\")\n\t\t}\n\t}\n\n\t// Precalculate a table of range names to avoid allocations in the\n\t// categorization path.\n\trangeNamesBuf := make([]string, n*n)\n\trangeNames := make([][]string, n)\n\tfor i := range rangeNames {\n\t\trangeNames[i] = rangeNamesBuf[:n]\n\t\trangeNamesBuf = rangeNamesBuf[n:]\n\t\tfor j := i + 1; j < n; j++ {\n\t\t\trangeNames[i][j] = categories[i].Name + \"-\" + categories[j].Name\n\t\t}\n\t}\n\treturn UserKeyCategories{\n\t\tcategories: categories,\n\t\tcmp:        cmp,\n\t\trangeNames: rangeNames,\n\t}\n}\n\n// Len returns the number of categories defined.\nfunc (kc *UserKeyCategories) Len() int {\n\treturn len(kc.categories)\n}\n\n// CategorizeKey returns the name of the category containing the key.\nfunc (kc *UserKeyCategories) CategorizeKey(userKey []byte) string {\n\tidx := sort.Search(len(kc.categories)-1, func(i int) bool {\n\t\treturn kc.cmp(userKey, kc.categories[i].UpperBound) < 0\n\t})\n\treturn kc.categories[idx].Name\n}\n\n// CategorizeKeyRange returns the name of the category containing the key range.\n// If the key range spans multiple categories, the result shows the first and\n// last category separated by a dash, e.g. `cat1-cat5`.\nfunc (kc *UserKeyCategories) CategorizeKeyRange(startUserKey, endUserKey []byte) string {\n\tn := len(kc.categories)\n\tp := sort.Search(n-1, func(i int) bool {\n\t\treturn kc.cmp(startUserKey, kc.categories[i].UpperBound) < 0\n\t})\n\tif p == n-1 || kc.cmp(endUserKey, kc.categories[p].UpperBound) < 0 {\n\t\t// Fast path for a single category.\n\t\treturn kc.categories[p].Name\n\t}\n\t// Binary search among the remaining categories.\n\tq := p + 1 + sort.Search(n-2-p, func(i int) bool {\n\t\treturn kc.cmp(endUserKey, kc.categories[p+1+i].UpperBound) < 0\n\t})\n\treturn kc.rangeNames[p][q]\n}\n"
        },
        {
          "name": "options_test.go",
          "type": "blob",
          "size": 12.4921875,
          "content": "// Copyright 2018 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"fmt\"\n\t\"math/rand/v2\"\n\t\"runtime\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/testkeys\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/cockroachdb/pebble/wal\"\n\t\"github.com/stretchr/testify/require\"\n)\n\n// testingRandomized randomizes some default options. Currently, it's\n// used for testing under a random format major version in some tests.\nfunc (o *Options) testingRandomized(t testing.TB) *Options {\n\tif o == nil {\n\t\to = &Options{}\n\t}\n\tif o.Logger == nil {\n\t\to.Logger = testLogger{t: t}\n\t}\n\tif o.FormatMajorVersion == FormatDefault {\n\t\t// Pick a random format major version from the range\n\t\t// [FormatMinSupported, FormatNewest].\n\t\tn := rand.IntN(int(internalFormatNewest - FormatMinSupported + 1))\n\t\to.FormatMajorVersion = FormatMinSupported + FormatMajorVersion(n)\n\t\tt.Logf(\"Running %s with format major version %s\", t.Name(), o.FormatMajorVersion.String())\n\t}\n\t// Enable columnar blocks if using a format major version that supports it.\n\tif o.FormatMajorVersion >= FormatColumnarBlocks && o.Experimental.EnableColumnarBlocks == nil && rand.Int64N(4) > 0 {\n\t\to.Experimental.EnableColumnarBlocks = func() bool { return true }\n\t}\n\to.EnsureDefaults()\n\treturn o\n}\n\nfunc testingRandomized(t testing.TB, o *Options) *Options {\n\to.testingRandomized(t)\n\treturn o\n}\n\nfunc TestLevelOptions(t *testing.T) {\n\tvar opts *Options\n\topts = opts.EnsureDefaults()\n\n\ttestCases := []struct {\n\t\tlevel          int\n\t\ttargetFileSize int64\n\t}{\n\t\t{0, 2 << 20},\n\t\t{1, (2 * 2) << 20},\n\t\t{2, (4 * 2) << 20},\n\t\t{3, (8 * 2) << 20},\n\t\t{4, (16 * 2) << 20},\n\t\t{5, (32 * 2) << 20},\n\t\t{6, (64 * 2) << 20},\n\t}\n\tfor _, c := range testCases {\n\t\tl := opts.Level(c.level)\n\t\tif c.targetFileSize != l.TargetFileSize {\n\t\t\tt.Fatalf(\"%d: expected target-file-size %d, but found %d\",\n\t\t\t\tc.level, c.targetFileSize, l.TargetFileSize)\n\t\t}\n\t}\n}\n\nfunc TestOptionsString(t *testing.T) {\n\tn := runtime.GOMAXPROCS(8)\n\tdefer runtime.GOMAXPROCS(n)\n\n\tconst expected = `[Version]\n  pebble_version=0.1\n\n[Options]\n  bytes_per_sync=524288\n  cache_size=8388608\n  cleaner=delete\n  compaction_debt_concurrency=1073741824\n  comparer=leveldb.BytewiseComparator\n  disable_wal=false\n  flush_delay_delete_range=0s\n  flush_delay_range_key=0s\n  flush_split_bytes=4194304\n  format_major_version=13\n  key_schema=DefaultKeySchema(leveldb.BytewiseComparator,16)\n  l0_compaction_concurrency=10\n  l0_compaction_file_threshold=500\n  l0_compaction_threshold=4\n  l0_stop_writes_threshold=12\n  lbase_max_bytes=67108864\n  max_concurrent_compactions=1\n  max_concurrent_downloads=1\n  max_manifest_file_size=134217728\n  max_open_files=1000\n  mem_table_size=4194304\n  mem_table_stop_writes_threshold=2\n  min_deletion_rate=0\n  merger=pebble.concatenate\n  multilevel_compaction_heuristic=wamp(0.00, false)\n  read_compaction_rate=16000\n  read_sampling_multiplier=16\n  num_deletions_threshold=100\n  deletion_size_ratio_threshold=0.500000\n  tombstone_dense_compaction_threshold=0.100000\n  strict_wal_tail=true\n  table_cache_shards=8\n  validate_on_ingest=false\n  wal_dir=\n  wal_bytes_per_sync=0\n  max_writer_concurrency=0\n  force_writer_parallelism=false\n  secondary_cache_size_bytes=0\n  create_on_shared=0\n\n[Level \"0\"]\n  block_restart_interval=16\n  block_size=4096\n  block_size_threshold=90\n  compression=Snappy\n  filter_policy=none\n  filter_type=table\n  index_block_size=4096\n  target_file_size=2097152\n`\n\n\tvar opts *Options\n\topts = opts.EnsureDefaults()\n\trequire.Equal(t, expected, opts.String())\n}\n\nfunc TestOptionsCheckCompatibility(t *testing.T) {\n\tvar opts *Options\n\topts = opts.EnsureDefaults()\n\ts := opts.String()\n\trequire.NoError(t, opts.CheckCompatibility(s))\n\trequire.Regexp(t, `invalid key=value syntax`, opts.CheckCompatibility(\"foo\\n\"))\n\n\ttmp := *opts\n\ttmp.Comparer = &Comparer{Name: \"foo\"}\n\trequire.Regexp(t, `comparer name from file.*!=.*`, tmp.CheckCompatibility(s))\n\n\ttmp = *opts\n\ttmp.Merger = &Merger{Name: \"foo\"}\n\trequire.Regexp(t, `merger name from file.*!=.*`, tmp.CheckCompatibility(s))\n\n\t// RocksDB uses a similar (INI-style) syntax for the OPTIONS file, but\n\t// different section names and keys.\n\ts = `\n[CFOptions \"default\"]\n  comparator=rocksdb-comparer\n  merge_operator=rocksdb-merger\n`\n\ttmp = *opts\n\ttmp.Comparer = &Comparer{Name: \"foo\"}\n\trequire.Regexp(t, `comparer name from file.*!=.*`, tmp.CheckCompatibility(s))\n\n\ttmp.Comparer = &Comparer{Name: \"rocksdb-comparer\"}\n\ttmp.Merger = &Merger{Name: \"foo\"}\n\trequire.Regexp(t, `merger name from file.*!=.*`, tmp.CheckCompatibility(s))\n\n\ttmp.Merger = &Merger{Name: \"rocksdb-merger\"}\n\trequire.NoError(t, tmp.CheckCompatibility(s))\n\n\t// RocksDB allows the merge operator to be unspecified, in which case it\n\t// shows up as \"nullptr\".\n\ts = `\n[CFOptions \"default\"]\n  merge_operator=nullptr\n`\n\ttmp = *opts\n\trequire.NoError(t, tmp.CheckCompatibility(s))\n\n\t// Check that an OPTIONS file that configured an explicit WALDir that will\n\t// no longer be used errors if it's not also present in WALRecoveryDirs.\n\trequire.Equal(t, ErrMissingWALRecoveryDir{Dir: \"external-wal-dir\"},\n\t\t(&Options{}).EnsureDefaults().CheckCompatibility(`\n[Options]\n  wal_dir=external-wal-dir\n`))\n\t// But not if it's configured as a WALRecoveryDir or current WALDir.\n\trequire.NoError(t,\n\t\t(&Options{WALRecoveryDirs: []wal.Dir{{Dirname: \"external-wal-dir\"}}}).EnsureDefaults().CheckCompatibility(`\n[Options]\n  wal_dir=external-wal-dir\n`))\n\trequire.NoError(t,\n\t\t(&Options{WALDir: \"external-wal-dir\"}).EnsureDefaults().CheckCompatibility(`\n[Options]\n  wal_dir=external-wal-dir\n`))\n\n\t// Check that an OPTIONS file that configured a secondary failover WAL dir\n\t// that will no longer be used errors if it's not also present in\n\t// WALRecoveryDirs.\n\trequire.Equal(t, ErrMissingWALRecoveryDir{Dir: \"failover-wal-dir\"},\n\t\t(&Options{}).EnsureDefaults().CheckCompatibility(`\n[Options]\n\n[WAL Failover]\n  secondary_dir=failover-wal-dir\n`))\n\t// But not if it's configured as a WALRecoveryDir or current failover\n\t// secondary dir.\n\trequire.NoError(t, (&Options{WALRecoveryDirs: []wal.Dir{{Dirname: \"failover-wal-dir\"}}}).EnsureDefaults().CheckCompatibility(`\n[Options]\n\n[WAL Failover]\n  secondary_dir=failover-wal-dir\n`))\n\trequire.NoError(t, (&Options{WALFailover: &WALFailoverOptions{Secondary: wal.Dir{Dirname: \"failover-wal-dir\"}}}).EnsureDefaults().CheckCompatibility(`\n[Options]\n\n[WAL Failover]\n  secondary_dir=failover-wal-dir\n`))\n}\n\ntype testCleaner struct{}\n\nfunc (testCleaner) Clean(fs vfs.FS, fileType base.FileType, path string) error {\n\treturn nil\n}\n\nfunc (testCleaner) String() string {\n\treturn \"test-cleaner\"\n}\n\nfunc TestOptionsParse(t *testing.T) {\n\ttestComparer := *DefaultComparer\n\ttestComparer.Name = \"test-comparer\"\n\ttestMerger := *DefaultMerger\n\ttestMerger.Name = \"test-merger\"\n\tvar newCacheSize int64\n\n\thooks := &ParseHooks{\n\t\tNewCache: func(size int64) *Cache {\n\t\t\tnewCacheSize = size\n\t\t\treturn nil\n\t\t},\n\t\tNewCleaner: func(name string) (Cleaner, error) {\n\t\t\tif name == (testCleaner{}).String() {\n\t\t\t\treturn testCleaner{}, nil\n\t\t\t}\n\t\t\treturn nil, errors.Errorf(\"unknown cleaner: %q\", name)\n\t\t},\n\t\tNewComparer: func(name string) (*Comparer, error) {\n\t\t\tif name == testComparer.Name {\n\t\t\t\treturn &testComparer, nil\n\t\t\t}\n\t\t\treturn nil, errors.Errorf(\"unknown comparer: %q\", name)\n\t\t},\n\t\tNewMerger: func(name string) (*Merger, error) {\n\t\t\tif name == testMerger.Name {\n\t\t\t\treturn &testMerger, nil\n\t\t\t}\n\t\t\treturn nil, errors.Errorf(\"unknown merger: %q\", name)\n\t\t},\n\t}\n\n\ttestCases := []struct {\n\t\tcleaner  Cleaner\n\t\tcomparer *Comparer\n\t\tmerger   *Merger\n\t}{\n\t\t{testCleaner{}, nil, nil},\n\t\t{nil, &testComparer, nil},\n\t\t{nil, nil, &testMerger},\n\t}\n\tfor _, c := range testCases {\n\t\tt.Run(\"\", func(t *testing.T) {\n\t\t\tvar opts Options\n\t\t\topts.Comparer = c.comparer\n\t\t\topts.Merger = c.merger\n\t\t\topts.WALDir = \"wal\"\n\t\t\topts.Levels = make([]LevelOptions, 3)\n\t\t\topts.Levels[0].BlockSize = 1024\n\t\t\topts.Levels[1].BlockSize = 2048\n\t\t\topts.Levels[2].BlockSize = 4096\n\t\t\topts.Experimental.CompactionDebtConcurrency = 100\n\t\t\topts.FlushDelayDeleteRange = 10 * time.Second\n\t\t\topts.FlushDelayRangeKey = 11 * time.Second\n\t\t\topts.Experimental.LevelMultiplier = 5\n\t\t\topts.TargetByteDeletionRate = 200\n\t\t\topts.WALFailover = &WALFailoverOptions{\n\t\t\t\tSecondary: wal.Dir{Dirname: \"wal_secondary\", FS: vfs.Default},\n\t\t\t}\n\t\t\topts.Experimental.ReadCompactionRate = 300\n\t\t\topts.Experimental.ReadSamplingMultiplier = 400\n\t\t\topts.Experimental.NumDeletionsThreshold = 500\n\t\t\topts.Experimental.DeletionSizeRatioThreshold = 0.7\n\t\t\topts.Experimental.TombstoneDenseCompactionThreshold = 0.2\n\t\t\topts.Experimental.FileCacheShards = 500\n\t\t\topts.Experimental.MaxWriterConcurrency = 1\n\t\t\topts.Experimental.ForceWriterParallelism = true\n\t\t\topts.Experimental.SecondaryCacheSizeBytes = 1024\n\t\t\topts.EnsureDefaults()\n\t\t\tstr := opts.String()\n\n\t\t\tnewCacheSize = 0\n\t\t\tvar parsedOptions Options\n\t\t\trequire.NoError(t, parsedOptions.Parse(str, hooks))\n\t\t\tparsedStr := parsedOptions.String()\n\t\t\tif str != parsedStr {\n\t\t\t\tt.Fatalf(\"expected\\n%s\\nbut found\\n%s\", str, parsedStr)\n\t\t\t}\n\t\t\trequire.Nil(t, parsedOptions.Cache)\n\t\t\trequire.NotEqual(t, newCacheSize, 0)\n\t\t})\n\t}\n}\n\nfunc TestOptionsParseComparerOverwrite(t *testing.T) {\n\t// Test that an unrecognized comparer in the OPTIONS file does not nil out\n\t// the Comparer field.\n\to := &Options{Comparer: testkeys.Comparer}\n\terr := o.Parse(`[Options]\ncomparer=unrecognized`, nil)\n\trequire.NoError(t, err)\n\trequire.Equal(t, testkeys.Comparer, o.Comparer)\n}\n\nfunc TestOptionsValidate(t *testing.T) {\n\ttestCases := []struct {\n\t\toptions  string\n\t\texpected string\n\t}{\n\t\t{``, ``},\n\t\t{`\n[Options]\n  l0_compaction_concurrency=0\n`,\n\t\t\t`L0CompactionConcurrency \\(0\\) must be >= 1`,\n\t\t},\n\t\t{`\n[Options]\n  l0_compaction_threshold=2\n  l0_stop_writes_threshold=1\n`,\n\t\t\t`L0StopWritesThreshold .* must be >= L0CompactionThreshold .*`,\n\t\t},\n\t\t{`\n[Options]\n  mem_table_size=4294967296\n`,\n\t\t\t`MemTableSize \\(4\\.0GB\\) must be < [2|4]\\.0GB`,\n\t\t},\n\t\t{`\n[Options]\n  mem_table_stop_writes_threshold=1\n`,\n\t\t\t`MemTableStopWritesThreshold .* must be >= 2`,\n\t\t},\n\t}\n\n\tfor _, c := range testCases {\n\t\tt.Run(\"\", func(t *testing.T) {\n\t\t\tvar opts Options\n\t\t\topts.EnsureDefaults()\n\t\t\trequire.NoError(t, opts.Parse(c.options, nil))\n\t\t\terr := opts.Validate()\n\t\t\tif c.expected == \"\" {\n\t\t\t\trequire.NoError(t, err)\n\t\t\t} else {\n\t\t\t\trequire.Error(t, err)\n\t\t\t\trequire.Regexp(t, c.expected, err.Error())\n\t\t\t}\n\t\t})\n\t}\n}\n\n// This test isn't being done in TestOptionsValidate\n// cause it doesn't support setting pointers.\nfunc TestOptionsValidateCache(t *testing.T) {\n\tvar opts Options\n\topts.EnsureDefaults()\n\topts.Cache = NewCache(8 << 20)\n\tdefer opts.Cache.Unref()\n\topts.FileCache = NewFileCache(NewCache(8<<20), 10, 1)\n\tdefer opts.FileCache.cache.Unref()\n\tdefer opts.FileCache.Unref()\n\n\terr := opts.Validate()\n\trequire.Error(t, err)\n\tif fmt.Sprint(err) != \"underlying cache in the FileCache and the Cache dont match\" {\n\t\tt.Errorf(\"Unexpected error message\")\n\t}\n}\n\nfunc TestKeyCategories(t *testing.T) {\n\tkc := MakeUserKeyCategories(base.DefaultComparer.Compare, []UserKeyCategory{\n\t\t{Name: \"b\", UpperBound: []byte(\"b\")},\n\t\t{Name: \"dd\", UpperBound: []byte(\"dd\")},\n\t\t{Name: \"e\", UpperBound: []byte(\"e\")},\n\t\t{Name: \"h\", UpperBound: []byte(\"h\")},\n\t\t{Name: \"o\", UpperBound: nil},\n\t}...)\n\n\tfor _, tc := range []struct {\n\t\tkey      string\n\t\texpected string\n\t}{\n\t\t{key: \"a\", expected: \"b\"},\n\t\t{key: \"a123\", expected: \"b\"},\n\t\t{key: \"az\", expected: \"b\"},\n\t\t{key: \"b\", expected: \"dd\"},\n\t\t{key: \"b123\", expected: \"dd\"},\n\t\t{key: \"c\", expected: \"dd\"},\n\t\t{key: \"d\", expected: \"dd\"},\n\t\t{key: \"dd\", expected: \"e\"},\n\t\t{key: \"dd0\", expected: \"e\"},\n\t\t{key: \"de\", expected: \"e\"},\n\t\t{key: \"e\", expected: \"h\"},\n\t\t{key: \"e1\", expected: \"h\"},\n\t\t{key: \"f\", expected: \"h\"},\n\t\t{key: \"h\", expected: \"o\"},\n\t\t{key: \"h1\", expected: \"o\"},\n\t\t{key: \"z\", expected: \"o\"},\n\t} {\n\t\tt.Run(tc.key, func(t *testing.T) {\n\t\t\tres := kc.CategorizeKey([]byte(tc.key))\n\t\t\trequire.Equal(t, tc.expected, res)\n\t\t})\n\t}\n\n\tfor _, tc := range []struct {\n\t\trng      string\n\t\texpected string\n\t}{\n\t\t{rng: \"a-a\", expected: \"b\"},\n\t\t{rng: \"a-a1\", expected: \"b\"},\n\t\t{rng: \"a-b\", expected: \"b-dd\"},\n\t\t{rng: \"b1-b5\", expected: \"dd\"},\n\t\t{rng: \"b1-dd0\", expected: \"dd-e\"},\n\t\t{rng: \"b1-g\", expected: \"dd-h\"},\n\t\t{rng: \"b1-j\", expected: \"dd-o\"},\n\t\t{rng: \"b1-z\", expected: \"dd-o\"},\n\t\t{rng: \"dd-e\", expected: \"e-h\"},\n\t\t{rng: \"h-i\", expected: \"o\"},\n\t\t{rng: \"i-i\", expected: \"o\"},\n\t\t{rng: \"i-j\", expected: \"o\"},\n\t\t{rng: \"a-z\", expected: \"b-o\"},\n\t} {\n\t\tt.Run(tc.rng, func(t *testing.T) {\n\t\t\tkeys := strings.SplitN(tc.rng, \"-\", 2)\n\t\t\tres := kc.CategorizeKeyRange([]byte(keys[0]), []byte(keys[1]))\n\t\t\trequire.Equal(t, tc.expected, res)\n\t\t})\n\t}\n}\n"
        },
        {
          "name": "overlap.go",
          "type": "blob",
          "size": 2.037109375,
          "content": "// Copyright 2024 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"context\"\n\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n\t\"github.com/cockroachdb/pebble/internal/overlap\"\n)\n\n// An overlapChecker provides facilities for checking whether any keys within a\n// particular LSM version overlap a set of bounds. It is a thin wrapper for\n// dataoverlap.Checker.\ntype overlapChecker struct {\n\tcomparer *base.Comparer\n\tnewIters tableNewIters\n\topts     IterOptions\n\tv        *version\n}\n\n// DetermineLSMOverlap calculates the overlap.WithLSM for the given bounds.\nfunc (c *overlapChecker) DetermineLSMOverlap(\n\tctx context.Context, bounds base.UserKeyBounds,\n) (overlap.WithLSM, error) {\n\tchecker := overlap.MakeChecker(c.comparer.Compare, c)\n\treturn checker.LSMOverlap(ctx, bounds, c.v)\n}\n\nvar _ overlap.IteratorFactory = (*overlapChecker)(nil)\n\n// Points is part of the overlap.IteratorFactory implementation.\nfunc (c *overlapChecker) Points(\n\tctx context.Context, m *manifest.FileMetadata,\n) (base.InternalIterator, error) {\n\titers, err := c.newIters(ctx, m, &c.opts, internalIterOpts{}, iterPointKeys)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn iters.point, nil\n}\n\n// RangeDels is part of the overlap.IteratorFactory implementation.\nfunc (c *overlapChecker) RangeDels(\n\tctx context.Context, m *manifest.FileMetadata,\n) (keyspan.FragmentIterator, error) {\n\titers, err := c.newIters(ctx, m, &c.opts, internalIterOpts{}, iterRangeDeletions)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn iters.rangeDeletion, nil\n}\n\n// RangeKeys is part of the overlap.IteratorFactory implementation.\nfunc (c *overlapChecker) RangeKeys(\n\tctx context.Context, m *manifest.FileMetadata,\n) (keyspan.FragmentIterator, error) {\n\titers, err := c.newIters(ctx, m, &c.opts, internalIterOpts{}, iterRangeKeys)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn iters.rangeKey, nil\n}\n"
        },
        {
          "name": "pacer.go",
          "type": "blob",
          "size": 6.06640625,
          "content": "// Copyright 2019 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/crlib/crtime\"\n)\n\n// deletionPacerInfo contains any info from the db necessary to make deletion\n// pacing decisions (to limit background IO usage so that it does not contend\n// with foreground traffic).\ntype deletionPacerInfo struct {\n\tfreeBytes     uint64\n\tobsoleteBytes uint64\n\tliveBytes     uint64\n}\n\n// deletionPacer rate limits deletions of obsolete files. This is necessary to\n// prevent overloading the disk with too many deletions too quickly after a\n// large compaction, or an iterator close. On some SSDs, disk performance can be\n// negatively impacted if too many blocks are deleted very quickly, so this\n// mechanism helps mitigate that.\ntype deletionPacer struct {\n\t// If there are less than freeSpaceThreshold bytes of free space on\n\t// disk, increase the pace of deletions such that we delete enough bytes to\n\t// get back to the threshold within the freeSpaceTimeframe.\n\tfreeSpaceThreshold uint64\n\tfreeSpaceTimeframe time.Duration\n\n\t// If the ratio of obsolete bytes to live bytes is greater than\n\t// obsoleteBytesMaxRatio, increase the pace of deletions such that we delete\n\t// enough bytes to get back to the threshold within the obsoleteBytesTimeframe.\n\tobsoleteBytesMaxRatio  float64\n\tobsoleteBytesTimeframe time.Duration\n\n\tmu struct {\n\t\tsync.Mutex\n\n\t\t// history keeps rack of recent deletion history; it used to increase the\n\t\t// deletion rate to match the pace of deletions.\n\t\thistory history\n\t}\n\n\ttargetByteDeletionRate int64\n\n\tgetInfo func() deletionPacerInfo\n}\n\nconst deletePacerHistory = 5 * time.Minute\n\n// newDeletionPacer instantiates a new deletionPacer for use when deleting\n// obsolete files.\n//\n// targetByteDeletionRate is the rate (in bytes/sec) at which we want to\n// normally limit deletes (when we are not falling behind or running out of\n// space). A value of 0.0 disables pacing.\nfunc newDeletionPacer(\n\tnow crtime.Mono, targetByteDeletionRate int64, getInfo func() deletionPacerInfo,\n) *deletionPacer {\n\td := &deletionPacer{\n\t\tfreeSpaceThreshold: 16 << 30, // 16 GB\n\t\tfreeSpaceTimeframe: 10 * time.Second,\n\n\t\tobsoleteBytesMaxRatio:  0.20,\n\t\tobsoleteBytesTimeframe: 5 * time.Minute,\n\n\t\ttargetByteDeletionRate: targetByteDeletionRate,\n\t\tgetInfo:                getInfo,\n\t}\n\td.mu.history.Init(now, deletePacerHistory)\n\treturn d\n}\n\n// ReportDeletion is used to report a deletion to the pacer. The pacer uses it\n// to keep track of the recent rate of deletions and potentially increase the\n// deletion rate accordingly.\n//\n// ReportDeletion is thread-safe.\nfunc (p *deletionPacer) ReportDeletion(now crtime.Mono, bytesToDelete uint64) {\n\tp.mu.Lock()\n\tdefer p.mu.Unlock()\n\tp.mu.history.Add(now, int64(bytesToDelete))\n}\n\n// PacingDelay returns the recommended pacing wait time (in seconds) for\n// deleting the given number of bytes.\n//\n// PacingDelay is thread-safe.\nfunc (p *deletionPacer) PacingDelay(now crtime.Mono, bytesToDelete uint64) (waitSeconds float64) {\n\tif p.targetByteDeletionRate == 0 {\n\t\t// Pacing disabled.\n\t\treturn 0.0\n\t}\n\n\tbaseRate := float64(p.targetByteDeletionRate)\n\t// If recent deletion rate is more than our target, use that so that we don't\n\t// fall behind.\n\thistoricRate := func() float64 {\n\t\tp.mu.Lock()\n\t\tdefer p.mu.Unlock()\n\t\treturn float64(p.mu.history.Sum(now)) / deletePacerHistory.Seconds()\n\t}()\n\tif historicRate > baseRate {\n\t\tbaseRate = historicRate\n\t}\n\n\t// Apply heuristics to increase the deletion rate.\n\tvar extraRate float64\n\tinfo := p.getInfo()\n\tif info.freeBytes <= p.freeSpaceThreshold {\n\t\t// Increase the rate so that we can free up enough bytes within the timeframe.\n\t\textraRate = float64(p.freeSpaceThreshold-info.freeBytes) / p.freeSpaceTimeframe.Seconds()\n\t}\n\tif info.liveBytes == 0 {\n\t\t// We don't know the obsolete bytes ratio. Disable pacing altogether.\n\t\treturn 0.0\n\t}\n\tobsoleteBytesRatio := float64(info.obsoleteBytes) / float64(info.liveBytes)\n\tif obsoleteBytesRatio >= p.obsoleteBytesMaxRatio {\n\t\t// Increase the rate so that we can free up enough bytes within the timeframe.\n\t\tr := (obsoleteBytesRatio - p.obsoleteBytesMaxRatio) * float64(info.liveBytes) / p.obsoleteBytesTimeframe.Seconds()\n\t\tif extraRate < r {\n\t\t\textraRate = r\n\t\t}\n\t}\n\n\treturn float64(bytesToDelete) / (baseRate + extraRate)\n}\n\n// history is a helper used to keep track of the recent history of a set of\n// data points (in our case deleted bytes), at limited granularity.\n// Specifically, we split the desired timeframe into 100 \"epochs\" and all times\n// are effectively rounded down to the nearest epoch boundary.\ntype history struct {\n\tepochDuration time.Duration\n\tstartTime     crtime.Mono\n\t// currEpoch is the epoch of the most recent operation.\n\tcurrEpoch int64\n\t// val contains the recent epoch values.\n\t// val[currEpoch % historyEpochs] is the current epoch.\n\t// val[(currEpoch + 1) % historyEpochs] is the oldest epoch.\n\tval [historyEpochs]int64\n\t// sum is always equal to the sum of values in val.\n\tsum int64\n}\n\nconst historyEpochs = 100\n\n// Init the history helper to keep track of data over the given number of\n// seconds.\nfunc (h *history) Init(now crtime.Mono, timeframe time.Duration) {\n\t*h = history{\n\t\tepochDuration: timeframe / time.Duration(historyEpochs),\n\t\tstartTime:     now,\n\t\tcurrEpoch:     0,\n\t\tsum:           0,\n\t}\n}\n\n// Add adds a value for the current time.\nfunc (h *history) Add(now crtime.Mono, val int64) {\n\th.advance(now)\n\th.val[h.currEpoch%historyEpochs] += val\n\th.sum += val\n}\n\n// Sum returns the sum of recent values. The result is approximate in that the\n// cut-off time is within 1% of the exact one.\nfunc (h *history) Sum(now crtime.Mono) int64 {\n\th.advance(now)\n\treturn h.sum\n}\n\nfunc (h *history) epoch(t crtime.Mono) int64 {\n\treturn int64(t.Sub(h.startTime) / h.epochDuration)\n}\n\n// advance advances the time to the given time.\nfunc (h *history) advance(now crtime.Mono) {\n\tepoch := h.epoch(now)\n\tfor h.currEpoch < epoch {\n\t\th.currEpoch++\n\t\t// Forget the data for the oldest epoch.\n\t\th.sum -= h.val[h.currEpoch%historyEpochs]\n\t\th.val[h.currEpoch%historyEpochs] = 0\n\t}\n}\n"
        },
        {
          "name": "pacer_test.go",
          "type": "blob",
          "size": 5.435546875,
          "content": "// Copyright 2019 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"cmp\"\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand/v2\"\n\t\"slices\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/crlib/crtime\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestDeletionPacer(t *testing.T) {\n\tconst MB = 1 << 20\n\tconst GB = 1 << 30\n\ttestCases := []struct {\n\t\tfreeBytes     uint64\n\t\tobsoleteBytes uint64\n\t\tliveBytes     uint64\n\t\t// history of deletion reporting; first value in the pair is the time,\n\t\t// second value is the deleted bytes. The time of pacing is the same as the\n\t\t// last time in the history.\n\t\thistory [][2]int64\n\t\t// expected pacing rate in MB/s.\n\t\texpected float64\n\t}{\n\t\t{\n\t\t\tfreeBytes:     160 * GB,\n\t\t\tobsoleteBytes: 1 * MB,\n\t\t\tliveBytes:     160 * MB,\n\t\t\texpected:      100.0,\n\t\t},\n\t\t// As freeBytes is 2GB below the free space threshold, rate should be\n\t\t// increased by 204.8MB/s.\n\t\t{\n\t\t\tfreeBytes:     14 * GB,\n\t\t\tobsoleteBytes: 1 * MB,\n\t\t\tliveBytes:     100 * MB,\n\t\t\texpected:      304.8,\n\t\t},\n\t\t// As freeBytes is 10GB below the free space threshold, rate should be\n\t\t// increased to by 1GB/s.\n\t\t{\n\t\t\tfreeBytes:     6 * GB,\n\t\t\tobsoleteBytes: 1 * MB,\n\t\t\tliveBytes:     100 * MB,\n\t\t\texpected:      1124.0,\n\t\t},\n\t\t// obsoleteBytesRatio is 50%. We need to delete 30GB within 5 minutes.\n\t\t{\n\t\t\tfreeBytes:     500 * GB,\n\t\t\tobsoleteBytes: 50 * GB,\n\t\t\tliveBytes:     100 * GB,\n\t\t\texpected:      202.4,\n\t\t},\n\t\t// When obsolete ratio unknown, there should be no throttling.\n\t\t{\n\t\t\tfreeBytes:     500 * GB,\n\t\t\tobsoleteBytes: 0,\n\t\t\tliveBytes:     0,\n\t\t\texpected:      math.Inf(1),\n\t\t},\n\t\t// History shows 200MB/sec deletions on average over last 5 minutes.\n\t\t{\n\t\t\tfreeBytes:     160 * GB,\n\t\t\tobsoleteBytes: 1 * MB,\n\t\t\tliveBytes:     160 * MB,\n\t\t\thistory:       [][2]int64{{0, 5 * 60 * 200 * MB}},\n\t\t\texpected:      200.0,\n\t\t},\n\t\t// History shows 200MB/sec deletions on average over last 5 minutes and\n\t\t// freeBytes is 10GB below the threshold.\n\t\t{\n\t\t\tfreeBytes:     6 * GB,\n\t\t\tobsoleteBytes: 1 * MB,\n\t\t\tliveBytes:     160 * MB,\n\t\t\thistory:       [][2]int64{{0, 5 * 60 * 200 * MB}},\n\t\t\texpected:      1224.0,\n\t\t},\n\t\t// History shows 200MB/sec deletions on average over last 5 minutes and\n\t\t// obsoleteBytesRatio is 50%.\n\t\t{\n\t\t\tfreeBytes:     500 * GB,\n\t\t\tobsoleteBytes: 50 * GB,\n\t\t\tliveBytes:     100 * GB,\n\t\t\thistory:       [][2]int64{{0, 5 * 60 * 200 * MB}},\n\t\t\texpected:      302.4,\n\t\t},\n\t\t// History shows 1000MB/sec deletions on average over last 5 minutes.\n\t\t{\n\t\t\tfreeBytes:     160 * GB,\n\t\t\tobsoleteBytes: 1 * MB,\n\t\t\tliveBytes:     160 * MB,\n\t\t\thistory:       [][2]int64{{0, 60 * 1000 * MB}, {3 * 60, 60 * 4 * 1000 * MB}, {4 * 60, 0}},\n\t\t\texpected:      1000.0,\n\t\t},\n\t\t// First entry in history is too old, it should be discarded.\n\t\t{\n\t\t\tfreeBytes:     160 * GB,\n\t\t\tobsoleteBytes: 1 * MB,\n\t\t\tliveBytes:     160 * MB,\n\t\t\thistory:       [][2]int64{{0, 10 * 60 * 10000 * MB}, {3 * 60, 4 * 60 * 200 * MB}, {7 * 60, 1 * 60 * 200 * MB}},\n\t\t\texpected:      200.0,\n\t\t},\n\t}\n\tfor tcIdx, tc := range testCases {\n\t\tt.Run(fmt.Sprintf(\"%d\", tcIdx), func(t *testing.T) {\n\t\t\tgetInfo := func() deletionPacerInfo {\n\t\t\t\treturn deletionPacerInfo{\n\t\t\t\t\tfreeBytes:     tc.freeBytes,\n\t\t\t\t\tliveBytes:     tc.liveBytes,\n\t\t\t\t\tobsoleteBytes: tc.obsoleteBytes,\n\t\t\t\t}\n\t\t\t}\n\t\t\tstart := crtime.NowMono()\n\t\t\tlast := start\n\t\t\tpacer := newDeletionPacer(start, 100*MB, getInfo)\n\t\t\tfor _, h := range tc.history {\n\t\t\t\tlast = start + crtime.Mono(time.Second*time.Duration(h[0]))\n\t\t\t\tpacer.ReportDeletion(last, uint64(h[1]))\n\t\t\t}\n\t\t\tresult := 1.0 / pacer.PacingDelay(last, 1*MB)\n\t\t\trequire.InDelta(t, tc.expected, result, 1e-7)\n\t\t})\n\t}\n}\n\n// TestDeletionPacerHistory tests the history helper by crosschecking Sum()\n// against a naive implementation.\nfunc TestDeletionPacerHistory(t *testing.T) {\n\ttype event struct {\n\t\ttime crtime.Mono\n\t\t// If report is 0, this event is a Sum(). Otherwise it is an Add().\n\t\treport int64\n\t}\n\tnumEvents := 1 + rand.IntN(200)\n\ttimeframe := time.Duration(1+rand.IntN(60*100)) * time.Second\n\tevents := make([]event, numEvents)\n\tstartTime := crtime.NowMono()\n\tfor i := range events {\n\t\tevents[i].time = startTime + crtime.Mono(rand.Int64N(int64(timeframe)))\n\t\tif rand.IntN(3) == 0 {\n\t\t\tevents[i].report = 0\n\t\t} else {\n\t\t\tevents[i].report = int64(rand.IntN(100000))\n\t\t}\n\t}\n\tslices.SortFunc(events, func(a, b event) int { return cmp.Compare(a.time, b.time) })\n\n\tvar h history\n\th.Init(startTime, timeframe)\n\n\t// partialSums[i] := SUM_j<i events[j].report\n\tpartialSums := make([]int64, len(events)+1)\n\tfor i := range events {\n\t\tpartialSums[i+1] = partialSums[i] + events[i].report\n\t}\n\n\tfor i, e := range events {\n\t\tif e.report != 0 {\n\t\t\th.Add(e.time, e.report)\n\t\t\tcontinue\n\t\t}\n\n\t\tresult := h.Sum(e.time)\n\n\t\t// getIdx returns the largest event index <= i that is before the cutoff\n\t\t// time.\n\t\tgetIdx := func(cutoff crtime.Mono) int {\n\t\t\tfor j := i; j >= 0; j-- {\n\t\t\t\tif events[j].time < cutoff {\n\t\t\t\t\treturn j\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn -1\n\t\t}\n\n\t\t// Sum all report values in the last timeframe, and see if recent events\n\t\t// (allowing 1% error in the cutoff time) match the result.\n\t\ta := getIdx(e.time - crtime.Mono(timeframe*(historyEpochs+1)/historyEpochs))\n\t\tb := getIdx(e.time - crtime.Mono(timeframe*(historyEpochs-1)/historyEpochs))\n\t\tfound := false\n\t\tfor j := a; j <= b; j++ {\n\t\t\tif partialSums[i+1]-partialSums[j+1] == result {\n\t\t\t\tfound = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif !found {\n\t\t\tt.Fatalf(\"incorrect Sum() result %d; %v\", result, events[a+1:i+1])\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "range_del_test.go",
          "type": "blob",
          "size": 17.47265625,
          "content": "// Copyright 2018 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"math/rand/v2\"\n\t\"runtime\"\n\t\"strings\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/datadriven\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/testkeys\"\n\t\"github.com/cockroachdb/pebble/objstorage/objstorageprovider\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestRangeDel(t *testing.T) {\n\tvar d *DB\n\tdefer func() {\n\t\tif d != nil {\n\t\t\trequire.NoError(t, closeAllSnapshots(d))\n\t\t\trequire.NoError(t, d.Close())\n\t\t}\n\t}()\n\topts := &Options{}\n\topts.DisableAutomaticCompactions = true\n\n\tdatadriven.RunTest(t, \"testdata/range_del\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"define\":\n\t\t\tif d != nil {\n\t\t\t\tif err := closeAllSnapshots(d); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t\tif err := d.Close(); err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tvar err error\n\t\t\tif d, err = runDBDefineCmd(td, opts); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\n\t\t\td.mu.Lock()\n\t\t\t// Disable the \"dynamic base level\" code for this test.\n\t\t\td.mu.versions.picker.forceBaseLevel1()\n\t\t\ts := fmt.Sprintf(\"mem: %d\\n%s\", len(d.mu.mem.queue), d.mu.versions.currentVersion().String())\n\t\t\td.mu.Unlock()\n\t\t\treturn s\n\n\t\tcase \"wait-pending-table-stats\":\n\t\t\treturn runTableStatsCmd(td, d)\n\n\t\tcase \"compact\":\n\t\t\tif err := runCompactCmd(td, d); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\td.mu.Lock()\n\t\t\t// Disable the \"dynamic base level\" code for this test.\n\t\t\td.mu.versions.picker.forceBaseLevel1()\n\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\td.mu.Unlock()\n\t\t\treturn s\n\n\t\tcase \"get\":\n\t\t\treturn runGetCmd(t, td, d)\n\n\t\tcase \"iter\":\n\t\t\tsnap := Snapshot{\n\t\t\t\tdb:     d,\n\t\t\t\tseqNum: base.SeqNumMax,\n\t\t\t}\n\t\t\tif td.HasArg(\"seq\") {\n\t\t\t\tvar n uint64\n\t\t\t\ttd.ScanArgs(t, \"seq\", &n)\n\t\t\t\tsnap.seqNum = base.SeqNum(n)\n\t\t\t}\n\t\t\titer, _ := snap.NewIter(nil)\n\t\t\treturn runIterCmd(td, iter, true)\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc TestFlushDelay(t *testing.T) {\n\topts := &Options{\n\t\tFS:                    vfs.NewMem(),\n\t\tComparer:              testkeys.Comparer,\n\t\tFlushDelayDeleteRange: 10 * time.Millisecond,\n\t\tFlushDelayRangeKey:    10 * time.Millisecond,\n\t\tFormatMajorVersion:    internalFormatNewest,\n\t\tLogger:                testLogger{t: t},\n\t}\n\td, err := Open(\"\", opts)\n\trequire.NoError(t, err)\n\n\t// Ensure that all the various means of writing a rangedel or range key\n\t// trigger their respective flush delays.\n\tcases := []func(){\n\t\tfunc() {\n\t\t\trequire.NoError(t, d.DeleteRange([]byte(\"a\"), []byte(\"z\"), nil))\n\t\t},\n\t\tfunc() {\n\t\t\tb := d.NewBatch()\n\t\t\trequire.NoError(t, b.DeleteRange([]byte(\"a\"), []byte(\"z\"), nil))\n\t\t\trequire.NoError(t, b.Commit(nil))\n\t\t},\n\t\tfunc() {\n\t\t\tb := d.NewBatch()\n\t\t\top := b.DeleteRangeDeferred(1, 1)\n\t\t\top.Key[0] = 'a'\n\t\t\top.Value[0] = 'z'\n\t\t\top.Finish()\n\t\t\trequire.NoError(t, b.Commit(nil))\n\t\t},\n\t\tfunc() {\n\t\t\tb := d.NewBatch()\n\t\t\tb2 := d.NewBatch()\n\t\t\trequire.NoError(t, b.DeleteRange([]byte(\"a\"), []byte(\"z\"), nil))\n\t\t\trequire.NoError(t, b2.SetRepr(b.Repr()))\n\t\t\trequire.NoError(t, b2.Commit(nil))\n\t\t\trequire.NoError(t, b.Close())\n\t\t},\n\t\tfunc() {\n\t\t\tb := d.NewBatch()\n\t\t\tb2 := d.NewBatch()\n\t\t\trequire.NoError(t, b.DeleteRange([]byte(\"a\"), []byte(\"z\"), nil))\n\t\t\trequire.NoError(t, b2.Apply(b, nil))\n\t\t\trequire.NoError(t, b2.Commit(nil))\n\t\t\trequire.NoError(t, b.Close())\n\t\t},\n\t\tfunc() {\n\t\t\trequire.NoError(t, d.RangeKeySet([]byte(\"a\"), []byte(\"z\"), nil, nil, nil))\n\t\t},\n\t\tfunc() {\n\t\t\trequire.NoError(t, d.RangeKeyUnset([]byte(\"a\"), []byte(\"z\"), nil, nil))\n\t\t},\n\t\tfunc() {\n\t\t\trequire.NoError(t, d.RangeKeyDelete([]byte(\"a\"), []byte(\"z\"), nil))\n\t\t},\n\t\tfunc() {\n\t\t\tb := d.NewBatch()\n\t\t\trequire.NoError(t, b.RangeKeySet([]byte(\"a\"), []byte(\"z\"), nil, nil, nil))\n\t\t\trequire.NoError(t, b.Commit(nil))\n\t\t},\n\t\tfunc() {\n\t\t\tb := d.NewBatch()\n\t\t\trequire.NoError(t, b.RangeKeyUnset([]byte(\"a\"), []byte(\"z\"), nil, nil))\n\t\t\trequire.NoError(t, b.Commit(nil))\n\t\t},\n\t\tfunc() {\n\t\t\tb := d.NewBatch()\n\t\t\trequire.NoError(t, b.RangeKeyDelete([]byte(\"a\"), []byte(\"z\"), nil))\n\t\t\trequire.NoError(t, b.Commit(nil))\n\t\t},\n\t\tfunc() {\n\t\t\tb := d.NewBatch()\n\t\t\tb2 := d.NewBatch()\n\t\t\trequire.NoError(t, b.RangeKeySet([]byte(\"a\"), []byte(\"z\"), nil, nil, nil))\n\t\t\trequire.NoError(t, b2.SetRepr(b.Repr()))\n\t\t\trequire.NoError(t, b2.Commit(nil))\n\t\t\trequire.NoError(t, b.Close())\n\t\t},\n\t\tfunc() {\n\t\t\tb := d.NewBatch()\n\t\t\tb2 := d.NewBatch()\n\t\t\trequire.NoError(t, b.RangeKeySet([]byte(\"a\"), []byte(\"z\"), nil, nil, nil))\n\t\t\trequire.NoError(t, b2.Apply(b, nil))\n\t\t\trequire.NoError(t, b2.Commit(nil))\n\t\t\trequire.NoError(t, b.Close())\n\t\t},\n\t}\n\n\tfor _, f := range cases {\n\t\td.mu.Lock()\n\t\tflushed := d.mu.mem.queue[len(d.mu.mem.queue)-1].flushed\n\t\td.mu.Unlock()\n\t\tf()\n\t\t<-flushed\n\t}\n\trequire.NoError(t, d.Close())\n}\n\nfunc TestFlushDelayStress(t *testing.T) {\n\trng := rand.New(rand.NewPCG(0, uint64(time.Now().UnixNano())))\n\topts := &Options{\n\t\tFS:                    vfs.NewMem(),\n\t\tComparer:              testkeys.Comparer,\n\t\tFlushDelayDeleteRange: time.Duration(rng.IntN(10)+1) * time.Millisecond,\n\t\tFlushDelayRangeKey:    time.Duration(rng.IntN(10)+1) * time.Millisecond,\n\t\tFormatMajorVersion:    internalFormatNewest,\n\t\tMemTableSize:          8192,\n\t\tLogger:                testLogger{t: t},\n\t}\n\n\tconst runs = 100\n\tfor run := 0; run < runs; run++ {\n\t\td, err := Open(\"\", opts)\n\t\trequire.NoError(t, err)\n\n\t\tnow := time.Now().UnixNano()\n\t\twriters := runtime.GOMAXPROCS(0)\n\t\tvar wg sync.WaitGroup\n\t\twg.Add(writers)\n\t\tfor i := 0; i < writers; i++ {\n\t\t\trng := rand.New(rand.NewPCG(0, uint64(now)+uint64(i)))\n\t\t\tgo func() {\n\t\t\t\tconst ops = 100\n\t\t\t\tdefer wg.Done()\n\n\t\t\t\tvar k1, k2 [32]byte\n\t\t\t\tfor j := 0; j < ops; j++ {\n\t\t\t\t\tswitch rng.IntN(3) {\n\t\t\t\t\tcase 0:\n\t\t\t\t\t\trandStr(k1[:], rng)\n\t\t\t\t\t\trandStr(k2[:], rng)\n\t\t\t\t\t\trequire.NoError(t, d.DeleteRange(k1[:], k2[:], nil))\n\t\t\t\t\tcase 1:\n\t\t\t\t\t\trandStr(k1[:], rng)\n\t\t\t\t\t\trandStr(k2[:], rng)\n\t\t\t\t\t\trequire.NoError(t, d.RangeKeySet(k1[:], k2[:], []byte(\"@2\"), nil, nil))\n\t\t\t\t\tcase 2:\n\t\t\t\t\t\trandStr(k1[:], rng)\n\t\t\t\t\t\trandStr(k2[:], rng)\n\t\t\t\t\t\trequire.NoError(t, d.Set(k1[:], k2[:], nil))\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tpanic(\"unreachable\")\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}()\n\t\t}\n\t\twg.Wait()\n\t\ttime.Sleep(time.Duration(rng.IntN(10)+1) * time.Millisecond)\n\t\trequire.NoError(t, d.Close())\n\t}\n}\n\n// Verify that range tombstones at higher levels do not unintentionally delete\n// newer keys at lower levels. This test sets up one such scenario. The base\n// problem is that range tombstones are not truncated to sstable boundaries on\n// disk, only in memory.\nfunc TestRangeDelCompactionTruncation(t *testing.T) {\n\trunTest := func(formatVersion FormatMajorVersion) {\n\t\t// Use a small target file size so that there is a single key per sstable.\n\t\td, err := Open(\"\", &Options{\n\t\t\tFS: vfs.NewMem(),\n\t\t\tLevels: []LevelOptions{\n\t\t\t\t{TargetFileSize: 100},\n\t\t\t\t{TargetFileSize: 100},\n\t\t\t\t{TargetFileSize: 1},\n\t\t\t},\n\t\t\tDebugCheck:         DebugCheckLevels,\n\t\t\tFormatMajorVersion: formatVersion,\n\t\t})\n\t\trequire.NoError(t, err)\n\t\tdefer d.Close()\n\n\t\td.mu.Lock()\n\t\td.mu.versions.dynamicBaseLevel = false\n\t\td.mu.Unlock()\n\n\t\tlsm := func() string {\n\t\t\td.mu.Lock()\n\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\td.mu.Unlock()\n\t\t\treturn s\n\t\t}\n\t\texpectLSM := func(expected string) {\n\t\t\tt.Helper()\n\t\t\texpected = strings.TrimSpace(expected)\n\t\t\tactual := strings.TrimSpace(lsm())\n\t\t\tif expected != actual {\n\t\t\t\tt.Fatalf(\"expected\\n%s\\nbut found\\n%s\", expected, actual)\n\t\t\t}\n\t\t}\n\n\t\trequire.NoError(t, d.Set([]byte(\"a\"), bytes.Repeat([]byte(\"b\"), 100), nil))\n\t\tsnap1 := d.NewSnapshot()\n\t\tdefer snap1.Close()\n\t\t// Flush so that each version of \"a\" ends up in its own L0 table. If we\n\t\t// allowed both versions in the same L0 table, compaction could trivially\n\t\t// move the single L0 table to L1.\n\t\trequire.NoError(t, d.Flush())\n\t\trequire.NoError(t, d.Set([]byte(\"b\"), bytes.Repeat([]byte(\"c\"), 100), nil))\n\n\t\tsnap2 := d.NewSnapshot()\n\t\tdefer snap2.Close()\n\t\trequire.NoError(t, d.DeleteRange([]byte(\"a\"), []byte(\"d\"), nil))\n\n\t\t// Compact to produce the L1 tables.\n\t\trequire.NoError(t, d.Compact([]byte(\"c\"), []byte(\"c\\x00\"), false))\n\t\texpectLSM(`\nL1:\n  000008:[a#12,RANGEDEL-b#inf,RANGEDEL]\n  000009:[b#12,RANGEDEL-d#inf,RANGEDEL]\n`)\n\n\t\t// Compact again to move one of the tables to L2.\n\t\trequire.NoError(t, d.Compact([]byte(\"c\"), []byte(\"c\\x00\"), false))\n\t\texpectLSM(`\nL1:\n  000008:[a#12,RANGEDEL-b#inf,RANGEDEL]\nL2:\n  000009:[b#12,RANGEDEL-d#inf,RANGEDEL]\n`)\n\n\t\t// Write \"b\" and \"c\" to a new table.\n\t\trequire.NoError(t, d.Set([]byte(\"b\"), []byte(\"d\"), nil))\n\t\trequire.NoError(t, d.Set([]byte(\"c\"), []byte(\"e\"), nil))\n\t\trequire.NoError(t, d.Flush())\n\t\texpectLSM(`\nL0.0:\n  000011:[b#13,SET-c#14,SET]\nL1:\n  000008:[a#12,RANGEDEL-b#inf,RANGEDEL]\nL2:\n  000009:[b#12,RANGEDEL-d#inf,RANGEDEL]\n`)\n\n\t\t// \"b\" is still visible at this point as it should be.\n\t\tif _, closer, err := d.Get([]byte(\"b\")); err != nil {\n\t\t\tt.Fatalf(\"expected success, but found %v\", err)\n\t\t} else {\n\t\t\tcloser.Close()\n\t\t}\n\n\t\tkeys := func() string {\n\t\t\titer, _ := d.NewIter(nil)\n\t\t\tdefer iter.Close()\n\t\t\tvar buf bytes.Buffer\n\t\t\tvar sep string\n\t\t\tfor iter.First(); iter.Valid(); iter.Next() {\n\t\t\t\tfmt.Fprintf(&buf, \"%s%s\", sep, iter.Key())\n\t\t\t\tsep = \" \"\n\t\t\t}\n\t\t\treturn buf.String()\n\t\t}\n\n\t\tif expected, actual := `b c`, keys(); expected != actual {\n\t\t\tt.Fatalf(\"expected %q, but found %q\", expected, actual)\n\t\t}\n\n\t\t// Compact the L0 table. This will compact the L0 table into L1 and do to the\n\t\t// sstable target size settings will create 2 tables in L1. Then L1 table\n\t\t// containing \"c\" will be compacted again with the L2 table creating two\n\t\t// tables in L2. Lastly, the L2 table containing \"c\" will be compacted\n\t\t// creating the L3 table.\n\t\trequire.NoError(t, d.Compact([]byte(\"c\"), []byte(\"c\\x00\"), false))\n\t\texpectLSM(`\nL1:\n  000008:[a#12,RANGEDEL-b#inf,RANGEDEL]\nL2:\n  000012:[b#13,SET-c#inf,RANGEDEL]\nL3:\n  000013:[c#14,SET-d#inf,RANGEDEL]\n`)\n\n\t\t// The L1 table still contains a tombstone from [a,d) which will improperly\n\t\t// delete the newer version of \"b\" in L2.\n\t\tif _, closer, err := d.Get([]byte(\"b\")); err != nil {\n\t\t\tt.Errorf(\"expected success, but found %v\", err)\n\t\t} else {\n\t\t\tcloser.Close()\n\t\t}\n\n\t\tif expected, actual := `b c`, keys(); expected != actual {\n\t\t\tt.Errorf(\"expected %q, but found %q\", expected, actual)\n\t\t}\n\t}\n\n\tversions := []FormatMajorVersion{\n\t\tFormatMinSupported,\n\t\tFormatNewest,\n\t}\n\tfor _, version := range versions {\n\t\tt.Run(fmt.Sprintf(\"version-%s\", version), func(t *testing.T) {\n\t\t\trunTest(version)\n\t\t})\n\t}\n}\n\n// This is an alternate scenario to the one created in\n// TestRangeDelCompactionTruncation that would result in the bounds for an\n// sstable expanding to overlap its left neighbor if we failed to truncate an\n// sstable's boundaries to the compaction input boundaries.\nfunc TestRangeDelCompactionTruncation2(t *testing.T) {\n\t// Use a small target file size so that there is a single key per sstable.\n\td, err := Open(\"\", &Options{\n\t\tFS: vfs.NewMem(),\n\t\tLevels: []LevelOptions{\n\t\t\t{TargetFileSize: 200},\n\t\t\t{TargetFileSize: 200},\n\t\t\t{TargetFileSize: 1},\n\t\t},\n\t\tDebugCheck: DebugCheckLevels,\n\t})\n\trequire.NoError(t, err)\n\tdefer d.Close()\n\n\tlsm := func() string {\n\t\td.mu.Lock()\n\t\ts := d.mu.versions.currentVersion().String()\n\t\td.mu.Unlock()\n\t\treturn s\n\t}\n\texpectLSM := func(expected string) {\n\t\tt.Helper()\n\t\texpected = strings.TrimSpace(expected)\n\t\tactual := strings.TrimSpace(lsm())\n\t\tif expected != actual {\n\t\t\tt.Fatalf(\"expected\\n%s\\nbut found\\n%s\", expected, actual)\n\t\t}\n\t}\n\n\trequire.NoError(t, d.Set([]byte(\"b\"), bytes.Repeat([]byte(\"b\"), 100), nil))\n\tsnap1 := d.NewSnapshot()\n\tdefer snap1.Close()\n\t// Flush so that each version of \"b\" ends up in its own L0 table. If we\n\t// allowed both versions in the same L0 table, compaction could trivially\n\t// move the single L0 table to L1.\n\trequire.NoError(t, d.Flush())\n\trequire.NoError(t, d.Set([]byte(\"b\"), bytes.Repeat([]byte(\"c\"), 100), nil))\n\tsnap2 := d.NewSnapshot()\n\tdefer snap2.Close()\n\trequire.NoError(t, d.DeleteRange([]byte(\"a\"), []byte(\"d\"), nil))\n\n\t// Compact to produce the L1 tables.\n\trequire.NoError(t, d.Compact([]byte(\"b\"), []byte(\"b\\x00\"), false))\n\texpectLSM(`\nL6:\n  000009:[a#12,RANGEDEL-d#inf,RANGEDEL]\n`)\n\n\trequire.NoError(t, d.Set([]byte(\"c\"), bytes.Repeat([]byte(\"d\"), 100), nil))\n\trequire.NoError(t, d.Compact([]byte(\"c\"), []byte(\"c\\x00\"), false))\n\texpectLSM(`\nL6:\n  000012:[a#12,RANGEDEL-c#inf,RANGEDEL]\n  000013:[c#13,SET-d#inf,RANGEDEL]\n`)\n}\n\n// TODO(peter): rewrite this test, TestRangeDelCompactionTruncation, and\n// TestRangeDelCompactionTruncation2 as data-driven tests.\nfunc TestRangeDelCompactionTruncation3(t *testing.T) {\n\t// Use a small target file size so that there is a single key per sstable.\n\td, err := Open(\"tmp\", &Options{\n\t\tCleaner: ArchiveCleaner{},\n\t\tFS:      vfs.NewMem(),\n\t\tLevels: []LevelOptions{\n\t\t\t{TargetFileSize: 200},\n\t\t\t{TargetFileSize: 200},\n\t\t\t{TargetFileSize: 1},\n\t\t},\n\t\tDebugCheck: DebugCheckLevels,\n\t})\n\trequire.NoError(t, err)\n\tdefer d.Close()\n\n\td.mu.Lock()\n\td.mu.versions.dynamicBaseLevel = false\n\td.mu.Unlock()\n\n\tlsm := func() string {\n\t\td.mu.Lock()\n\t\ts := d.mu.versions.currentVersion().String()\n\t\td.mu.Unlock()\n\t\treturn s\n\t}\n\texpectLSM := func(expected string) {\n\t\tt.Helper()\n\t\texpected = strings.TrimSpace(expected)\n\t\tactual := strings.TrimSpace(lsm())\n\t\tif expected != actual {\n\t\t\tt.Fatalf(\"expected\\n%s\\nbut found\\n%s\", expected, actual)\n\t\t}\n\t}\n\n\trequire.NoError(t, d.Set([]byte(\"b\"), bytes.Repeat([]byte(\"b\"), 100), nil))\n\tsnap1 := d.NewSnapshot()\n\tdefer snap1.Close()\n\n\t// Flush so that each version of \"b\" ends up in its own L0 table. If we\n\t// allowed both versions in the same L0 table, compaction could trivially\n\t// move the single L0 table to L1.\n\trequire.NoError(t, d.Flush())\n\trequire.NoError(t, d.Set([]byte(\"b\"), bytes.Repeat([]byte(\"c\"), 100), nil))\n\tsnap2 := d.NewSnapshot()\n\tdefer snap2.Close()\n\n\trequire.NoError(t, d.DeleteRange([]byte(\"a\"), []byte(\"d\"), nil))\n\tsnap3 := d.NewSnapshot()\n\tdefer snap3.Close()\n\n\tif _, _, err := d.Get([]byte(\"b\")); err != ErrNotFound {\n\t\tt.Fatalf(\"expected not found, but found %v\", err)\n\t}\n\n\t// Compact a few times to move the tables down to L3.\n\tfor i := 0; i < 3; i++ {\n\t\trequire.NoError(t, d.Compact([]byte(\"b\"), []byte(\"b\\x00\"), false))\n\t}\n\texpectLSM(`\nL3:\n  000009:[a#12,RANGEDEL-d#inf,RANGEDEL]\n`)\n\n\trequire.NoError(t, d.Set([]byte(\"c\"), bytes.Repeat([]byte(\"d\"), 100), nil))\n\n\trequire.NoError(t, d.Compact([]byte(\"c\"), []byte(\"c\\x00\"), false))\n\texpectLSM(`\nL3:\n  000013:[a#12,RANGEDEL-b#inf,RANGEDEL]\n  000014:[b#12,RANGEDEL-c#inf,RANGEDEL]\nL4:\n  000015:[c#13,SET-d#inf,RANGEDEL]\n`)\n\n\trequire.NoError(t, d.Compact([]byte(\"c\"), []byte(\"c\\x00\"), false))\n\texpectLSM(`\nL3:\n  000013:[a#12,RANGEDEL-b#inf,RANGEDEL]\n  000014:[b#12,RANGEDEL-c#inf,RANGEDEL]\nL5:\n  000015:[c#13,SET-d#inf,RANGEDEL]\n`)\n\n\tif _, _, err := d.Get([]byte(\"b\")); err != ErrNotFound {\n\t\tt.Fatalf(\"expected not found, but found %v\", err)\n\t}\n\n\trequire.NoError(t, d.Compact([]byte(\"a\"), []byte(\"a\\x00\"), false))\n\texpectLSM(`\nL3:\n  000014:[b#12,RANGEDEL-c#inf,RANGEDEL]\nL4:\n  000013:[a#12,RANGEDEL-b#inf,RANGEDEL]\nL5:\n  000015:[c#13,SET-d#inf,RANGEDEL]\n`)\n\n\tif v, _, err := d.Get([]byte(\"b\")); err != ErrNotFound {\n\t\tt.Fatalf(\"expected not found, but found %v [%s]\", err, v)\n\t}\n}\n\nfunc BenchmarkRangeDelIterate(b *testing.B) {\n\tfor _, entries := range []int{10, 1000, 100000} {\n\t\tb.Run(fmt.Sprintf(\"entries=%d\", entries), func(b *testing.B) {\n\t\t\tfor _, deleted := range []int{entries, entries - 1} {\n\t\t\t\tb.Run(fmt.Sprintf(\"deleted=%d\", deleted), func(b *testing.B) {\n\t\t\t\t\tfor _, snapshotCompact := range []bool{false, true} {\n\t\t\t\t\t\tb.Run(fmt.Sprintf(\"snapshotAndCompact=%t\", snapshotCompact), func(b *testing.B) {\n\t\t\t\t\t\t\tbenchmarkRangeDelIterate(b, entries, deleted, snapshotCompact)\n\t\t\t\t\t\t})\n\t\t\t\t\t}\n\t\t\t\t})\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc benchmarkRangeDelIterate(b *testing.B, entries, deleted int, snapshotCompact bool) {\n\tmem := vfs.NewMem()\n\tcache := NewCache(128 << 20) // 128 MB\n\tdefer cache.Unref()\n\n\td, err := Open(\"\", &Options{\n\t\tCache:      cache,\n\t\tFS:         mem,\n\t\tDebugCheck: DebugCheckLevels,\n\t})\n\tif err != nil {\n\t\tb.Fatal(err)\n\t}\n\tdefer d.Close()\n\n\tmakeKey := func(i int) []byte {\n\t\treturn []byte(fmt.Sprintf(\"%09d\", i))\n\t}\n\n\t// Create an sstable with N entries and ingest it. This is a fast way\n\t// to get a lot of entries into pebble.\n\tf, err := mem.Create(\"ext\", vfs.WriteCategoryUnspecified)\n\tif err != nil {\n\t\tb.Fatal(err)\n\t}\n\tw := sstable.NewRawWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{\n\t\tBlockSize: 32 << 10, // 32 KB\n\t})\n\tfor i := 0; i < entries; i++ {\n\t\tkey := base.MakeInternalKey(makeKey(i), 0, InternalKeyKindSet)\n\t\tif err := w.AddWithForceObsolete(key, nil, false /* forceObsolete */); err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n\tif err := w.Close(); err != nil {\n\t\tb.Fatal(err)\n\t}\n\tif err := d.Ingest(context.Background(), []string{\"ext\"}); err != nil {\n\t\tb.Fatal(err)\n\t}\n\n\t// Some benchmarks test snapshots that force the range tombstone into the\n\t// same level as the covered data.\n\t// See https://github.com/cockroachdb/pebble/issues/1070.\n\tif snapshotCompact {\n\t\ts := d.NewSnapshot()\n\t\tdefer func() { require.NoError(b, s.Close()) }()\n\t}\n\n\t// Create a range tombstone that deletes most (or all) of those entries.\n\tfrom := makeKey(0)\n\tto := makeKey(deleted)\n\tif err := d.DeleteRange(from, to, nil); err != nil {\n\t\tb.Fatal(err)\n\t}\n\n\tif snapshotCompact {\n\t\trequire.NoError(b, d.Compact(makeKey(0), makeKey(entries), false))\n\t}\n\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\titer, _ := d.NewIter(nil)\n\t\titer.SeekGE(from)\n\t\tif deleted < entries {\n\t\t\tif !iter.Valid() {\n\t\t\t\tb.Fatal(\"key not found\")\n\t\t\t}\n\t\t} else if iter.Valid() {\n\t\t\tb.Fatal(\"unexpected key found\")\n\t\t}\n\t\tif err := iter.Close(); err != nil {\n\t\t\tb.Fatal(err)\n\t\t}\n\t}\n}\n"
        },
        {
          "name": "range_keys.go",
          "type": "blob",
          "size": 29.3154296875,
          "content": "// Copyright 2021 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"context\"\n\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/invariants\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n\t\"github.com/cockroachdb/pebble/internal/treeprinter\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n)\n\n// constructRangeKeyIter constructs the range-key iterator stack, populating\n// i.rangeKey.rangeKeyIter with the resulting iterator.\nfunc (i *Iterator) constructRangeKeyIter() {\n\ti.rangeKey.rangeKeyIter = i.rangeKey.iterConfig.Init(\n\t\t&i.comparer, i.seqNum, i.opts.LowerBound, i.opts.UpperBound,\n\t\t&i.hasPrefix, &i.prefixOrFullSeekKey, false /* internalKeys */, &i.rangeKey.rangeKeyBuffers.internal)\n\n\tif i.opts.DebugRangeKeyStack {\n\t\t// The default logger is preferable to i.opts.getLogger(), at least in the\n\t\t// metamorphic test.\n\t\ti.rangeKey.rangeKeyIter = keyspan.InjectLogging(i.rangeKey.rangeKeyIter, base.DefaultLogger)\n\t}\n\n\t// If there's an indexed batch with range keys, include it.\n\tif i.batch != nil {\n\t\tif i.batch.index == nil {\n\t\t\t// This isn't an indexed batch. We shouldn't have gotten this far.\n\t\t\tpanic(errors.AssertionFailedf(\"creating an iterator over an unindexed batch\"))\n\t\t} else {\n\t\t\t// Only include the batch's range key iterator if it has any keys.\n\t\t\t// NB: This can force reconstruction of the rangekey iterator stack\n\t\t\t// in SetOptions if subsequently range keys are added. See\n\t\t\t// SetOptions.\n\t\t\tif i.batch.countRangeKeys > 0 {\n\t\t\t\ti.batch.initRangeKeyIter(&i.opts, &i.batchRangeKeyIter, i.batchSeqNum)\n\t\t\t\ti.rangeKey.iterConfig.AddLevel(&i.batchRangeKeyIter)\n\t\t\t}\n\t\t}\n\t}\n\n\tif !i.batchOnlyIter {\n\t\t// Next are the flushables: memtables and large batches.\n\t\tif i.readState != nil {\n\t\t\tfor j := len(i.readState.memtables) - 1; j >= 0; j-- {\n\t\t\t\tmem := i.readState.memtables[j]\n\t\t\t\t// We only need to read from memtables which contain sequence numbers older\n\t\t\t\t// than seqNum.\n\t\t\t\tif logSeqNum := mem.logSeqNum; logSeqNum >= i.seqNum {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif rki := mem.newRangeKeyIter(&i.opts); rki != nil {\n\t\t\t\t\ti.rangeKey.iterConfig.AddLevel(rki)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tcurrent := i.version\n\t\tif current == nil {\n\t\t\tcurrent = i.readState.current\n\t\t}\n\t\t// Next are the file levels: L0 sub-levels followed by lower levels.\n\n\t\t// Add file-specific iterators for L0 files containing range keys. We\n\t\t// maintain a separate manifest.LevelMetadata for each level containing only\n\t\t// files that contain range keys, however we don't compute a separate\n\t\t// L0Sublevels data structure too.\n\t\t//\n\t\t// We first use L0's LevelMetadata to peek and see whether L0 contains any\n\t\t// range keys at all. If it does, we create a range key level iterator per\n\t\t// level that contains range keys using the information from L0Sublevels.\n\t\t// Some sublevels may not contain any range keys, and we need to iterate\n\t\t// through the fileMetadata to determine that. Since L0's file count should\n\t\t// not significantly exceed ~1000 files (see L0CompactionFileThreshold),\n\t\t// this should be okay.\n\t\tif !current.RangeKeyLevels[0].Empty() {\n\t\t\t// L0 contains at least 1 file containing range keys.\n\t\t\t// Add level iterators for the L0 sublevels, iterating from newest to\n\t\t\t// oldest.\n\t\t\tfor j := len(current.L0SublevelFiles) - 1; j >= 0; j-- {\n\t\t\t\titer := current.L0SublevelFiles[j].Iter()\n\t\t\t\tif !containsAnyRangeKeys(iter) {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tli := i.rangeKey.iterConfig.NewLevelIter()\n\t\t\t\tli.Init(\n\t\t\t\t\ti.ctx,\n\t\t\t\t\ti.opts.SpanIterOptions(),\n\t\t\t\t\ti.cmp,\n\t\t\t\t\ti.newIterRangeKey,\n\t\t\t\t\titer.Filter(manifest.KeyTypeRange),\n\t\t\t\t\tmanifest.L0Sublevel(j),\n\t\t\t\t\tmanifest.KeyTypeRange,\n\t\t\t\t)\n\t\t\t\ti.rangeKey.iterConfig.AddLevel(li)\n\t\t\t}\n\t\t}\n\n\t\t// Add level iterators for the non-empty non-L0 levels.\n\t\tfor level := 1; level < len(current.RangeKeyLevels); level++ {\n\t\t\tif current.RangeKeyLevels[level].Empty() {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tli := i.rangeKey.iterConfig.NewLevelIter()\n\t\t\tspanIterOpts := i.opts.SpanIterOptions()\n\t\t\tli.Init(i.ctx, spanIterOpts, i.cmp, i.newIterRangeKey, current.RangeKeyLevels[level].Iter(),\n\t\t\t\tmanifest.Level(level), manifest.KeyTypeRange)\n\t\t\ti.rangeKey.iterConfig.AddLevel(li)\n\t\t}\n\t}\n}\n\nfunc containsAnyRangeKeys(iter manifest.LevelIterator) bool {\n\tfor f := iter.First(); f != nil; f = iter.Next() {\n\t\tif f.HasRangeKeys {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// Range key masking\n//\n// Pebble iterators may be configured such that range keys with suffixes mask\n// point keys with lower suffixes. The intended use is implementing a MVCC\n// delete range operation using range keys, when suffixes are MVCC timestamps.\n//\n// To enable masking, the user populates the IterOptions's RangeKeyMasking\n// field. The Suffix field configures which range keys act as masks. The\n// intended use is to hold a MVCC read timestamp. When implementing a MVCC\n// delete range operation, only range keys that are visible at the read\n// timestamp should be visible. If a range key has a suffix ≤\n// RangeKeyMasking.Suffix, it acts as a mask.\n//\n// Range key masking is facilitated by the keyspan.InterleavingIter. The\n// interleaving iterator interleaves range keys and point keys during combined\n// iteration. During user iteration, the interleaving iterator is configured\n// with a keyspan.SpanMask, implemented by the rangeKeyMasking struct below.\n// The SpanMask interface defines two methods: SpanChanged and SkipPoint.\n//\n// SpanChanged is used to keep the current mask up-to-date. Whenever the point\n// iterator has stepped into or out of the bounds of a range key, the\n// interleaving iterator invokes SpanChanged passing the current covering range\n// key. The below rangeKeyMasking implementation scans the range keys looking\n// for the range key with the largest suffix that's still ≤ the suffix supplied\n// to IterOptions.RangeKeyMasking.Suffix (the \"read timestamp\"). If it finds a\n// range key that meets the condition, the range key should act as a mask. The\n// span and the relevant range key's suffix are saved.\n//\n// The above ensures that `rangeKeyMasking.maskActiveSuffix` always contains the\n// current masking suffix such that any point keys with lower suffixes should be\n// skipped.\n//\n// There are two ways in which masked point keys are skipped.\n//\n//   1. Interleaving iterator SkipPoint\n//\n// Whenever the interleaving iterator encounters a point key that falls within\n// the bounds of a range key, it invokes SkipPoint. The interleaving iterator\n// guarantees that the SpanChanged method described above has already been\n// invoked with the covering range key. The below rangeKeyMasking implementation\n// of SkipPoint splits the key into prefix and suffix, compares the suffix to\n// the `maskActiveSuffix` updated by SpanChanged and returns true if\n// suffix(point) < maskActiveSuffix.\n//\n// The SkipPoint logic is sufficient to ensure that the Pebble iterator filters\n// out all masked point keys. However, it requires the iterator read each masked\n// point key. For broad range keys that mask many points, this may be expensive.\n//\n//   2. Block property filter\n//\n// For more efficient handling of braad range keys that mask many points, the\n// IterOptions.RangeKeyMasking field has an optional Filter option. This Filter\n// field takes a superset of the block-property filter interface, adding a\n// method to dynamically configure the filter's filtering criteria.\n//\n// To make use of the Filter option, the user is required to define and\n// configure a block-property collector that collects a property containing at\n// least the maximum suffix of a key within a block.\n//\n// When the SpanChanged method described above is invoked, rangeKeyMasking also\n// reconfigures the user-provided filter. It invokes a SetSuffix method,\n// providing the `maskActiveSuffix`, requesting that from now on the\n// block-property filter return Intersects()=false for any properties indicating\n// that a block contains exclusively keys with suffixes greater than the\n// provided suffix.\n//\n// Note that unlike other block-property filters, the filter used for masking\n// must not apply across the entire keyspace. It must only filter blocks that\n// lie within the bounds of the range key that set the mask suffix. To\n// accommodate this, rangeKeyMasking implements a special interface:\n// sstable.BoundLimitedBlockPropertyFilter. This interface extends the block\n// property filter interface with two new methods: KeyIsWithinLowerBound and\n// KeyIsWithinUpperBound. The rangeKeyMasking type wraps the user-provided block\n// property filter, implementing these two methods and overriding Intersects to\n// always return true if there is no active mask.\n//\n// The logic to ensure that a mask block-property filter is only applied within\n// the bounds of the masking range key is subtle. The interleaving iterator\n// guarantees that it never invokes SpanChanged until the point iterator is\n// positioned within the range key. During forward iteration, this guarantees\n// that any block that a sstable reader might attempt to load contains only keys\n// greater than or equal to the range key's lower bound. During backward\n// iteration, it provides the analagous guarantee on the range key's upper\n// bound.\n//\n// The above ensures that an sstable reader only needs to verify that a block\n// that it skips meets the opposite bound. This is where the\n// KeyIsWithinLowerBound and KeyIsWithinUpperBound methods are used. When an\n// sstable iterator is configured with a BoundLimitedBlockPropertyFilter, it\n// checks for intersection with the block-property filter before every block\n// load, like ordinary block-property filters. However, if the bound-limited\n// block property filter indicates that it does NOT intersect, the filter's\n// relevant KeyIsWithin{Lower,Upper}Bound method is queried, using a block\n// index separator as the bound. If the method indicates that the provided index\n// separator does not fall within the range key bounds, the no-intersection\n// result is ignored, and the block is read.\n\ntype rangeKeyMasking struct {\n\tcmp       base.Compare\n\tsuffixCmp base.CompareRangeSuffixes\n\tsplit     base.Split\n\tfilter    BlockPropertyFilterMask\n\t// maskActiveSuffix holds the suffix of a range key currently acting as a\n\t// mask, hiding point keys with suffixes greater than it. maskActiveSuffix\n\t// is only ever non-nil if IterOptions.RangeKeyMasking.Suffix is non-nil.\n\t// maskActiveSuffix is updated whenever the iterator passes over a new range\n\t// key. The maskActiveSuffix should only be used if maskSpan is non-nil.\n\t//\n\t// See SpanChanged.\n\tmaskActiveSuffix []byte\n\t// maskSpan holds the span from which the active mask suffix was extracted.\n\t// The span is used for bounds comparisons, to ensure that a range-key mask\n\t// is not applied beyond the bounds of the range key.\n\tmaskSpan *keyspan.Span\n\tparent   *Iterator\n}\n\nfunc (m *rangeKeyMasking) init(parent *Iterator, c *base.Comparer) {\n\tm.cmp = c.Compare\n\tm.suffixCmp = c.CompareRangeSuffixes\n\tm.split = c.Split\n\tif parent.opts.RangeKeyMasking.Filter != nil {\n\t\tm.filter = parent.opts.RangeKeyMasking.Filter()\n\t}\n\tm.parent = parent\n}\n\n// SpanChanged implements the keyspan.SpanMask interface, used during range key\n// iteration.\nfunc (m *rangeKeyMasking) SpanChanged(s *keyspan.Span) {\n\tif s == nil && m.maskSpan == nil {\n\t\treturn\n\t}\n\tm.maskSpan = nil\n\tm.maskActiveSuffix = m.maskActiveSuffix[:0]\n\n\t// Find the smallest suffix of a range key contained within the Span,\n\t// excluding suffixes less than m.opts.RangeKeyMasking.Suffix.\n\tif s != nil {\n\t\tm.parent.rangeKey.stale = true\n\t\tif m.parent.opts.RangeKeyMasking.Suffix != nil {\n\t\t\tfor j := range s.Keys {\n\t\t\t\tif s.Keys[j].Suffix == nil {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif m.suffixCmp(s.Keys[j].Suffix, m.parent.opts.RangeKeyMasking.Suffix) < 0 {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif len(m.maskActiveSuffix) == 0 || m.suffixCmp(m.maskActiveSuffix, s.Keys[j].Suffix) > 0 {\n\t\t\t\t\tm.maskSpan = s\n\t\t\t\t\tm.maskActiveSuffix = append(m.maskActiveSuffix[:0], s.Keys[j].Suffix...)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif m.maskSpan != nil && m.parent.opts.RangeKeyMasking.Filter != nil {\n\t\t// Update the  block-property filter to filter point keys with suffixes\n\t\t// greater than m.maskActiveSuffix.\n\t\terr := m.filter.SetSuffix(m.maskActiveSuffix)\n\t\tif err != nil {\n\t\t\tm.parent.err = err\n\t\t}\n\t}\n\t// If no span is active, we leave the inner block-property filter configured\n\t// with its existing suffix. That's okay, because Intersects calls are first\n\t// evaluated by iteratorRangeKeyState.Intersects, which considers all blocks\n\t// as intersecting if there's no active mask.\n}\n\n// SkipPoint implements the keyspan.SpanMask interface, used during range key\n// iteration. Whenever a point key is covered by a non-empty Span, the\n// interleaving iterator invokes SkipPoint. This function is responsible for\n// performing range key masking.\n//\n// If a non-nil IterOptions.RangeKeyMasking.Suffix is set, range key masking is\n// enabled. Masking hides point keys, transparently skipping over the keys.\n// Whether or not a point key is masked is determined by comparing the point\n// key's suffix, the overlapping span's keys' suffixes, and the user-configured\n// IterOption's RangeKeyMasking.Suffix. When configured with a masking threshold\n// _t_, and there exists a span with suffix _r_ covering a point key with suffix\n// _p_, and\n//\n//\t_t_ ≤ _r_ < _p_\n//\n// then the point key is elided. Consider the following rendering, where using\n// integer suffixes with higher integers sort before suffixes with lower\n// integers, (for example @7 ≤ @6 < @5):\n//\n//\t     ^\n//\t  @9 |        •―――――――――――――――○ [e,m)@9\n//\ts  8 |                      • l@8\n//\tu  7 |------------------------------------ @7 RangeKeyMasking.Suffix\n//\tf  6 |      [h,q)@6 •―――――――――――――――――○            (threshold)\n//\tf  5 |              • h@5\n//\tf  4 |                          • n@4\n//\ti  3 |          •―――――――――――○ [f,l)@3\n//\tx  2 |  • b@2\n//\t   1 |\n//\t   0 |___________________________________\n//\t      a b c d e f g h i j k l m n o p q\n//\n// An iterator scanning the entire keyspace with the masking threshold set to @7\n// will observe point keys b@2 and l@8. The span keys [h,q)@6 and [f,l)@3 serve\n// as masks, because cmp(@6,@7) ≥ 0 and cmp(@3,@7) ≥ 0. The span key [e,m)@9\n// does not serve as a mask, because cmp(@9,@7) < 0.\n//\n// Although point l@8 falls within the user key bounds of [e,m)@9, [e,m)@9 is\n// non-masking due to its suffix. The point key l@8 also falls within the user\n// key bounds of [h,q)@6, but since cmp(@6,@8) ≥ 0, l@8 is unmasked.\n//\n// Invariant: The userKey is within the user key bounds of the span most\n// recently provided to `SpanChanged`.\nfunc (m *rangeKeyMasking) SkipPoint(userKey []byte) bool {\n\tm.parent.stats.RangeKeyStats.ContainedPoints++\n\tif m.maskSpan == nil {\n\t\t// No range key is currently acting as a mask, so don't skip.\n\t\treturn false\n\t}\n\t// Range key masking is enabled and the current span includes a range key\n\t// that is being used as a mask. (NB: SpanChanged already verified that the\n\t// range key's suffix is ≥ RangeKeyMasking.Suffix).\n\t//\n\t// This point key falls within the bounds of the range key (guaranteed by\n\t// the InterleavingIter). Skip the point key if the range key's suffix is\n\t// greater than the point key's suffix.\n\tpointSuffix := userKey[m.split(userKey):]\n\tif len(pointSuffix) > 0 && m.suffixCmp(m.maskActiveSuffix, pointSuffix) < 0 {\n\t\tm.parent.stats.RangeKeyStats.SkippedPoints++\n\t\treturn true\n\t}\n\treturn false\n}\n\n// The iteratorRangeKeyState type implements the sstable package's\n// BoundLimitedBlockPropertyFilter interface in order to use block property\n// filters for range key masking. The iteratorRangeKeyState implementation wraps\n// the block-property filter provided in Options.RangeKeyMasking.Filter.\n//\n// Using a block-property filter for range-key masking requires limiting the\n// filter's effect to the bounds of the range key currently acting as a mask.\n// Consider the range key [a,m)@10, and an iterator positioned just before the\n// below block, bounded by index separators `c` and `z`:\n//\n//\t          c                          z\n//\t   x      |  c@9 c@5 c@1 d@7 e@4 y@4 | ...\n//\titer pos\n//\n// The next block cannot be skipped, despite the range key suffix @10 is greater\n// than all the block's keys' suffixes, because it contains a key (y@4) outside\n// the bounds of the range key.\n//\n// This extended BoundLimitedBlockPropertyFilter interface adds two new methods,\n// KeyIsWithinLowerBound and KeyIsWithinUpperBound, for testing whether a\n// particular block is within bounds.\n//\n// The iteratorRangeKeyState implements these new methods by first checking if\n// the iterator is currently positioned within a range key. If not, the provided\n// key is considered out-of-bounds. If the iterator is positioned within a range\n// key, it compares the corresponding range key bound.\nvar _ sstable.BoundLimitedBlockPropertyFilter = (*rangeKeyMasking)(nil)\n\n// Name implements the limitedBlockPropertyFilter interface defined in the\n// sstable package by passing through to the user-defined block property filter.\nfunc (m *rangeKeyMasking) Name() string {\n\treturn m.filter.Name()\n}\n\n// Intersects implements the limitedBlockPropertyFilter interface defined in the\n// sstable package by passing the intersection decision to the user-provided\n// block property filter only if a range key is covering the current iterator\n// position.\nfunc (m *rangeKeyMasking) Intersects(prop []byte) (bool, error) {\n\tif m.maskSpan == nil {\n\t\t// No span is actively masking.\n\t\treturn true, nil\n\t}\n\treturn m.filter.Intersects(prop)\n}\n\nfunc (m *rangeKeyMasking) SyntheticSuffixIntersects(prop []byte, suffix []byte) (bool, error) {\n\tif m.maskSpan == nil {\n\t\t// No span is actively masking.\n\t\treturn true, nil\n\t}\n\treturn m.filter.SyntheticSuffixIntersects(prop, suffix)\n}\n\n// KeyIsWithinLowerBound implements the limitedBlockPropertyFilter interface\n// defined in the sstable package. It's used to restrict the masking block\n// property filter to only applying within the bounds of the active range key.\nfunc (m *rangeKeyMasking) KeyIsWithinLowerBound(key []byte) bool {\n\t// Invariant: m.maskSpan != nil\n\t//\n\t// The provided `key` is an inclusive lower bound of the block we're\n\t// considering skipping.\n\treturn m.cmp(m.maskSpan.Start, key) <= 0\n}\n\n// KeyIsWithinUpperBound implements the limitedBlockPropertyFilter interface\n// defined in the sstable package. It's used to restrict the masking block\n// property filter to only applying within the bounds of the active range key.\nfunc (m *rangeKeyMasking) KeyIsWithinUpperBound(key []byte) bool {\n\t// Invariant: m.maskSpan != nil\n\t//\n\t// The provided `key` is an *inclusive* upper bound of the block we're\n\t// considering skipping, so the range key's end must be strictly greater\n\t// than the block bound for the block to be within bounds.\n\treturn m.cmp(m.maskSpan.End, key) > 0\n}\n\n// lazyCombinedIter implements the internalIterator interface, wrapping a\n// pointIter. It requires the pointIter's the levelIters be configured with\n// pointers to its combinedIterState. When the levelIter observes a file\n// containing a range key, the lazyCombinedIter constructs the combined\n// range+point key iterator stack and switches to it.\ntype lazyCombinedIter struct {\n\t// parent holds a pointer to the root *pebble.Iterator containing this\n\t// iterator. It's used to mutate the internalIterator in use when switching\n\t// to combined iteration.\n\tparent            *Iterator\n\tpointIter         internalIterator\n\tcombinedIterState combinedIterState\n}\n\n// combinedIterState encapsulates the current state of combined iteration.\n// Various low-level iterators (mergingIter, leveliter) hold pointers to the\n// *pebble.Iterator's combinedIterState. This allows them to check whether or\n// not they must monitor for files containing range keys (!initialized), or not.\n//\n// When !initialized, low-level iterators watch for files containing range keys.\n// When one is discovered, they set triggered=true and key to the smallest\n// (forward direction) or largest (reverse direction) range key that's been\n// observed.\ntype combinedIterState struct {\n\t// key holds the smallest (forward direction) or largest (backward\n\t// direction) user key from a range key bound discovered during the iterator\n\t// operation that triggered the switch to combined iteration.\n\t//\n\t// Slices stored here must be stable. This is possible because callers pass\n\t// a Smallest/Largest bound from a fileMetadata, which are immutable. A key\n\t// slice's bytes must not be overwritten.\n\tkey         []byte\n\ttriggered   bool\n\tinitialized bool\n}\n\n// Assert that *lazyCombinedIter implements internalIterator.\nvar _ internalIterator = (*lazyCombinedIter)(nil)\n\n// initCombinedIteration is invoked after a pointIter positioning operation\n// resulted in i.combinedIterState.triggered=true.\n//\n// The `dir` parameter is `+1` or `-1` indicating forward iteration or backward\n// iteration respectively.\n//\n// The `pointKey` and `pointValue` parameters provide the new point key-value\n// pair that the iterator was just positioned to. The combined iterator should\n// be seeded with this point key-value pair and return the smaller (forward\n// iteration) or largest (backward iteration) of the two.\n//\n// The `seekKey` parameter is non-nil only if the iterator operation that\n// triggered the switch to combined iteration was a SeekGE, SeekPrefixGE or\n// SeekLT. It provides the seek key supplied and is used to seek the range-key\n// iterator using the same key. This is necessary for SeekGE/SeekPrefixGE\n// operations that land in the middle of a range key and must truncate to the\n// user-provided seek key.\nfunc (i *lazyCombinedIter) initCombinedIteration(\n\tdir int8, pointKV *base.InternalKV, seekKey []byte,\n) *base.InternalKV {\n\t// Invariant: i.parent.rangeKey is nil.\n\t// Invariant: !i.combinedIterState.initialized.\n\tif invariants.Enabled {\n\t\tif i.combinedIterState.initialized {\n\t\t\tpanic(\"pebble: combined iterator already initialized\")\n\t\t}\n\t\tif i.parent.rangeKey != nil {\n\t\t\tpanic(\"pebble: iterator already has a range-key iterator stack\")\n\t\t}\n\t}\n\n\t// We need to determine the key to seek the range key iterator to. If\n\t// seekKey is not nil, the user-initiated operation that triggered the\n\t// switch to combined iteration was itself a seek, and we can use that key.\n\t// Otherwise, a First/Last or relative positioning operation triggered the\n\t// switch to combined iteration.\n\t//\n\t// The levelIter that observed a file containing range keys populated\n\t// combinedIterState.key with the smallest (forward) or largest (backward)\n\t// range key it observed. If multiple levelIters observed files with range\n\t// keys during the same operation on the mergingIter, combinedIterState.key\n\t// is the smallest [during forward iteration; largest in reverse iteration]\n\t// such key.\n\tif seekKey == nil {\n\t\t// Use the levelIter-populated key.\n\t\tseekKey = i.combinedIterState.key\n\n\t\t// We may need to adjust the levelIter-populated seek key to the\n\t\t// surfaced point key. If the key observed is beyond [in the iteration\n\t\t// direction] the current point key, there may still exist a range key\n\t\t// at an earlier key. Consider the following example:\n\t\t//\n\t\t//   L5:  000003:[bar.DEL.5, foo.RANGEKEYSET.9]\n\t\t//   L6:  000001:[bar.SET.2] 000002:[bax.RANGEKEYSET.8]\n\t\t//\n\t\t// A call to First() seeks the levels to files L5.000003 and L6.000001.\n\t\t// The L5 levelIter observes that L5.000003 contains the range key with\n\t\t// start key `foo`, and triggers a switch to combined iteration, setting\n\t\t// `combinedIterState.key` = `foo`.\n\t\t//\n\t\t// The L6 levelIter did not observe the true first range key\n\t\t// (bax.RANGEKEYSET.8), because it appears in a later sstable. When the\n\t\t// combined iterator is initialized, the range key iterator must be\n\t\t// seeked to a key that will find `bax`. To accomplish this, we seek the\n\t\t// key instead to `bar`. It is guaranteed that no range key exists\n\t\t// earlier than `bar`, otherwise a levelIter would've observed it and\n\t\t// set `combinedIterState.key` to its start key.\n\t\tif pointKV != nil {\n\t\t\tif dir == +1 && i.parent.cmp(i.combinedIterState.key, pointKV.K.UserKey) > 0 {\n\t\t\t\tseekKey = pointKV.K.UserKey\n\t\t\t} else if dir == -1 && i.parent.cmp(seekKey, pointKV.K.UserKey) < 0 {\n\t\t\t\tseekKey = pointKV.K.UserKey\n\t\t\t}\n\t\t}\n\t}\n\n\t// An operation on the point iterator observed a file containing range keys,\n\t// so we must switch to combined interleaving iteration. First, construct\n\t// the range key iterator stack. It must not exist, otherwise we'd already\n\t// be performing combined iteration.\n\ti.parent.rangeKey = iterRangeKeyStateAllocPool.Get().(*iteratorRangeKeyState)\n\ti.parent.rangeKey.init(i.parent.comparer.Compare, i.parent.comparer.Split, &i.parent.opts)\n\ti.parent.constructRangeKeyIter()\n\n\t// Initialize the Iterator's interleaving iterator.\n\ti.parent.rangeKey.iiter.Init(\n\t\t&i.parent.comparer, i.parent.pointIter, i.parent.rangeKey.rangeKeyIter,\n\t\tkeyspan.InterleavingIterOpts{\n\t\t\tMask:       &i.parent.rangeKeyMasking,\n\t\t\tLowerBound: i.parent.opts.LowerBound,\n\t\t\tUpperBound: i.parent.opts.UpperBound,\n\t\t})\n\n\t// Set the parent's primary iterator to point to the combined, interleaving\n\t// iterator that's now initialized with our current state.\n\ti.parent.iter = &i.parent.rangeKey.iiter\n\ti.combinedIterState.initialized = true\n\ti.combinedIterState.key = nil\n\n\t// All future iterator operations will go directly through the combined\n\t// iterator.\n\t//\n\t// Initialize the interleaving iterator. We pass the point key-value pair so\n\t// that the interleaving iterator knows where the point iterator is\n\t// positioned. Additionally, we pass the seek key to which the range-key\n\t// iterator should be seeked in order to initialize its position.\n\t//\n\t// In the forward direction (invert for backwards), the seek key is a key\n\t// guaranteed to find the smallest range key that's greater than the last\n\t// key the iterator returned. The range key may be less than pointKV, in\n\t// which case the range key will be interleaved next instead of the point\n\t// key.\n\tif dir == +1 {\n\t\tvar prefix []byte\n\t\tif i.parent.hasPrefix {\n\t\t\tprefix = i.parent.prefixOrFullSeekKey\n\t\t}\n\t\treturn i.parent.rangeKey.iiter.InitSeekGE(prefix, seekKey, pointKV)\n\t}\n\treturn i.parent.rangeKey.iiter.InitSeekLT(seekKey, pointKV)\n}\n\nfunc (i *lazyCombinedIter) SeekGE(key []byte, flags base.SeekGEFlags) *base.InternalKV {\n\tif i.combinedIterState.initialized {\n\t\treturn i.parent.rangeKey.iiter.SeekGE(key, flags)\n\t}\n\tkv := i.pointIter.SeekGE(key, flags)\n\tif i.combinedIterState.triggered {\n\t\treturn i.initCombinedIteration(+1, kv, key)\n\t}\n\treturn kv\n}\n\nfunc (i *lazyCombinedIter) SeekPrefixGE(\n\tprefix, key []byte, flags base.SeekGEFlags,\n) *base.InternalKV {\n\tif i.combinedIterState.initialized {\n\t\treturn i.parent.rangeKey.iiter.SeekPrefixGE(prefix, key, flags)\n\t}\n\tkv := i.pointIter.SeekPrefixGE(prefix, key, flags)\n\tif i.combinedIterState.triggered {\n\t\treturn i.initCombinedIteration(+1, kv, key)\n\t}\n\treturn kv\n}\n\nfunc (i *lazyCombinedIter) SeekLT(key []byte, flags base.SeekLTFlags) *base.InternalKV {\n\tif i.combinedIterState.initialized {\n\t\treturn i.parent.rangeKey.iiter.SeekLT(key, flags)\n\t}\n\tkv := i.pointIter.SeekLT(key, flags)\n\tif i.combinedIterState.triggered {\n\t\treturn i.initCombinedIteration(-1, kv, key)\n\t}\n\treturn kv\n}\n\nfunc (i *lazyCombinedIter) First() *base.InternalKV {\n\tif i.combinedIterState.initialized {\n\t\treturn i.parent.rangeKey.iiter.First()\n\t}\n\tkv := i.pointIter.First()\n\tif i.combinedIterState.triggered {\n\t\treturn i.initCombinedIteration(+1, kv, nil)\n\t}\n\treturn kv\n}\n\nfunc (i *lazyCombinedIter) Last() *base.InternalKV {\n\tif i.combinedIterState.initialized {\n\t\treturn i.parent.rangeKey.iiter.Last()\n\t}\n\tkv := i.pointIter.Last()\n\tif i.combinedIterState.triggered {\n\t\treturn i.initCombinedIteration(-1, kv, nil)\n\t}\n\treturn kv\n}\n\nfunc (i *lazyCombinedIter) Next() *base.InternalKV {\n\tif i.combinedIterState.initialized {\n\t\treturn i.parent.rangeKey.iiter.Next()\n\t}\n\tkv := i.pointIter.Next()\n\tif i.combinedIterState.triggered {\n\t\treturn i.initCombinedIteration(+1, kv, nil)\n\t}\n\treturn kv\n}\n\nfunc (i *lazyCombinedIter) NextPrefix(succKey []byte) *base.InternalKV {\n\tif i.combinedIterState.initialized {\n\t\treturn i.parent.rangeKey.iiter.NextPrefix(succKey)\n\t}\n\tkv := i.pointIter.NextPrefix(succKey)\n\tif i.combinedIterState.triggered {\n\t\treturn i.initCombinedIteration(+1, kv, nil)\n\t}\n\treturn kv\n}\n\nfunc (i *lazyCombinedIter) Prev() *base.InternalKV {\n\tif i.combinedIterState.initialized {\n\t\treturn i.parent.rangeKey.iiter.Prev()\n\t}\n\tkv := i.pointIter.Prev()\n\tif i.combinedIterState.triggered {\n\t\treturn i.initCombinedIteration(-1, kv, nil)\n\t}\n\treturn kv\n}\n\nfunc (i *lazyCombinedIter) Error() error {\n\tif i.combinedIterState.initialized {\n\t\treturn i.parent.rangeKey.iiter.Error()\n\t}\n\treturn i.pointIter.Error()\n}\n\nfunc (i *lazyCombinedIter) Close() error {\n\tif i.combinedIterState.initialized {\n\t\treturn i.parent.rangeKey.iiter.Close()\n\t}\n\treturn i.pointIter.Close()\n}\n\nfunc (i *lazyCombinedIter) SetBounds(lower, upper []byte) {\n\tif i.combinedIterState.initialized {\n\t\ti.parent.rangeKey.iiter.SetBounds(lower, upper)\n\t\treturn\n\t}\n\ti.pointIter.SetBounds(lower, upper)\n}\n\nfunc (i *lazyCombinedIter) SetContext(ctx context.Context) {\n\tif i.combinedIterState.initialized {\n\t\ti.parent.rangeKey.iiter.SetContext(ctx)\n\t\treturn\n\t}\n\ti.pointIter.SetContext(ctx)\n}\n\n// DebugTree is part of the InternalIterator interface.\nfunc (i *lazyCombinedIter) DebugTree(tp treeprinter.Node) {\n\tn := tp.Childf(\"%T(%p)\", i, i)\n\tif i.combinedIterState.initialized {\n\t\ti.parent.rangeKey.iiter.DebugTree(n)\n\t} else {\n\t\ti.pointIter.DebugTree(n)\n\t}\n}\n\nfunc (i *lazyCombinedIter) String() string {\n\tif i.combinedIterState.initialized {\n\t\treturn i.parent.rangeKey.iiter.String()\n\t}\n\treturn i.pointIter.String()\n}\n"
        },
        {
          "name": "rangekey",
          "type": "tree",
          "content": null
        },
        {
          "name": "read_compaction_queue.go",
          "type": "blob",
          "size": 2.5810546875,
          "content": "package pebble\n\nimport \"github.com/cockroachdb/pebble/internal/base\"\n\n// The maximum number of elements in the readCompactions queue.\n// We want to limit the number of elements so that we only do\n// compactions for ranges which are being read recently.\nconst readCompactionMaxQueueSize = 5\n\n// The readCompactionQueue is a queue of read compactions with\n// 0 overlapping ranges.\ntype readCompactionQueue struct {\n\t// Invariant: A contiguous prefix of the queue contains\n\t// all the elements in the queue, in order of insertion.\n\t// When we remove duplicates from the queue, we break\n\t// the invariant that a contiguous prefix of the queue\n\t// has all the elements in it. To fix this, we shift\n\t// the elements of the queue to the left. This is cheap\n\t// because the queue has a max length of 5.\n\tqueue [readCompactionMaxQueueSize]*readCompaction\n\n\t// The size of the queue which is occupied.\n\t// A size of k, implies that the first k elements\n\t// of the queue are occupied.\n\t// The size will be <= readCompactionMaxQueueSize.\n\tsize int\n}\n\n// combine should be used to combine an older queue with a newer\n// queue.\nfunc (qu *readCompactionQueue) combine(newQu *readCompactionQueue, cmp base.Compare) {\n\n\tfor i := 0; i < newQu.size; i++ {\n\t\tqu.add(newQu.queue[i], cmp)\n\t}\n}\n\n// add adds read compactions to the queue, while maintaining the invariant\n// that there are no overlapping ranges in the queue.\nfunc (qu *readCompactionQueue) add(rc *readCompaction, cmp base.Compare) {\n\tsz := qu.size\n\tfor i := 0; i < sz; i++ {\n\t\tleft := qu.queue[i]\n\t\tright := rc\n\t\tif cmp(left.start, right.start) > 0 {\n\t\t\tleft, right = right, left\n\t\t}\n\t\tif cmp(right.start, left.end) <= 0 {\n\t\t\tqu.queue[i] = nil\n\t\t\tqu.size--\n\t\t}\n\t}\n\n\t// Get rid of the holes which may have been formed\n\t// in the queue.\n\tqu.shiftLeft()\n\n\tif qu.size == readCompactionMaxQueueSize {\n\t\t// Make space at the end.\n\t\tcopy(qu.queue[0:], qu.queue[1:])\n\t\tqu.queue[qu.size-1] = rc\n\t} else {\n\t\tqu.size++\n\t\tqu.queue[qu.size-1] = rc\n\t}\n}\n\n// Shifts the non-nil elements of the queue to the left so\n// that a continguous prefix of the queue is non-nil.\nfunc (qu *readCompactionQueue) shiftLeft() {\n\tnilPos := -1\n\tfor i := 0; i < readCompactionMaxQueueSize; i++ {\n\t\tif qu.queue[i] == nil && nilPos == -1 {\n\t\t\tnilPos = i\n\t\t} else if qu.queue[i] != nil && nilPos != -1 {\n\t\t\tqu.queue[nilPos] = qu.queue[i]\n\t\t\tqu.queue[i] = nil\n\t\t\tnilPos++\n\t\t}\n\t}\n}\n\n// remove will remove the oldest element from the queue.\nfunc (qu *readCompactionQueue) remove() *readCompaction {\n\tif qu.size == 0 {\n\t\treturn nil\n\t}\n\n\tc := qu.queue[0]\n\tcopy(qu.queue[0:], qu.queue[1:])\n\tqu.queue[qu.size-1] = nil\n\tqu.size--\n\treturn c\n}\n"
        },
        {
          "name": "read_state.go",
          "type": "blob",
          "size": 3.1513671875,
          "content": "// Copyright 2019 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport \"sync/atomic\"\n\n// readState encapsulates the state needed for reading (the current version and\n// list of memtables). Loading the readState is done without grabbing\n// DB.mu. Instead, a separate DB.readState.RWMutex is used for\n// synchronization. This mutex solely covers the current readState object which\n// means it is rarely or ever contended.\n//\n// Note that various fancy lock-free mechanisms can be imagined for loading the\n// readState, but benchmarking showed the ones considered to purely be\n// pessimizations. The RWMutex version is a single atomic increment for the\n// RLock and an atomic decrement for the RUnlock. It is difficult to do better\n// than that without something like thread-local storage which isn't available\n// in Go.\ntype readState struct {\n\tdb        *DB\n\trefcnt    atomic.Int32\n\tcurrent   *version\n\tmemtables flushableList\n}\n\n// ref adds a reference to the readState.\nfunc (s *readState) ref() {\n\ts.refcnt.Add(1)\n}\n\n// unref removes a reference to the readState. If this was the last reference,\n// the reference the readState holds on the version is released. Requires DB.mu\n// is NOT held as version.unref() will acquire it. See unrefLocked() if DB.mu\n// is held by the caller.\nfunc (s *readState) unref() {\n\tif s.refcnt.Add(-1) != 0 {\n\t\treturn\n\t}\n\ts.current.Unref()\n\tfor _, mem := range s.memtables {\n\t\tmem.readerUnref(true)\n\t}\n\n\t// The last reference to the readState was released. Check to see if there\n\t// are new obsolete tables to delete.\n\ts.db.maybeScheduleObsoleteTableDeletion()\n}\n\n// unrefLocked removes a reference to the readState. If this was the last\n// reference, the reference the readState holds on the version is\n// released.\n//\n// DB.mu must be held. See unref() if DB.mu is NOT held by the caller.\nfunc (s *readState) unrefLocked() {\n\tif s.refcnt.Add(-1) != 0 {\n\t\treturn\n\t}\n\ts.current.UnrefLocked()\n\tfor _, mem := range s.memtables {\n\t\tmem.readerUnrefLocked(true)\n\t}\n\n\t// In this code path, the caller is responsible for scheduling obsolete table\n\t// deletion as necessary.\n}\n\n// loadReadState returns the current readState. The returned readState must be\n// unreferenced when the caller is finished with it.\nfunc (d *DB) loadReadState() *readState {\n\td.readState.RLock()\n\tstate := d.readState.val\n\tstate.ref()\n\td.readState.RUnlock()\n\treturn state\n}\n\n// updateReadStateLocked creates a new readState from the current version and\n// list of memtables. Requires DB.mu is held. If checker is not nil, it is\n// called after installing the new readState.\nfunc (d *DB) updateReadStateLocked(checker func(*DB) error) {\n\ts := &readState{\n\t\tdb:        d,\n\t\tcurrent:   d.mu.versions.currentVersion(),\n\t\tmemtables: d.mu.mem.queue,\n\t}\n\ts.refcnt.Store(1)\n\ts.current.Ref()\n\tfor _, mem := range s.memtables {\n\t\tmem.readerRef()\n\t}\n\n\td.readState.Lock()\n\told := d.readState.val\n\td.readState.val = s\n\td.readState.Unlock()\n\tif checker != nil {\n\t\tif err := checker(d); err != nil {\n\t\t\td.opts.Logger.Fatalf(\"checker failed with error: %s\", err)\n\t\t}\n\t}\n\tif old != nil {\n\t\told.unrefLocked()\n\t}\n}\n"
        },
        {
          "name": "read_state_test.go",
          "type": "blob",
          "size": 0.90234375,
          "content": "// Copyright 2019 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"fmt\"\n\t\"math/rand/v2\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/pebble/vfs\"\n)\n\nfunc BenchmarkReadState(b *testing.B) {\n\td, err := Open(\"\", &Options{\n\t\tFS: vfs.NewMem(),\n\t})\n\tif err != nil {\n\t\tb.Fatal(err)\n\t}\n\n\tfor _, updateFrac := range []float32{0, 0.1, 0.5} {\n\t\tb.Run(fmt.Sprintf(\"updates=%.0f\", updateFrac*100), func(b *testing.B) {\n\t\t\tb.RunParallel(func(pb *testing.PB) {\n\t\t\t\trng := rand.New(rand.NewPCG(0, uint64(time.Now().UnixNano())))\n\n\t\t\t\tfor pb.Next() {\n\t\t\t\t\tif rng.Float32() < updateFrac {\n\t\t\t\t\t\td.mu.Lock()\n\t\t\t\t\t\td.updateReadStateLocked(nil)\n\t\t\t\t\t\td.mu.Unlock()\n\t\t\t\t\t} else {\n\t\t\t\t\t\ts := d.loadReadState()\n\t\t\t\t\t\ts.unref()\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t})\n\t\t})\n\t}\n\n\tif err := d.Close(); err != nil {\n\t\tb.Fatal(err)\n\t}\n}\n"
        },
        {
          "name": "record",
          "type": "tree",
          "content": null
        },
        {
          "name": "replay",
          "type": "tree",
          "content": null
        },
        {
          "name": "scan_internal.go",
          "type": "blob",
          "size": 39.865234375,
          "content": "// Copyright 2023 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"slices\"\n\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/invariants\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan/keyspanimpl\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n\t\"github.com/cockroachdb/pebble/internal/treeprinter\"\n\t\"github.com/cockroachdb/pebble/objstorage\"\n\t\"github.com/cockroachdb/pebble/objstorage/remote\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n)\n\nconst (\n\t// In skip-shared iteration mode, keys in levels greater than\n\t// sharedLevelsStart (i.e. lower in the LSM) are skipped. Keys\n\t// in sharedLevelsStart are returned iff they are not in a\n\t// shared file.\n\tsharedLevelsStart = remote.SharedLevelsStart\n\n\t// In skip-external iteration mode, keys in levels greater\n\t// than externalSkipStart are skipped. Keys in\n\t// externalSkipStart are returned iff they are not in an\n\t// external file.\n\texternalSkipStart = 6\n)\n\n// ErrInvalidSkipSharedIteration is returned by ScanInternal if it was called\n// with a shared file visitor function, and a file in a shareable level (i.e.\n// level >= sharedLevelsStart) was found to not be in shared storage according\n// to objstorage.Provider, or not shareable for another reason such as for\n// containing keys newer than the snapshot sequence number.\nvar ErrInvalidSkipSharedIteration = errors.New(\"pebble: cannot use skip-shared iteration due to non-shareable files in lower levels\")\n\n// SharedSSTMeta represents an sstable on shared storage that can be ingested\n// by another pebble instance. This struct must contain all fields that are\n// required for a Pebble instance to ingest a foreign sstable on shared storage,\n// including constructing any relevant objstorage.Provider / remoteobjcat.Catalog\n// data structures, as well as creating virtual FileMetadatas.\n//\n// Note that the Pebble instance creating and returning a SharedSSTMeta might\n// not be the one that created the underlying sstable on shared storage to begin\n// with; it's possible for a Pebble instance to reshare an sstable that was\n// shared to it.\ntype SharedSSTMeta struct {\n\t// Backing is the shared object underlying this SST. Can be attached to an\n\t// objstorage.Provider.\n\tBacking objstorage.RemoteObjectBackingHandle\n\n\t// Smallest and Largest internal keys for the overall bounds. The kind and\n\t// SeqNum of these will reflect what is physically present on the source Pebble\n\t// instance's view of the sstable; it's up to the ingesting instance to set the\n\t// sequence number in the trailer to match the read-time sequence numbers\n\t// reserved for the level this SST is being ingested into. The Kind is expected\n\t// to remain unchanged by the ingesting instance.\n\t//\n\t// Note that these bounds could be narrower than the bounds of the underlying\n\t// sstable; ScanInternal is expected to truncate sstable bounds to the user key\n\t// bounds passed into that method.\n\tSmallest, Largest InternalKey\n\n\t// SmallestRangeKey and LargestRangeKey are internal keys that denote the\n\t// range key bounds of this sstable. Must lie within [Smallest, Largest].\n\tSmallestRangeKey, LargestRangeKey InternalKey\n\n\t// SmallestPointKey and LargestPointKey are internal keys that denote the\n\t// point key bounds of this sstable. Must lie within [Smallest, Largest].\n\tSmallestPointKey, LargestPointKey InternalKey\n\n\t// Level denotes the level at which this file was present at read time.\n\t// For files visited by ScanInternal, this value will only be 5 or 6.\n\tLevel uint8\n\n\t// Size contains an estimate of the size of this sstable.\n\tSize uint64\n\n\t// fileNum at time of creation in the creator instance. Only used for\n\t// debugging/tests.\n\tfileNum base.FileNum\n}\n\nfunc (s *SharedSSTMeta) cloneFromFileMeta(f *fileMetadata) {\n\t*s = SharedSSTMeta{\n\t\tSmallest:         f.Smallest.Clone(),\n\t\tLargest:          f.Largest.Clone(),\n\t\tSmallestRangeKey: f.SmallestRangeKey.Clone(),\n\t\tLargestRangeKey:  f.LargestRangeKey.Clone(),\n\t\tSmallestPointKey: f.SmallestPointKey.Clone(),\n\t\tLargestPointKey:  f.LargestPointKey.Clone(),\n\t\tSize:             f.Size,\n\t\tfileNum:          f.FileNum,\n\t}\n}\n\ntype sharedByLevel []SharedSSTMeta\n\nfunc (s sharedByLevel) Len() int           { return len(s) }\nfunc (s sharedByLevel) Swap(i, j int)      { s[i], s[j] = s[j], s[i] }\nfunc (s sharedByLevel) Less(i, j int) bool { return s[i].Level < s[j].Level }\n\ntype pcIterPos int\n\nconst (\n\tpcIterPosCur pcIterPos = iota\n\tpcIterPosNext\n)\n\n// pointCollapsingIterator is an internalIterator that collapses point keys and\n// returns at most one point internal key for each user key. Merges and\n// SingleDels are not supported and result in a panic if encountered. Point keys\n// deleted by rangedels are considered shadowed and not exposed.\n//\n// Only used in ScanInternal to return at most one internal key per user key.\ntype pointCollapsingIterator struct {\n\titer     keyspan.InterleavingIter\n\tpos      pcIterPos\n\tcomparer *base.Comparer\n\tmerge    base.Merge\n\terr      error\n\tseqNum   base.SeqNum\n\t// The current position of `iter`. Always owned by the underlying iter.\n\titerKV *base.InternalKV\n\t// The last saved key. findNextEntry and similar methods are expected to save\n\t// the current value of iterKey to savedKey if they're iterating away from the\n\t// current key but still need to retain it. See comments in findNextEntry on\n\t// how this field is used.\n\t//\n\t// At the end of a positioning call:\n\t//  - if pos == pcIterPosNext, iterKey is pointing to the next user key owned\n\t//    by `iter` while savedKey is holding a copy to our current key.\n\t//  - If pos == pcIterPosCur, iterKey is pointing to an `iter`-owned current\n\t//    key, and savedKey is either undefined or pointing to a version of the\n\t//    current key owned by this iterator (i.e. backed by savedKeyBuf).\n\tsavedKey    InternalKey\n\tsavedKeyBuf []byte\n\t// If fixedSeqNum is non-zero, all emitted points are verified to have this\n\t// fixed sequence number.\n\tfixedSeqNum base.SeqNum\n}\n\nfunc (p *pointCollapsingIterator) Span() *keyspan.Span {\n\treturn p.iter.Span()\n}\n\n// SeekPrefixGE implements the InternalIterator interface.\nfunc (p *pointCollapsingIterator) SeekPrefixGE(\n\tprefix, key []byte, flags base.SeekGEFlags,\n) *base.InternalKV {\n\tp.resetKey()\n\tp.iterKV = p.iter.SeekPrefixGE(prefix, key, flags)\n\tp.pos = pcIterPosCur\n\tif p.iterKV == nil {\n\t\treturn nil\n\t}\n\treturn p.findNextEntry()\n}\n\n// SeekGE implements the InternalIterator interface.\nfunc (p *pointCollapsingIterator) SeekGE(key []byte, flags base.SeekGEFlags) *base.InternalKV {\n\tp.resetKey()\n\tp.iterKV = p.iter.SeekGE(key, flags)\n\tp.pos = pcIterPosCur\n\tif p.iterKV == nil {\n\t\treturn nil\n\t}\n\treturn p.findNextEntry()\n}\n\n// SeekLT implements the InternalIterator interface.\nfunc (p *pointCollapsingIterator) SeekLT(key []byte, flags base.SeekLTFlags) *base.InternalKV {\n\tpanic(\"unimplemented\")\n}\n\nfunc (p *pointCollapsingIterator) resetKey() {\n\tp.savedKey.UserKey = p.savedKeyBuf[:0]\n\tp.savedKey.Trailer = 0\n\tp.iterKV = nil\n\tp.pos = pcIterPosCur\n}\n\nfunc (p *pointCollapsingIterator) verifySeqNum(kv *base.InternalKV) *base.InternalKV {\n\tif !invariants.Enabled {\n\t\treturn kv\n\t}\n\tif p.fixedSeqNum == 0 || kv == nil || kv.Kind() == InternalKeyKindRangeDelete {\n\t\treturn kv\n\t}\n\tif kv.SeqNum() != p.fixedSeqNum {\n\t\tpanic(fmt.Sprintf(\"expected foreign point key to have seqnum %d, got %d\", p.fixedSeqNum, kv.SeqNum()))\n\t}\n\treturn kv\n}\n\n// findNextEntry is called to return the next key. p.iter must be positioned at the\n// start of the first user key we are interested in.\nfunc (p *pointCollapsingIterator) findNextEntry() *base.InternalKV {\n\tp.saveKey()\n\t// Saves a comparison in the fast path\n\tfirstIteration := true\n\tfor p.iterKV != nil {\n\t\t// NB: p.savedKey is either the current key (iff p.iterKV == firstKey),\n\t\t// or the previous key.\n\t\tif !firstIteration && !p.comparer.Equal(p.iterKV.K.UserKey, p.savedKey.UserKey) {\n\t\t\tp.saveKey()\n\t\t\tcontinue\n\t\t}\n\t\tfirstIteration = false\n\t\tif s := p.iter.Span(); s != nil && s.CoversAt(p.seqNum, p.iterKV.SeqNum()) {\n\t\t\t// All future keys for this user key must be deleted.\n\t\t\tif p.savedKey.Kind() == InternalKeyKindSingleDelete {\n\t\t\t\tpanic(\"cannot process singledel key in point collapsing iterator\")\n\t\t\t}\n\t\t\t// Fast forward to the next user key.\n\t\t\tp.saveKey()\n\t\t\tp.iterKV = p.iter.Next()\n\t\t\tfor p.iterKV != nil && p.savedKey.SeqNum() >= p.iterKV.SeqNum() && p.comparer.Equal(p.iterKV.K.UserKey, p.savedKey.UserKey) {\n\t\t\t\tp.iterKV = p.iter.Next()\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tswitch p.savedKey.Kind() {\n\t\tcase InternalKeyKindSet, InternalKeyKindDelete, InternalKeyKindSetWithDelete, InternalKeyKindDeleteSized:\n\t\t\t// Note that we return SETs directly, even if they would otherwise get\n\t\t\t// compacted into a Del to turn into a SetWithDelete. This is a fast\n\t\t\t// path optimization that can break SINGLEDEL determinism. To lead to\n\t\t\t// consistent SINGLEDEL behaviour, this iterator should *not* be used for\n\t\t\t// a keyspace where SINGLEDELs could be in use. If this iterator observes\n\t\t\t// a SINGLEDEL as the first internal key for a user key, it will panic.\n\t\t\t//\n\t\t\t// As p.value is a lazy value owned by the child iterator, we can thread\n\t\t\t// it through without loading it into p.valueBuf.\n\t\t\t//\n\t\t\t// TODO(bilal): We can even avoid saving the key in this fast path if\n\t\t\t// we are in a block where setHasSamePrefix = false in a v3 sstable,\n\t\t\t// guaranteeing that there's only one internal key for each user key.\n\t\t\t// Thread this logic through the sstable iterators and/or consider\n\t\t\t// collapsing (ha) this logic into the sstable iterators that are aware\n\t\t\t// of blocks and can determine user key changes without doing key saves\n\t\t\t// or comparisons.\n\t\t\tp.pos = pcIterPosCur\n\t\t\treturn p.verifySeqNum(p.iterKV)\n\t\tcase InternalKeyKindSingleDelete:\n\t\t\t// Panic, as this iterator is not expected to observe single deletes.\n\t\t\tpanic(\"cannot process singledel key in point collapsing iterator\")\n\t\tcase InternalKeyKindMerge:\n\t\t\t// Panic, as this iterator is not expected to observe merges.\n\t\t\tpanic(\"cannot process merge key in point collapsing iterator\")\n\t\tcase InternalKeyKindRangeDelete:\n\t\t\t// These are interleaved by the interleaving iterator ahead of all points.\n\t\t\t// We should pass them as-is, but also account for any points ahead of\n\t\t\t// them.\n\t\t\tp.pos = pcIterPosCur\n\t\t\treturn p.verifySeqNum(p.iterKV)\n\t\tdefault:\n\t\t\tpanic(fmt.Sprintf(\"unexpected kind: %d\", p.iterKV.Kind()))\n\t\t}\n\t}\n\tp.resetKey()\n\treturn nil\n}\n\n// First implements the InternalIterator interface.\nfunc (p *pointCollapsingIterator) First() *base.InternalKV {\n\tp.resetKey()\n\tp.iterKV = p.iter.First()\n\tp.pos = pcIterPosCur\n\tif p.iterKV == nil {\n\t\treturn nil\n\t}\n\treturn p.findNextEntry()\n}\n\n// Last implements the InternalIterator interface.\nfunc (p *pointCollapsingIterator) Last() *base.InternalKV {\n\tpanic(\"unimplemented\")\n}\n\nfunc (p *pointCollapsingIterator) saveKey() {\n\tif p.iterKV == nil {\n\t\tp.savedKey = InternalKey{UserKey: p.savedKeyBuf[:0]}\n\t\treturn\n\t}\n\tp.savedKeyBuf = append(p.savedKeyBuf[:0], p.iterKV.K.UserKey...)\n\tp.savedKey = InternalKey{UserKey: p.savedKeyBuf, Trailer: p.iterKV.K.Trailer}\n}\n\n// Next implements the InternalIterator interface.\nfunc (p *pointCollapsingIterator) Next() *base.InternalKV {\n\tswitch p.pos {\n\tcase pcIterPosCur:\n\t\tp.saveKey()\n\t\tif p.iterKV != nil && p.iterKV.Kind() == InternalKeyKindRangeDelete {\n\t\t\t// Step over the interleaved range delete and process the very next\n\t\t\t// internal key, even if it's at the same user key. This is because a\n\t\t\t// point for that user key has not been returned yet.\n\t\t\tp.iterKV = p.iter.Next()\n\t\t\tbreak\n\t\t}\n\t\t// Fast forward to the next user key.\n\t\tkv := p.iter.Next()\n\t\t// p.iterKV.SeqNum() >= key.SeqNum() is an optimization that allows us to\n\t\t// use p.iterKV.SeqNum() < key.SeqNum() as a sign that the user key has\n\t\t// changed, without needing to do the full key comparison.\n\t\tfor kv != nil && p.savedKey.SeqNum() >= kv.SeqNum() &&\n\t\t\tp.comparer.Equal(p.savedKey.UserKey, kv.K.UserKey) {\n\t\t\tkv = p.iter.Next()\n\t\t}\n\t\tif kv == nil {\n\t\t\t// There are no keys to return.\n\t\t\tp.resetKey()\n\t\t\treturn nil\n\t\t}\n\t\tp.iterKV = kv\n\tcase pcIterPosNext:\n\t\tp.pos = pcIterPosCur\n\t}\n\tif p.iterKV == nil {\n\t\tp.resetKey()\n\t\treturn nil\n\t}\n\treturn p.findNextEntry()\n}\n\n// NextPrefix implements the InternalIterator interface.\nfunc (p *pointCollapsingIterator) NextPrefix(succKey []byte) *base.InternalKV {\n\tpanic(\"unimplemented\")\n}\n\n// Prev implements the InternalIterator interface.\nfunc (p *pointCollapsingIterator) Prev() *base.InternalKV {\n\tpanic(\"unimplemented\")\n}\n\n// Error implements the InternalIterator interface.\nfunc (p *pointCollapsingIterator) Error() error {\n\tif p.err != nil {\n\t\treturn p.err\n\t}\n\treturn p.iter.Error()\n}\n\n// Close implements the InternalIterator interface.\nfunc (p *pointCollapsingIterator) Close() error {\n\treturn p.iter.Close()\n}\n\n// SetBounds implements the InternalIterator interface.\nfunc (p *pointCollapsingIterator) SetBounds(lower, upper []byte) {\n\tp.resetKey()\n\tp.iter.SetBounds(lower, upper)\n}\n\nfunc (p *pointCollapsingIterator) SetContext(ctx context.Context) {\n\tp.iter.SetContext(ctx)\n}\n\n// DebugTree is part of the InternalIterator interface.\nfunc (p *pointCollapsingIterator) DebugTree(tp treeprinter.Node) {\n\tn := tp.Childf(\"%T(%p)\", p, p)\n\tp.iter.DebugTree(n)\n}\n\n// String implements the InternalIterator interface.\nfunc (p *pointCollapsingIterator) String() string {\n\treturn p.iter.String()\n}\n\nvar _ internalIterator = &pointCollapsingIterator{}\n\n// IteratorLevelKind is used to denote whether the current ScanInternal iterator\n// is unknown, belongs to a flushable, or belongs to an LSM level type.\ntype IteratorLevelKind int8\n\nconst (\n\t// IteratorLevelUnknown indicates an unknown LSM level.\n\tIteratorLevelUnknown IteratorLevelKind = iota\n\t// IteratorLevelLSM indicates an LSM level.\n\tIteratorLevelLSM\n\t// IteratorLevelFlushable indicates a flushable (i.e. memtable).\n\tIteratorLevelFlushable\n)\n\n// IteratorLevel is used with scanInternalIterator to surface additional iterator-specific info where possible.\n// Note: this is struct is only provided for point keys.\ntype IteratorLevel struct {\n\tKind IteratorLevelKind\n\t// FlushableIndex indicates the position within the flushable queue of this level.\n\t// Only valid if kind == IteratorLevelFlushable.\n\tFlushableIndex int\n\t// The level within the LSM. Only valid if Kind == IteratorLevelLSM.\n\tLevel int\n\t// Sublevel is only valid if Kind == IteratorLevelLSM and Level == 0.\n\tSublevel int\n}\n\n// scanInternalIterator is an iterator that returns all internal keys, including\n// tombstones. For instance, an InternalKeyKindDelete would be returned as an\n// InternalKeyKindDelete instead of the iterator skipping over to the next key.\n// Internal keys within a user key are collapsed, eg. if there are two SETs, the\n// one with the higher sequence is returned. Useful if an external user of Pebble\n// needs to observe and rebuild Pebble's history of internal keys, such as in\n// node-to-node replication. For use with {db,snapshot}.ScanInternal().\n//\n// scanInternalIterator is expected to ignore point keys deleted by range\n// deletions, and range keys shadowed by a range key unset or delete, however it\n// *must* return the range delete as well as the range key unset/delete that did\n// the shadowing.\ntype scanInternalIterator struct {\n\tctx             context.Context\n\tdb              *DB\n\topts            scanInternalOptions\n\tcomparer        *base.Comparer\n\tmerge           Merge\n\titer            internalIterator\n\treadState       *readState\n\tversion         *version\n\trangeKey        *iteratorRangeKeyState\n\tpointKeyIter    internalIterator\n\titerKV          *base.InternalKV\n\talloc           *iterAlloc\n\tnewIters        tableNewIters\n\tnewIterRangeKey keyspanimpl.TableNewSpanIter\n\tseqNum          base.SeqNum\n\titerLevels      []IteratorLevel\n\tmergingIter     *mergingIter\n\n\t// boundsBuf holds two buffers used to store the lower and upper bounds.\n\t// Whenever the InternalIterator's bounds change, the new bounds are copied\n\t// into boundsBuf[boundsBufIdx]. The two bounds share a slice to reduce\n\t// allocations. opts.LowerBound and opts.UpperBound point into this slice.\n\tboundsBuf    [2][]byte\n\tboundsBufIdx int\n}\n\n// truncateExternalFile truncates an External file's [SmallestUserKey,\n// LargestUserKey] fields to [lower, upper). A ExternalFile is\n// produced that is suitable for external consumption by other Pebble\n// instances.\n//\n// truncateSharedFile reads the file to try to create the smallest\n// possible bounds.  Here, we blindly truncate them. This may mean we\n// include this SST in iterations it isn't really needed in. Since we\n// don't expect External files to be long-lived in the pebble\n// instance, We think this is OK.\n//\n// TODO(ssd) 2024-01-26: Potentially de-duplicate with\n// truncateSharedFile.\nfunc (d *DB) truncateExternalFile(\n\tctx context.Context,\n\tlower, upper []byte,\n\tlevel int,\n\tfile *fileMetadata,\n\tobjMeta objstorage.ObjectMetadata,\n) (*ExternalFile, error) {\n\tcmp := d.cmp\n\tsst := &ExternalFile{\n\t\tLevel:           uint8(level),\n\t\tObjName:         objMeta.Remote.CustomObjectName,\n\t\tLocator:         objMeta.Remote.Locator,\n\t\tHasPointKey:     file.HasPointKeys,\n\t\tHasRangeKey:     file.HasRangeKeys,\n\t\tSize:            file.Size,\n\t\tSyntheticPrefix: slices.Clone(file.SyntheticPrefixAndSuffix.Prefix()),\n\t\tSyntheticSuffix: slices.Clone(file.SyntheticPrefixAndSuffix.Suffix()),\n\t}\n\n\tneedsLowerTruncate := cmp(lower, file.Smallest.UserKey) > 0\n\tif needsLowerTruncate {\n\t\tsst.StartKey = slices.Clone(lower)\n\t} else {\n\t\tsst.StartKey = slices.Clone(file.Smallest.UserKey)\n\t}\n\n\tcmpUpper := cmp(upper, file.Largest.UserKey)\n\tneedsUpperTruncate := cmpUpper < 0\n\tif needsUpperTruncate {\n\t\tsst.EndKey = slices.Clone(upper)\n\t\tsst.EndKeyIsInclusive = false\n\t} else {\n\t\tsst.EndKey = slices.Clone(file.Largest.UserKey)\n\t\tsst.EndKeyIsInclusive = !file.Largest.IsExclusiveSentinel()\n\t}\n\n\tif cmp(sst.StartKey, sst.EndKey) > 0 {\n\t\treturn nil, base.AssertionFailedf(\"pebble: invalid external file bounds after truncation [%q, %q)\", sst.StartKey, sst.EndKey)\n\t}\n\n\tif cmp(sst.StartKey, sst.EndKey) == 0 && !sst.EndKeyIsInclusive {\n\t\treturn nil, base.AssertionFailedf(\"pebble: invalid external file bounds after truncation [%q, %q)\", sst.StartKey, sst.EndKey)\n\t}\n\n\treturn sst, nil\n}\n\n// truncateSharedFile truncates a shared file's [Smallest, Largest] fields to\n// [lower, upper), potentially opening iterators on the file to find keys within\n// the requested bounds. A SharedSSTMeta is produced that is suitable for\n// external consumption by other Pebble instances. If shouldSkip is true, this\n// file does not contain any keys in [lower, upper) and can be skipped.\n//\n// TODO(bilal): If opening iterators and doing reads in this method is too\n// inefficient, consider producing non-tight file bounds instead.\nfunc (d *DB) truncateSharedFile(\n\tctx context.Context,\n\tlower, upper []byte,\n\tlevel int,\n\tfile *fileMetadata,\n\tobjMeta objstorage.ObjectMetadata,\n) (sst *SharedSSTMeta, shouldSkip bool, err error) {\n\tcmp := d.cmp\n\tsst = &SharedSSTMeta{}\n\tsst.cloneFromFileMeta(file)\n\tsst.Level = uint8(level)\n\tsst.Backing, err = d.objProvider.RemoteObjectBacking(&objMeta)\n\tif err != nil {\n\t\treturn nil, false, err\n\t}\n\tneedsLowerTruncate := cmp(lower, file.Smallest.UserKey) > 0\n\tneedsUpperTruncate := cmp(upper, file.Largest.UserKey) < 0 || (cmp(upper, file.Largest.UserKey) == 0 && !file.Largest.IsExclusiveSentinel())\n\t// Fast path: file is entirely within [lower, upper).\n\tif !needsLowerTruncate && !needsUpperTruncate {\n\t\treturn sst, false, nil\n\t}\n\n\t// We will need to truncate file bounds in at least one direction. Open all\n\t// relevant iterators.\n\titers, err := d.newIters(ctx, file, &IterOptions{\n\t\tLowerBound: lower,\n\t\tUpperBound: upper,\n\t\tlayer:      manifest.Level(level),\n\t}, internalIterOpts{}, iterPointKeys|iterRangeDeletions|iterRangeKeys)\n\tif err != nil {\n\t\treturn nil, false, err\n\t}\n\tdefer iters.CloseAll()\n\titer := iters.point\n\trangeDelIter := iters.rangeDeletion\n\trangeKeyIter := iters.rangeKey\n\tif rangeDelIter != nil {\n\t\trangeDelIter = keyspan.Truncate(cmp, rangeDelIter, base.UserKeyBoundsEndExclusive(lower, upper))\n\t}\n\tif rangeKeyIter != nil {\n\t\trangeKeyIter = keyspan.Truncate(cmp, rangeKeyIter, base.UserKeyBoundsEndExclusive(lower, upper))\n\t}\n\t// Check if we need to truncate on the left side. This means finding a new\n\t// LargestPointKey and LargestRangeKey that is >= lower.\n\tif needsLowerTruncate {\n\t\tsst.SmallestPointKey.UserKey = sst.SmallestPointKey.UserKey[:0]\n\t\tsst.SmallestPointKey.Trailer = 0\n\t\tkv := iter.SeekGE(lower, base.SeekGEFlagsNone)\n\t\tfoundPointKey := kv != nil\n\t\tif kv != nil {\n\t\t\tsst.SmallestPointKey.CopyFrom(kv.K)\n\t\t}\n\t\tif rangeDelIter != nil {\n\t\t\tif span, err := rangeDelIter.SeekGE(lower); err != nil {\n\t\t\t\treturn nil, false, err\n\t\t\t} else if span != nil && (len(sst.SmallestPointKey.UserKey) == 0 || base.InternalCompare(cmp, span.SmallestKey(), sst.SmallestPointKey) < 0) {\n\t\t\t\tsst.SmallestPointKey.CopyFrom(span.SmallestKey())\n\t\t\t\tfoundPointKey = true\n\t\t\t}\n\t\t}\n\t\tif !foundPointKey {\n\t\t\t// There are no point keys in the span we're interested in.\n\t\t\tsst.SmallestPointKey = InternalKey{}\n\t\t\tsst.LargestPointKey = InternalKey{}\n\t\t}\n\t\tsst.SmallestRangeKey.UserKey = sst.SmallestRangeKey.UserKey[:0]\n\t\tsst.SmallestRangeKey.Trailer = 0\n\t\tif rangeKeyIter != nil {\n\t\t\tspan, err := rangeKeyIter.SeekGE(lower)\n\t\t\tswitch {\n\t\t\tcase err != nil:\n\t\t\t\treturn nil, false, err\n\t\t\tcase span != nil:\n\t\t\t\tsst.SmallestRangeKey.CopyFrom(span.SmallestKey())\n\t\t\tdefault:\n\t\t\t\t// There are no range keys in the span we're interested in.\n\t\t\t\tsst.SmallestRangeKey = InternalKey{}\n\t\t\t\tsst.LargestRangeKey = InternalKey{}\n\t\t\t}\n\t\t}\n\t}\n\t// Check if we need to truncate on the right side. This means finding a new\n\t// LargestPointKey and LargestRangeKey that is < upper.\n\tif needsUpperTruncate {\n\t\tsst.LargestPointKey.UserKey = sst.LargestPointKey.UserKey[:0]\n\t\tsst.LargestPointKey.Trailer = 0\n\t\tkv := iter.SeekLT(upper, base.SeekLTFlagsNone)\n\t\tfoundPointKey := kv != nil\n\t\tif kv != nil {\n\t\t\tsst.LargestPointKey.CopyFrom(kv.K)\n\t\t}\n\t\tif rangeDelIter != nil {\n\t\t\tif span, err := rangeDelIter.SeekLT(upper); err != nil {\n\t\t\t\treturn nil, false, err\n\t\t\t} else if span != nil && (len(sst.LargestPointKey.UserKey) == 0 || base.InternalCompare(cmp, span.LargestKey(), sst.LargestPointKey) > 0) {\n\t\t\t\tsst.LargestPointKey.CopyFrom(span.LargestKey())\n\t\t\t\tfoundPointKey = true\n\t\t\t}\n\t\t}\n\t\tif !foundPointKey {\n\t\t\t// There are no point keys in the span we're interested in.\n\t\t\tsst.SmallestPointKey = InternalKey{}\n\t\t\tsst.LargestPointKey = InternalKey{}\n\t\t}\n\t\tsst.LargestRangeKey.UserKey = sst.LargestRangeKey.UserKey[:0]\n\t\tsst.LargestRangeKey.Trailer = 0\n\t\tif rangeKeyIter != nil {\n\t\t\tspan, err := rangeKeyIter.SeekLT(upper)\n\t\t\tswitch {\n\t\t\tcase err != nil:\n\t\t\t\treturn nil, false, err\n\t\t\tcase span != nil:\n\t\t\t\tsst.LargestRangeKey.CopyFrom(span.LargestKey())\n\t\t\tdefault:\n\t\t\t\t// There are no range keys in the span we're interested in.\n\t\t\t\tsst.SmallestRangeKey = InternalKey{}\n\t\t\t\tsst.LargestRangeKey = InternalKey{}\n\t\t\t}\n\t\t}\n\t}\n\t// Set overall bounds based on {Smallest,Largest}{Point,Range}Key.\n\tswitch {\n\tcase len(sst.SmallestRangeKey.UserKey) == 0:\n\t\tsst.Smallest = sst.SmallestPointKey\n\tcase len(sst.SmallestPointKey.UserKey) == 0:\n\t\tsst.Smallest = sst.SmallestRangeKey\n\tdefault:\n\t\tsst.Smallest = sst.SmallestPointKey\n\t\tif base.InternalCompare(cmp, sst.SmallestRangeKey, sst.SmallestPointKey) < 0 {\n\t\t\tsst.Smallest = sst.SmallestRangeKey\n\t\t}\n\t}\n\tswitch {\n\tcase len(sst.LargestRangeKey.UserKey) == 0:\n\t\tsst.Largest = sst.LargestPointKey\n\tcase len(sst.LargestPointKey.UserKey) == 0:\n\t\tsst.Largest = sst.LargestRangeKey\n\tdefault:\n\t\tsst.Largest = sst.LargestPointKey\n\t\tif base.InternalCompare(cmp, sst.LargestRangeKey, sst.LargestPointKey) > 0 {\n\t\t\tsst.Largest = sst.LargestRangeKey\n\t\t}\n\t}\n\t// On rare occasion, a file might overlap with [lower, upper) but not actually\n\t// have any keys within those bounds. Skip such files.\n\tif len(sst.Smallest.UserKey) == 0 {\n\t\treturn nil, true, nil\n\t}\n\tsst.Size, err = d.fileCache.estimateSize(file, sst.Smallest.UserKey, sst.Largest.UserKey)\n\tif err != nil {\n\t\treturn nil, false, err\n\t}\n\t// On occasion, estimateSize gives us a low estimate, i.e. a 0 file size. This\n\t// can cause panics in places where we divide by file sizes. Correct for it\n\t// here.\n\tif sst.Size == 0 {\n\t\tsst.Size = 1\n\t}\n\treturn sst, false, nil\n}\n\nfunc scanInternalImpl(\n\tctx context.Context, lower, upper []byte, iter *scanInternalIterator, opts *scanInternalOptions,\n) error {\n\tif opts.visitSharedFile != nil && (lower == nil || upper == nil) {\n\t\tpanic(\"lower and upper bounds must be specified in skip-shared iteration mode\")\n\t}\n\tif opts.visitSharedFile != nil && opts.visitExternalFile != nil {\n\t\treturn base.AssertionFailedf(\"cannot provide both a shared-file and external-file visitor\")\n\t}\n\n\t// Before starting iteration, check if any files in levels sharedLevelsStart\n\t// and below are *not* shared. Error out if that is the case, as skip-shared\n\t// iteration will not produce a consistent point-in-time view of this range\n\t// of keys. For files that are shared, call visitSharedFile with a truncated\n\t// version of that file.\n\tcmp := iter.comparer.Compare\n\tprovider := iter.db.ObjProvider()\n\tseqNum := iter.seqNum\n\tcurrent := iter.version\n\tif current == nil {\n\t\tcurrent = iter.readState.current\n\t}\n\n\tif opts.visitSharedFile != nil || opts.visitExternalFile != nil {\n\t\tif provider == nil {\n\t\t\tpanic(\"expected non-nil Provider in skip-shared iteration mode\")\n\t\t}\n\n\t\tfirstLevelWithRemote := opts.skipLevelForOpts()\n\t\tfor level := firstLevelWithRemote; level < numLevels; level++ {\n\t\t\tfiles := current.Levels[level].Iter()\n\t\t\tfor f := files.SeekGE(cmp, lower); f != nil && cmp(f.Smallest.UserKey, upper) < 0; f = files.Next() {\n\t\t\t\tif cmp(lower, f.Largest.UserKey) == 0 && f.Largest.IsExclusiveSentinel() {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tvar objMeta objstorage.ObjectMetadata\n\t\t\t\tvar err error\n\t\t\t\tobjMeta, err = provider.Lookup(fileTypeTable, f.FileBacking.DiskFileNum)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\n\t\t\t\t// We allow a mix of files at the first level.\n\t\t\t\tif level != firstLevelWithRemote {\n\t\t\t\t\tif !objMeta.IsShared() && !objMeta.IsExternal() {\n\t\t\t\t\t\treturn errors.Wrapf(ErrInvalidSkipSharedIteration, \"file %s is not shared or external\", objMeta.DiskFileNum)\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tif objMeta.IsShared() && opts.visitSharedFile == nil {\n\t\t\t\t\treturn errors.Wrapf(ErrInvalidSkipSharedIteration, \"shared file is present but no shared file visitor is defined\")\n\t\t\t\t}\n\n\t\t\t\tif objMeta.IsExternal() && opts.visitExternalFile == nil {\n\t\t\t\t\treturn errors.Wrapf(ErrInvalidSkipSharedIteration, \"external file is present but no external file visitor is defined\")\n\t\t\t\t}\n\n\t\t\t\tif !base.Visible(f.LargestSeqNum, seqNum, base.SeqNumMax) {\n\t\t\t\t\treturn errors.Wrapf(ErrInvalidSkipSharedIteration, \"file %s contains keys newer than snapshot\", objMeta.DiskFileNum)\n\t\t\t\t}\n\n\t\t\t\tif level != firstLevelWithRemote && (!objMeta.IsShared() && !objMeta.IsExternal()) {\n\t\t\t\t\treturn errors.Wrapf(ErrInvalidSkipSharedIteration, \"file %s is not shared or external\", objMeta.DiskFileNum)\n\t\t\t\t}\n\n\t\t\t\tif objMeta.IsShared() {\n\t\t\t\t\tvar sst *SharedSSTMeta\n\t\t\t\t\tvar skip bool\n\t\t\t\t\tsst, skip, err = iter.db.truncateSharedFile(ctx, lower, upper, level, f, objMeta)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err\n\t\t\t\t\t}\n\t\t\t\t\tif skip {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tif err = opts.visitSharedFile(sst); err != nil {\n\t\t\t\t\t\treturn err\n\t\t\t\t\t}\n\t\t\t\t} else if objMeta.IsExternal() {\n\t\t\t\t\tsst, err := iter.db.truncateExternalFile(ctx, lower, upper, level, f, objMeta)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err\n\t\t\t\t\t}\n\t\t\t\t\tif err := opts.visitExternalFile(sst); err != nil {\n\t\t\t\t\t\treturn err\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t}\n\t\t}\n\t}\n\n\tfor valid := iter.seekGE(lower); valid && iter.error() == nil; valid = iter.next() {\n\t\tkey := iter.unsafeKey()\n\n\t\tif opts.rateLimitFunc != nil {\n\t\t\tif err := opts.rateLimitFunc(key, iter.lazyValue()); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\n\t\tswitch key.Kind() {\n\t\tcase InternalKeyKindRangeKeyDelete, InternalKeyKindRangeKeyUnset, InternalKeyKindRangeKeySet:\n\t\t\tif opts.visitRangeKey != nil {\n\t\t\t\tspan := iter.unsafeSpan()\n\t\t\t\t// NB: The caller isn't interested in the sequence numbers of these\n\t\t\t\t// range keys. Rather, the caller wants them to be in trailer order\n\t\t\t\t// _after_ zeroing of sequence numbers. Copy span.Keys, sort it, and then\n\t\t\t\t// call visitRangeKey.\n\t\t\t\tkeysCopy := make([]keyspan.Key, len(span.Keys))\n\t\t\t\tfor i := range span.Keys {\n\t\t\t\t\tkeysCopy[i].CopyFrom(span.Keys[i])\n\t\t\t\t\tkeysCopy[i].Trailer = base.MakeTrailer(0, span.Keys[i].Kind())\n\t\t\t\t}\n\t\t\t\tkeyspan.SortKeysByTrailer(keysCopy)\n\t\t\t\tif err := opts.visitRangeKey(span.Start, span.End, keysCopy); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\tcase InternalKeyKindRangeDelete:\n\t\t\tif opts.visitRangeDel != nil {\n\t\t\t\trangeDel := iter.unsafeRangeDel()\n\t\t\t\tif err := opts.visitRangeDel(rangeDel.Start, rangeDel.End, rangeDel.LargestSeqNum()); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\tdefault:\n\t\t\tif opts.visitPointKey != nil {\n\t\t\t\tvar info IteratorLevel\n\t\t\t\tif len(iter.mergingIter.heap.items) > 0 {\n\t\t\t\t\tmergingIterIdx := iter.mergingIter.heap.items[0].index\n\t\t\t\t\tinfo = iter.iterLevels[mergingIterIdx]\n\t\t\t\t} else {\n\t\t\t\t\tinfo = IteratorLevel{Kind: IteratorLevelUnknown}\n\t\t\t\t}\n\t\t\t\tval := iter.lazyValue()\n\t\t\t\tif err := opts.visitPointKey(key, val, info); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (opts *scanInternalOptions) skipLevelForOpts() int {\n\tif opts.visitSharedFile != nil {\n\t\treturn sharedLevelsStart\n\t}\n\tif opts.visitExternalFile != nil {\n\t\treturn externalSkipStart\n\t}\n\treturn numLevels\n}\n\n// constructPointIter constructs a merging iterator and sets i.iter to it.\nfunc (i *scanInternalIterator) constructPointIter(\n\tcategory sstable.Category, memtables flushableList, buf *iterAlloc,\n) error {\n\t// Merging levels and levels from iterAlloc.\n\tmlevels := buf.mlevels[:0]\n\tlevels := buf.levels[:0]\n\n\t// We compute the number of levels needed ahead of time and reallocate a slice if\n\t// the array from the iterAlloc isn't large enough. Doing this allocation once\n\t// should improve the performance.\n\tnumMergingLevels := len(memtables)\n\tnumLevelIters := 0\n\n\tcurrent := i.version\n\tif current == nil {\n\t\tcurrent = i.readState.current\n\t}\n\tnumMergingLevels += len(current.L0SublevelFiles)\n\tnumLevelIters += len(current.L0SublevelFiles)\n\n\tskipStart := i.opts.skipLevelForOpts()\n\tfor level := 1; level < len(current.Levels); level++ {\n\t\tif current.Levels[level].Empty() {\n\t\t\tcontinue\n\t\t}\n\t\tif level > skipStart {\n\t\t\tcontinue\n\t\t}\n\t\tnumMergingLevels++\n\t\tnumLevelIters++\n\t}\n\n\tif numMergingLevels > cap(mlevels) {\n\t\tmlevels = make([]mergingIterLevel, 0, numMergingLevels)\n\t}\n\tif numLevelIters > cap(levels) {\n\t\tlevels = make([]levelIter, 0, numLevelIters)\n\t}\n\t// TODO(bilal): Push these into the iterAlloc buf.\n\tvar rangeDelMiter keyspanimpl.MergingIter\n\trangeDelIters := make([]keyspan.FragmentIterator, 0, numMergingLevels)\n\trangeDelLevels := make([]keyspanimpl.LevelIter, 0, numLevelIters)\n\n\ti.iterLevels = make([]IteratorLevel, numMergingLevels)\n\tmlevelsIndex := 0\n\n\t// Next are the memtables.\n\tfor j := len(memtables) - 1; j >= 0; j-- {\n\t\tmem := memtables[j]\n\t\tmlevels = append(mlevels, mergingIterLevel{\n\t\t\titer: mem.newIter(&i.opts.IterOptions),\n\t\t})\n\t\ti.iterLevels[mlevelsIndex] = IteratorLevel{\n\t\t\tKind:           IteratorLevelFlushable,\n\t\t\tFlushableIndex: j,\n\t\t}\n\t\tmlevelsIndex++\n\t\tif rdi := mem.newRangeDelIter(&i.opts.IterOptions); rdi != nil {\n\t\t\trangeDelIters = append(rangeDelIters, rdi)\n\t\t}\n\t}\n\n\t// Next are the file levels: L0 sub-levels followed by lower levels.\n\tlevelsIndex := len(levels)\n\tmlevels = mlevels[:numMergingLevels]\n\tlevels = levels[:numLevelIters]\n\trangeDelLevels = rangeDelLevels[:numLevelIters]\n\ti.opts.IterOptions.snapshotForHideObsoletePoints = i.seqNum\n\ti.opts.IterOptions.Category = category\n\taddLevelIterForFiles := func(files manifest.LevelIterator, level manifest.Layer) {\n\t\tli := &levels[levelsIndex]\n\t\trli := &rangeDelLevels[levelsIndex]\n\n\t\tli.init(\n\t\t\ti.ctx, i.opts.IterOptions, i.comparer, i.newIters, files, level,\n\t\t\tinternalIterOpts{})\n\t\tmlevels[mlevelsIndex].iter = li\n\t\trli.Init(i.ctx, keyspan.SpanIterOptions{RangeKeyFilters: i.opts.RangeKeyFilters},\n\t\t\ti.comparer.Compare, tableNewRangeDelIter(i.newIters), files, level,\n\t\t\tmanifest.KeyTypePoint)\n\t\trangeDelIters = append(rangeDelIters, rli)\n\n\t\tlevelsIndex++\n\t\tmlevelsIndex++\n\t}\n\n\tfor j := len(current.L0SublevelFiles) - 1; j >= 0; j-- {\n\t\ti.iterLevels[mlevelsIndex] = IteratorLevel{\n\t\t\tKind:     IteratorLevelLSM,\n\t\t\tLevel:    0,\n\t\t\tSublevel: j,\n\t\t}\n\t\taddLevelIterForFiles(current.L0SublevelFiles[j].Iter(), manifest.L0Sublevel(j))\n\t}\n\t// Add level iterators for the non-empty non-L0 levels.\n\tfor level := 1; level < numLevels; level++ {\n\t\tif current.Levels[level].Empty() {\n\t\t\tcontinue\n\t\t}\n\n\t\tif level > skipStart {\n\t\t\tcontinue\n\t\t}\n\t\ti.iterLevels[mlevelsIndex] = IteratorLevel{Kind: IteratorLevelLSM, Level: level}\n\t\tlevIter := current.Levels[level].Iter()\n\t\tif level == skipStart {\n\t\t\tnonRemoteFiles := make([]*manifest.FileMetadata, 0)\n\t\t\tfor f := levIter.First(); f != nil; f = levIter.Next() {\n\t\t\t\tmeta, err := i.db.objProvider.Lookup(fileTypeTable, f.FileBacking.DiskFileNum)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tif (meta.IsShared() && i.opts.visitSharedFile != nil) ||\n\t\t\t\t\t(meta.IsExternal() && i.opts.visitExternalFile != nil) {\n\t\t\t\t\t// Skip this file.\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tnonRemoteFiles = append(nonRemoteFiles, f)\n\t\t\t}\n\t\t\tlevSlice := manifest.NewLevelSliceKeySorted(i.db.cmp, nonRemoteFiles)\n\t\t\tlevIter = levSlice.Iter()\n\t\t}\n\n\t\taddLevelIterForFiles(levIter, manifest.Level(level))\n\t}\n\n\tbuf.merging.init(&i.opts.IterOptions, &InternalIteratorStats{}, i.comparer.Compare, i.comparer.Split, mlevels...)\n\tbuf.merging.snapshot = i.seqNum\n\trangeDelMiter.Init(i.comparer, keyspan.VisibleTransform(i.seqNum), new(keyspanimpl.MergingBuffers), rangeDelIters...)\n\n\tif i.opts.includeObsoleteKeys {\n\t\tiiter := &keyspan.InterleavingIter{}\n\t\tiiter.Init(i.comparer, &buf.merging, &rangeDelMiter,\n\t\t\tkeyspan.InterleavingIterOpts{\n\t\t\t\tLowerBound: i.opts.LowerBound,\n\t\t\t\tUpperBound: i.opts.UpperBound,\n\t\t\t})\n\t\ti.pointKeyIter = iiter\n\t} else {\n\t\tpcIter := &pointCollapsingIterator{\n\t\t\tcomparer: i.comparer,\n\t\t\tmerge:    i.merge,\n\t\t\tseqNum:   i.seqNum,\n\t\t}\n\t\tpcIter.iter.Init(i.comparer, &buf.merging, &rangeDelMiter, keyspan.InterleavingIterOpts{\n\t\t\tLowerBound: i.opts.LowerBound,\n\t\t\tUpperBound: i.opts.UpperBound,\n\t\t})\n\t\ti.pointKeyIter = pcIter\n\t}\n\ti.iter = i.pointKeyIter\n\treturn nil\n}\n\n// constructRangeKeyIter constructs the range-key iterator stack, populating\n// i.rangeKey.rangeKeyIter with the resulting iterator. This is similar to\n// Iterator.constructRangeKeyIter, except it doesn't handle batches and ensures\n// iterConfig does *not* elide unsets/deletes.\nfunc (i *scanInternalIterator) constructRangeKeyIter() error {\n\t// We want the bounded iter from iterConfig, but not the collapsing of\n\t// RangeKeyUnsets and RangeKeyDels.\n\ti.rangeKey.rangeKeyIter = i.rangeKey.iterConfig.Init(\n\t\ti.comparer, i.seqNum, i.opts.LowerBound, i.opts.UpperBound,\n\t\tnil /* hasPrefix */, nil /* prefix */, true, /* internalKeys */\n\t\t&i.rangeKey.rangeKeyBuffers.internal)\n\n\t// Next are the flushables: memtables and large batches.\n\tif i.readState != nil {\n\t\tfor j := len(i.readState.memtables) - 1; j >= 0; j-- {\n\t\t\tmem := i.readState.memtables[j]\n\t\t\t// We only need to read from memtables which contain sequence numbers older\n\t\t\t// than seqNum.\n\t\t\tif logSeqNum := mem.logSeqNum; logSeqNum >= i.seqNum {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif rki := mem.newRangeKeyIter(&i.opts.IterOptions); rki != nil {\n\t\t\t\ti.rangeKey.iterConfig.AddLevel(rki)\n\t\t\t}\n\t\t}\n\t}\n\n\tcurrent := i.version\n\tif current == nil {\n\t\tcurrent = i.readState.current\n\t}\n\t// Next are the file levels: L0 sub-levels followed by lower levels.\n\t//\n\t// Add file-specific iterators for L0 files containing range keys. This is less\n\t// efficient than using levelIters for sublevels of L0 files containing\n\t// range keys, but range keys are expected to be sparse anyway, reducing the\n\t// cost benefit of maintaining a separate L0Sublevels instance for range key\n\t// files and then using it here.\n\t//\n\t// NB: We iterate L0's files in reverse order. They're sorted by\n\t// LargestSeqNum ascending, and we need to add them to the merging iterator\n\t// in LargestSeqNum descending to preserve the merging iterator's invariants\n\t// around Key InternalKeyTrailer order.\n\titer := current.RangeKeyLevels[0].Iter()\n\tfor f := iter.Last(); f != nil; f = iter.Prev() {\n\t\tspanIter, err := i.newIterRangeKey(i.ctx, f, i.opts.SpanIterOptions())\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\ti.rangeKey.iterConfig.AddLevel(spanIter)\n\t}\n\t// Add level iterators for the non-empty non-L0 levels.\n\tskipStart := i.opts.skipLevelForOpts()\n\tfor level := 1; level < len(current.RangeKeyLevels); level++ {\n\t\tif current.RangeKeyLevels[level].Empty() {\n\t\t\tcontinue\n\t\t}\n\t\tif level > skipStart {\n\t\t\tcontinue\n\t\t}\n\t\tli := i.rangeKey.iterConfig.NewLevelIter()\n\t\tspanIterOpts := i.opts.SpanIterOptions()\n\t\tlevIter := current.RangeKeyLevels[level].Iter()\n\t\tif level == skipStart {\n\t\t\tnonRemoteFiles := make([]*manifest.FileMetadata, 0)\n\t\t\tfor f := levIter.First(); f != nil; f = levIter.Next() {\n\t\t\t\tmeta, err := i.db.objProvider.Lookup(fileTypeTable, f.FileBacking.DiskFileNum)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tif (meta.IsShared() && i.opts.visitSharedFile != nil) ||\n\t\t\t\t\t(meta.IsExternal() && i.opts.visitExternalFile != nil) {\n\t\t\t\t\t// Skip this file.\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tnonRemoteFiles = append(nonRemoteFiles, f)\n\t\t\t}\n\t\t\tlevSlice := manifest.NewLevelSliceKeySorted(i.db.cmp, nonRemoteFiles)\n\t\t\tlevIter = levSlice.Iter()\n\t\t}\n\t\tli.Init(i.ctx, spanIterOpts, i.comparer.Compare, i.newIterRangeKey, levIter,\n\t\t\tmanifest.Level(level), manifest.KeyTypeRange)\n\t\ti.rangeKey.iterConfig.AddLevel(li)\n\t}\n\treturn nil\n}\n\n// seekGE seeks this iterator to the first key that's greater than or equal\n// to the specified user key.\nfunc (i *scanInternalIterator) seekGE(key []byte) bool {\n\ti.iterKV = i.iter.SeekGE(key, base.SeekGEFlagsNone)\n\treturn i.iterKV != nil\n}\n\n// unsafeKey returns the unsafe InternalKey at the current position. The value\n// is nil if the iterator is invalid or exhausted.\nfunc (i *scanInternalIterator) unsafeKey() *InternalKey {\n\treturn &i.iterKV.K\n}\n\n// lazyValue returns a value pointer to the value at the current iterator\n// position. Behaviour undefined if unsafeKey() returns a Range key or Rangedel\n// kind key.\nfunc (i *scanInternalIterator) lazyValue() LazyValue {\n\treturn i.iterKV.V\n}\n\n// unsafeRangeDel returns a range key span. Behaviour undefined if UnsafeKey returns\n// a non-rangedel kind.\nfunc (i *scanInternalIterator) unsafeRangeDel() *keyspan.Span {\n\ttype spanInternalIterator interface {\n\t\tSpan() *keyspan.Span\n\t}\n\treturn i.pointKeyIter.(spanInternalIterator).Span()\n}\n\n// unsafeSpan returns a range key span. Behaviour undefined if UnsafeKey returns\n// a non-rangekey type.\nfunc (i *scanInternalIterator) unsafeSpan() *keyspan.Span {\n\treturn i.rangeKey.iiter.Span()\n}\n\n// next advances the iterator in the forward direction, and returns the\n// iterator's new validity state.\nfunc (i *scanInternalIterator) next() bool {\n\ti.iterKV = i.iter.Next()\n\treturn i.iterKV != nil\n}\n\n// error returns an error from the internal iterator, if there's any.\nfunc (i *scanInternalIterator) error() error {\n\treturn i.iter.Error()\n}\n\n// close closes this iterator, and releases any pooled objects.\nfunc (i *scanInternalIterator) close() error {\n\tif err := i.iter.Close(); err != nil {\n\t\treturn err\n\t}\n\tif i.readState != nil {\n\t\ti.readState.unref()\n\t}\n\tif i.version != nil {\n\t\ti.version.Unref()\n\t}\n\tif i.rangeKey != nil {\n\t\ti.rangeKey.PrepareForReuse()\n\t\t*i.rangeKey = iteratorRangeKeyState{\n\t\t\trangeKeyBuffers: i.rangeKey.rangeKeyBuffers,\n\t\t}\n\t\titerRangeKeyStateAllocPool.Put(i.rangeKey)\n\t\ti.rangeKey = nil\n\t}\n\tif alloc := i.alloc; alloc != nil {\n\t\tfor j := range i.boundsBuf {\n\t\t\tif cap(i.boundsBuf[j]) >= maxKeyBufCacheSize {\n\t\t\t\talloc.boundsBuf[j] = nil\n\t\t\t} else {\n\t\t\t\talloc.boundsBuf[j] = i.boundsBuf[j]\n\t\t\t}\n\t\t}\n\t\t*alloc = iterAlloc{\n\t\t\tkeyBuf:              alloc.keyBuf[:0],\n\t\t\tboundsBuf:           alloc.boundsBuf,\n\t\t\tprefixOrFullSeekKey: alloc.prefixOrFullSeekKey[:0],\n\t\t}\n\t\titerAllocPool.Put(alloc)\n\t\ti.alloc = nil\n\t}\n\treturn nil\n}\n\nfunc (i *scanInternalIterator) initializeBoundBufs(lower, upper []byte) {\n\tbuf := i.boundsBuf[i.boundsBufIdx][:0]\n\tif lower != nil {\n\t\tbuf = append(buf, lower...)\n\t\ti.opts.LowerBound = buf\n\t} else {\n\t\ti.opts.LowerBound = nil\n\t}\n\tif upper != nil {\n\t\tbuf = append(buf, upper...)\n\t\ti.opts.UpperBound = buf[len(buf)-len(upper):]\n\t} else {\n\t\ti.opts.UpperBound = nil\n\t}\n\ti.boundsBuf[i.boundsBufIdx] = buf\n\ti.boundsBufIdx = 1 - i.boundsBufIdx\n}\n"
        },
        {
          "name": "scan_internal_test.go",
          "type": "blob",
          "size": 17.8369140625,
          "content": "// Copyright 2023 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"context\"\n\t\"encoding/hex\"\n\t\"fmt\"\n\t\"math\"\n\t\"strconv\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/datadriven\"\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/bloom\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/itertest\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan\"\n\t\"github.com/cockroachdb/pebble/internal/testkeys\"\n\t\"github.com/cockroachdb/pebble/objstorage\"\n\t\"github.com/cockroachdb/pebble/objstorage/objstorageprovider\"\n\t\"github.com/cockroachdb/pebble/objstorage/remote\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestScanStatistics(t *testing.T) {\n\tvar d *DB\n\ttype scanInternalReader interface {\n\t\tScanStatistics(\n\t\t\tctx context.Context,\n\t\t\tlower, upper []byte,\n\t\t\topts ScanStatisticsOptions,\n\t\t) (LSMKeyStatistics, error)\n\t}\n\tbatches := map[string]*Batch{}\n\tsnaps := map[string]*Snapshot{}\n\tctx := context.TODO()\n\n\tgetOpts := func() *Options {\n\t\topts := &Options{\n\t\t\tFS:                 vfs.NewMem(),\n\t\t\tLogger:             testLogger{t: t},\n\t\t\tComparer:           testkeys.Comparer,\n\t\t\tFormatMajorVersion: FormatMinForSharedObjects,\n\t\t\tBlockPropertyCollectors: []func() BlockPropertyCollector{\n\t\t\t\tsstable.NewTestKeysBlockPropertyCollector,\n\t\t\t},\n\t\t}\n\t\topts.Experimental.RemoteStorage = remote.MakeSimpleFactory(map[remote.Locator]remote.Storage{\n\t\t\t\"\": remote.NewInMem(),\n\t\t})\n\t\topts.Experimental.CreateOnShared = remote.CreateOnSharedAll\n\t\topts.Experimental.CreateOnSharedLocator = \"\"\n\t\topts.DisableAutomaticCompactions = true\n\t\topts.EnsureDefaults()\n\t\topts.WithFSDefaults()\n\t\treturn opts\n\t}\n\tcleanup := func() (err error) {\n\t\tfor key, batch := range batches {\n\t\t\terr = firstError(err, batch.Close())\n\t\t\tdelete(batches, key)\n\t\t}\n\t\tfor key, snap := range snaps {\n\t\t\terr = firstError(err, snap.Close())\n\t\t\tdelete(snaps, key)\n\t\t}\n\t\tif d != nil {\n\t\t\terr = firstError(err, d.Close())\n\t\t\td = nil\n\t\t}\n\t\treturn err\n\t}\n\tdefer cleanup()\n\n\tdatadriven.RunTest(t, \"testdata/scan_statistics\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"reset\":\n\t\t\tif err := cleanup(); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tvar err error\n\t\t\td, err = Open(\"\", getOpts())\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.NoError(t, d.SetCreatorID(1))\n\t\t\treturn \"\"\n\t\tcase \"snapshot\":\n\t\t\ts := d.NewSnapshot()\n\t\t\tvar name string\n\t\t\ttd.ScanArgs(t, \"name\", &name)\n\t\t\tsnaps[name] = s\n\t\t\treturn \"\"\n\t\tcase \"batch\":\n\t\t\tvar name string\n\t\t\ttd.MaybeScanArgs(t, \"name\", &name)\n\t\t\tcommit := td.HasArg(\"commit\")\n\t\t\tb := d.NewIndexedBatch()\n\t\t\trequire.NoError(t, runBatchDefineCmd(td, b))\n\t\t\tvar err error\n\t\t\tif commit {\n\t\t\t\tfunc() {\n\t\t\t\t\tdefer func() {\n\t\t\t\t\t\tif r := recover(); r != nil {\n\t\t\t\t\t\t\terr = errors.New(r.(string))\n\t\t\t\t\t\t}\n\t\t\t\t\t}()\n\t\t\t\t\terr = b.Commit(nil)\n\t\t\t\t}()\n\t\t\t} else if name != \"\" {\n\t\t\t\tbatches[name] = b\n\t\t\t}\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tcount := b.Count()\n\t\t\tif commit {\n\t\t\t\treturn fmt.Sprintf(\"committed %d keys\\n\", count)\n\t\t\t}\n\t\t\treturn fmt.Sprintf(\"wrote %d keys to batch %q\\n\", count, name)\n\t\tcase \"compact\":\n\t\t\tif err := runCompactCmd(td, d); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn runLSMCmd(td, d)\n\t\tcase \"flush\":\n\t\t\terr := d.Flush()\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\t\tcase \"commit\":\n\t\t\tname := pluckStringCmdArg(td, \"batch\")\n\t\t\tb := batches[name]\n\t\t\tdefer b.Close()\n\t\t\tcount := b.Count()\n\t\t\trequire.NoError(t, d.Apply(b, nil))\n\t\t\tdelete(batches, name)\n\t\t\treturn fmt.Sprintf(\"committed %d keys\\n\", count)\n\t\tcase \"scan-statistics\":\n\t\t\tvar lower, upper []byte\n\t\t\tvar reader scanInternalReader = d\n\t\t\tvar b strings.Builder\n\t\t\tvar showSnapshotPinned = false\n\t\t\tvar keyKindsToDisplay []InternalKeyKind\n\t\t\tvar showLevels []string\n\n\t\t\tfor _, arg := range td.CmdArgs {\n\t\t\t\tswitch arg.Key {\n\t\t\t\tcase \"lower\":\n\t\t\t\t\tlower = []byte(arg.Vals[0])\n\t\t\t\tcase \"upper\":\n\t\t\t\t\tupper = []byte(arg.Vals[0])\n\t\t\t\tcase \"show-snapshot-pinned\":\n\t\t\t\t\tshowSnapshotPinned = true\n\t\t\t\tcase \"keys\":\n\t\t\t\t\tfor _, key := range arg.Vals {\n\t\t\t\t\t\tkeyKindsToDisplay = append(keyKindsToDisplay, base.ParseKind(key))\n\t\t\t\t\t}\n\t\t\t\tcase \"levels\":\n\t\t\t\t\tshowLevels = append(showLevels, arg.Vals...)\n\t\t\t\tdefault:\n\t\t\t\t}\n\t\t\t}\n\t\t\tstats, err := reader.ScanStatistics(ctx, lower, upper, ScanStatisticsOptions{})\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\n\t\t\tfor _, level := range showLevels {\n\t\t\t\tlvl, err := strconv.Atoi(level)\n\t\t\t\tif err != nil || lvl >= numLevels {\n\t\t\t\t\treturn fmt.Sprintf(\"invalid level %s\", level)\n\t\t\t\t}\n\n\t\t\t\tfmt.Fprintf(&b, \"Level %d:\\n\", lvl)\n\t\t\t\tif showSnapshotPinned {\n\t\t\t\t\tfmt.Fprintf(&b, \"  compaction pinned count: %d\\n\", stats.Levels[lvl].SnapshotPinnedKeys)\n\t\t\t\t}\n\t\t\t\tfor _, kind := range keyKindsToDisplay {\n\t\t\t\t\tfmt.Fprintf(&b, \"  %s key count: %d\\n\", kind.String(), stats.Levels[lvl].KindsCount[kind])\n\t\t\t\t\tif stats.Levels[lvl].LatestKindsCount[kind] > 0 {\n\t\t\t\t\t\tfmt.Fprintf(&b, \"  %s latest count: %d\\n\", kind.String(), stats.Levels[lvl].LatestKindsCount[kind])\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfmt.Fprintf(&b, \"Aggregate:\\n\")\n\t\t\tif showSnapshotPinned {\n\t\t\t\tfmt.Fprintf(&b, \"  snapshot pinned count: %d\\n\", stats.Accumulated.SnapshotPinnedKeys)\n\t\t\t}\n\t\t\tfor _, kind := range keyKindsToDisplay {\n\t\t\t\tfmt.Fprintf(&b, \"  %s key count: %d\\n\", kind.String(), stats.Accumulated.KindsCount[kind])\n\t\t\t\tif stats.Accumulated.LatestKindsCount[kind] > 0 {\n\t\t\t\t\tfmt.Fprintf(&b, \"  %s latest count: %d\\n\", kind.String(), stats.Accumulated.LatestKindsCount[kind])\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn b.String()\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command %q\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc TestScanInternal(t *testing.T) {\n\tvar d *DB\n\ttype scanInternalReader interface {\n\t\tScanInternal(\n\t\t\tctx context.Context,\n\t\t\tcategory sstable.Category,\n\t\t\tlower, upper []byte,\n\t\t\tvisitPointKey func(key *InternalKey, value LazyValue, iterInfo IteratorLevel) error,\n\t\t\tvisitRangeDel func(start, end []byte, seqNum base.SeqNum) error,\n\t\t\tvisitRangeKey func(start, end []byte, keys []keyspan.Key) error,\n\t\t\tvisitSharedFile func(sst *SharedSSTMeta) error,\n\t\t\tvisitExternalFile func(sst *ExternalFile) error,\n\t\t) error\n\t}\n\tbatches := map[string]*Batch{}\n\tsnaps := map[string]*Snapshot{}\n\tefos := map[string]*EventuallyFileOnlySnapshot{}\n\textStorage := remote.NewInMem()\n\tparseOpts := func(td *datadriven.TestData) (*Options, error) {\n\t\topts := &Options{\n\t\t\tFS:                 vfs.NewMem(),\n\t\t\tLogger:             testLogger{t: t},\n\t\t\tComparer:           testkeys.Comparer,\n\t\t\tFormatMajorVersion: FormatVirtualSSTables,\n\t\t\tBlockPropertyCollectors: []func() BlockPropertyCollector{\n\t\t\t\tsstable.NewTestKeysBlockPropertyCollector,\n\t\t\t},\n\t\t}\n\t\topts.Experimental.RemoteStorage = remote.MakeSimpleFactory(map[remote.Locator]remote.Storage{\n\t\t\t\"external-storage\": extStorage,\n\t\t\t\"\":                 remote.NewInMem(),\n\t\t})\n\t\topts.Experimental.CreateOnShared = remote.CreateOnSharedAll\n\t\topts.Experimental.CreateOnSharedLocator = \"\"\n\t\topts.DisableAutomaticCompactions = true\n\t\topts.EnsureDefaults()\n\t\topts.WithFSDefaults()\n\n\t\tfor _, cmdArg := range td.CmdArgs {\n\t\t\tswitch cmdArg.Key {\n\t\t\tcase \"format-major-version\":\n\t\t\t\tv, err := strconv.Atoi(cmdArg.Vals[0])\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\t// Override the DB version.\n\t\t\t\topts.FormatMajorVersion = FormatMajorVersion(v)\n\t\t\tcase \"block-size\":\n\t\t\t\tv, err := strconv.Atoi(cmdArg.Vals[0])\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\tfor i := range opts.Levels {\n\t\t\t\t\topts.Levels[i].BlockSize = v\n\t\t\t\t}\n\t\t\tcase \"index-block-size\":\n\t\t\t\tv, err := strconv.Atoi(cmdArg.Vals[0])\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\tfor i := range opts.Levels {\n\t\t\t\t\topts.Levels[i].IndexBlockSize = v\n\t\t\t\t}\n\t\t\tcase \"target-file-size\":\n\t\t\t\tv, err := strconv.Atoi(cmdArg.Vals[0])\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\tfor i := range opts.Levels {\n\t\t\t\t\topts.Levels[i].TargetFileSize = int64(v)\n\t\t\t\t}\n\t\t\tcase \"bloom-bits-per-key\":\n\t\t\t\tv, err := strconv.Atoi(cmdArg.Vals[0])\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\tfp := bloom.FilterPolicy(v)\n\t\t\t\topts.Filters = map[string]FilterPolicy{fp.Name(): fp}\n\t\t\t\tfor i := range opts.Levels {\n\t\t\t\t\topts.Levels[i].FilterPolicy = fp\n\t\t\t\t}\n\t\t\tcase \"merger\":\n\t\t\t\tswitch cmdArg.Vals[0] {\n\t\t\t\tcase \"appender\":\n\t\t\t\t\topts.Merger = base.DefaultMerger\n\t\t\t\tdefault:\n\t\t\t\t\treturn nil, errors.Newf(\"unrecognized Merger %q\\n\", cmdArg.Vals[0])\n\t\t\t\t}\n\t\t\tcase \"create-on-shared\":\n\t\t\t\tv, err := strconv.ParseBool(cmdArg.Vals[0])\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\tif !v {\n\t\t\t\t\topts.Experimental.CreateOnShared = remote.CreateOnSharedNone\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn opts, nil\n\t}\n\tcleanup := func() (err error) {\n\t\tfor key, batch := range batches {\n\t\t\terr = firstError(err, batch.Close())\n\t\t\tdelete(batches, key)\n\t\t}\n\t\tfor key, snap := range snaps {\n\t\t\terr = firstError(err, snap.Close())\n\t\t\tdelete(snaps, key)\n\t\t}\n\t\tfor key, es := range efos {\n\t\t\terr = firstError(err, es.Close())\n\t\t\tdelete(efos, key)\n\t\t}\n\t\tif d != nil {\n\t\t\terr = firstError(err, d.Close())\n\t\t\td = nil\n\t\t}\n\t\treturn err\n\t}\n\tdefer cleanup()\n\n\tdatadriven.RunTest(t, \"testdata/scan_internal\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"define\":\n\t\t\tif err := cleanup(); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\topts, err := parseOpts(td)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\td, err = runDBDefineCmd(td, opts)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn runLSMCmd(td, d)\n\n\t\tcase \"reset\":\n\t\t\tif err := cleanup(); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\topts, err := parseOpts(td)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t\treturn err.Error()\n\t\t\t}\n\n\t\t\td, err = Open(\"\", opts)\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.NoError(t, d.SetCreatorID(1))\n\t\t\treturn \"\"\n\t\tcase \"snapshot\":\n\t\t\ts := d.NewSnapshot()\n\t\t\tvar name string\n\t\t\ttd.ScanArgs(t, \"name\", &name)\n\t\t\tsnaps[name] = s\n\t\t\treturn \"\"\n\t\tcase \"wait-for-file-only-snapshot\":\n\t\t\tif len(td.CmdArgs) != 1 {\n\t\t\t\tpanic(\"insufficient args for file-only-snapshot command\")\n\t\t\t}\n\t\t\tname := td.CmdArgs[0].Key\n\t\t\tes := efos[name]\n\t\t\tif err := es.WaitForFileOnlySnapshot(context.TODO(), 1*time.Millisecond); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"ok\"\n\t\tcase \"file-only-snapshot\":\n\t\t\tif len(td.CmdArgs) != 1 {\n\t\t\t\tpanic(\"insufficient args for file-only-snapshot command\")\n\t\t\t}\n\t\t\tname := td.CmdArgs[0].Key\n\t\t\tvar keyRanges []KeyRange\n\t\t\tfor _, line := range strings.Split(td.Input, \"\\n\") {\n\t\t\t\tfields := strings.Fields(line)\n\t\t\t\tif len(fields) != 2 {\n\t\t\t\t\treturn \"expected two fields for file-only snapshot KeyRanges\"\n\t\t\t\t}\n\t\t\t\tkr := KeyRange{Start: []byte(fields[0]), End: []byte(fields[1])}\n\t\t\t\tkeyRanges = append(keyRanges, kr)\n\t\t\t}\n\n\t\t\ts := d.NewEventuallyFileOnlySnapshot(keyRanges)\n\t\t\tefos[name] = s\n\t\t\treturn \"ok\"\n\t\tcase \"batch\":\n\t\t\tvar name string\n\t\t\ttd.MaybeScanArgs(t, \"name\", &name)\n\t\t\tcommit := td.HasArg(\"commit\")\n\t\t\tingest := td.HasArg(\"ingest\")\n\t\t\tingestExternal := td.HasArg(\"ingest-external\")\n\t\t\tb := d.NewIndexedBatch()\n\t\t\trequire.NoError(t, runBatchDefineCmd(td, b))\n\n\t\t\twriteSST := func(\n\t\t\t\tpoints internalIterator,\n\t\t\t\trangeDels keyspan.FragmentIterator,\n\t\t\t\trangeKeys keyspan.FragmentIterator,\n\t\t\t\twriter objstorage.Writable) {\n\t\t\t\tw := sstable.NewWriter(writer, d.opts.MakeWriterOptions(0, sstable.TableFormatPebblev4))\n\t\t\t\t{\n\t\t\t\t\tspan, err := rangeDels.First()\n\t\t\t\t\tfor ; span != nil; span, err = rangeDels.Next() {\n\t\t\t\t\t\trequire.NoError(t, w.DeleteRange(span.Start, span.End))\n\t\t\t\t\t}\n\t\t\t\t\trequire.NoError(t, err)\n\t\t\t\t\trangeDels.Close()\n\t\t\t\t}\n\t\t\t\t{\n\t\t\t\t\tspan, err := rangeKeys.First()\n\t\t\t\t\tfor ; span != nil; span, err = rangeKeys.Next() {\n\t\t\t\t\t\tkeys := []keyspan.Key{}\n\t\t\t\t\t\tfor i := range span.Keys {\n\t\t\t\t\t\t\tkeys = append(keys, span.Keys[i])\n\t\t\t\t\t\t\tkeys[i].Trailer = base.MakeTrailer(0, keys[i].Kind())\n\t\t\t\t\t\t}\n\t\t\t\t\t\tkeyspan.SortKeysByTrailer(keys)\n\t\t\t\t\t\trequire.NoError(t, w.Raw().EncodeSpan(keyspan.Span{\n\t\t\t\t\t\t\tStart: span.Start,\n\t\t\t\t\t\t\tEnd:   span.End,\n\t\t\t\t\t\t\tKeys:  keys,\n\t\t\t\t\t\t}))\n\t\t\t\t\t}\n\t\t\t\t\trequire.NoError(t, err)\n\t\t\t\t}\n\t\t\t\trangeKeys.Close()\n\t\t\t\tfor kv := points.First(); kv != nil; kv = points.Next() {\n\t\t\t\t\tt.Logf(\"writing %s\", kv.K)\n\t\t\t\t\tvar value []byte\n\t\t\t\t\tvar err error\n\t\t\t\t\tvalue, _, err = kv.Value(value)\n\t\t\t\t\trequire.NoError(t, err)\n\t\t\t\t\trequire.NoError(t, w.Raw().AddWithForceObsolete(kv.K, value, false))\n\t\t\t\t}\n\t\t\t\tpoints.Close()\n\t\t\t\trequire.NoError(t, w.Close())\n\t\t\t}\n\n\t\t\tvar err error\n\t\t\tif commit {\n\t\t\t\tfunc() {\n\t\t\t\t\tdefer func() {\n\t\t\t\t\t\tif r := recover(); r != nil {\n\t\t\t\t\t\t\terr = errors.New(r.(string))\n\t\t\t\t\t\t}\n\t\t\t\t\t}()\n\t\t\t\t\terr = b.Commit(nil)\n\t\t\t\t}()\n\t\t\t} else if ingest {\n\t\t\t\tpoints, rangeDels, rangeKeys := batchSort(b)\n\t\t\t\tfile, err := d.opts.FS.Create(\"temp0.sst\", vfs.WriteCategoryUnspecified)\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\twriteSST(points, rangeDels, rangeKeys, objstorageprovider.NewFileWritable(file))\n\t\t\t\trequire.NoError(t, d.Ingest(context.Background(), []string{\"temp0.sst\"}))\n\t\t\t} else if ingestExternal {\n\t\t\t\tpoints, rangeDels, rangeKeys := batchSort(b)\n\t\t\t\tlargestUnsafe := points.Last()\n\t\t\t\tlargest := largestUnsafe.K.Clone()\n\t\t\t\tsmallestUnsafe := points.First()\n\t\t\t\tsmallest := smallestUnsafe.K.Clone()\n\t\t\t\tvar objName string\n\t\t\t\ttd.MaybeScanArgs(t, \"ingest-external\", &objName)\n\t\t\t\tfile, err := extStorage.CreateObject(objName)\n\t\t\t\trequire.NoError(t, err)\n\t\t\t\tobjstorageprovider.NewRemoteWritable(file)\n\n\t\t\t\twriteSST(points, rangeDels, rangeKeys, objstorageprovider.NewRemoteWritable(file))\n\t\t\t\tef := ExternalFile{\n\t\t\t\t\tObjName:           objName,\n\t\t\t\t\tLocator:           remote.Locator(\"external-storage\"),\n\t\t\t\t\tSize:              10,\n\t\t\t\t\tStartKey:          smallest.UserKey,\n\t\t\t\t\tEndKey:            largest.UserKey,\n\t\t\t\t\tEndKeyIsInclusive: true,\n\t\t\t\t\tHasPointKey:       true,\n\t\t\t\t}\n\t\t\t\t_, err = d.IngestExternalFiles(context.Background(), []ExternalFile{ef})\n\t\t\t\trequire.NoError(t, err)\n\t\t\t} else if name != \"\" {\n\t\t\t\tbatches[name] = b\n\t\t\t}\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tcount := b.Count()\n\t\t\tif commit {\n\t\t\t\treturn fmt.Sprintf(\"committed %d keys\\n\", count)\n\t\t\t}\n\t\t\treturn fmt.Sprintf(\"wrote %d keys to batch %q\\n\", count, name)\n\t\tcase \"compact\":\n\t\t\tif err := runCompactCmd(td, d); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn runLSMCmd(td, d)\n\t\tcase \"flush\":\n\t\t\terr := d.Flush()\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\t\tcase \"lsm\":\n\t\t\treturn runLSMCmd(td, d)\n\t\tcase \"commit\":\n\t\t\tname := pluckStringCmdArg(td, \"batch\")\n\t\t\tb := batches[name]\n\t\t\tdefer b.Close()\n\t\t\tcount := b.Count()\n\t\t\trequire.NoError(t, d.Apply(b, nil))\n\t\t\tdelete(batches, name)\n\t\t\treturn fmt.Sprintf(\"committed %d keys\\n\", count)\n\t\tcase \"scan-internal\":\n\t\t\tvar lower, upper []byte\n\t\t\tvar reader scanInternalReader = d\n\t\t\tvar b strings.Builder\n\t\t\tvar sharedFileVisitor func(sst *SharedSSTMeta) error\n\t\t\tvar externalFileVisitor func(sst *ExternalFile) error\n\t\t\tfor _, arg := range td.CmdArgs {\n\t\t\t\tswitch arg.Key {\n\t\t\t\tcase \"lower\":\n\t\t\t\t\tlower = []byte(arg.Vals[0])\n\t\t\t\tcase \"upper\":\n\t\t\t\t\tupper = []byte(arg.Vals[0])\n\t\t\t\tcase \"snapshot\":\n\t\t\t\t\tname := arg.Vals[0]\n\t\t\t\t\tsnap, ok := snaps[name]\n\t\t\t\t\tif !ok {\n\t\t\t\t\t\treturn fmt.Sprintf(\"no snapshot found for name %s\", name)\n\t\t\t\t\t}\n\t\t\t\t\treader = snap\n\t\t\t\tcase \"file-only-snapshot\":\n\t\t\t\t\tname := arg.Vals[0]\n\t\t\t\t\tefos, ok := efos[name]\n\t\t\t\t\tif !ok {\n\t\t\t\t\t\treturn fmt.Sprintf(\"no snapshot found for name %s\", name)\n\t\t\t\t\t}\n\t\t\t\t\treader = efos\n\t\t\t\tcase \"skip-shared\":\n\t\t\t\t\tsharedFileVisitor = func(sst *SharedSSTMeta) error {\n\t\t\t\t\t\tfmt.Fprintf(&b, \"shared file: %s [%s-%s] [point=%s-%s] [range=%s-%s]\\n\", sst.fileNum, sst.Smallest.String(), sst.Largest.String(), sst.SmallestPointKey.String(), sst.LargestPointKey.String(), sst.SmallestRangeKey.String(), sst.LargestRangeKey.String())\n\t\t\t\t\t\treturn nil\n\t\t\t\t\t}\n\t\t\t\tcase \"skip-external\":\n\t\t\t\t\texternalFileVisitor = func(sst *ExternalFile) error {\n\t\t\t\t\t\tfmt.Fprintf(&b, \"external file: %s %s [0x%s-0x%s] (hasPoint: %v, hasRange: %v)\\n\",\n\t\t\t\t\t\t\tsst.Locator, sst.ObjName,\n\t\t\t\t\t\t\thex.EncodeToString(sst.StartKey),\n\t\t\t\t\t\t\thex.EncodeToString(sst.EndKey),\n\t\t\t\t\t\t\tsst.HasPointKey, sst.HasRangeKey)\n\t\t\t\t\t\treturn nil\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\terr := reader.ScanInternal(context.TODO(), sstable.CategoryUnknown, lower, upper,\n\t\t\t\tfunc(key *InternalKey, value LazyValue, _ IteratorLevel) error {\n\t\t\t\t\tv := value.InPlaceValue()\n\t\t\t\t\tfmt.Fprintf(&b, \"%s (%s)\\n\", key, v)\n\t\t\t\t\treturn nil\n\t\t\t\t},\n\t\t\t\tfunc(start, end []byte, seqNum base.SeqNum) error {\n\t\t\t\t\tfmt.Fprintf(&b, \"%s-%s#%d,RANGEDEL\\n\", start, end, seqNum)\n\t\t\t\t\treturn nil\n\t\t\t\t},\n\t\t\t\tfunc(start, end []byte, keys []keyspan.Key) error {\n\t\t\t\t\ts := keyspan.Span{Start: start, End: end, Keys: keys}\n\t\t\t\t\tfmt.Fprintf(&b, \"%s\\n\", s.String())\n\t\t\t\t\treturn nil\n\t\t\t\t},\n\t\t\t\tsharedFileVisitor,\n\t\t\t\texternalFileVisitor,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn b.String()\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command %q\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc TestPointCollapsingIter(t *testing.T) {\n\tvar def string\n\tdatadriven.RunTest(t, \"testdata/point_collapsing_iter\", func(t *testing.T, d *datadriven.TestData) string {\n\t\tswitch d.Cmd {\n\t\tcase \"define\":\n\t\t\tdef = d.Input\n\t\t\treturn \"\"\n\n\t\tcase \"iter\":\n\t\t\tvar kvs []base.InternalKV\n\t\t\tvar spans []keyspan.Span\n\t\t\tfor _, line := range strings.Split(def, \"\\n\") {\n\t\t\t\tfor _, key := range strings.Fields(line) {\n\t\t\t\t\tj := strings.Index(key, \":\")\n\t\t\t\t\tk := base.ParseInternalKey(key[:j])\n\t\t\t\t\tv := []byte(key[j+1:])\n\t\t\t\t\tif k.Kind() == InternalKeyKindRangeDelete {\n\t\t\t\t\t\tspans = append(spans, keyspan.Span{\n\t\t\t\t\t\t\tStart:     k.UserKey,\n\t\t\t\t\t\t\tEnd:       v,\n\t\t\t\t\t\t\tKeys:      []keyspan.Key{{Trailer: k.Trailer}},\n\t\t\t\t\t\t\tKeysOrder: 0,\n\t\t\t\t\t\t})\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tkvs = append(kvs, base.MakeInternalKV(k, v))\n\t\t\t\t}\n\t\t\t}\n\t\t\tf := base.NewFakeIter(kvs)\n\n\t\t\tksIter := keyspan.NewIter(base.DefaultComparer.Compare, spans)\n\t\t\tpcIter := &pointCollapsingIterator{\n\t\t\t\tcomparer: base.DefaultComparer,\n\t\t\t\tmerge:    base.DefaultMerger.Merge,\n\t\t\t\tseqNum:   math.MaxUint64,\n\t\t\t}\n\t\t\tpcIter.iter.Init(base.DefaultComparer, f, ksIter, keyspan.InterleavingIterOpts{})\n\t\t\tdefer pcIter.Close()\n\t\t\treturn itertest.RunInternalIterCmd(t, d, pcIter, itertest.Verbose)\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", d.Cmd)\n\t\t}\n\t})\n}\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "snapshot.go",
          "type": "blob",
          "size": 15.4658203125,
          "content": "// Copyright 2012 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"context\"\n\t\"io\"\n\t\"math\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/invariants\"\n\t\"github.com/cockroachdb/pebble/rangekey\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n)\n\n// Snapshot provides a read-only point-in-time view of the DB state.\ntype Snapshot struct {\n\t// The db the snapshot was created from.\n\tdb     *DB\n\tseqNum base.SeqNum\n\n\t// Set if part of an EventuallyFileOnlySnapshot.\n\tefos *EventuallyFileOnlySnapshot\n\n\t// The list the snapshot is linked into.\n\tlist *snapshotList\n\n\t// The next/prev link for the snapshotList doubly-linked list of snapshots.\n\tprev, next *Snapshot\n}\n\nvar _ Reader = (*Snapshot)(nil)\n\n// Get gets the value for the given key. It returns ErrNotFound if the Snapshot\n// does not contain the key.\n//\n// The caller should not modify the contents of the returned slice, but it is\n// safe to modify the contents of the argument after Get returns. The returned\n// slice will remain valid until the returned Closer is closed. On success, the\n// caller MUST call closer.Close() or a memory leak will occur.\nfunc (s *Snapshot) Get(key []byte) ([]byte, io.Closer, error) {\n\tif s.db == nil {\n\t\tpanic(ErrClosed)\n\t}\n\treturn s.db.getInternal(key, nil /* batch */, s)\n}\n\n// NewIter returns an iterator that is unpositioned (Iterator.Valid() will\n// return false). The iterator can be positioned via a call to SeekGE,\n// SeekLT, First or Last.\nfunc (s *Snapshot) NewIter(o *IterOptions) (*Iterator, error) {\n\treturn s.NewIterWithContext(context.Background(), o)\n}\n\n// NewIterWithContext is like NewIter, and additionally accepts a context for\n// tracing.\nfunc (s *Snapshot) NewIterWithContext(ctx context.Context, o *IterOptions) (*Iterator, error) {\n\tif s.db == nil {\n\t\tpanic(ErrClosed)\n\t}\n\treturn s.db.newIter(ctx, nil /* batch */, newIterOpts{\n\t\tsnapshot: snapshotIterOpts{seqNum: s.seqNum},\n\t}, o), nil\n}\n\n// ScanInternal scans all internal keys within the specified bounds, truncating\n// any rangedels and rangekeys to those bounds. For use when an external user\n// needs to be aware of all internal keys that make up a key range.\n//\n// See comment on db.ScanInternal for the behaviour that can be expected of\n// point keys deleted by range dels and keys masked by range keys.\nfunc (s *Snapshot) ScanInternal(\n\tctx context.Context,\n\tcategory sstable.Category,\n\tlower, upper []byte,\n\tvisitPointKey func(key *InternalKey, value LazyValue, iterInfo IteratorLevel) error,\n\tvisitRangeDel func(start, end []byte, seqNum base.SeqNum) error,\n\tvisitRangeKey func(start, end []byte, keys []rangekey.Key) error,\n\tvisitSharedFile func(sst *SharedSSTMeta) error,\n\tvisitExternalFile func(sst *ExternalFile) error,\n) error {\n\tif s.db == nil {\n\t\tpanic(ErrClosed)\n\t}\n\tscanInternalOpts := &scanInternalOptions{\n\t\tcategory:          category,\n\t\tvisitPointKey:     visitPointKey,\n\t\tvisitRangeDel:     visitRangeDel,\n\t\tvisitRangeKey:     visitRangeKey,\n\t\tvisitSharedFile:   visitSharedFile,\n\t\tvisitExternalFile: visitExternalFile,\n\t\tIterOptions: IterOptions{\n\t\t\tKeyTypes:   IterKeyTypePointsAndRanges,\n\t\t\tLowerBound: lower,\n\t\t\tUpperBound: upper,\n\t\t},\n\t}\n\n\titer, err := s.db.newInternalIter(ctx, snapshotIterOpts{seqNum: s.seqNum}, scanInternalOpts)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer iter.close()\n\n\treturn scanInternalImpl(ctx, lower, upper, iter, scanInternalOpts)\n}\n\n// closeLocked is similar to Close(), except it requires that db.mu be held\n// by the caller.\nfunc (s *Snapshot) closeLocked() error {\n\ts.db.mu.snapshots.remove(s)\n\n\t// If s was the previous earliest snapshot, we might be able to reclaim\n\t// disk space by dropping obsolete records that were pinned by s.\n\tif e := s.db.mu.snapshots.earliest(); e > s.seqNum {\n\t\ts.db.maybeScheduleCompactionPicker(pickElisionOnly)\n\t}\n\ts.db = nil\n\treturn nil\n}\n\n// Close closes the snapshot, releasing its resources. Close must be called.\n// Failure to do so will result in a tiny memory leak and a large leak of\n// resources on disk due to the entries the snapshot is preventing from being\n// deleted.\n//\n// d.mu must NOT be held by the caller.\nfunc (s *Snapshot) Close() error {\n\tdb := s.db\n\tif db == nil {\n\t\tpanic(ErrClosed)\n\t}\n\tdb.mu.Lock()\n\tdefer db.mu.Unlock()\n\treturn s.closeLocked()\n}\n\ntype snapshotList struct {\n\troot Snapshot\n}\n\nfunc (l *snapshotList) init() {\n\tl.root.next = &l.root\n\tl.root.prev = &l.root\n}\n\nfunc (l *snapshotList) empty() bool {\n\treturn l.root.next == &l.root\n}\n\nfunc (l *snapshotList) count() int {\n\tif l.empty() {\n\t\treturn 0\n\t}\n\tvar count int\n\tfor i := l.root.next; i != &l.root; i = i.next {\n\t\tcount++\n\t}\n\treturn count\n}\n\nfunc (l *snapshotList) earliest() base.SeqNum {\n\tv := base.SeqNum(math.MaxUint64)\n\tif !l.empty() {\n\t\tv = l.root.next.seqNum\n\t}\n\treturn v\n}\n\nfunc (l *snapshotList) toSlice() []base.SeqNum {\n\tif l.empty() {\n\t\treturn nil\n\t}\n\tvar results []base.SeqNum\n\tfor i := l.root.next; i != &l.root; i = i.next {\n\t\tresults = append(results, i.seqNum)\n\t}\n\treturn results\n}\n\nfunc (l *snapshotList) pushBack(s *Snapshot) {\n\tif s.list != nil || s.prev != nil || s.next != nil {\n\t\tpanic(\"pebble: snapshot list is inconsistent\")\n\t}\n\ts.prev = l.root.prev\n\ts.prev.next = s\n\ts.next = &l.root\n\ts.next.prev = s\n\ts.list = l\n}\n\nfunc (l *snapshotList) remove(s *Snapshot) {\n\tif s == &l.root {\n\t\tpanic(\"pebble: cannot remove snapshot list root node\")\n\t}\n\tif s.list != l {\n\t\tpanic(\"pebble: snapshot list is inconsistent\")\n\t}\n\ts.prev.next = s.next\n\ts.next.prev = s.prev\n\ts.next = nil // avoid memory leaks\n\ts.prev = nil // avoid memory leaks\n\ts.list = nil // avoid memory leaks\n}\n\n// EventuallyFileOnlySnapshot (aka EFOS) provides a read-only point-in-time view\n// of the database state, similar to Snapshot. An EventuallyFileOnlySnapshot\n// induces less write amplification than Snapshot, at the cost of increased space\n// amplification. While a Snapshot may increase write amplification across all\n// flushes and compactions for the duration of its lifetime, an\n// EventuallyFileOnlySnapshot only incurs that cost for flushes/compactions if\n// memtables at the time of EFOS instantiation contained keys that the EFOS is\n// interested in (i.e. its protectedRanges). In that case, the EFOS prevents\n// elision of keys visible to it, similar to a Snapshot, until those memtables\n// are flushed, and once that happens, the \"EventuallyFileOnlySnapshot\"\n// transitions to a file-only snapshot state in which it pins zombies sstables\n// like an open Iterator would, without pinning any memtables. Callers that can\n// tolerate the increased space amplification of pinning zombie sstables until\n// the snapshot is closed may prefer EventuallyFileOnlySnapshots for their\n// reduced write amplification. Callers that desire the benefits of the file-only\n// state that requires no pinning of memtables should call\n// `WaitForFileOnlySnapshot()` before relying on the EFOS to keep producing iterators\n// with zero write-amp and zero pinning of memtables in memory.\n//\n// EventuallyFileOnlySnapshots interact with the IngestAndExcise operation in\n// subtle ways. The IngestAndExcise can force the transition of an EFOS to a\n// file-only snapshot if an excise overlaps with the EFOS bounds.\ntype EventuallyFileOnlySnapshot struct {\n\tmu struct {\n\t\t// NB: If both this mutex and db.mu are being grabbed, db.mu should be\n\t\t// grabbed _before_ grabbing this one.\n\t\tsync.Mutex\n\n\t\t// Either the snap field is set below, or the version is set at any given\n\t\t// point of time. If a snapshot is referenced, this is not a file-only\n\t\t// snapshot yet, and if a version is set (and ref'd) this is a file-only\n\t\t// snapshot.\n\n\t\t// The wrapped regular snapshot, if not a file-only snapshot yet.\n\t\tsnap *Snapshot\n\t\t// The wrapped version reference, if a file-only snapshot.\n\t\tvers *version\n\t}\n\n\t// Key ranges to watch for an excise on.\n\tprotectedRanges []KeyRange\n\n\t// The db the snapshot was created from.\n\tdb     *DB\n\tseqNum base.SeqNum\n\tclosed chan struct{}\n}\n\nfunc (d *DB) makeEventuallyFileOnlySnapshot(keyRanges []KeyRange) *EventuallyFileOnlySnapshot {\n\tisFileOnly := true\n\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\tseqNum := d.mu.versions.visibleSeqNum.Load()\n\t// Check if any of the keyRanges overlap with a memtable.\n\tfor i := range d.mu.mem.queue {\n\t\td.mu.mem.queue[i].computePossibleOverlaps(func(bounded) shouldContinue {\n\t\t\tisFileOnly = false\n\t\t\treturn stopIteration\n\t\t}, sliceAsBounded(keyRanges)...)\n\t}\n\tes := &EventuallyFileOnlySnapshot{\n\t\tdb:              d,\n\t\tseqNum:          seqNum,\n\t\tprotectedRanges: keyRanges,\n\t\tclosed:          make(chan struct{}),\n\t}\n\tif isFileOnly {\n\t\tes.mu.vers = d.mu.versions.currentVersion()\n\t\tes.mu.vers.Ref()\n\t} else {\n\t\ts := &Snapshot{\n\t\t\tdb:     d,\n\t\t\tseqNum: seqNum,\n\t\t}\n\t\ts.efos = es\n\t\tes.mu.snap = s\n\t\td.mu.snapshots.pushBack(s)\n\t}\n\treturn es\n}\n\n// Transitions this EventuallyFileOnlySnapshot to a file-only snapshot. Requires\n// earliestUnflushedSeqNum and vers to correspond to the same Version from the\n// current or a past acquisition of db.mu. vers must have been Ref()'d before\n// that mutex was released, if it was released.\n//\n// NB: The caller is expected to check for es.excised before making this\n// call.\n//\n// d.mu must be held when calling this method.\nfunc (es *EventuallyFileOnlySnapshot) transitionToFileOnlySnapshot(vers *version) error {\n\tes.mu.Lock()\n\tselect {\n\tcase <-es.closed:\n\t\tvers.UnrefLocked()\n\t\tes.mu.Unlock()\n\t\treturn ErrClosed\n\tdefault:\n\t}\n\tif es.mu.snap == nil {\n\t\tes.mu.Unlock()\n\t\tpanic(\"pebble: tried to transition an eventually-file-only-snapshot twice\")\n\t}\n\t// The caller has already called Ref() on vers.\n\tes.mu.vers = vers\n\t// NB: The callers should have already done a check of es.excised.\n\toldSnap := es.mu.snap\n\tes.mu.snap = nil\n\tes.mu.Unlock()\n\treturn oldSnap.closeLocked()\n}\n\n// hasTransitioned returns true if this EFOS has transitioned to a file-only\n// snapshot.\nfunc (es *EventuallyFileOnlySnapshot) hasTransitioned() bool {\n\tes.mu.Lock()\n\tdefer es.mu.Unlock()\n\treturn es.mu.vers != nil\n}\n\n// waitForFlush waits for a flush on any memtables that need to be flushed\n// before this EFOS can transition to a file-only snapshot. If this EFOS is\n// waiting on a flush of the mutable memtable, it forces a rotation within\n// `dur` duration. For immutable memtables, it schedules a flush and waits for\n// it to finish.\nfunc (es *EventuallyFileOnlySnapshot) waitForFlush(ctx context.Context, dur time.Duration) error {\n\tes.db.mu.Lock()\n\tdefer es.db.mu.Unlock()\n\n\tearliestUnflushedSeqNum := es.db.getEarliestUnflushedSeqNumLocked()\n\tfor earliestUnflushedSeqNum < es.seqNum {\n\t\tselect {\n\t\tcase <-es.closed:\n\t\t\treturn ErrClosed\n\t\tcase <-ctx.Done():\n\t\t\treturn ctx.Err()\n\t\tdefault:\n\t\t}\n\t\t// Check if the current mutable memtable contains keys less than seqNum.\n\t\t// If so, rotate it.\n\t\tif es.db.mu.mem.mutable.logSeqNum < es.seqNum && dur.Nanoseconds() > 0 {\n\t\t\tes.db.maybeScheduleDelayedFlush(es.db.mu.mem.mutable, dur)\n\t\t} else {\n\t\t\t// Find the last memtable that contains seqNums less than es.seqNum,\n\t\t\t// and force a flush on it.\n\t\t\tvar mem *flushableEntry\n\t\t\tfor i := range es.db.mu.mem.queue {\n\t\t\t\tif es.db.mu.mem.queue[i].logSeqNum < es.seqNum {\n\t\t\t\t\tmem = es.db.mu.mem.queue[i]\n\t\t\t\t}\n\t\t\t}\n\t\t\tmem.flushForced = true\n\t\t\tes.db.maybeScheduleFlush()\n\t\t}\n\t\tes.db.mu.compact.cond.Wait()\n\n\t\tearliestUnflushedSeqNum = es.db.getEarliestUnflushedSeqNumLocked()\n\t}\n\treturn nil\n}\n\n// WaitForFileOnlySnapshot blocks the calling goroutine until this snapshot\n// has been converted into a file-only snapshot (i.e. all memtables containing\n// keys < seqNum are flushed). A duration can be passed in, and if nonzero,\n// a delayed flush will be scheduled at that duration if necessary.\n//\n// Idempotent; can be called multiple times with no side effects.\nfunc (es *EventuallyFileOnlySnapshot) WaitForFileOnlySnapshot(\n\tctx context.Context, dur time.Duration,\n) error {\n\tif es.hasTransitioned() {\n\t\treturn nil\n\t}\n\n\tif err := es.waitForFlush(ctx, dur); err != nil {\n\t\treturn err\n\t}\n\n\tif invariants.Enabled {\n\t\t// Since we aren't returning an error, we _must_ have transitioned to a\n\t\t// file-only snapshot by now.\n\t\tif !es.hasTransitioned() {\n\t\t\tpanic(\"expected EFOS to have transitioned to file-only snapshot after flush\")\n\t\t}\n\t}\n\treturn nil\n}\n\n// Close closes the file-only snapshot and releases all referenced resources.\n// Not idempotent.\nfunc (es *EventuallyFileOnlySnapshot) Close() error {\n\tclose(es.closed)\n\tes.db.mu.Lock()\n\tdefer es.db.mu.Unlock()\n\tes.mu.Lock()\n\tdefer es.mu.Unlock()\n\n\tif es.mu.snap != nil {\n\t\tif err := es.mu.snap.closeLocked(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif es.mu.vers != nil {\n\t\tes.mu.vers.UnrefLocked()\n\t}\n\treturn nil\n}\n\n// Get implements the Reader interface.\nfunc (es *EventuallyFileOnlySnapshot) Get(key []byte) (value []byte, closer io.Closer, err error) {\n\t// TODO(jackson): Use getInternal.\n\titer, err := es.NewIter(nil)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tif !iter.SeekPrefixGE(key) {\n\t\tif err = firstError(iter.Error(), iter.Close()); err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t\treturn nil, nil, ErrNotFound\n\t}\n\tif !es.db.equal(iter.Key(), key) {\n\t\treturn nil, nil, firstError(iter.Close(), ErrNotFound)\n\t}\n\treturn iter.Value(), iter, nil\n}\n\n// NewIter returns an iterator that is unpositioned (Iterator.Valid() will\n// return false). The iterator can be positioned via a call to SeekGE,\n// SeekLT, First or Last.\nfunc (es *EventuallyFileOnlySnapshot) NewIter(o *IterOptions) (*Iterator, error) {\n\treturn es.NewIterWithContext(context.Background(), o)\n}\n\n// NewIterWithContext is like NewIter, and additionally accepts a context for\n// tracing.\nfunc (es *EventuallyFileOnlySnapshot) NewIterWithContext(\n\tctx context.Context, o *IterOptions,\n) (*Iterator, error) {\n\tselect {\n\tcase <-es.closed:\n\t\tpanic(ErrClosed)\n\tdefault:\n\t}\n\n\tes.mu.Lock()\n\tdefer es.mu.Unlock()\n\tif es.mu.vers != nil {\n\t\tsOpts := snapshotIterOpts{seqNum: es.seqNum, vers: es.mu.vers}\n\t\treturn es.db.newIter(ctx, nil /* batch */, newIterOpts{snapshot: sOpts}, o), nil\n\t}\n\n\tsOpts := snapshotIterOpts{seqNum: es.seqNum}\n\titer := es.db.newIter(ctx, nil /* batch */, newIterOpts{snapshot: sOpts}, o)\n\treturn iter, nil\n}\n\n// ScanInternal scans all internal keys within the specified bounds, truncating\n// any rangedels and rangekeys to those bounds. For use when an external user\n// needs to be aware of all internal keys that make up a key range.\n//\n// See comment on db.ScanInternal for the behaviour that can be expected of\n// point keys deleted by range dels and keys masked by range keys.\nfunc (es *EventuallyFileOnlySnapshot) ScanInternal(\n\tctx context.Context,\n\tcategory sstable.Category,\n\tlower, upper []byte,\n\tvisitPointKey func(key *InternalKey, value LazyValue, iterInfo IteratorLevel) error,\n\tvisitRangeDel func(start, end []byte, seqNum base.SeqNum) error,\n\tvisitRangeKey func(start, end []byte, keys []rangekey.Key) error,\n\tvisitSharedFile func(sst *SharedSSTMeta) error,\n\tvisitExternalFile func(sst *ExternalFile) error,\n) error {\n\tif es.db == nil {\n\t\tpanic(ErrClosed)\n\t}\n\tvar sOpts snapshotIterOpts\n\topts := &scanInternalOptions{\n\t\tcategory: category,\n\t\tIterOptions: IterOptions{\n\t\t\tKeyTypes:   IterKeyTypePointsAndRanges,\n\t\t\tLowerBound: lower,\n\t\t\tUpperBound: upper,\n\t\t},\n\t\tvisitPointKey:     visitPointKey,\n\t\tvisitRangeDel:     visitRangeDel,\n\t\tvisitRangeKey:     visitRangeKey,\n\t\tvisitSharedFile:   visitSharedFile,\n\t\tvisitExternalFile: visitExternalFile,\n\t}\n\tes.mu.Lock()\n\tif es.mu.vers != nil {\n\t\tsOpts = snapshotIterOpts{\n\t\t\tseqNum: es.seqNum,\n\t\t\tvers:   es.mu.vers,\n\t\t}\n\t} else {\n\t\tsOpts = snapshotIterOpts{\n\t\t\tseqNum: es.seqNum,\n\t\t}\n\t}\n\tes.mu.Unlock()\n\titer, err := es.db.newInternalIter(ctx, sOpts, opts)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer iter.close()\n\n\treturn scanInternalImpl(ctx, lower, upper, iter, opts)\n}\n"
        },
        {
          "name": "snapshot_test.go",
          "type": "blob",
          "size": 9.7333984375,
          "content": "// Copyright 2012 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"math/rand/v2\"\n\t\"reflect\"\n\t\"runtime\"\n\t\"strings\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/cockroachdb/crlib/crstrings\"\n\t\"github.com/cockroachdb/datadriven\"\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestSnapshotListToSlice(t *testing.T) {\n\ttestCases := []struct {\n\t\tvals []base.SeqNum\n\t}{\n\t\t{nil},\n\t\t{[]base.SeqNum{1}},\n\t\t{[]base.SeqNum{1, 2, 3}},\n\t\t{[]base.SeqNum{3, 2, 1}},\n\t}\n\tfor _, c := range testCases {\n\t\tt.Run(\"\", func(t *testing.T) {\n\t\t\tvar l snapshotList\n\t\t\tl.init()\n\t\t\tfor _, v := range c.vals {\n\t\t\t\tl.pushBack(&Snapshot{seqNum: v})\n\t\t\t}\n\t\t\tslice := l.toSlice()\n\t\t\tif !reflect.DeepEqual(c.vals, slice) {\n\t\t\t\tt.Fatalf(\"expected %d, but got %d\", c.vals, slice)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc testSnapshotImpl(t *testing.T, newSnapshot func(d *DB) Reader) {\n\tvar d *DB\n\tvar snapshots map[string]Reader\n\n\tclose := func() {\n\t\tfor _, s := range snapshots {\n\t\t\trequire.NoError(t, s.Close())\n\t\t}\n\t\tsnapshots = nil\n\t\tif d != nil {\n\t\t\trequire.NoError(t, d.Close())\n\t\t\td = nil\n\t\t}\n\t}\n\tdefer close()\n\n\trandVersion := func() FormatMajorVersion {\n\t\treturn FormatMinSupported + FormatMajorVersion(rand.IntN(int(internalFormatNewest-FormatMinSupported)+1))\n\t}\n\tdatadriven.RunTest(t, \"testdata/snapshot\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"define\":\n\t\t\tclose()\n\n\t\t\tvar err error\n\t\t\toptions := &Options{\n\t\t\t\tFS:                 vfs.NewMem(),\n\t\t\t\tFormatMajorVersion: randVersion(),\n\t\t\t}\n\t\t\tif td.HasArg(\"block-size\") {\n\t\t\t\tvar blockSize int\n\t\t\t\ttd.ScanArgs(t, \"block-size\", &blockSize)\n\t\t\t\toptions.Levels = make([]LevelOptions, 1)\n\t\t\t\toptions.Levels[0].BlockSize = blockSize\n\t\t\t\toptions.Levels[0].IndexBlockSize = blockSize\n\t\t\t}\n\t\t\td, err = Open(\"\", options)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tsnapshots = make(map[string]Reader)\n\n\t\t\tfor _, line := range crstrings.Lines(td.Input) {\n\t\t\t\tparts := strings.Fields(line)\n\t\t\t\tvar err error\n\t\t\t\tswitch parts[0] {\n\t\t\t\tcase \"set\":\n\t\t\t\t\tif len(parts) != 3 {\n\t\t\t\t\t\treturn fmt.Sprintf(\"%s expects 2 arguments\", parts[0])\n\t\t\t\t\t}\n\t\t\t\t\terr = d.Set([]byte(parts[1]), []byte(parts[2]), nil)\n\t\t\t\tcase \"del\":\n\t\t\t\t\tif len(parts) != 2 {\n\t\t\t\t\t\treturn fmt.Sprintf(\"%s expects 1 argument\", parts[0])\n\t\t\t\t\t}\n\t\t\t\t\terr = d.Delete([]byte(parts[1]), nil)\n\t\t\t\tcase \"merge\":\n\t\t\t\t\tif len(parts) != 3 {\n\t\t\t\t\t\treturn fmt.Sprintf(\"%s expects 2 arguments\", parts[0])\n\t\t\t\t\t}\n\t\t\t\t\terr = d.Merge([]byte(parts[1]), []byte(parts[2]), nil)\n\t\t\t\tcase \"snapshot\":\n\t\t\t\t\tif len(parts) != 2 {\n\t\t\t\t\t\treturn fmt.Sprintf(\"%s expects 1 argument\", parts[0])\n\t\t\t\t\t}\n\t\t\t\t\tsnapshots[parts[1]] = newSnapshot(d)\n\t\t\t\tcase \"compact\":\n\t\t\t\t\tif len(parts) != 2 {\n\t\t\t\t\t\treturn fmt.Sprintf(\"%s expects 1 argument\", parts[0])\n\t\t\t\t\t}\n\t\t\t\t\tkeys := strings.Split(parts[1], \"-\")\n\t\t\t\t\tif len(keys) != 2 {\n\t\t\t\t\t\treturn fmt.Sprintf(\"malformed key range: %s\", parts[1])\n\t\t\t\t\t}\n\t\t\t\t\terr = d.Compact([]byte(keys[0]), []byte(keys[1]), false)\n\t\t\t\tdefault:\n\t\t\t\t\treturn fmt.Sprintf(\"unknown op: %s\", parts[0])\n\t\t\t\t}\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"db-state\":\n\t\t\td.mu.Lock()\n\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\td.mu.Unlock()\n\t\t\treturn s\n\n\t\tcase \"iter\":\n\t\t\tvar iter *Iterator\n\t\t\tif len(td.CmdArgs) == 1 {\n\t\t\t\tif td.CmdArgs[0].Key != \"snapshot\" {\n\t\t\t\t\treturn fmt.Sprintf(\"unknown argument: %s\", td.CmdArgs[0])\n\t\t\t\t}\n\t\t\t\tif len(td.CmdArgs[0].Vals) != 1 {\n\t\t\t\t\treturn fmt.Sprintf(\"%s expects 1 value: %s\", td.CmdArgs[0].Key, td.CmdArgs[0])\n\t\t\t\t}\n\t\t\t\tname := td.CmdArgs[0].Vals[0]\n\t\t\t\tsnapshot := snapshots[name]\n\t\t\t\tif snapshot == nil {\n\t\t\t\t\treturn fmt.Sprintf(\"unable to find snapshot \\\"%s\\\"\", name)\n\t\t\t\t}\n\t\t\t\titer, _ = snapshot.NewIter(nil)\n\t\t\t} else {\n\t\t\t\titer, _ = d.NewIter(nil)\n\t\t\t}\n\t\t\tdefer iter.Close()\n\n\t\t\tvar b bytes.Buffer\n\t\t\tfor _, line := range crstrings.Lines(td.Input) {\n\t\t\t\tparts := strings.Fields(line)\n\t\t\t\tswitch parts[0] {\n\t\t\t\tcase \"first\":\n\t\t\t\t\titer.First()\n\t\t\t\tcase \"last\":\n\t\t\t\t\titer.Last()\n\t\t\t\tcase \"seek-ge\":\n\t\t\t\t\tif len(parts) != 2 {\n\t\t\t\t\t\treturn \"seek-ge <key>\\n\"\n\t\t\t\t\t}\n\t\t\t\t\titer.SeekGE([]byte(strings.TrimSpace(parts[1])))\n\t\t\t\tcase \"seek-lt\":\n\t\t\t\t\tif len(parts) != 2 {\n\t\t\t\t\t\treturn \"seek-lt <key>\\n\"\n\t\t\t\t\t}\n\t\t\t\t\titer.SeekLT([]byte(strings.TrimSpace(parts[1])))\n\t\t\t\tcase \"next\":\n\t\t\t\t\titer.Next()\n\t\t\t\tcase \"prev\":\n\t\t\t\t\titer.Prev()\n\t\t\t\tdefault:\n\t\t\t\t\treturn fmt.Sprintf(\"unknown op: %s\", parts[0])\n\t\t\t\t}\n\t\t\t\tif iter.Valid() {\n\t\t\t\t\tfmt.Fprintf(&b, \"%s:%s\\n\", iter.Key(), iter.Value())\n\t\t\t\t} else if err := iter.Error(); err != nil {\n\t\t\t\t\tfmt.Fprintf(&b, \"err=%v\\n\", err)\n\t\t\t\t} else {\n\t\t\t\t\tfmt.Fprintf(&b, \".\\n\")\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn b.String()\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc TestSnapshot(t *testing.T) {\n\ttestSnapshotImpl(t, func(d *DB) Reader {\n\t\treturn d.NewSnapshot()\n\t})\n}\n\nfunc TestEventuallyFileOnlySnapshot(t *testing.T) {\n\ttestSnapshotImpl(t, func(d *DB) Reader {\n\t\t// NB: all keys in testdata/snapshot fall within the ASCII keyrange a-z.\n\t\treturn d.NewEventuallyFileOnlySnapshot([]KeyRange{{Start: []byte(\"a\"), End: []byte(\"z\")}})\n\t})\n}\n\nfunc TestSnapshotClosed(t *testing.T) {\n\td, err := Open(\"\", &Options{\n\t\tFS: vfs.NewMem(),\n\t})\n\trequire.NoError(t, err)\n\n\tcatch := func(f func()) (err error) {\n\t\tdefer func() {\n\t\t\tif r := recover(); r != nil {\n\t\t\t\terr = r.(error)\n\t\t\t}\n\t\t}()\n\t\tf()\n\t\treturn nil\n\t}\n\n\tsnap := d.NewSnapshot()\n\trequire.NoError(t, snap.Close())\n\trequire.True(t, errors.Is(catch(func() { _ = snap.Close() }), ErrClosed))\n\trequire.True(t, errors.Is(catch(func() { _, _, _ = snap.Get(nil) }), ErrClosed))\n\trequire.True(t, errors.Is(catch(func() { snap.NewIter(nil) }), ErrClosed))\n\n\trequire.NoError(t, d.Close())\n}\n\nfunc TestSnapshotRangeDeletionStress(t *testing.T) {\n\tconst runs = 200\n\tconst middleKey = runs * runs\n\n\td, err := Open(\"\", &Options{\n\t\tFS: vfs.NewMem(),\n\t})\n\trequire.NoError(t, err)\n\n\tmkkey := func(k int) []byte {\n\t\treturn []byte(fmt.Sprintf(\"%08d\", k))\n\t}\n\tv := []byte(\"hello world\")\n\n\tsnapshots := make([]*Snapshot, 0, runs)\n\tfor r := 0; r < runs; r++ {\n\t\t// We use a keyspace that is 2*runs*runs wide. In other words there are\n\t\t// 2*runs sections of the keyspace, each with runs elements. On every\n\t\t// run, we write to the r-th element of each section of the keyspace.\n\t\tfor i := 0; i < 2*runs; i++ {\n\t\t\terr := d.Set(mkkey(runs*i+r), v, nil)\n\t\t\trequire.NoError(t, err)\n\t\t}\n\n\t\t// Now we delete some of the keyspace through a DeleteRange. We delete from\n\t\t// the middle of the keyspace outwards. The keyspace is made of 2*runs\n\t\t// sections, and we delete an additional two of these sections per run.\n\t\terr := d.DeleteRange(mkkey(middleKey-runs*r), mkkey(middleKey+runs*r), nil)\n\t\trequire.NoError(t, err)\n\n\t\tsnapshots = append(snapshots, d.NewSnapshot())\n\t}\n\n\t// Check that all the snapshots contain the expected number of keys.\n\t// Iterating over so many keys is slow, so do it in parallel.\n\tvar wg sync.WaitGroup\n\tsem := make(chan struct{}, runtime.GOMAXPROCS(0))\n\tfor r := range snapshots {\n\t\twg.Add(1)\n\t\tsem <- struct{}{}\n\t\tgo func(r int) {\n\t\t\tdefer func() {\n\t\t\t\t<-sem\n\t\t\t\twg.Done()\n\t\t\t}()\n\n\t\t\t// Count the keys at this snapshot.\n\t\t\titer, _ := snapshots[r].NewIter(nil)\n\t\t\tvar keysFound int\n\t\t\tfor iter.First(); iter.Valid(); iter.Next() {\n\t\t\t\tkeysFound++\n\t\t\t}\n\t\t\terr := firstError(iter.Error(), iter.Close())\n\t\t\tif err != nil {\n\t\t\t\tt.Error(err)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// At the time that this snapshot was taken, (r+1)*2*runs unique keys\n\t\t\t// were Set (one in each of the 2*runs sections per run).  But this\n\t\t\t// run also deleted the 2*r middlemost sections.  When this snapshot\n\t\t\t// was taken, a Set to each of those sections had been made (r+1)\n\t\t\t// times, so 2*r*(r+1) previously-set keys are now deleted.\n\n\t\t\tkeysExpected := (r+1)*2*runs - 2*r*(r+1)\n\t\t\tif keysFound != keysExpected {\n\t\t\t\tt.Errorf(\"%d: found %d keys, want %d\", r, keysFound, keysExpected)\n\t\t\t}\n\t\t\tif err := snapshots[r].Close(); err != nil {\n\t\t\t\tt.Error(err)\n\t\t\t}\n\t\t}(r)\n\t}\n\twg.Wait()\n\trequire.NoError(t, d.Close())\n}\n\n// TestNewSnapshotRace tests atomicity of NewSnapshot.\n//\n// It tests for a regression of a previous race condition in which NewSnapshot\n// would retrieve the visible sequence number for a new snapshot before\n// locking the database mutex to add the snapshot. A write and flush that\n// that occurred between the reading of the sequence number and appending the\n// snapshot could drop keys required by the snapshot.\nfunc TestNewSnapshotRace(t *testing.T) {\n\tconst runs = 10\n\td, err := Open(\"\", &Options{FS: vfs.NewMem()})\n\trequire.NoError(t, err)\n\n\tv := []byte(`foo`)\n\tch := make(chan string)\n\tvar wg sync.WaitGroup\n\twg.Add(1)\n\n\tgo func() {\n\t\tdefer wg.Done()\n\t\tfor k := range ch {\n\t\t\tif err := d.Set([]byte(k), v, nil); err != nil {\n\t\t\t\tt.Error(err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif err := d.Flush(); err != nil {\n\t\t\t\tt.Error(err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n\tfor i := 0; i < runs; i++ {\n\t\t// This main test goroutine sets `k` before creating a new snapshot.\n\t\t// The key `k` should always be present within the snapshot.\n\t\tk := fmt.Sprintf(\"key%06d\", i)\n\t\trequire.NoError(t, d.Set([]byte(k), v, nil))\n\n\t\t// Lock d.mu in another goroutine so that our call to NewSnapshot\n\t\t// will need to contend for d.mu.\n\t\twg.Add(1)\n\t\tlocked := make(chan struct{})\n\t\tgo func() {\n\t\t\tdefer wg.Done()\n\t\t\td.mu.Lock()\n\t\t\tclose(locked)\n\t\t\ttime.Sleep(20 * time.Millisecond)\n\t\t\td.mu.Unlock()\n\t\t}()\n\t\t<-locked\n\n\t\t// Tell the other goroutine to overwrite `k` with a later sequence\n\t\t// number. It's indeterminate which key we'll read, but we should\n\t\t// always read one of them.\n\t\tch <- k\n\t\ts := d.NewSnapshot()\n\t\t_, c, err := s.Get([]byte(k))\n\t\trequire.NoError(t, err)\n\t\trequire.NoError(t, c.Close())\n\t\trequire.NoError(t, s.Close())\n\t}\n\tclose(ch)\n\twg.Wait()\n\trequire.NoError(t, d.Close())\n}\n"
        },
        {
          "name": "sstable",
          "type": "tree",
          "content": null
        },
        {
          "name": "table_stats.go",
          "type": "blob",
          "size": 41.423828125,
          "content": "// Copyright 2020 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"math\"\n\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/invariants\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan/keyspanimpl\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n\t\"github.com/cockroachdb/pebble/sstable/block\"\n)\n\n// In-memory statistics about tables help inform compaction picking, but may\n// be expensive to calculate or load from disk. Every time a database is\n// opened, these statistics must be reloaded or recalculated. To minimize\n// impact on user activity and compactions, we load these statistics\n// asynchronously in the background and store loaded statistics in each\n// table's *FileMetadata.\n//\n// This file implements the asynchronous loading of statistics by maintaining\n// a list of files that require statistics, alongside their LSM levels.\n// Whenever new files are added to the LSM, the files are appended to\n// d.mu.tableStats.pending. If a stats collection job is not currently\n// running, one is started in a separate goroutine.\n//\n// The stats collection job grabs and clears the pending list, computes table\n// statistics relative to the current readState and updates the tables' file\n// metadata. New pending files may accumulate during a stats collection job,\n// so a completing job triggers a new job if necessary. Only one job runs at a\n// time.\n//\n// When an existing database is opened, all files lack in-memory statistics.\n// These files' stats are loaded incrementally whenever the pending list is\n// empty by scanning a current readState for files missing statistics. Once a\n// job completes a scan without finding any remaining files without\n// statistics, it flips a `loadedInitial` flag. From then on, the stats\n// collection job only needs to load statistics for new files appended to the\n// pending list.\n\nfunc (d *DB) maybeCollectTableStatsLocked() {\n\tif d.shouldCollectTableStatsLocked() {\n\t\tgo d.collectTableStats()\n\t}\n}\n\n// updateTableStatsLocked is called when new files are introduced, after the\n// read state has been updated. It may trigger a new stat collection.\n// DB.mu must be locked when calling.\nfunc (d *DB) updateTableStatsLocked(newFiles []manifest.NewFileEntry) {\n\tvar needStats bool\n\tfor _, nf := range newFiles {\n\t\tif !nf.Meta.StatsValid() {\n\t\t\tneedStats = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif !needStats {\n\t\treturn\n\t}\n\n\td.mu.tableStats.pending = append(d.mu.tableStats.pending, newFiles...)\n\td.maybeCollectTableStatsLocked()\n}\n\nfunc (d *DB) shouldCollectTableStatsLocked() bool {\n\treturn !d.mu.tableStats.loading &&\n\t\td.closed.Load() == nil &&\n\t\t!d.opts.DisableTableStats &&\n\t\t(len(d.mu.tableStats.pending) > 0 || !d.mu.tableStats.loadedInitial)\n}\n\n// collectTableStats runs a table stats collection job, returning true if the\n// invocation did the collection work, false otherwise (e.g. if another job was\n// already running).\nfunc (d *DB) collectTableStats() bool {\n\tconst maxTableStatsPerScan = 50\n\n\td.mu.Lock()\n\tif !d.shouldCollectTableStatsLocked() {\n\t\td.mu.Unlock()\n\t\treturn false\n\t}\n\n\tpending := d.mu.tableStats.pending\n\td.mu.tableStats.pending = nil\n\td.mu.tableStats.loading = true\n\tjobID := d.newJobIDLocked()\n\tloadedInitial := d.mu.tableStats.loadedInitial\n\t// Drop DB.mu before performing IO.\n\td.mu.Unlock()\n\n\t// Every run of collectTableStats either collects stats from the pending\n\t// list (if non-empty) or from scanning the version (loadedInitial is\n\t// false). This job only runs if at least one of those conditions holds.\n\n\t// Grab a read state to scan for tables.\n\trs := d.loadReadState()\n\tvar collected []collectedStats\n\tvar hints []deleteCompactionHint\n\tif len(pending) > 0 {\n\t\tcollected, hints = d.loadNewFileStats(rs, pending)\n\t} else {\n\t\tvar moreRemain bool\n\t\tvar buf [maxTableStatsPerScan]collectedStats\n\t\tcollected, hints, moreRemain = d.scanReadStateTableStats(rs, buf[:0])\n\t\tloadedInitial = !moreRemain\n\t}\n\trs.unref()\n\n\t// Update the FileMetadata with the loaded stats while holding d.mu.\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\td.mu.tableStats.loading = false\n\tif loadedInitial && !d.mu.tableStats.loadedInitial {\n\t\td.mu.tableStats.loadedInitial = loadedInitial\n\t\td.opts.EventListener.TableStatsLoaded(TableStatsInfo{\n\t\t\tJobID: int(jobID),\n\t\t})\n\t}\n\n\tmaybeCompact := false\n\tfor _, c := range collected {\n\t\tc.fileMetadata.Stats = c.TableStats\n\t\tmaybeCompact = maybeCompact || fileCompensation(c.fileMetadata) > 0\n\t\tc.fileMetadata.StatsMarkValid()\n\t}\n\n\td.mu.tableStats.cond.Broadcast()\n\td.maybeCollectTableStatsLocked()\n\tif len(hints) > 0 && !d.opts.private.disableDeleteOnlyCompactions {\n\t\t// Verify that all of the hint tombstones' files still exist in the\n\t\t// current version. Otherwise, the tombstone itself may have been\n\t\t// compacted into L6 and more recent keys may have had their sequence\n\t\t// numbers zeroed.\n\t\t//\n\t\t// Note that it's possible that the tombstone file is being compacted\n\t\t// presently. In that case, the file will be present in v. When the\n\t\t// compaction finishes compacting the tombstone file, it will detect\n\t\t// and clear the hint.\n\t\t//\n\t\t// See DB.maybeUpdateDeleteCompactionHints.\n\t\tv := d.mu.versions.currentVersion()\n\t\tkeepHints := hints[:0]\n\t\tfor _, h := range hints {\n\t\t\tif v.Contains(h.tombstoneLevel, h.tombstoneFile) {\n\t\t\t\tkeepHints = append(keepHints, h)\n\t\t\t}\n\t\t}\n\t\td.mu.compact.deletionHints = append(d.mu.compact.deletionHints, keepHints...)\n\t}\n\tif maybeCompact {\n\t\td.maybeScheduleCompaction()\n\t}\n\treturn true\n}\n\ntype collectedStats struct {\n\t*fileMetadata\n\tmanifest.TableStats\n}\n\nfunc (d *DB) loadNewFileStats(\n\trs *readState, pending []manifest.NewFileEntry,\n) ([]collectedStats, []deleteCompactionHint) {\n\tvar hints []deleteCompactionHint\n\tcollected := make([]collectedStats, 0, len(pending))\n\tfor _, nf := range pending {\n\t\t// A file's stats might have been populated by an earlier call to\n\t\t// loadNewFileStats if the file was moved.\n\t\t// NB: We're not holding d.mu which protects f.Stats, but only\n\t\t// collectTableStats updates f.Stats for active files, and we\n\t\t// ensure only one goroutine runs it at a time through\n\t\t// d.mu.tableStats.loading.\n\t\tif nf.Meta.StatsValid() {\n\t\t\tcontinue\n\t\t}\n\n\t\t// The file isn't guaranteed to still be live in the readState's\n\t\t// version. It may have been deleted or moved. Skip it if it's not in\n\t\t// the expected level.\n\t\tif !rs.current.Contains(nf.Level, nf.Meta) {\n\t\t\tcontinue\n\t\t}\n\n\t\tstats, newHints, err := d.loadTableStats(\n\t\t\trs.current, nf.Level,\n\t\t\tnf.Meta,\n\t\t)\n\t\tif err != nil {\n\t\t\td.opts.EventListener.BackgroundError(err)\n\t\t\tcontinue\n\t\t}\n\t\t// NB: We don't update the FileMetadata yet, because we aren't\n\t\t// holding DB.mu. We'll copy it to the FileMetadata after we're\n\t\t// finished with IO.\n\t\tcollected = append(collected, collectedStats{\n\t\t\tfileMetadata: nf.Meta,\n\t\t\tTableStats:   stats,\n\t\t})\n\t\thints = append(hints, newHints...)\n\t}\n\treturn collected, hints\n}\n\n// scanReadStateTableStats is run by an active stat collection job when there\n// are no pending new files, but there might be files that existed at Open for\n// which we haven't loaded table stats.\nfunc (d *DB) scanReadStateTableStats(\n\trs *readState, fill []collectedStats,\n) ([]collectedStats, []deleteCompactionHint, bool) {\n\tmoreRemain := false\n\tvar hints []deleteCompactionHint\n\tsizesChecked := make(map[base.DiskFileNum]struct{})\n\tfor l, levelMetadata := range rs.current.Levels {\n\t\titer := levelMetadata.Iter()\n\t\tfor f := iter.First(); f != nil; f = iter.Next() {\n\t\t\t// NB: We're not holding d.mu which protects f.Stats, but only the\n\t\t\t// active stats collection job updates f.Stats for active files,\n\t\t\t// and we ensure only one goroutine runs it at a time through\n\t\t\t// d.mu.tableStats.loading. This makes it safe to read validity\n\t\t\t// through f.Stats.ValidLocked despite not holding d.mu.\n\t\t\tif f.StatsValid() {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Limit how much work we do per read state. The older the read\n\t\t\t// state is, the higher the likelihood files are no longer being\n\t\t\t// used in the current version. If we've exhausted our allowance,\n\t\t\t// return true for the last return value to signal there's more\n\t\t\t// work to do.\n\t\t\tif len(fill) == cap(fill) {\n\t\t\t\tmoreRemain = true\n\t\t\t\treturn fill, hints, moreRemain\n\t\t\t}\n\n\t\t\t// If the file is remote and not SharedForeign, we should check if its size\n\t\t\t// matches. This is because checkConsistency skips over remote files.\n\t\t\t//\n\t\t\t// SharedForeign and External files are skipped as their sizes are allowed\n\t\t\t// to have a mismatch; the size stored in the FileBacking is just the part\n\t\t\t// of the file that is referenced by this Pebble instance, not the size of\n\t\t\t// the whole object.\n\t\t\tobjMeta, err := d.objProvider.Lookup(fileTypeTable, f.FileBacking.DiskFileNum)\n\t\t\tif err != nil {\n\t\t\t\t// Set `moreRemain` so we'll try again.\n\t\t\t\tmoreRemain = true\n\t\t\t\td.opts.EventListener.BackgroundError(err)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tshouldCheckSize := objMeta.IsRemote() &&\n\t\t\t\t!d.objProvider.IsSharedForeign(objMeta) &&\n\t\t\t\t!objMeta.IsExternal()\n\t\t\tif _, ok := sizesChecked[f.FileBacking.DiskFileNum]; !ok && shouldCheckSize {\n\t\t\t\tsize, err := d.objProvider.Size(objMeta)\n\t\t\t\tfileSize := f.FileBacking.Size\n\t\t\t\tif err != nil {\n\t\t\t\t\tmoreRemain = true\n\t\t\t\t\td.opts.EventListener.BackgroundError(err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif size != int64(fileSize) {\n\t\t\t\t\terr := errors.Errorf(\n\t\t\t\t\t\t\"during consistency check in loadTableStats: L%d: %s: object size mismatch (%s): %d (provider) != %d (MANIFEST)\",\n\t\t\t\t\t\terrors.Safe(l), f.FileNum, d.objProvider.Path(objMeta),\n\t\t\t\t\t\terrors.Safe(size), errors.Safe(fileSize))\n\t\t\t\t\td.opts.EventListener.BackgroundError(err)\n\t\t\t\t\td.opts.Logger.Fatalf(\"%s\", err)\n\t\t\t\t}\n\n\t\t\t\tsizesChecked[f.FileBacking.DiskFileNum] = struct{}{}\n\t\t\t}\n\n\t\t\tstats, newHints, err := d.loadTableStats(\n\t\t\t\trs.current, l, f,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\t// Set `moreRemain` so we'll try again.\n\t\t\t\tmoreRemain = true\n\t\t\t\td.opts.EventListener.BackgroundError(err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfill = append(fill, collectedStats{\n\t\t\t\tfileMetadata: f,\n\t\t\t\tTableStats:   stats,\n\t\t\t})\n\t\t\thints = append(hints, newHints...)\n\t\t}\n\t}\n\treturn fill, hints, moreRemain\n}\n\nfunc (d *DB) loadTableStats(\n\tv *version, level int, meta *fileMetadata,\n) (manifest.TableStats, []deleteCompactionHint, error) {\n\tvar stats manifest.TableStats\n\tvar compactionHints []deleteCompactionHint\n\terr := d.fileCache.withCommonReader(\n\t\tmeta, func(r sstable.CommonReader) (err error) {\n\t\t\tprops := r.CommonProperties()\n\t\t\tstats.NumEntries = props.NumEntries\n\t\t\tstats.NumDeletions = props.NumDeletions\n\t\t\tstats.NumRangeKeySets = props.NumRangeKeySets\n\t\t\tstats.ValueBlocksSize = props.ValueBlocksSize\n\t\t\tstats.CompressionType = block.CompressionFromString(props.CompressionName)\n\t\t\tif props.NumDataBlocks > 0 {\n\t\t\t\tstats.TombstoneDenseBlocksRatio = float64(props.NumTombstoneDenseBlocks) / float64(props.NumDataBlocks)\n\t\t\t}\n\n\t\t\tif props.NumPointDeletions() > 0 {\n\t\t\t\tif err = d.loadTablePointKeyStats(props, v, level, meta, &stats); err != nil {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t\tif props.NumRangeDeletions > 0 || props.NumRangeKeyDels > 0 {\n\t\t\t\tif compactionHints, err = d.loadTableRangeDelStats(\n\t\t\t\t\tr, v, level, meta, &stats,\n\t\t\t\t); err != nil {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn\n\t\t})\n\tif err != nil {\n\t\treturn stats, nil, err\n\t}\n\treturn stats, compactionHints, nil\n}\n\n// loadTablePointKeyStats calculates the point key statistics for the given\n// table. The provided manifest.TableStats are updated.\nfunc (d *DB) loadTablePointKeyStats(\n\tprops *sstable.CommonProperties,\n\tv *version,\n\tlevel int,\n\tmeta *fileMetadata,\n\tstats *manifest.TableStats,\n) error {\n\t// TODO(jackson): If the file has a wide keyspace, the average\n\t// value size beneath the entire file might not be representative\n\t// of the size of the keys beneath the point tombstones.\n\t// We could write the ranges of 'clusters' of point tombstones to\n\t// a sstable property and call averageValueSizeBeneath for each of\n\t// these narrower ranges to improve the estimate.\n\tavgValLogicalSize, compressionRatio, err := d.estimateSizesBeneath(v, level, meta, props)\n\tif err != nil {\n\t\treturn err\n\t}\n\tstats.PointDeletionsBytesEstimate =\n\t\tpointDeletionsBytesEstimate(meta.Size, props, avgValLogicalSize, compressionRatio)\n\treturn nil\n}\n\n// loadTableRangeDelStats calculates the range deletion and range key deletion\n// statistics for the given table.\nfunc (d *DB) loadTableRangeDelStats(\n\tr sstable.CommonReader, v *version, level int, meta *fileMetadata, stats *manifest.TableStats,\n) ([]deleteCompactionHint, error) {\n\titer, err := newCombinedDeletionKeyspanIter(d.opts.Comparer, r, meta)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer iter.Close()\n\tvar compactionHints []deleteCompactionHint\n\t// We iterate over the defragmented range tombstones and range key deletions,\n\t// which ensures we don't double count ranges deleted at different sequence\n\t// numbers. Also, merging abutting tombstones reduces the number of calls to\n\t// estimateReclaimedSizeBeneath which is costly, and improves the accuracy of\n\t// our overall estimate.\n\ts, err := iter.First()\n\tfor ; s != nil; s, err = iter.Next() {\n\t\tstart, end := s.Start, s.End\n\t\t// We only need to consider deletion size estimates for tables that contain\n\t\t// RANGEDELs.\n\t\tvar maxRangeDeleteSeqNum base.SeqNum\n\t\tfor _, k := range s.Keys {\n\t\t\tif k.Kind() == base.InternalKeyKindRangeDelete && maxRangeDeleteSeqNum < k.SeqNum() {\n\t\t\t\tmaxRangeDeleteSeqNum = k.SeqNum()\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\n\t\t// If the file is in the last level of the LSM, there is no data beneath\n\t\t// it. The fact that there is still a range tombstone in a bottommost file\n\t\t// indicates two possibilites:\n\t\t//   1. an open snapshot kept the tombstone around, and the data the\n\t\t//      tombstone deletes is contained within the file itself.\n\t\t//   2. the file was ingested.\n\t\t// In the first case, we'd like to estimate disk usage within the file\n\t\t// itself since compacting the file will drop that covered data. In the\n\t\t// second case, we expect that compacting the file will NOT drop any\n\t\t// data and rewriting the file is a waste of write bandwidth. We can\n\t\t// distinguish these cases by looking at the file metadata's sequence\n\t\t// numbers. A file's range deletions can only delete data within the\n\t\t// file at lower sequence numbers. All keys in an ingested sstable adopt\n\t\t// the same sequence number, preventing tombstones from deleting keys\n\t\t// within the same file. We check here if the largest RANGEDEL sequence\n\t\t// number is greater than the file's smallest sequence number. If it is,\n\t\t// the RANGEDEL could conceivably (although inconclusively) delete data\n\t\t// within the same file.\n\t\t//\n\t\t// Note that this heuristic is imperfect. If a table containing a range\n\t\t// deletion is ingested into L5 and subsequently compacted into L6 but\n\t\t// an open snapshot prevents elision of covered keys in L6, the\n\t\t// resulting RangeDeletionsBytesEstimate will incorrectly include all\n\t\t// covered keys.\n\t\t//\n\t\t// TODO(jackson): We could prevent the above error in the heuristic by\n\t\t// computing the file's RangeDeletionsBytesEstimate during the\n\t\t// compaction itself. It's unclear how common this is.\n\t\t//\n\t\t// NOTE: If the span `s` wholly contains a table containing range keys,\n\t\t// the returned size estimate will be slightly inflated by the range key\n\t\t// block. However, in practice, range keys are expected to be rare, and\n\t\t// the size of the range key block relative to the overall size of the\n\t\t// table is expected to be small.\n\t\tif level == numLevels-1 && meta.SmallestSeqNum < maxRangeDeleteSeqNum {\n\t\t\tsize, err := r.EstimateDiskUsage(start, end)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tstats.RangeDeletionsBytesEstimate += size\n\n\t\t\t// As the file is in the bottommost level, there is no need to collect a\n\t\t\t// deletion hint.\n\t\t\tcontinue\n\t\t}\n\n\t\t// While the size estimates for point keys should only be updated if this\n\t\t// span contains a range del, the sequence numbers are required for the\n\t\t// hint. Unconditionally descend, but conditionally update the estimates.\n\t\thintType := compactionHintFromKeys(s.Keys)\n\t\testimate, hintSeqNum, err := d.estimateReclaimedSizeBeneath(v, level, start, end, hintType)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tstats.RangeDeletionsBytesEstimate += estimate\n\n\t\t// hintSeqNum is the smallest sequence number contained in any\n\t\t// file overlapping with the hint and in a level below it.\n\t\tif hintSeqNum == math.MaxUint64 {\n\t\t\tcontinue\n\t\t}\n\t\thint := deleteCompactionHint{\n\t\t\thintType:                hintType,\n\t\t\tstart:                   make([]byte, len(start)),\n\t\t\tend:                     make([]byte, len(end)),\n\t\t\ttombstoneFile:           meta,\n\t\t\ttombstoneLevel:          level,\n\t\t\ttombstoneLargestSeqNum:  s.LargestSeqNum(),\n\t\t\ttombstoneSmallestSeqNum: s.SmallestSeqNum(),\n\t\t\tfileSmallestSeqNum:      hintSeqNum,\n\t\t}\n\t\tcopy(hint.start, start)\n\t\tcopy(hint.end, end)\n\t\tcompactionHints = append(compactionHints, hint)\n\t}\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn compactionHints, nil\n}\n\nfunc (d *DB) estimateSizesBeneath(\n\tv *version, level int, meta *fileMetadata, fileProps *sstable.CommonProperties,\n) (avgValueLogicalSize, compressionRatio float64, err error) {\n\t// Find all files in lower levels that overlap with meta,\n\t// summing their value sizes and entry counts.\n\tfile := meta\n\tvar fileSum, keySum, valSum, entryCount uint64\n\t// Include the file itself. This is important because in some instances, the\n\t// computed compression ratio is applied to the tombstones contained within\n\t// `meta` itself. If there are no files beneath `meta` in the LSM, we would\n\t// calculate a compression ratio of 0 which is not accurate for the file's\n\t// own tombstones.\n\tfileSum += file.Size\n\tentryCount += fileProps.NumEntries\n\tkeySum += fileProps.RawKeySize\n\tvalSum += fileProps.RawValueSize\n\n\taddPhysicalTableStats := func(r *sstable.Reader) (err error) {\n\t\tfileSum += file.Size\n\t\tentryCount += r.Properties.NumEntries\n\t\tkeySum += r.Properties.RawKeySize\n\t\tvalSum += r.Properties.RawValueSize\n\t\treturn nil\n\t}\n\taddVirtualTableStats := func(v sstable.VirtualReader) (err error) {\n\t\tfileSum += file.Size\n\t\tentryCount += file.Stats.NumEntries\n\t\tkeySum += v.Properties.RawKeySize\n\t\tvalSum += v.Properties.RawValueSize\n\t\treturn nil\n\t}\n\n\tfor l := level + 1; l < numLevels; l++ {\n\t\toverlaps := v.Overlaps(l, meta.UserKeyBounds())\n\t\titer := overlaps.Iter()\n\t\tfor file = iter.First(); file != nil; file = iter.Next() {\n\t\t\tvar err error\n\t\t\tif file.Virtual {\n\t\t\t\terr = d.fileCache.withVirtualReader(file.VirtualMeta(), addVirtualTableStats)\n\t\t\t} else {\n\t\t\t\terr = d.fileCache.withReader(file.PhysicalMeta(), addPhysicalTableStats)\n\t\t\t}\n\t\t\tif err != nil {\n\t\t\t\treturn 0, 0, err\n\t\t\t}\n\t\t}\n\t}\n\tif entryCount == 0 {\n\t\treturn 0, 0, nil\n\t}\n\t// RawKeySize and RawValueSize are uncompressed totals. We'll need to scale\n\t// the value sum according to the data size to account for compression,\n\t// index blocks and metadata overhead. Eg:\n\t//\n\t//    Compression rate        ×  Average uncompressed value size\n\t//\n\t//                            ↓\n\t//\n\t//         FileSize              RawValueSize\n\t//   -----------------------  ×  ------------\n\t//   RawKeySize+RawValueSize     NumEntries\n\t//\n\t// We return the average logical value size plus the compression ratio,\n\t// leaving the scaling to the caller. This allows the caller to perform\n\t// additional compression ratio scaling if necessary.\n\tuncompressedSum := float64(keySum + valSum)\n\tcompressionRatio = float64(fileSum) / uncompressedSum\n\tavgValueLogicalSize = (float64(valSum) / float64(entryCount))\n\treturn avgValueLogicalSize, compressionRatio, nil\n}\n\nfunc (d *DB) estimateReclaimedSizeBeneath(\n\tv *version, level int, start, end []byte, hintType deleteCompactionHintType,\n) (estimate uint64, hintSeqNum base.SeqNum, err error) {\n\t// Find all files in lower levels that overlap with the deleted range\n\t// [start, end).\n\t//\n\t// An overlapping file might be completely contained by the range\n\t// tombstone, in which case we can count the entire file size in\n\t// our estimate without doing any additional I/O.\n\t//\n\t// Otherwise, estimating the range for the file requires\n\t// additional I/O to read the file's index blocks.\n\thintSeqNum = math.MaxUint64\n\tfor l := level + 1; l < numLevels; l++ {\n\t\toverlaps := v.Overlaps(l, base.UserKeyBoundsEndExclusive(start, end))\n\t\titer := overlaps.Iter()\n\t\tfor file := iter.First(); file != nil; file = iter.Next() {\n\t\t\t// Determine whether we need to update size estimates and hint seqnums\n\t\t\t// based on the type of hint and the type of keys in this file.\n\t\t\tvar updateEstimates, updateHints bool\n\t\t\tswitch hintType {\n\t\t\tcase deleteCompactionHintTypePointKeyOnly:\n\t\t\t\t// The range deletion byte estimates should only be updated if this\n\t\t\t\t// table contains point keys. This ends up being an overestimate in\n\t\t\t\t// the case that table also has range keys, but such keys are expected\n\t\t\t\t// to contribute a negligible amount of the table's overall size,\n\t\t\t\t// relative to point keys.\n\t\t\t\tif file.HasPointKeys {\n\t\t\t\t\tupdateEstimates = true\n\t\t\t\t}\n\t\t\t\t// As the initiating span contained only range dels, hints can only be\n\t\t\t\t// updated if this table does _not_ contain range keys.\n\t\t\t\tif !file.HasRangeKeys {\n\t\t\t\t\tupdateHints = true\n\t\t\t\t}\n\t\t\tcase deleteCompactionHintTypeRangeKeyOnly:\n\t\t\t\t// The initiating span contained only range key dels. The estimates\n\t\t\t\t// apply only to point keys, and are therefore not updated.\n\t\t\t\tupdateEstimates = false\n\t\t\t\t// As the initiating span contained only range key dels, hints can\n\t\t\t\t// only be updated if this table does _not_ contain point keys.\n\t\t\t\tif !file.HasPointKeys {\n\t\t\t\t\tupdateHints = true\n\t\t\t\t}\n\t\t\tcase deleteCompactionHintTypePointAndRangeKey:\n\t\t\t\t// Always update the estimates and hints, as this hint type can drop a\n\t\t\t\t// file, irrespective of the mixture of keys. Similar to above, the\n\t\t\t\t// range del bytes estimates is an overestimate.\n\t\t\t\tupdateEstimates, updateHints = true, true\n\t\t\tdefault:\n\t\t\t\tpanic(fmt.Sprintf(\"pebble: unknown hint type %s\", hintType))\n\t\t\t}\n\t\t\tstartCmp := d.cmp(start, file.Smallest.UserKey)\n\t\t\tendCmp := d.cmp(file.Largest.UserKey, end)\n\t\t\tif startCmp <= 0 && (endCmp < 0 || endCmp == 0 && file.Largest.IsExclusiveSentinel()) {\n\t\t\t\t// The range fully contains the file, so skip looking it up in table\n\t\t\t\t// cache/looking at its indexes and add the full file size.\n\t\t\t\tif updateEstimates {\n\t\t\t\t\testimate += file.Size\n\t\t\t\t}\n\t\t\t\tif updateHints && hintSeqNum > file.SmallestSeqNum {\n\t\t\t\t\thintSeqNum = file.SmallestSeqNum\n\t\t\t\t}\n\t\t\t} else if d.cmp(file.Smallest.UserKey, end) <= 0 && d.cmp(start, file.Largest.UserKey) <= 0 {\n\t\t\t\t// Partial overlap.\n\t\t\t\tif hintType == deleteCompactionHintTypeRangeKeyOnly {\n\t\t\t\t\t// If the hint that generated this overlap contains only range keys,\n\t\t\t\t\t// there is no need to calculate disk usage, as the reclaimable space\n\t\t\t\t\t// is expected to be minimal relative to point keys.\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tvar size uint64\n\t\t\t\tvar err error\n\t\t\t\tif file.Virtual {\n\t\t\t\t\terr = d.fileCache.withVirtualReader(\n\t\t\t\t\t\tfile.VirtualMeta(), func(r sstable.VirtualReader) (err error) {\n\t\t\t\t\t\t\tsize, err = r.EstimateDiskUsage(start, end)\n\t\t\t\t\t\t\treturn err\n\t\t\t\t\t\t})\n\t\t\t\t} else {\n\t\t\t\t\terr = d.fileCache.withReader(\n\t\t\t\t\t\tfile.PhysicalMeta(), func(r *sstable.Reader) (err error) {\n\t\t\t\t\t\t\tsize, err = r.EstimateDiskUsage(start, end)\n\t\t\t\t\t\t\treturn err\n\t\t\t\t\t\t})\n\t\t\t\t}\n\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn 0, hintSeqNum, err\n\t\t\t\t}\n\t\t\t\testimate += size\n\t\t\t\tif updateHints && hintSeqNum > file.SmallestSeqNum && d.FormatMajorVersion() >= FormatVirtualSSTables {\n\t\t\t\t\t// If the format major version is past Virtual SSTables, deletion only\n\t\t\t\t\t// hints can also apply to partial overlaps with sstables.\n\t\t\t\t\thintSeqNum = file.SmallestSeqNum\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn estimate, hintSeqNum, nil\n}\n\nfunc maybeSetStatsFromProperties(meta physicalMeta, props *sstable.Properties) bool {\n\t// If a table contains range deletions or range key deletions, we defer the\n\t// stats collection. There are two main reasons for this:\n\t//\n\t//  1. Estimating the potential for reclaimed space due to a range deletion\n\t//     tombstone requires scanning the LSM - a potentially expensive operation\n\t//     that should be deferred.\n\t//  2. Range deletions and / or range key deletions present an opportunity to\n\t//     compute \"deletion hints\", which also requires a scan of the LSM to\n\t//     compute tables that would be eligible for deletion.\n\t//\n\t// These two tasks are deferred to the table stats collector goroutine.\n\tif props.NumRangeDeletions != 0 || props.NumRangeKeyDels != 0 {\n\t\treturn false\n\t}\n\n\t// If a table is more than 10% point deletions without user-provided size\n\t// estimates, don't calculate the PointDeletionsBytesEstimate statistic\n\t// using our limited knowledge. The table stats collector can populate the\n\t// stats and calculate an average of value size of all the tables beneath\n\t// the table in the LSM, which will be more accurate.\n\tif unsizedDels := (props.NumDeletions - props.NumSizedDeletions); unsizedDels > props.NumEntries/10 {\n\t\treturn false\n\t}\n\n\tvar pointEstimate uint64\n\tif props.NumEntries > 0 {\n\t\t// Use the file's own average key and value sizes as an estimate. This\n\t\t// doesn't require any additional IO and since the number of point\n\t\t// deletions in the file is low, the error introduced by this crude\n\t\t// estimate is expected to be small.\n\t\tcommonProps := &props.CommonProperties\n\t\tavgValSize, compressionRatio := estimatePhysicalSizes(meta.Size, commonProps)\n\t\tpointEstimate = pointDeletionsBytesEstimate(meta.Size, commonProps, avgValSize, compressionRatio)\n\t}\n\n\tmeta.Stats.NumEntries = props.NumEntries\n\tmeta.Stats.NumDeletions = props.NumDeletions\n\tmeta.Stats.NumRangeKeySets = props.NumRangeKeySets\n\tmeta.Stats.PointDeletionsBytesEstimate = pointEstimate\n\tmeta.Stats.RangeDeletionsBytesEstimate = 0\n\tmeta.Stats.ValueBlocksSize = props.ValueBlocksSize\n\tmeta.Stats.CompressionType = block.CompressionFromString(props.CompressionName)\n\tmeta.StatsMarkValid()\n\treturn true\n}\n\nfunc pointDeletionsBytesEstimate(\n\tfileSize uint64, props *sstable.CommonProperties, avgValLogicalSize, compressionRatio float64,\n) (estimate uint64) {\n\tif props.NumEntries == 0 {\n\t\treturn 0\n\t}\n\tnumPointDels := props.NumPointDeletions()\n\tif numPointDels == 0 {\n\t\treturn 0\n\t}\n\t// Estimate the potential space to reclaim using the table's own properties.\n\t// There may or may not be keys covered by any individual point tombstone.\n\t// If not, compacting the point tombstone into L6 will at least allow us to\n\t// drop the point deletion key and will reclaim the tombstone's key bytes.\n\t// If there are covered key(s), we also get to drop key and value bytes for\n\t// each covered key.\n\t//\n\t// Some point tombstones (DELSIZEDs) carry a user-provided estimate of the\n\t// uncompressed size of entries that will be elided by fully compacting the\n\t// tombstone. For these tombstones, there's no guesswork—we use the\n\t// RawPointTombstoneValueSizeHint property which is the sum of all these\n\t// tombstones' encoded values.\n\t//\n\t// For un-sized point tombstones (DELs), we estimate assuming that each\n\t// point tombstone on average covers 1 key and using average value sizes.\n\t// This is almost certainly an overestimate, but that's probably okay\n\t// because point tombstones can slow range iterations even when they don't\n\t// cover a key.\n\t//\n\t// TODO(jackson): This logic doesn't directly incorporate fixed per-key\n\t// overhead (8-byte trailer, plus at least 1 byte encoding the length of the\n\t// key and 1 byte encoding the length of the value). This overhead is\n\t// indirectly incorporated through the compression ratios, but that results\n\t// in the overhead being smeared per key-byte and value-byte, rather than\n\t// per-entry. This per-key fixed overhead can be nontrivial, especially for\n\t// dense swaths of point tombstones. Give some thought as to whether we\n\t// should directly include fixed per-key overhead in the calculations.\n\n\t// Below, we calculate the tombstone contributions and the shadowed keys'\n\t// contributions separately.\n\tvar tombstonesLogicalSize float64\n\tvar shadowedLogicalSize float64\n\n\t// 1. Calculate the contribution of the tombstone keys themselves.\n\tif props.RawPointTombstoneKeySize > 0 {\n\t\ttombstonesLogicalSize += float64(props.RawPointTombstoneKeySize)\n\t} else {\n\t\t// This sstable predates the existence of the RawPointTombstoneKeySize\n\t\t// property. We can use the average key size within the file itself and\n\t\t// the count of point deletions to estimate the size.\n\t\ttombstonesLogicalSize += float64(numPointDels * props.RawKeySize / props.NumEntries)\n\t}\n\n\t// 2. Calculate the contribution of the keys shadowed by tombstones.\n\t//\n\t// 2a. First account for keys shadowed by DELSIZED tombstones. THE DELSIZED\n\t// tombstones encode the size of both the key and value of the shadowed KV\n\t// entries. These sizes are aggregated into a sstable property.\n\tshadowedLogicalSize += float64(props.RawPointTombstoneValueSize)\n\n\t// 2b. Calculate the contribution of the KV entries shadowed by ordinary DEL\n\t// keys.\n\tnumUnsizedDels := numPointDels - props.NumSizedDeletions\n\t{\n\t\t// The shadowed keys have the same exact user keys as the tombstones\n\t\t// themselves, so we can use the `tombstonesLogicalSize` we computed\n\t\t// earlier as an estimate. There's a complication that\n\t\t// `tombstonesLogicalSize` may include DELSIZED keys we already\n\t\t// accounted for.\n\t\tshadowedLogicalSize += float64(tombstonesLogicalSize) / float64(numPointDels) * float64(numUnsizedDels)\n\n\t\t// Calculate the contribution of the deleted values. The caller has\n\t\t// already computed an average logical size (possibly computed across\n\t\t// many sstables).\n\t\tshadowedLogicalSize += float64(numUnsizedDels) * avgValLogicalSize\n\t}\n\n\t// Scale both tombstone and shadowed totals by logical:physical ratios to\n\t// account for compression, metadata overhead, etc.\n\t//\n\t//      Physical             FileSize\n\t//     -----------  = -----------------------\n\t//      Logical       RawKeySize+RawValueSize\n\t//\n\treturn uint64((tombstonesLogicalSize + shadowedLogicalSize) * compressionRatio)\n}\n\nfunc estimatePhysicalSizes(\n\tfileSize uint64, props *sstable.CommonProperties,\n) (avgValLogicalSize, compressionRatio float64) {\n\t// RawKeySize and RawValueSize are uncompressed totals. Scale according to\n\t// the data size to account for compression, index blocks and metadata\n\t// overhead. Eg:\n\t//\n\t//    Compression rate        ×  Average uncompressed value size\n\t//\n\t//                            ↓\n\t//\n\t//         FileSize              RawValSize\n\t//   -----------------------  ×  ----------\n\t//   RawKeySize+RawValueSize     NumEntries\n\t//\n\tuncompressedSum := props.RawKeySize + props.RawValueSize\n\tcompressionRatio = float64(fileSize) / float64(uncompressedSum)\n\tavgValLogicalSize = (float64(props.RawValueSize) / float64(props.NumEntries))\n\treturn avgValLogicalSize, compressionRatio\n}\n\n// newCombinedDeletionKeyspanIter returns a keyspan.FragmentIterator that\n// returns \"ranged deletion\" spans for a single table, providing a combined view\n// of both range deletion and range key deletion spans. The\n// tableRangedDeletionIter is intended for use in the specific case of computing\n// the statistics and deleteCompactionHints for a single table.\n//\n// As an example, consider the following set of spans from the range deletion\n// and range key blocks of a table:\n//\n//\t\t      |---------|     |---------|         |-------| RANGEKEYDELs\n//\t\t|-----------|-------------|           |-----|       RANGEDELs\n//\t  __________________________________________________________\n//\t\ta b c d e f g h i j k l m n o p q r s t u v w x y z\n//\n// The tableRangedDeletionIter produces the following set of output spans, where\n// '1' indicates a span containing only range deletions, '2' is a span\n// containing only range key deletions, and '3' is a span containing a mixture\n// of both range deletions and range key deletions.\n//\n//\t\t   1       3       1    3    2          1  3   2\n//\t\t|-----|---------|-----|---|-----|     |---|-|-----|\n//\t  __________________________________________________________\n//\t\ta b c d e f g h i j k l m n o p q r s t u v w x y z\n//\n// Algorithm.\n//\n// The iterator first defragments the range deletion and range key blocks\n// separately. During this defragmentation, the range key block is also filtered\n// so that keys other than range key deletes are ignored. The range delete and\n// range key delete keyspaces are then merged.\n//\n// Note that the only fragmentation introduced by merging is from where a range\n// del span overlaps with a range key del span. Within the bounds of any overlap\n// there is guaranteed to be no further fragmentation, as the constituent spans\n// have already been defragmented. To the left and right of any overlap, the\n// same reasoning applies. For example,\n//\n//\t\t         |--------|         |-------| RANGEKEYDEL\n//\t\t|---------------------------|         RANGEDEL\n//\t\t|----1---|----3---|----1----|---2---| Merged, fragmented spans.\n//\t  __________________________________________________________\n//\t\ta b c d e f g h i j k l m n o p q r s t u v w x y z\n//\n// Any fragmented abutting spans produced by the merging iter will be of\n// differing types (i.e. a transition from a span with homogenous key kinds to a\n// heterogeneous span, or a transition from a span with exclusively range dels\n// to a span with exclusively range key dels). Therefore, further\n// defragmentation is not required.\n//\n// Each span returned by the tableRangeDeletionIter will have at most four keys,\n// corresponding to the largest and smallest sequence numbers encountered across\n// the range deletes and range keys deletes that comprised the merged spans.\nfunc newCombinedDeletionKeyspanIter(\n\tcomparer *base.Comparer, cr sstable.CommonReader, m *fileMetadata,\n) (keyspan.FragmentIterator, error) {\n\t// The range del iter and range key iter are each wrapped in their own\n\t// defragmenting iter. For each iter, abutting spans can always be merged.\n\tvar equal = keyspan.DefragmentMethodFunc(func(_ base.CompareRangeSuffixes, a, b *keyspan.Span) bool { return true })\n\t// Reduce keys by maintaining a slice of at most length two, corresponding to\n\t// the largest and smallest keys in the defragmented span. This maintains the\n\t// contract that the emitted slice is sorted by (SeqNum, Kind) descending.\n\treducer := func(current, incoming []keyspan.Key) []keyspan.Key {\n\t\tif len(current) == 0 && len(incoming) == 0 {\n\t\t\t// While this should never occur in practice, a defensive return is used\n\t\t\t// here to preserve correctness.\n\t\t\treturn current\n\t\t}\n\t\tvar largest, smallest keyspan.Key\n\t\tvar set bool\n\t\tfor _, keys := range [2][]keyspan.Key{current, incoming} {\n\t\t\tif len(keys) == 0 {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfirst, last := keys[0], keys[len(keys)-1]\n\t\t\tif !set {\n\t\t\t\tlargest, smallest = first, last\n\t\t\t\tset = true\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif first.Trailer > largest.Trailer {\n\t\t\t\tlargest = first\n\t\t\t}\n\t\t\tif last.Trailer < smallest.Trailer {\n\t\t\t\tsmallest = last\n\t\t\t}\n\t\t}\n\t\tif largest.Equal(comparer.CompareRangeSuffixes, smallest) {\n\t\t\tcurrent = append(current[:0], largest)\n\t\t} else {\n\t\t\tcurrent = append(current[:0], largest, smallest)\n\t\t}\n\t\treturn current\n\t}\n\n\t// The separate iters for the range dels and range keys are wrapped in a\n\t// merging iter to join the keyspaces into a single keyspace. The separate\n\t// iters are only added if the particular key kind is present.\n\tmIter := &keyspanimpl.MergingIter{}\n\tvar transform = keyspan.TransformerFunc(func(_ base.CompareRangeSuffixes, in keyspan.Span, out *keyspan.Span) error {\n\t\tif in.KeysOrder != keyspan.ByTrailerDesc {\n\t\t\tpanic(\"pebble: combined deletion iter encountered keys in non-trailer descending order\")\n\t\t}\n\t\tout.Start, out.End = in.Start, in.End\n\t\tout.Keys = append(out.Keys[:0], in.Keys...)\n\t\tout.KeysOrder = keyspan.ByTrailerDesc\n\t\t// NB: The order of by-trailer descending may have been violated,\n\t\t// because we've layered rangekey and rangedel iterators from the same\n\t\t// sstable into the same keyspanimpl.MergingIter. The MergingIter will\n\t\t// return the keys in the order that the child iterators were provided.\n\t\t// Sort the keys to ensure they're sorted by trailer descending.\n\t\tkeyspan.SortKeysByTrailer(out.Keys)\n\t\treturn nil\n\t})\n\tmIter.Init(comparer, transform, new(keyspanimpl.MergingBuffers))\n\n\titer, err := cr.NewRawRangeDelIter(context.TODO(), m.FragmentIterTransforms())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif iter != nil {\n\t\t// Assert expected bounds. In previous versions of Pebble, range\n\t\t// deletions persisted to sstables could exceed the bounds of the\n\t\t// containing files due to \"split user keys.\" This required readers to\n\t\t// constrain the tombstones' bounds to the containing file at read time.\n\t\t// See docs/range_deletions.md for an extended discussion of the design\n\t\t// and invariants at that time.\n\t\t//\n\t\t// We've since compacted away all 'split user-keys' and in the process\n\t\t// eliminated all \"untruncated range tombstones\" for physical sstables.\n\t\t// We no longer need to perform truncation at read time for these\n\t\t// sstables.\n\t\t//\n\t\t// At the same time, we've also introduced the concept of \"virtual\n\t\t// SSTables\" where the file metadata's effective bounds can again be\n\t\t// reduced to be narrower than the contained tombstones. These virtual\n\t\t// SSTables handle truncation differently, performing it using\n\t\t// keyspan.Truncate when the sstable's range deletion iterator is\n\t\t// opened.\n\t\t//\n\t\t// Together, these mean that we should never see untruncated range\n\t\t// tombstones any more—and the merging iterator no longer accounts for\n\t\t// their existence. Since there's abundant subtlety that we're relying\n\t\t// on, we choose to be conservative and assert that these invariants\n\t\t// hold. We could (and previously did) choose to only validate these\n\t\t// bounds in invariants builds, but the most likely avenue for these\n\t\t// tombstones' existence is through a bug in a migration and old data\n\t\t// sitting around in an old store from long ago.\n\t\t//\n\t\t// The table stats collector will read all files range deletions\n\t\t// asynchronously after Open, and provides a perfect opportunity to\n\t\t// validate our invariants without harming user latency. We also\n\t\t// previously performed truncation here which similarly required key\n\t\t// comparisons, so replacing those key comparisons with assertions\n\t\t// should be roughly similar in performance.\n\t\t//\n\t\t// TODO(jackson): Only use AssertBounds in invariants builds in the\n\t\t// following release.\n\t\titer = keyspan.AssertBounds(\n\t\t\titer, m.SmallestPointKey, m.LargestPointKey.UserKey, comparer.Compare,\n\t\t)\n\t\tdIter := &keyspan.DefragmentingIter{}\n\t\tdIter.Init(comparer, iter, equal, reducer, new(keyspan.DefragmentingBuffers))\n\t\titer = dIter\n\t\tmIter.AddLevel(iter)\n\t}\n\n\titer, err = cr.NewRawRangeKeyIter(context.TODO(), m.FragmentIterTransforms())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif iter != nil {\n\t\t// Assert expected bounds in tests.\n\t\tif invariants.Sometimes(50) {\n\t\t\titer = keyspan.AssertBounds(\n\t\t\t\titer, m.SmallestRangeKey, m.LargestRangeKey.UserKey, comparer.Compare,\n\t\t\t)\n\t\t}\n\t\t// Wrap the range key iterator in a filter that elides keys other than range\n\t\t// key deletions.\n\t\titer = keyspan.Filter(iter, func(in *keyspan.Span, buf []keyspan.Key) []keyspan.Key {\n\t\t\tkeys := buf[:0]\n\t\t\tfor _, k := range in.Keys {\n\t\t\t\tif k.Kind() != base.InternalKeyKindRangeKeyDelete {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tkeys = append(keys, k)\n\t\t\t}\n\t\t\treturn keys\n\t\t}, comparer.Compare)\n\t\tdIter := &keyspan.DefragmentingIter{}\n\t\tdIter.Init(comparer, iter, equal, reducer, new(keyspan.DefragmentingBuffers))\n\t\titer = dIter\n\t\tmIter.AddLevel(iter)\n\t}\n\n\treturn mIter, nil\n}\n\n// rangeKeySetsAnnotator is a manifest.Annotator that annotates B-Tree nodes\n// with the sum of the files' counts of range key fragments. The count of range\n// key sets may change once a table's stats are loaded asynchronously, so its\n// values are marked as cacheable only if a file's stats have been loaded.\nvar rangeKeySetsAnnotator = manifest.SumAnnotator(func(f *manifest.FileMetadata) (uint64, bool) {\n\treturn f.Stats.NumRangeKeySets, f.StatsValid()\n})\n\n// tombstonesAnnotator is a manifest.Annotator that annotates B-Tree nodes\n// with the sum of the files' counts of tombstones (DEL, SINGLEDEL and RANGEDEL\n// keys). The count of tombstones may change once a table's stats are loaded\n// asynchronously, so its values are marked as cacheable only if a file's stats\n// have been loaded.\nvar tombstonesAnnotator = manifest.SumAnnotator(func(f *manifest.FileMetadata) (uint64, bool) {\n\treturn f.Stats.NumDeletions, f.StatsValid()\n})\n\n// valueBlocksSizeAnnotator is a manifest.Annotator that annotates B-Tree\n// nodes with the sum of the files' Properties.ValueBlocksSize. The value block\n// size may change once a table's stats are loaded asynchronously, so its\n// values are marked as cacheable only if a file's stats have been loaded.\nvar valueBlockSizeAnnotator = manifest.SumAnnotator(func(f *fileMetadata) (uint64, bool) {\n\treturn f.Stats.ValueBlocksSize, f.StatsValid()\n})\n\n// compressionTypeAnnotator is a manifest.Annotator that annotates B-tree\n// nodes with the compression type of the file. Its annotation type is\n// compressionTypes. The compression type may change once a table's stats are\n// loaded asynchronously, so its values are marked as cacheable only if a file's\n// stats have been loaded.\nvar compressionTypeAnnotator = manifest.Annotator[compressionTypes]{\n\tAggregator: compressionTypeAggregator{},\n}\n\ntype compressionTypeAggregator struct{}\n\ntype compressionTypes struct {\n\tsnappy, zstd, none, unknown uint64\n}\n\nfunc (a compressionTypeAggregator) Zero(dst *compressionTypes) *compressionTypes {\n\tif dst == nil {\n\t\treturn new(compressionTypes)\n\t}\n\t*dst = compressionTypes{}\n\treturn dst\n}\n\nfunc (a compressionTypeAggregator) Accumulate(\n\tf *fileMetadata, dst *compressionTypes,\n) (v *compressionTypes, cacheOK bool) {\n\tswitch f.Stats.CompressionType {\n\tcase SnappyCompression:\n\t\tdst.snappy++\n\tcase ZstdCompression:\n\t\tdst.zstd++\n\tcase NoCompression:\n\t\tdst.none++\n\tdefault:\n\t\tdst.unknown++\n\t}\n\treturn dst, f.StatsValid()\n}\n\nfunc (a compressionTypeAggregator) Merge(\n\tsrc *compressionTypes, dst *compressionTypes,\n) *compressionTypes {\n\tdst.snappy += src.snappy\n\tdst.zstd += src.zstd\n\tdst.none += src.none\n\tdst.unknown += src.unknown\n\treturn dst\n}\n"
        },
        {
          "name": "table_stats_test.go",
          "type": "blob",
          "size": 6.5849609375,
          "content": "// Copyright 2020 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/cockroachdb/datadriven\"\n\t\"github.com/cockroachdb/pebble/internal/keyspan\"\n\t\"github.com/cockroachdb/pebble/internal/testkeys\"\n\t\"github.com/cockroachdb/pebble/objstorage/objstorageprovider\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n\t\"github.com/cockroachdb/pebble/sstable/colblk\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestTableStats(t *testing.T) {\n\t// loadedInfo is protected by d.mu.\n\tvar loadedInfo *TableStatsInfo\n\topts := &Options{\n\t\tComparer:                    testkeys.Comparer,\n\t\tDisableAutomaticCompactions: true,\n\t\tFormatMajorVersion:          FormatMinSupported,\n\t\tFS:                          vfs.NewMem(),\n\t\tEventListener: &EventListener{\n\t\t\tTableStatsLoaded: func(info TableStatsInfo) {\n\t\t\t\tloadedInfo = &info\n\t\t\t},\n\t\t},\n\t\tLogger: testLogger{t},\n\t}\n\n\td, err := Open(\"\", opts)\n\trequire.NoError(t, err)\n\tdefer func() {\n\t\tif d != nil {\n\t\t\trequire.NoError(t, closeAllSnapshots(d))\n\t\t\trequire.NoError(t, d.Close())\n\t\t}\n\t}()\n\n\tdatadriven.RunTest(t, \"testdata/table_stats\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch td.Cmd {\n\t\tcase \"disable\":\n\t\t\td.mu.Lock()\n\t\t\td.opts.DisableTableStats = true\n\t\t\td.mu.Unlock()\n\t\t\treturn \"\"\n\n\t\tcase \"enable\":\n\t\t\td.mu.Lock()\n\t\t\td.opts.DisableTableStats = false\n\t\t\td.maybeCollectTableStatsLocked()\n\t\t\td.mu.Unlock()\n\t\t\treturn \"\"\n\n\t\tcase \"define\":\n\t\t\trequire.NoError(t, closeAllSnapshots(d))\n\t\t\trequire.NoError(t, d.Close())\n\t\t\tloadedInfo = nil\n\n\t\t\td, err = runDBDefineCmd(td, opts)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\td.mu.Lock()\n\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\td.mu.Unlock()\n\t\t\treturn s\n\n\t\tcase \"reopen\":\n\t\t\trequire.NoError(t, d.Close())\n\t\t\tloadedInfo = nil\n\n\t\t\t// Open using existing file system.\n\t\t\td, err = Open(\"\", opts)\n\t\t\trequire.NoError(t, err)\n\t\t\treturn \"\"\n\n\t\tcase \"batch\":\n\t\t\tb := d.NewBatch()\n\t\t\tif err := runBatchDefineCmd(td, b); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tb.Commit(nil)\n\t\t\treturn \"\"\n\n\t\tcase \"flush\":\n\t\t\tif err := d.Flush(); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\n\t\t\td.mu.Lock()\n\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\td.mu.Unlock()\n\t\t\treturn s\n\n\t\tcase \"ingest\":\n\t\t\tif err = runBuildCmd(td, d, d.opts.FS); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tif err = runIngestCmd(td, d, d.opts.FS); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\td.mu.Lock()\n\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\td.mu.Unlock()\n\t\t\treturn s\n\n\t\tcase \"metric\":\n\t\t\tm := d.Metrics()\n\t\t\t// TODO(jackson): Make a generalized command that uses reflection to\n\t\t\t// pull out arbitrary Metrics fields.\n\t\t\tvar buf bytes.Buffer\n\t\t\tfor _, arg := range td.CmdArgs {\n\t\t\t\tswitch arg.String() {\n\t\t\t\tcase \"keys.missized-tombstones-count\":\n\t\t\t\t\tfmt.Fprintf(&buf, \"%s: %d\", arg.String(), m.Keys.MissizedTombstonesCount)\n\t\t\t\tdefault:\n\t\t\t\t\treturn fmt.Sprintf(\"unrecognized metric %s\", arg)\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn buf.String()\n\n\t\tcase \"lsm\":\n\t\t\td.mu.Lock()\n\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\td.mu.Unlock()\n\t\t\treturn s\n\n\t\tcase \"build\":\n\t\t\tif err := runBuildCmd(td, d, d.opts.FS); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\treturn \"\"\n\n\t\tcase \"ingest-and-excise\":\n\t\t\tif err := runIngestAndExciseCmd(td, d, d.opts.FS); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\t// Wait for a possible flush.\n\t\t\td.mu.Lock()\n\t\t\tfor d.mu.compact.flushing {\n\t\t\t\td.mu.compact.cond.Wait()\n\t\t\t}\n\t\t\td.mu.Unlock()\n\t\t\treturn \"\"\n\n\t\tcase \"wait-pending-table-stats\":\n\t\t\treturn runTableStatsCmd(td, d)\n\n\t\tcase \"wait-loaded-initial\":\n\t\t\td.mu.Lock()\n\t\t\tfor d.mu.tableStats.loading || !d.mu.tableStats.loadedInitial {\n\t\t\t\td.mu.tableStats.cond.Wait()\n\t\t\t}\n\t\t\ts := loadedInfo.String()\n\t\t\td.mu.Unlock()\n\t\t\treturn s\n\n\t\tcase \"compact\":\n\t\t\tif err := runCompactCmd(td, d); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\td.mu.Lock()\n\t\t\t// Disable the \"dynamic base level\" code for this test.\n\t\t\td.mu.versions.picker.forceBaseLevel1()\n\t\t\ts := d.mu.versions.currentVersion().String()\n\t\t\td.mu.Unlock()\n\t\t\treturn s\n\n\t\tcase \"metadata-stats\":\n\t\t\t// Prints some metadata about some sstable which is currently in the\n\t\t\t// latest version.\n\t\t\treturn runMetadataCommand(t, td, d)\n\n\t\tcase \"properties\":\n\t\t\treturn runSSTablePropertiesCmd(t, td, d)\n\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", td.Cmd)\n\t\t}\n\t})\n}\n\nfunc TestTableRangeDeletionIter(t *testing.T) {\n\tvar m *fileMetadata\n\tcmp := testkeys.Comparer\n\tkeySchema := colblk.DefaultKeySchema(cmp, 16)\n\tfs := vfs.NewMem()\n\tdatadriven.RunTest(t, \"testdata/table_stats_deletion_iter\", func(t *testing.T, td *datadriven.TestData) string {\n\t\tswitch cmd := td.Cmd; cmd {\n\t\tcase \"build\":\n\t\t\tf, err := fs.Create(\"tmp.sst\", vfs.WriteCategoryUnspecified)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tw := sstable.NewRawWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{\n\t\t\t\tComparer:    cmp,\n\t\t\t\tKeySchema:   &keySchema,\n\t\t\t\tTableFormat: sstable.TableFormatMax,\n\t\t\t})\n\t\t\tm = &fileMetadata{}\n\t\t\tfor _, line := range strings.Split(td.Input, \"\\n\") {\n\t\t\t\terr = w.EncodeSpan(keyspan.ParseSpan(line))\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err.Error()\n\t\t\t\t}\n\t\t\t}\n\t\t\tif err = w.Close(); err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tmeta, err := w.Metadata()\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tif meta.HasPointKeys {\n\t\t\t\tm.ExtendPointKeyBounds(cmp.Compare, meta.SmallestPoint, meta.LargestPoint)\n\t\t\t}\n\t\t\tif meta.HasRangeDelKeys {\n\t\t\t\tm.ExtendPointKeyBounds(cmp.Compare, meta.SmallestRangeDel, meta.LargestRangeDel)\n\t\t\t}\n\t\t\tif meta.HasRangeKeys {\n\t\t\t\tm.ExtendRangeKeyBounds(cmp.Compare, meta.SmallestRangeKey, meta.LargestRangeKey)\n\t\t\t}\n\t\t\treturn m.DebugString(cmp.FormatKey, false /* verbose */)\n\t\tcase \"spans\":\n\t\t\tf, err := fs.Open(\"tmp.sst\")\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tvar r *sstable.Reader\n\t\t\treadable, err := sstable.NewSimpleReadable(f)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tr, err = sstable.NewReader(context.Background(), readable, sstable.ReaderOptions{\n\t\t\t\tComparer:   cmp,\n\t\t\t\tKeySchemas: sstable.KeySchemas{keySchema.Name: &keySchema},\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tdefer r.Close()\n\t\t\titer, err := newCombinedDeletionKeyspanIter(cmp, r, m)\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tdefer iter.Close()\n\t\t\tvar buf bytes.Buffer\n\t\t\ts, err := iter.First()\n\t\t\tfor ; s != nil; s, err = iter.Next() {\n\t\t\t\tbuf.WriteString(s.String() + \"\\n\")\n\t\t\t}\n\t\t\tif err != nil {\n\t\t\t\treturn err.Error()\n\t\t\t}\n\t\t\tif buf.Len() == 0 {\n\t\t\t\treturn \"(none)\"\n\t\t\t}\n\t\t\treturn buf.String()\n\t\tdefault:\n\t\t\treturn fmt.Sprintf(\"unknown command: %s\", cmd)\n\t\t}\n\t})\n}\n"
        },
        {
          "name": "testdata",
          "type": "tree",
          "content": null
        },
        {
          "name": "tool",
          "type": "tree",
          "content": null
        },
        {
          "name": "version_set.go",
          "type": "blob",
          "size": 37.544921875,
          "content": "// Copyright 2012 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"fmt\"\n\t\"io\"\n\t\"sync\"\n\t\"sync/atomic\"\n\n\t\"github.com/cockroachdb/errors\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/invariants\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n\t\"github.com/cockroachdb/pebble/objstorage\"\n\t\"github.com/cockroachdb/pebble/record\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/cockroachdb/pebble/vfs/atomicfs\"\n)\n\nconst numLevels = manifest.NumLevels\n\nconst manifestMarkerName = `manifest`\n\n// Provide type aliases for the various manifest structs.\ntype bulkVersionEdit = manifest.BulkVersionEdit\ntype deletedFileEntry = manifest.DeletedFileEntry\ntype fileMetadata = manifest.FileMetadata\ntype physicalMeta = manifest.PhysicalFileMeta\ntype virtualMeta = manifest.VirtualFileMeta\ntype fileBacking = manifest.FileBacking\ntype newFileEntry = manifest.NewFileEntry\ntype version = manifest.Version\ntype versionEdit = manifest.VersionEdit\ntype versionList = manifest.VersionList\n\n// versionSet manages a collection of immutable versions, and manages the\n// creation of a new version from the most recent version. A new version is\n// created from an existing version by applying a version edit which is just\n// like it sounds: a delta from the previous version. Version edits are logged\n// to the MANIFEST file, which is replayed at startup.\ntype versionSet struct {\n\t// Next seqNum to use for WAL writes.\n\tlogSeqNum base.AtomicSeqNum\n\n\t// The upper bound on sequence numbers that have been assigned so far. A\n\t// suffix of these sequence numbers may not have been written to a WAL. Both\n\t// logSeqNum and visibleSeqNum are atomically updated by the commitPipeline.\n\t// visibleSeqNum is <= logSeqNum.\n\tvisibleSeqNum base.AtomicSeqNum\n\n\t// Number of bytes present in sstables being written by in-progress\n\t// compactions. This value will be zero if there are no in-progress\n\t// compactions. Updated and read atomically.\n\tatomicInProgressBytes atomic.Int64\n\n\t// Immutable fields.\n\tdirname  string\n\tprovider objstorage.Provider\n\t// Set to DB.mu.\n\tmu   *sync.Mutex\n\topts *Options\n\tfs   vfs.FS\n\tcmp  *base.Comparer\n\t// Dynamic base level allows the dynamic base level computation to be\n\t// disabled. Used by tests which want to create specific LSM structures.\n\tdynamicBaseLevel bool\n\n\t// Mutable fields.\n\tversions versionList\n\tpicker   compactionPicker\n\n\t// Not all metrics are kept here. See DB.Metrics().\n\tmetrics Metrics\n\n\t// A pointer to versionSet.addObsoleteLocked. Avoids allocating a new closure\n\t// on the creation of every version.\n\tobsoleteFn        func(obsolete []*fileBacking)\n\tobsoleteTables    []tableInfo\n\tobsoleteManifests []fileInfo\n\tobsoleteOptions   []fileInfo\n\n\t// Zombie tables which have been removed from the current version but are\n\t// still referenced by an inuse iterator.\n\tzombieTables map[base.DiskFileNum]tableInfo\n\n\t// virtualBackings contains information about the FileBackings which support\n\t// virtual sstables in the latest version. It is mainly used to determine when\n\t// a backing is no longer in use by the tables in the latest version; this is\n\t// not a trivial problem because compactions and other operations can happen\n\t// in parallel (and they can finish in unpredictable order).\n\t//\n\t// This mechanism is complementary to the backing Ref/Unref mechanism, which\n\t// determines when a backing is no longer used by *any* live version and can\n\t// be removed.\n\t//\n\t// In principle this should have been a copy-on-write structure, with each\n\t// Version having its own record of its virtual backings (similar to the\n\t// B-tree that holds the tables). However, in practice we only need it for the\n\t// latest version, so we use a simpler structure and store it in the\n\t// versionSet instead.\n\t//\n\t// virtualBackings is modified under DB.mu and the log lock. If it is accessed\n\t// under DB.mu and a version update is in progress, it reflects the state of\n\t// the next version.\n\tvirtualBackings manifest.VirtualBackings\n\n\t// minUnflushedLogNum is the smallest WAL log file number corresponding to\n\t// mutations that have not been flushed to an sstable.\n\tminUnflushedLogNum base.DiskFileNum\n\n\t// The next file number. A single counter is used to assign file\n\t// numbers for the WAL, MANIFEST, sstable, and OPTIONS files.\n\tnextFileNum atomic.Uint64\n\n\t// The current manifest file number.\n\tmanifestFileNum base.DiskFileNum\n\tmanifestMarker  *atomicfs.Marker\n\n\tmanifestFile          vfs.File\n\tmanifest              *record.Writer\n\tgetFormatMajorVersion func() FormatMajorVersion\n\n\twriting    bool\n\twriterCond sync.Cond\n\t// State for deciding when to write a snapshot. Protected by mu.\n\trotationHelper record.RotationHelper\n}\n\ntype tableInfo struct {\n\tfileInfo\n\tisLocal bool\n}\n\nfunc (vs *versionSet) init(\n\tdirname string,\n\tprovider objstorage.Provider,\n\topts *Options,\n\tmarker *atomicfs.Marker,\n\tgetFMV func() FormatMajorVersion,\n\tmu *sync.Mutex,\n) {\n\tvs.dirname = dirname\n\tvs.provider = provider\n\tvs.mu = mu\n\tvs.writerCond.L = mu\n\tvs.opts = opts\n\tvs.fs = opts.FS\n\tvs.cmp = opts.Comparer\n\tvs.dynamicBaseLevel = true\n\tvs.versions.Init(mu)\n\tvs.obsoleteFn = vs.addObsoleteLocked\n\tvs.zombieTables = make(map[base.DiskFileNum]tableInfo)\n\tvs.virtualBackings = manifest.MakeVirtualBackings()\n\tvs.nextFileNum.Store(1)\n\tvs.manifestMarker = marker\n\tvs.getFormatMajorVersion = getFMV\n}\n\n// create creates a version set for a fresh DB.\nfunc (vs *versionSet) create(\n\tjobID JobID,\n\tdirname string,\n\tprovider objstorage.Provider,\n\topts *Options,\n\tmarker *atomicfs.Marker,\n\tgetFormatMajorVersion func() FormatMajorVersion,\n\tmu *sync.Mutex,\n) error {\n\tvs.init(dirname, provider, opts, marker, getFormatMajorVersion, mu)\n\tvar bve bulkVersionEdit\n\tnewVersion, err := bve.Apply(nil /* curr */, opts.Comparer, opts.FlushSplitBytes, opts.Experimental.ReadCompactionRate)\n\tif err != nil {\n\t\treturn err\n\t}\n\tvs.append(newVersion)\n\n\tvs.picker = newCompactionPickerByScore(newVersion, &vs.virtualBackings, vs.opts, nil)\n\t// Note that a \"snapshot\" version edit is written to the manifest when it is\n\t// created.\n\tvs.manifestFileNum = vs.getNextDiskFileNum()\n\terr = vs.createManifest(vs.dirname, vs.manifestFileNum, vs.minUnflushedLogNum, vs.nextFileNum.Load(), nil /* virtualBackings */)\n\tif err == nil {\n\t\tif err = vs.manifest.Flush(); err != nil {\n\t\t\tvs.opts.Logger.Fatalf(\"MANIFEST flush failed: %v\", err)\n\t\t}\n\t}\n\tif err == nil {\n\t\tif err = vs.manifestFile.Sync(); err != nil {\n\t\t\tvs.opts.Logger.Fatalf(\"MANIFEST sync failed: %v\", err)\n\t\t}\n\t}\n\tif err == nil {\n\t\t// NB: Move() is responsible for syncing the data directory.\n\t\tif err = vs.manifestMarker.Move(base.MakeFilename(fileTypeManifest, vs.manifestFileNum)); err != nil {\n\t\t\tvs.opts.Logger.Fatalf(\"MANIFEST set current failed: %v\", err)\n\t\t}\n\t}\n\n\tvs.opts.EventListener.ManifestCreated(ManifestCreateInfo{\n\t\tJobID:   int(jobID),\n\t\tPath:    base.MakeFilepath(vs.fs, vs.dirname, fileTypeManifest, vs.manifestFileNum),\n\t\tFileNum: vs.manifestFileNum,\n\t\tErr:     err,\n\t})\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// load loads the version set from the manifest file.\nfunc (vs *versionSet) load(\n\tdirname string,\n\tprovider objstorage.Provider,\n\topts *Options,\n\tmanifestFileNum base.DiskFileNum,\n\tmarker *atomicfs.Marker,\n\tgetFormatMajorVersion func() FormatMajorVersion,\n\tmu *sync.Mutex,\n) error {\n\tvs.init(dirname, provider, opts, marker, getFormatMajorVersion, mu)\n\n\tvs.manifestFileNum = manifestFileNum\n\tmanifestPath := base.MakeFilepath(opts.FS, dirname, fileTypeManifest, vs.manifestFileNum)\n\tmanifestFilename := opts.FS.PathBase(manifestPath)\n\n\t// Read the versionEdits in the manifest file.\n\tvar bve bulkVersionEdit\n\tbve.AddedByFileNum = make(map[base.FileNum]*fileMetadata)\n\tmanifest, err := vs.fs.Open(manifestPath)\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"pebble: could not open manifest file %q for DB %q\",\n\t\t\terrors.Safe(manifestFilename), dirname)\n\t}\n\tdefer manifest.Close()\n\trr := record.NewReader(manifest, 0 /* logNum */)\n\tfor {\n\t\tr, err := rr.Next()\n\t\tif err == io.EOF || record.IsInvalidRecord(err) {\n\t\t\tbreak\n\t\t}\n\t\tif err != nil {\n\t\t\treturn errors.Wrapf(err, \"pebble: error when loading manifest file %q\",\n\t\t\t\terrors.Safe(manifestFilename))\n\t\t}\n\t\tvar ve versionEdit\n\t\terr = ve.Decode(r)\n\t\tif err != nil {\n\t\t\t// Break instead of returning an error if the record is corrupted\n\t\t\t// or invalid.\n\t\t\tif err == io.EOF || record.IsInvalidRecord(err) {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\t\tif ve.ComparerName != \"\" {\n\t\t\tif ve.ComparerName != vs.cmp.Name {\n\t\t\t\treturn errors.Errorf(\"pebble: manifest file %q for DB %q: \"+\n\t\t\t\t\t\"comparer name from file %q != comparer name from Options %q\",\n\t\t\t\t\terrors.Safe(manifestFilename), dirname, errors.Safe(ve.ComparerName), errors.Safe(vs.cmp.Name))\n\t\t\t}\n\t\t}\n\t\tif err := bve.Accumulate(&ve); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif ve.MinUnflushedLogNum != 0 {\n\t\t\tvs.minUnflushedLogNum = ve.MinUnflushedLogNum\n\t\t}\n\t\tif ve.NextFileNum != 0 {\n\t\t\tvs.nextFileNum.Store(ve.NextFileNum)\n\t\t}\n\t\tif ve.LastSeqNum != 0 {\n\t\t\t// logSeqNum is the _next_ sequence number that will be assigned,\n\t\t\t// while LastSeqNum is the last assigned sequence number. Note that\n\t\t\t// this behaviour mimics that in RocksDB; the first sequence number\n\t\t\t// assigned is one greater than the one present in the manifest\n\t\t\t// (assuming no WALs contain higher sequence numbers than the\n\t\t\t// manifest's LastSeqNum). Increment LastSeqNum by 1 to get the\n\t\t\t// next sequence number that will be assigned.\n\t\t\t//\n\t\t\t// If LastSeqNum is less than SeqNumStart, increase it to at least\n\t\t\t// SeqNumStart to leave ample room for reserved sequence numbers.\n\t\t\tif ve.LastSeqNum+1 < base.SeqNumStart {\n\t\t\t\tvs.logSeqNum.Store(base.SeqNumStart)\n\t\t\t} else {\n\t\t\t\tvs.logSeqNum.Store(ve.LastSeqNum + 1)\n\t\t\t}\n\t\t}\n\t}\n\t// We have already set vs.nextFileNum = 2 at the beginning of the\n\t// function and could have only updated it to some other non-zero value,\n\t// so it cannot be 0 here.\n\tif vs.minUnflushedLogNum == 0 {\n\t\tif vs.nextFileNum.Load() >= 2 {\n\t\t\t// We either have a freshly created DB, or a DB created by RocksDB\n\t\t\t// that has not had a single flushed SSTable yet. This is because\n\t\t\t// RocksDB bumps up nextFileNum in this case without bumping up\n\t\t\t// minUnflushedLogNum, even if WALs with non-zero file numbers are\n\t\t\t// present in the directory.\n\t\t} else {\n\t\t\treturn base.CorruptionErrorf(\"pebble: malformed manifest file %q for DB %q\",\n\t\t\t\terrors.Safe(manifestFilename), dirname)\n\t\t}\n\t}\n\tvs.markFileNumUsed(vs.minUnflushedLogNum)\n\n\t// Populate the fileBackingMap and the FileBacking for virtual sstables since\n\t// we have finished version edit accumulation.\n\tfor _, b := range bve.AddedFileBacking {\n\t\tvs.virtualBackings.AddAndRef(b)\n\t}\n\n\tfor _, addedLevel := range bve.Added {\n\t\tfor _, m := range addedLevel {\n\t\t\tif m.Virtual {\n\t\t\t\tvs.virtualBackings.AddTable(m)\n\t\t\t}\n\t\t}\n\t}\n\n\tif invariants.Enabled {\n\t\t// There should be no deleted tables or backings, since we're starting from\n\t\t// an empty state.\n\t\tfor _, deletedLevel := range bve.Deleted {\n\t\t\tif len(deletedLevel) != 0 {\n\t\t\t\tpanic(\"deleted files after manifest replay\")\n\t\t\t}\n\t\t}\n\t\tif len(bve.RemovedFileBacking) > 0 {\n\t\t\tpanic(\"deleted backings after manifest replay\")\n\t\t}\n\t}\n\n\tnewVersion, err := bve.Apply(nil, opts.Comparer, opts.FlushSplitBytes, opts.Experimental.ReadCompactionRate)\n\tif err != nil {\n\t\treturn err\n\t}\n\tnewVersion.L0Sublevels.InitCompactingFileInfo(nil /* in-progress compactions */)\n\tvs.append(newVersion)\n\n\tfor i := range vs.metrics.Levels {\n\t\tl := &vs.metrics.Levels[i]\n\t\tl.NumFiles = int64(newVersion.Levels[i].Len())\n\t\tfiles := newVersion.Levels[i].Slice()\n\t\tl.Size = int64(files.SizeSum())\n\t}\n\tfor _, l := range newVersion.Levels {\n\t\titer := l.Iter()\n\t\tfor f := iter.First(); f != nil; f = iter.Next() {\n\t\t\tif !f.Virtual {\n\t\t\t\t_, localSize := sizeIfLocal(f.FileBacking, vs.provider)\n\t\t\t\tvs.metrics.Table.Local.LiveSize = uint64(int64(vs.metrics.Table.Local.LiveSize) + localSize)\n\t\t\t}\n\t\t}\n\t}\n\tvs.virtualBackings.ForEach(func(backing *fileBacking) {\n\t\t_, localSize := sizeIfLocal(backing, vs.provider)\n\t\tvs.metrics.Table.Local.LiveSize = uint64(int64(vs.metrics.Table.Local.LiveSize) + localSize)\n\t})\n\n\tvs.picker = newCompactionPickerByScore(newVersion, &vs.virtualBackings, vs.opts, nil)\n\treturn nil\n}\n\nfunc (vs *versionSet) close() error {\n\tif vs.manifestFile != nil {\n\t\tif err := vs.manifestFile.Close(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif vs.manifestMarker != nil {\n\t\tif err := vs.manifestMarker.Close(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// logLock locks the manifest for writing. The lock must be released by either\n// a call to logUnlock or logAndApply.\n//\n// DB.mu must be held when calling this method, but the mutex may be dropped and\n// re-acquired during the course of this method.\nfunc (vs *versionSet) logLock() {\n\t// Wait for any existing writing to the manifest to complete, then mark the\n\t// manifest as busy.\n\tfor vs.writing {\n\t\t// Note: writerCond.L is DB.mu, so we unlock it while we wait.\n\t\tvs.writerCond.Wait()\n\t}\n\tvs.writing = true\n}\n\n// logUnlock releases the lock for manifest writing.\n//\n// DB.mu must be held when calling this method.\nfunc (vs *versionSet) logUnlock() {\n\tif !vs.writing {\n\t\tvs.opts.Logger.Fatalf(\"MANIFEST not locked for writing\")\n\t}\n\tvs.writing = false\n\tvs.writerCond.Signal()\n}\n\n// logAndApply logs the version edit to the manifest, applies the version edit\n// to the current version, and installs the new version.\n//\n// logAndApply fills in the following fields of the VersionEdit: NextFileNum,\n// LastSeqNum, RemovedBackingTables. The removed backing tables are those\n// backings that are no longer used (in the new version) after applying the edit\n// (as per vs.virtualBackings). Other than these fields, the VersionEdit must be\n// complete.\n//\n// New table backing references (FileBacking.Ref) are taken as part of applying\n// the version edit. The state of the virtual backings (vs.virtualBackings) is\n// updated before logging to the manifest and installing the latest version;\n// this is ok because any failure in those steps is fatal.\n// TODO(radu): remove the error return.\n//\n// DB.mu must be held when calling this method and will be released temporarily\n// while performing file I/O. Requires that the manifest is locked for writing\n// (see logLock). Will unconditionally release the manifest lock (via\n// logUnlock) even if an error occurs.\n//\n// inProgressCompactions is called while DB.mu is held, to get the list of\n// in-progress compactions.\nfunc (vs *versionSet) logAndApply(\n\tjobID JobID,\n\tve *versionEdit,\n\tmetrics map[int]*LevelMetrics,\n\tforceRotation bool,\n\tinProgressCompactions func() []compactionInfo,\n) error {\n\tif !vs.writing {\n\t\tvs.opts.Logger.Fatalf(\"MANIFEST not locked for writing\")\n\t}\n\tdefer vs.logUnlock()\n\n\tif ve.MinUnflushedLogNum != 0 {\n\t\tif ve.MinUnflushedLogNum < vs.minUnflushedLogNum ||\n\t\t\tvs.nextFileNum.Load() <= uint64(ve.MinUnflushedLogNum) {\n\t\t\tpanic(fmt.Sprintf(\"pebble: inconsistent versionEdit minUnflushedLogNum %d\",\n\t\t\t\tve.MinUnflushedLogNum))\n\t\t}\n\t}\n\n\t// This is the next manifest filenum, but if the current file is too big we\n\t// will write this ve to the next file which means what ve encodes is the\n\t// current filenum and not the next one.\n\t//\n\t// TODO(sbhola): figure out why this is correct and update comment.\n\tve.NextFileNum = vs.nextFileNum.Load()\n\n\t// LastSeqNum is set to the current upper bound on the assigned sequence\n\t// numbers. Note that this is exactly the behavior of RocksDB. LastSeqNum is\n\t// used to initialize versionSet.logSeqNum and versionSet.visibleSeqNum on\n\t// replay. It must be higher than or equal to any than any sequence number\n\t// written to an sstable, including sequence numbers in ingested files.\n\t// Note that LastSeqNum is not (and cannot be) the minimum unflushed sequence\n\t// number. This is fallout from ingestion which allows a sequence number X to\n\t// be assigned to an ingested sstable even though sequence number X-1 resides\n\t// in an unflushed memtable. logSeqNum is the _next_ sequence number that\n\t// will be assigned, so subtract that by 1 to get the upper bound on the\n\t// last assigned sequence number.\n\tlogSeqNum := vs.logSeqNum.Load()\n\tve.LastSeqNum = logSeqNum - 1\n\tif logSeqNum == 0 {\n\t\t// logSeqNum is initialized to 1 in Open() if there are no previous WAL\n\t\t// or manifest records, so this case should never happen.\n\t\tvs.opts.Logger.Fatalf(\"logSeqNum must be a positive integer: %d\", logSeqNum)\n\t}\n\n\tcurrentVersion := vs.currentVersion()\n\tvar newVersion *version\n\n\t// Generate a new manifest if we don't currently have one, or forceRotation\n\t// is true, or the current one is too large.\n\t//\n\t// For largeness, we do not exclusively use MaxManifestFileSize size\n\t// threshold since we have had incidents where due to either large keys or\n\t// large numbers of files, each edit results in a snapshot + write of the\n\t// edit. This slows the system down since each flush or compaction is\n\t// writing a new manifest snapshot. The primary goal of the size-based\n\t// rollover logic is to ensure that when reopening a DB, the number of edits\n\t// that need to be replayed on top of the snapshot is \"sane\". Rolling over\n\t// to a new manifest after each edit is not relevant to that goal.\n\t//\n\t// Consider the following cases:\n\t// - The number of live files F in the DB is roughly stable: after writing\n\t//   the snapshot (with F files), say we require that there be enough edits\n\t//   such that the cumulative number of files in those edits, E, be greater\n\t//   than F. This will ensure that the total amount of time in logAndApply\n\t//   that is spent in snapshot writing is ~50%.\n\t//\n\t// - The number of live files F in the DB is shrinking drastically, say from\n\t//   F to F/10: This can happen for various reasons, like wide range\n\t//   tombstones, or large numbers of smaller than usual files that are being\n\t//   merged together into larger files. And say the new files generated\n\t//   during this shrinkage is insignificant compared to F/10, and so for\n\t//   this example we will assume it is effectively 0. After this shrinking,\n\t//   E = 0.9F, and so if we used the previous snapshot file count, F, as the\n\t//   threshold that needs to be exceeded, we will further delay the snapshot\n\t//   writing. Which means on DB reopen we will need to replay 0.9F edits to\n\t//   get to a version with 0.1F files. It would be better to create a new\n\t//   snapshot when E exceeds the number of files in the current version.\n\t//\n\t// - The number of live files F in the DB is growing via perfect ingests\n\t//   into L6: Say we wrote the snapshot when there were F files and now we\n\t//   have 10F files, so E = 9F. We will further delay writing a new\n\t//   snapshot. This case can be critiqued as contrived, but we consider it\n\t//   nonetheless.\n\t//\n\t// The logic below uses the min of the last snapshot file count and the file\n\t// count in the current version.\n\tvs.rotationHelper.AddRecord(int64(len(ve.DeletedFiles) + len(ve.NewFiles)))\n\tsizeExceeded := vs.manifest.Size() >= vs.opts.MaxManifestFileSize\n\trequireRotation := forceRotation || vs.manifest == nil\n\n\tvar nextSnapshotFilecount int64\n\tfor i := range vs.metrics.Levels {\n\t\tnextSnapshotFilecount += vs.metrics.Levels[i].NumFiles\n\t}\n\tif sizeExceeded && !requireRotation {\n\t\trequireRotation = vs.rotationHelper.ShouldRotate(nextSnapshotFilecount)\n\t}\n\tvar newManifestFileNum base.DiskFileNum\n\tvar prevManifestFileSize uint64\n\tvar newManifestVirtualBackings []*fileBacking\n\tif requireRotation {\n\t\tnewManifestFileNum = vs.getNextDiskFileNum()\n\t\tprevManifestFileSize = uint64(vs.manifest.Size())\n\n\t\t// We want the virtual backings *before* applying the version edit, because\n\t\t// the new manifest will contain the pre-apply version plus the last version\n\t\t// edit.\n\t\tnewManifestVirtualBackings = vs.virtualBackings.Backings()\n\t}\n\n\t// Grab certain values before releasing vs.mu, in case createManifest() needs\n\t// to be called.\n\tminUnflushedLogNum := vs.minUnflushedLogNum\n\tnextFileNum := vs.nextFileNum.Load()\n\n\t// Note: this call populates ve.RemovedBackingTables.\n\tzombieBackings, removedVirtualBackings, localLiveSizeDelta :=\n\t\tgetZombiesAndUpdateVirtualBackings(ve, &vs.virtualBackings, vs.provider)\n\n\tif err := func() error {\n\t\tvs.mu.Unlock()\n\t\tdefer vs.mu.Lock()\n\n\t\tif vs.getFormatMajorVersion() < FormatVirtualSSTables && len(ve.CreatedBackingTables) > 0 {\n\t\t\treturn base.AssertionFailedf(\"MANIFEST cannot contain virtual sstable records due to format major version\")\n\t\t}\n\t\tvar b bulkVersionEdit\n\t\terr := b.Accumulate(ve)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"MANIFEST accumulate failed\")\n\t\t}\n\t\tnewVersion, err = b.Apply(\n\t\t\tcurrentVersion, vs.cmp, vs.opts.FlushSplitBytes, vs.opts.Experimental.ReadCompactionRate,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"MANIFEST apply failed\")\n\t\t}\n\n\t\tif newManifestFileNum != 0 {\n\t\t\tif err := vs.createManifest(vs.dirname, newManifestFileNum, minUnflushedLogNum, nextFileNum, newManifestVirtualBackings); err != nil {\n\t\t\t\tvs.opts.EventListener.ManifestCreated(ManifestCreateInfo{\n\t\t\t\t\tJobID:   int(jobID),\n\t\t\t\t\tPath:    base.MakeFilepath(vs.fs, vs.dirname, fileTypeManifest, newManifestFileNum),\n\t\t\t\t\tFileNum: newManifestFileNum,\n\t\t\t\t\tErr:     err,\n\t\t\t\t})\n\t\t\t\treturn errors.Wrap(err, \"MANIFEST create failed\")\n\t\t\t}\n\t\t}\n\n\t\tw, err := vs.manifest.Next()\n\t\tif err != nil {\n\t\t\treturn errors.Wrap(err, \"MANIFEST next record write failed\")\n\t\t}\n\n\t\t// NB: Any error from this point on is considered fatal as we don't know if\n\t\t// the MANIFEST write occurred or not. Trying to determine that is\n\t\t// fraught. Instead we rely on the standard recovery mechanism run when a\n\t\t// database is open. In particular, that mechanism generates a new MANIFEST\n\t\t// and ensures it is synced.\n\t\tif err := ve.Encode(w); err != nil {\n\t\t\treturn errors.Wrap(err, \"MANIFEST write failed\")\n\t\t}\n\t\tif err := vs.manifest.Flush(); err != nil {\n\t\t\treturn errors.Wrap(err, \"MANIFEST flush failed\")\n\t\t}\n\t\tif err := vs.manifestFile.Sync(); err != nil {\n\t\t\treturn errors.Wrap(err, \"MANIFEST sync failed\")\n\t\t}\n\t\tif newManifestFileNum != 0 {\n\t\t\t// NB: Move() is responsible for syncing the data directory.\n\t\t\tif err := vs.manifestMarker.Move(base.MakeFilename(fileTypeManifest, newManifestFileNum)); err != nil {\n\t\t\t\treturn errors.Wrap(err, \"MANIFEST set current failed\")\n\t\t\t}\n\t\t\tvs.opts.EventListener.ManifestCreated(ManifestCreateInfo{\n\t\t\t\tJobID:   int(jobID),\n\t\t\t\tPath:    base.MakeFilepath(vs.fs, vs.dirname, fileTypeManifest, newManifestFileNum),\n\t\t\t\tFileNum: newManifestFileNum,\n\t\t\t})\n\t\t}\n\t\treturn nil\n\t}(); err != nil {\n\t\t// Any error encountered during any of the operations in the previous\n\t\t// closure are considered fatal. Treating such errors as fatal is preferred\n\t\t// to attempting to unwind various file and b-tree reference counts, and\n\t\t// re-generating L0 sublevel metadata. This may change in the future, if\n\t\t// certain manifest / WAL operations become retryable. For more context, see\n\t\t// #1159 and #1792.\n\t\tvs.opts.Logger.Fatalf(\"%s\", err)\n\t\treturn err\n\t}\n\n\tif requireRotation {\n\t\t// Successfully rotated.\n\t\tvs.rotationHelper.Rotate(nextSnapshotFilecount)\n\t}\n\t// Now that DB.mu is held again, initialize compacting file info in\n\t// L0Sublevels.\n\tinProgress := inProgressCompactions()\n\n\tnewVersion.L0Sublevels.InitCompactingFileInfo(inProgressL0Compactions(inProgress))\n\n\t// Update the zombie tables set first, as installation of the new version\n\t// will unref the previous version which could result in addObsoleteLocked\n\t// being called.\n\tfor _, b := range zombieBackings {\n\t\tvs.zombieTables[b.backing.DiskFileNum] = tableInfo{\n\t\t\tfileInfo: fileInfo{\n\t\t\t\tFileNum:  b.backing.DiskFileNum,\n\t\t\t\tFileSize: b.backing.Size,\n\t\t\t},\n\t\t\tisLocal: b.isLocal,\n\t\t}\n\t}\n\n\t// Unref the removed backings and report those that already became obsolete.\n\t// Note that the only case where we report obsolete tables here is when\n\t// VirtualBackings.Protect/Unprotect was used to keep a backing alive without\n\t// it being used in the current version.\n\tvar obsoleteVirtualBackings []*fileBacking\n\tfor _, b := range removedVirtualBackings {\n\t\tif b.backing.Unref() == 0 {\n\t\t\tobsoleteVirtualBackings = append(obsoleteVirtualBackings, b.backing)\n\t\t}\n\t}\n\tvs.addObsoleteLocked(obsoleteVirtualBackings)\n\n\t// Install the new version.\n\tvs.append(newVersion)\n\n\tif ve.MinUnflushedLogNum != 0 {\n\t\tvs.minUnflushedLogNum = ve.MinUnflushedLogNum\n\t}\n\tif newManifestFileNum != 0 {\n\t\tif vs.manifestFileNum != 0 {\n\t\t\tvs.obsoleteManifests = append(vs.obsoleteManifests, fileInfo{\n\t\t\t\tFileNum:  vs.manifestFileNum,\n\t\t\t\tFileSize: prevManifestFileSize,\n\t\t\t})\n\t\t}\n\t\tvs.manifestFileNum = newManifestFileNum\n\t}\n\n\tfor level, update := range metrics {\n\t\tvs.metrics.Levels[level].Add(update)\n\t}\n\tfor i := range vs.metrics.Levels {\n\t\tl := &vs.metrics.Levels[i]\n\t\tl.NumFiles = int64(newVersion.Levels[i].Len())\n\t\tl.NumVirtualFiles = newVersion.Levels[i].NumVirtual\n\t\tl.VirtualSize = newVersion.Levels[i].VirtualSize\n\t\tl.Size = int64(newVersion.Levels[i].Size())\n\n\t\tl.Sublevels = 0\n\t\tif l.NumFiles > 0 {\n\t\t\tl.Sublevels = 1\n\t\t}\n\t\tif invariants.Enabled {\n\t\t\tlevelFiles := newVersion.Levels[i].Slice()\n\t\t\tif size := int64(levelFiles.SizeSum()); l.Size != size {\n\t\t\t\tvs.opts.Logger.Fatalf(\"versionSet metrics L%d Size = %d, actual size = %d\", i, l.Size, size)\n\t\t\t}\n\t\t\tif nVirtual := levelFiles.NumVirtual(); nVirtual != l.NumVirtualFiles {\n\t\t\t\tvs.opts.Logger.Fatalf(\n\t\t\t\t\t\"versionSet metrics L%d NumVirtual = %d, actual NumVirtual = %d\",\n\t\t\t\t\ti, l.NumVirtualFiles, nVirtual,\n\t\t\t\t)\n\t\t\t}\n\t\t\tif vSize := levelFiles.VirtualSizeSum(); vSize != l.VirtualSize {\n\t\t\t\tvs.opts.Logger.Fatalf(\n\t\t\t\t\t\"versionSet metrics L%d Virtual size = %d, actual size = %d\",\n\t\t\t\t\ti, l.VirtualSize, vSize,\n\t\t\t\t)\n\t\t\t}\n\t\t}\n\t}\n\tvs.metrics.Levels[0].Sublevels = int32(len(newVersion.L0SublevelFiles))\n\tvs.metrics.Table.Local.LiveSize = uint64(int64(vs.metrics.Table.Local.LiveSize) + localLiveSizeDelta)\n\n\tvs.picker = newCompactionPickerByScore(newVersion, &vs.virtualBackings, vs.opts, inProgress)\n\tif !vs.dynamicBaseLevel {\n\t\tvs.picker.forceBaseLevel1()\n\t}\n\treturn nil\n}\n\ntype fileBackingInfo struct {\n\tbacking *fileBacking\n\tisLocal bool\n}\n\n// getZombiesAndUpdateVirtualBackings updates the virtual backings with the\n// changes in the versionEdit and populates ve.RemovedBackingTables.\n// Returns:\n//   - zombieBackings: all backings (physical and virtual) that will no longer be\n//     needed when we apply ve.\n//   - removedVirtualBackings: the virtual backings that will be removed by the\n//     VersionEdit and which must be Unref()ed by the caller. These backings\n//     match ve.RemovedBackingTables.\n//   - localLiveSizeDelta: the delta in local live bytes.\nfunc getZombiesAndUpdateVirtualBackings(\n\tve *versionEdit, virtualBackings *manifest.VirtualBackings, provider objstorage.Provider,\n) (zombieBackings, removedVirtualBackings []fileBackingInfo, localLiveSizeDelta int64) {\n\t// First, deal with the physical tables.\n\t//\n\t// A physical backing has become unused if it is in DeletedFiles but not in\n\t// NewFiles or CreatedBackingTables.\n\t//\n\t// Note that for the common case where there are very few elements, the map\n\t// will stay on the stack.\n\tstillUsed := make(map[base.DiskFileNum]struct{})\n\tfor _, nf := range ve.NewFiles {\n\t\tif !nf.Meta.Virtual {\n\t\t\tstillUsed[nf.Meta.FileBacking.DiskFileNum] = struct{}{}\n\t\t\t_, localFileDelta := sizeIfLocal(nf.Meta.FileBacking, provider)\n\t\t\tlocalLiveSizeDelta += localFileDelta\n\t\t}\n\t}\n\tfor _, b := range ve.CreatedBackingTables {\n\t\tstillUsed[b.DiskFileNum] = struct{}{}\n\t}\n\tfor _, m := range ve.DeletedFiles {\n\t\tif !m.Virtual {\n\t\t\t// NB: this deleted file may also be in NewFiles or\n\t\t\t// CreatedBackingTables, due to a file moving between levels, or\n\t\t\t// becoming virtualized. In which case there is no change due to this\n\t\t\t// file in the localLiveSizeDelta -- the subtraction below compensates\n\t\t\t// for the addition.\n\t\t\tisLocal, localFileDelta := sizeIfLocal(m.FileBacking, provider)\n\t\t\tlocalLiveSizeDelta -= localFileDelta\n\t\t\tif _, ok := stillUsed[m.FileBacking.DiskFileNum]; !ok {\n\t\t\t\tzombieBackings = append(zombieBackings, fileBackingInfo{\n\t\t\t\t\tbacking: m.FileBacking,\n\t\t\t\t\tisLocal: isLocal,\n\t\t\t\t})\n\t\t\t}\n\t\t}\n\t}\n\n\t// Now deal with virtual tables.\n\t//\n\t// When a virtual table moves between levels we AddTable() then RemoveTable(),\n\t// which works out.\n\tfor _, b := range ve.CreatedBackingTables {\n\t\tvirtualBackings.AddAndRef(b)\n\t\t_, localFileDelta := sizeIfLocal(b, provider)\n\t\tlocalLiveSizeDelta += localFileDelta\n\t}\n\tfor _, nf := range ve.NewFiles {\n\t\tif nf.Meta.Virtual {\n\t\t\tvirtualBackings.AddTable(nf.Meta)\n\t\t}\n\t}\n\tfor _, m := range ve.DeletedFiles {\n\t\tif m.Virtual {\n\t\t\tvirtualBackings.RemoveTable(m)\n\t\t}\n\t}\n\n\tif unused := virtualBackings.Unused(); len(unused) > 0 {\n\t\t// Virtual backings that are no longer used are zombies and are also added\n\t\t// to RemovedBackingTables (before the version edit is written to disk).\n\t\tve.RemovedBackingTables = make([]base.DiskFileNum, len(unused))\n\t\tfor i, b := range unused {\n\t\t\tisLocal, localFileDelta := sizeIfLocal(b, provider)\n\t\t\tlocalLiveSizeDelta -= localFileDelta\n\t\t\tve.RemovedBackingTables[i] = b.DiskFileNum\n\t\t\tzombieBackings = append(zombieBackings, fileBackingInfo{\n\t\t\t\tbacking: b,\n\t\t\t\tisLocal: isLocal,\n\t\t\t})\n\t\t\tvirtualBackings.Remove(b.DiskFileNum)\n\t\t}\n\t\tremovedVirtualBackings = zombieBackings[len(zombieBackings)-len(unused):]\n\t}\n\treturn zombieBackings, removedVirtualBackings, localLiveSizeDelta\n}\n\n// sizeIfLocal returns backing.Size if the backing is a local file, else 0.\nfunc sizeIfLocal(\n\tbacking *fileBacking, provider objstorage.Provider,\n) (isLocal bool, localSize int64) {\n\tisLocal = objstorage.IsLocalTable(provider, backing.DiskFileNum)\n\tif isLocal {\n\t\treturn true, int64(backing.Size)\n\t}\n\treturn false, 0\n}\n\nfunc (vs *versionSet) incrementCompactions(\n\tkind compactionKind, extraLevels []*compactionLevel, pickerMetrics compactionPickerMetrics,\n) {\n\tswitch kind {\n\tcase compactionKindDefault:\n\t\tvs.metrics.Compact.Count++\n\t\tvs.metrics.Compact.DefaultCount++\n\n\tcase compactionKindFlush, compactionKindIngestedFlushable:\n\t\tvs.metrics.Flush.Count++\n\n\tcase compactionKindMove:\n\t\tvs.metrics.Compact.Count++\n\t\tvs.metrics.Compact.MoveCount++\n\n\tcase compactionKindDeleteOnly:\n\t\tvs.metrics.Compact.Count++\n\t\tvs.metrics.Compact.DeleteOnlyCount++\n\n\tcase compactionKindElisionOnly:\n\t\tvs.metrics.Compact.Count++\n\t\tvs.metrics.Compact.ElisionOnlyCount++\n\n\tcase compactionKindRead:\n\t\tvs.metrics.Compact.Count++\n\t\tvs.metrics.Compact.ReadCount++\n\n\tcase compactionKindTombstoneDensity:\n\t\tvs.metrics.Compact.Count++\n\t\tvs.metrics.Compact.TombstoneDensityCount++\n\n\tcase compactionKindRewrite:\n\t\tvs.metrics.Compact.Count++\n\t\tvs.metrics.Compact.RewriteCount++\n\n\tcase compactionKindCopy:\n\t\tvs.metrics.Compact.Count++\n\t\tvs.metrics.Compact.CopyCount++\n\n\tdefault:\n\t\tif invariants.Enabled {\n\t\t\tpanic(\"unhandled compaction kind\")\n\t\t}\n\t}\n\tif len(extraLevels) > 0 {\n\t\tvs.metrics.Compact.MultiLevelCount++\n\t}\n}\n\nfunc (vs *versionSet) incrementCompactionBytes(numBytes int64) {\n\tvs.atomicInProgressBytes.Add(numBytes)\n}\n\n// createManifest creates a manifest file that contains a snapshot of vs.\nfunc (vs *versionSet) createManifest(\n\tdirname string,\n\tfileNum, minUnflushedLogNum base.DiskFileNum,\n\tnextFileNum uint64,\n\tvirtualBackings []*fileBacking,\n) (err error) {\n\tvar (\n\t\tfilename     = base.MakeFilepath(vs.fs, dirname, fileTypeManifest, fileNum)\n\t\tmanifestFile vfs.File\n\t\tmanifest     *record.Writer\n\t)\n\tdefer func() {\n\t\tif manifest != nil {\n\t\t\tmanifest.Close()\n\t\t}\n\t\tif manifestFile != nil {\n\t\t\tmanifestFile.Close()\n\t\t}\n\t\tif err != nil {\n\t\t\tvs.fs.Remove(filename)\n\t\t}\n\t}()\n\tmanifestFile, err = vs.fs.Create(filename, \"pebble-manifest\")\n\tif err != nil {\n\t\treturn err\n\t}\n\tmanifest = record.NewWriter(manifestFile)\n\n\tsnapshot := versionEdit{\n\t\tComparerName: vs.cmp.Name,\n\t}\n\n\tfor level, levelMetadata := range vs.currentVersion().Levels {\n\t\titer := levelMetadata.Iter()\n\t\tfor meta := iter.First(); meta != nil; meta = iter.Next() {\n\t\t\tsnapshot.NewFiles = append(snapshot.NewFiles, newFileEntry{\n\t\t\t\tLevel: level,\n\t\t\t\tMeta:  meta,\n\t\t\t})\n\t\t}\n\t}\n\n\tsnapshot.CreatedBackingTables = virtualBackings\n\n\t// When creating a version snapshot for an existing DB, this snapshot VersionEdit will be\n\t// immediately followed by another VersionEdit (being written in logAndApply()). That\n\t// VersionEdit always contains a LastSeqNum, so we don't need to include that in the snapshot.\n\t// But it does not necessarily include MinUnflushedLogNum, NextFileNum, so we initialize those\n\t// using the corresponding fields in the versionSet (which came from the latest preceding\n\t// VersionEdit that had those fields).\n\tsnapshot.MinUnflushedLogNum = minUnflushedLogNum\n\tsnapshot.NextFileNum = nextFileNum\n\n\tw, err1 := manifest.Next()\n\tif err1 != nil {\n\t\treturn err1\n\t}\n\tif err := snapshot.Encode(w); err != nil {\n\t\treturn err\n\t}\n\n\tif vs.manifest != nil {\n\t\tvs.manifest.Close()\n\t\tvs.manifest = nil\n\t}\n\tif vs.manifestFile != nil {\n\t\tif err := vs.manifestFile.Close(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tvs.manifestFile = nil\n\t}\n\n\tvs.manifest, manifest = manifest, nil\n\tvs.manifestFile, manifestFile = manifestFile, nil\n\treturn nil\n}\n\n// NB: This method is not safe for concurrent use. It is only safe\n// to be called when concurrent changes to nextFileNum are not expected.\nfunc (vs *versionSet) markFileNumUsed(fileNum base.DiskFileNum) {\n\tif vs.nextFileNum.Load() <= uint64(fileNum) {\n\t\tvs.nextFileNum.Store(uint64(fileNum + 1))\n\t}\n}\n\n// getNextFileNum returns the next file number to be used.\n//\n// Can be called without the versionSet's mutex being held.\nfunc (vs *versionSet) getNextFileNum() base.FileNum {\n\tx := vs.nextFileNum.Add(1) - 1\n\treturn base.FileNum(x)\n}\n\n// Can be called without the versionSet's mutex being held.\nfunc (vs *versionSet) getNextDiskFileNum() base.DiskFileNum {\n\tx := vs.nextFileNum.Add(1) - 1\n\treturn base.DiskFileNum(x)\n}\n\nfunc (vs *versionSet) append(v *version) {\n\tif v.Refs() != 0 {\n\t\tpanic(\"pebble: version should be unreferenced\")\n\t}\n\tif !vs.versions.Empty() {\n\t\tvs.versions.Back().UnrefLocked()\n\t}\n\tv.Deleted = vs.obsoleteFn\n\tv.Ref()\n\tvs.versions.PushBack(v)\n\tif invariants.Enabled {\n\t\t// Verify that the virtualBackings contains all the backings referenced by\n\t\t// the version.\n\t\tfor _, l := range v.Levels {\n\t\t\titer := l.Iter()\n\t\t\tfor f := iter.First(); f != nil; f = iter.Next() {\n\t\t\t\tif f.Virtual {\n\t\t\t\t\tif _, ok := vs.virtualBackings.Get(f.FileBacking.DiskFileNum); !ok {\n\t\t\t\t\t\tpanic(fmt.Sprintf(\"%s is not in virtualBackings\", f.FileBacking.DiskFileNum))\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (vs *versionSet) currentVersion() *version {\n\treturn vs.versions.Back()\n}\n\nfunc (vs *versionSet) addLiveFileNums(m map[base.DiskFileNum]struct{}) {\n\tcurrent := vs.currentVersion()\n\tfor v := vs.versions.Front(); true; v = v.Next() {\n\t\tfor _, lm := range v.Levels {\n\t\t\titer := lm.Iter()\n\t\t\tfor f := iter.First(); f != nil; f = iter.Next() {\n\t\t\t\tm[f.FileBacking.DiskFileNum] = struct{}{}\n\t\t\t}\n\t\t}\n\t\tif v == current {\n\t\t\tbreak\n\t\t}\n\t}\n\t// virtualBackings contains backings that are referenced by some virtual\n\t// tables in the latest version (which are handled above), and backings that\n\t// are not but are still alive because of the protection mechanism (see\n\t// manifset.VirtualBackings). This loop ensures the latter get added to the\n\t// map.\n\tvs.virtualBackings.ForEach(func(b *fileBacking) {\n\t\tm[b.DiskFileNum] = struct{}{}\n\t})\n}\n\n// addObsoleteLocked will add the fileInfo associated with obsolete backing\n// sstables to the obsolete tables list.\n//\n// The file backings in the obsolete list must not appear more than once.\n//\n// DB.mu must be held when addObsoleteLocked is called.\nfunc (vs *versionSet) addObsoleteLocked(obsolete []*fileBacking) {\n\tif len(obsolete) == 0 {\n\t\treturn\n\t}\n\n\tobsoleteFileInfo := make([]tableInfo, len(obsolete))\n\tfor i, bs := range obsolete {\n\t\tobsoleteFileInfo[i].FileNum = bs.DiskFileNum\n\t\tobsoleteFileInfo[i].FileSize = bs.Size\n\t}\n\n\tif invariants.Enabled {\n\t\tdedup := make(map[base.DiskFileNum]struct{})\n\t\tfor _, fi := range obsoleteFileInfo {\n\t\t\tdedup[fi.FileNum] = struct{}{}\n\t\t}\n\t\tif len(dedup) != len(obsoleteFileInfo) {\n\t\t\tpanic(\"pebble: duplicate FileBacking present in obsolete list\")\n\t\t}\n\t}\n\n\tfor i, fi := range obsoleteFileInfo {\n\t\t// Note that the obsolete tables are no longer zombie by the definition of\n\t\t// zombie, but we leave them in the zombie tables map until they are\n\t\t// deleted from disk.\n\t\t//\n\t\t// TODO(sumeer): this means that the zombie metrics, like ZombieSize,\n\t\t// computed in DB.Metrics are also being counted in the obsolete metrics.\n\t\t// Was this intentional?\n\t\tinfo, ok := vs.zombieTables[fi.FileNum]\n\t\tif !ok {\n\t\t\tvs.opts.Logger.Fatalf(\"MANIFEST obsolete table %s not marked as zombie\", fi.FileNum)\n\t\t}\n\t\tobsoleteFileInfo[i].isLocal = info.isLocal\n\t}\n\n\tvs.obsoleteTables = append(vs.obsoleteTables, obsoleteFileInfo...)\n\tvs.updateObsoleteTableMetricsLocked()\n}\n\n// addObsolete will acquire DB.mu, so DB.mu must not be held when this is\n// called.\nfunc (vs *versionSet) addObsolete(obsolete []*fileBacking) {\n\tvs.mu.Lock()\n\tdefer vs.mu.Unlock()\n\tvs.addObsoleteLocked(obsolete)\n}\n\nfunc (vs *versionSet) updateObsoleteTableMetricsLocked() {\n\tvs.metrics.Table.ObsoleteCount = int64(len(vs.obsoleteTables))\n\tvs.metrics.Table.ObsoleteSize = 0\n\tvs.metrics.Table.Local.ObsoleteSize = 0\n\tfor _, fi := range vs.obsoleteTables {\n\t\tvs.metrics.Table.ObsoleteSize += fi.FileSize\n\t\tif fi.isLocal {\n\t\t\tvs.metrics.Table.Local.ObsoleteSize += fi.FileSize\n\t\t}\n\t}\n}\n\nfunc findCurrentManifest(\n\tfs vfs.FS, dirname string, ls []string,\n) (marker *atomicfs.Marker, manifestNum base.DiskFileNum, exists bool, err error) {\n\t// Locating a marker should succeed even if the marker has never been placed.\n\tvar filename string\n\tmarker, filename, err = atomicfs.LocateMarkerInListing(fs, dirname, manifestMarkerName, ls)\n\tif err != nil {\n\t\treturn nil, 0, false, err\n\t}\n\n\tif filename == \"\" {\n\t\t// The marker hasn't been set yet. This database doesn't exist.\n\t\treturn marker, 0, false, nil\n\t}\n\n\tvar ok bool\n\t_, manifestNum, ok = base.ParseFilename(fs, filename)\n\tif !ok {\n\t\treturn marker, 0, false, base.CorruptionErrorf(\"pebble: MANIFEST name %q is malformed\", errors.Safe(filename))\n\t}\n\treturn marker, manifestNum, true, nil\n}\n\nfunc newFileMetrics(newFiles []manifest.NewFileEntry) map[int]*LevelMetrics {\n\tm := map[int]*LevelMetrics{}\n\tfor _, nf := range newFiles {\n\t\tlm := m[nf.Level]\n\t\tif lm == nil {\n\t\t\tlm = &LevelMetrics{}\n\t\t\tm[nf.Level] = lm\n\t\t}\n\t\tlm.NumFiles++\n\t\tlm.Size += int64(nf.Meta.Size)\n\t}\n\treturn m\n}\n"
        },
        {
          "name": "version_set_test.go",
          "type": "blob",
          "size": 9.5791015625,
          "content": "// Copyright 2020 The LevelDB-Go and Pebble Authors. All rights reserved. Use\n// of this source code is governed by a BSD-style license that can be found in\n// the LICENSE file.\n\npackage pebble\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"math/rand/v2\"\n\t\"slices\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"testing\"\n\n\t\"github.com/cockroachdb/datadriven\"\n\t\"github.com/cockroachdb/pebble/internal/base\"\n\t\"github.com/cockroachdb/pebble/internal/manifest\"\n\t\"github.com/cockroachdb/pebble/objstorage\"\n\t\"github.com/cockroachdb/pebble/objstorage/objstorageprovider\"\n\t\"github.com/cockroachdb/pebble/record\"\n\t\"github.com/cockroachdb/pebble/sstable\"\n\t\"github.com/cockroachdb/pebble/vfs\"\n\t\"github.com/cockroachdb/pebble/vfs/atomicfs\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc writeAndIngest(t *testing.T, mem vfs.FS, d *DB, k InternalKey, v []byte, filename string) {\n\tpath := mem.PathJoin(\"ext\", filename)\n\tf, err := mem.Create(path, vfs.WriteCategoryUnspecified)\n\trequire.NoError(t, err)\n\tw := sstable.NewRawWriter(objstorageprovider.NewFileWritable(f), sstable.WriterOptions{})\n\trequire.NoError(t, w.AddWithForceObsolete(k, v, false /* forceObsolete */))\n\trequire.NoError(t, w.Close())\n\trequire.NoError(t, d.Ingest(context.Background(), []string{path}))\n}\n\nfunc TestVersionSet(t *testing.T) {\n\topts := &Options{\n\t\tFS:       vfs.NewMem(),\n\t\tComparer: base.DefaultComparer,\n\t\tLogger:   testLogger{t},\n\t}\n\topts.EnsureDefaults()\n\tmu := &sync.Mutex{}\n\tmarker, _, err := atomicfs.LocateMarker(opts.FS, \"\", manifestMarkerName)\n\trequire.NoError(t, err)\n\tprovider, err := objstorageprovider.Open(objstorageprovider.DefaultSettings(opts.FS, \"\" /* dirName */))\n\trequire.NoError(t, err)\n\tvar vs versionSet\n\trequire.NoError(t, vs.create(\n\t\t0 /* jobID */, \"\" /* dirname */, provider, opts, marker,\n\t\tfunc() FormatMajorVersion { return FormatVirtualSSTables },\n\t\tmu,\n\t))\n\tvs.logSeqNum.Store(100)\n\n\tmetas := make(map[base.FileNum]*manifest.FileMetadata)\n\tbackings := make(map[base.DiskFileNum]*manifest.FileBacking)\n\t// When we parse VersionEdits, we get a new FileBacking each time. We need to\n\t// deduplicate them, since they hold a ref count.\n\tdedupBacking := func(b *manifest.FileBacking) *manifest.FileBacking {\n\t\tif existing, ok := backings[b.DiskFileNum]; ok {\n\t\t\treturn existing\n\t\t}\n\t\t// The first time we see a backing, we also set a size.\n\t\tb.Size = uint64(b.DiskFileNum) * 1000\n\t\tbackings[b.DiskFileNum] = b\n\t\treturn b\n\t}\n\n\trefs := make(map[string]*version)\n\tdatadriven.RunTest(t, \"testdata/version_set\", func(t *testing.T, td *datadriven.TestData) (retVal string) {\n\t\t// createFile only exists to prevent versionSet from complaining that a\n\t\t// file that is becoming a zombie does not exist.\n\t\tcreateFile := func(fileNum base.DiskFileNum) {\n\t\t\tw, _, err := provider.Create(context.Background(), fileTypeTable, fileNum, objstorage.CreateOptions{})\n\t\t\trequire.NoError(t, err)\n\t\t\trequire.NoError(t, w.Finish())\n\t\t}\n\t\tvar buf strings.Builder\n\n\t\tswitch td.Cmd {\n\t\tcase \"apply\":\n\t\t\tve, err := manifest.ParseVersionEditDebug(td.Input)\n\t\t\tif err != nil {\n\t\t\t\ttd.Fatalf(t, \"%v\", err)\n\t\t\t}\n\t\t\tfor _, nf := range ve.NewFiles {\n\t\t\t\t// Set a size that depends on FileNum.\n\t\t\t\tnf.Meta.Size = uint64(nf.Meta.FileNum) * 100\n\t\t\t\tnf.Meta.FileBacking = dedupBacking(nf.Meta.FileBacking)\n\t\t\t\tmetas[nf.Meta.FileNum] = nf.Meta\n\t\t\t\tif !nf.Meta.Virtual {\n\t\t\t\t\tcreateFile(nf.Meta.FileBacking.DiskFileNum)\n\t\t\t\t}\n\t\t\t}\n\t\t\tfor de := range ve.DeletedFiles {\n\t\t\t\tm := metas[de.FileNum]\n\t\t\t\tif m == nil {\n\t\t\t\t\ttd.Fatalf(t, \"unknown FileNum %s\", de.FileNum)\n\t\t\t\t}\n\t\t\t\tve.DeletedFiles[de] = m\n\t\t\t}\n\t\t\tfor i := range ve.CreatedBackingTables {\n\t\t\t\tve.CreatedBackingTables[i] = dedupBacking(ve.CreatedBackingTables[i])\n\t\t\t\tcreateFile(ve.CreatedBackingTables[i].DiskFileNum)\n\t\t\t}\n\n\t\t\tfileMetrics := newFileMetrics(ve.NewFiles)\n\t\t\tfor de, f := range ve.DeletedFiles {\n\t\t\t\tlm := fileMetrics[de.Level]\n\t\t\t\tif lm == nil {\n\t\t\t\t\tlm = &LevelMetrics{}\n\t\t\t\t\tfileMetrics[de.Level] = lm\n\t\t\t\t}\n\t\t\t\tlm.NumFiles--\n\t\t\t\tlm.Size -= int64(f.Size)\n\t\t\t}\n\n\t\t\tmu.Lock()\n\t\t\tvs.logLock()\n\n\t\t\tforceRotation := rand.IntN(3) == 0\n\t\t\terr = vs.logAndApply(\n\t\t\t\t0 /* jobID */, ve, fileMetrics, forceRotation,\n\t\t\t\tfunc() []compactionInfo { return nil },\n\t\t\t)\n\t\t\tmu.Unlock()\n\t\t\tif err != nil {\n\t\t\t\ttd.Fatalf(t, \"%v\", err)\n\t\t\t}\n\t\t\t// Show the edit, so that we can see the fields populated by Apply. We\n\t\t\t// zero out the next file number because it is not deterministic (because\n\t\t\t// of the randomized forceRotation).\n\t\t\tve.NextFileNum = 0\n\t\t\tfmt.Fprintf(&buf, \"applied:\\n%s\", ve.String())\n\n\t\tcase \"protect-backing\":\n\t\t\tn, _ := strconv.Atoi(td.CmdArgs[0].String())\n\t\t\tvs.virtualBackings.Protect(base.DiskFileNum(n))\n\n\t\tcase \"unprotect-backing\":\n\t\t\tn, _ := strconv.Atoi(td.CmdArgs[0].String())\n\t\t\tvs.virtualBackings.Unprotect(base.DiskFileNum(n))\n\n\t\tcase \"ref-version\":\n\t\t\tname := td.CmdArgs[0].String()\n\t\t\trefs[name] = vs.currentVersion()\n\t\t\trefs[name].Ref()\n\n\t\tcase \"unref-version\":\n\t\t\tname := td.CmdArgs[0].String()\n\t\t\trefs[name].Unref()\n\n\t\tcase \"reopen\":\n\t\t\tvar err error\n\t\t\tvar filename string\n\t\t\tmarker, filename, err = atomicfs.LocateMarker(opts.FS, \"\", manifestMarkerName)\n\t\t\tif err != nil {\n\t\t\t\ttd.Fatalf(t, \"error locating marker: %v\", err)\n\t\t\t}\n\t\t\t_, manifestNum, ok := base.ParseFilename(opts.FS, filename)\n\t\t\tif !ok {\n\t\t\t\ttd.Fatalf(t, \"invalid manifest file name %q\", filename)\n\t\t\t}\n\t\t\tvs = versionSet{}\n\t\t\terr = vs.load(\n\t\t\t\t\"\", provider, opts, manifestNum, marker,\n\t\t\t\tfunc() FormatMajorVersion { return FormatVirtualSSTables }, mu,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\ttd.Fatalf(t, \"error loading manifest: %v\", err)\n\t\t\t}\n\n\t\t\t// Repopulate the maps.\n\t\t\tmetas = make(map[base.FileNum]*manifest.FileMetadata)\n\t\t\tbackings = make(map[base.DiskFileNum]*manifest.FileBacking)\n\t\t\tv := vs.currentVersion()\n\t\t\tfor _, l := range v.Levels {\n\t\t\t\tit := l.Iter()\n\t\t\t\tfor f := it.First(); f != nil; f = it.Next() {\n\t\t\t\t\tmetas[f.FileNum] = f\n\t\t\t\t\tdedupBacking(f.FileBacking)\n\t\t\t\t}\n\t\t\t}\n\n\t\tdefault:\n\t\t\ttd.Fatalf(t, \"unknown command: %s\", td.Cmd)\n\t\t}\n\n\t\tfmt.Fprintf(&buf, \"current version:\\n\")\n\t\tfor _, l := range strings.Split(vs.currentVersion().DebugString(), \"\\n\") {\n\t\t\tif l != \"\" {\n\t\t\t\tfmt.Fprintf(&buf, \"  %s\\n\", l)\n\t\t\t}\n\t\t}\n\t\tbuf.WriteString(vs.virtualBackings.String())\n\t\tif len(vs.zombieTables) == 0 {\n\t\t\tbuf.WriteString(\"no zombie tables\\n\")\n\t\t} else {\n\t\t\tvar nums []base.DiskFileNum\n\t\t\tfor k := range vs.zombieTables {\n\t\t\t\tnums = append(nums, k)\n\t\t\t}\n\t\t\tbuf.WriteString(\"zombie tables:\")\n\t\t\tslices.Sort(nums)\n\t\t\tfor _, n := range nums {\n\t\t\t\tfmt.Fprintf(&buf, \" %s\", n)\n\t\t\t}\n\t\t\tbuf.WriteString(\"\\n\")\n\t\t}\n\n\t\tif len(vs.obsoleteTables) == 0 {\n\t\t\tbuf.WriteString(\"no obsolete tables\\n\")\n\t\t} else {\n\t\t\tbuf.WriteString(\"obsolete tables:\")\n\t\t\tfor _, fi := range vs.obsoleteTables {\n\t\t\t\tfmt.Fprintf(&buf, \" %s\", fi.FileNum)\n\t\t\t}\n\t\t\tbuf.WriteString(\"\\n\")\n\t\t}\n\n\t\treturn buf.String()\n\t})\n}\n\nfunc TestVersionSetCheckpoint(t *testing.T) {\n\tmem := vfs.NewMem()\n\trequire.NoError(t, mem.MkdirAll(\"ext\", 0755))\n\n\topts := &Options{\n\t\tFS:                  mem,\n\t\tMaxManifestFileSize: 1,\n\t\tLogger:              testLogger{t: t},\n\t}\n\td, err := Open(\"\", opts)\n\trequire.NoError(t, err)\n\n\t// Multiple manifest files are created such that the latest one must have a correct snapshot\n\t// of the preceding state for the DB to be opened correctly and see the written data.\n\t// Snapshot has no files, so first edit will cause manifest rotation.\n\twriteAndIngest(t, mem, d, base.MakeInternalKey([]byte(\"a\"), 0, InternalKeyKindSet), []byte(\"b\"), \"a\")\n\t// Snapshot has no files, and manifest has an edit from the previous ingest,\n\t// so this second ingest will cause manifest rotation.\n\twriteAndIngest(t, mem, d, base.MakeInternalKey([]byte(\"c\"), 0, InternalKeyKindSet), []byte(\"d\"), \"c\")\n\trequire.NoError(t, d.Close())\n\td, err = Open(\"\", opts)\n\trequire.NoError(t, err)\n\tcheckValue := func(k string, expected string) {\n\t\tv, closer, err := d.Get([]byte(k))\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, expected, string(v))\n\t\tcloser.Close()\n\t}\n\tcheckValue(\"a\", \"b\")\n\tcheckValue(\"c\", \"d\")\n\trequire.NoError(t, d.Close())\n}\n\nfunc TestVersionSetSeqNums(t *testing.T) {\n\tmem := vfs.NewMem()\n\trequire.NoError(t, mem.MkdirAll(\"ext\", 0755))\n\n\topts := &Options{\n\t\tFS:                  mem,\n\t\tMaxManifestFileSize: 1,\n\t\tLogger:              testLogger{t: t},\n\t}\n\td, err := Open(\"\", opts)\n\trequire.NoError(t, err)\n\n\t// Snapshot has no files, so first edit will cause manifest rotation.\n\twriteAndIngest(t, mem, d, base.MakeInternalKey([]byte(\"a\"), 0, InternalKeyKindSet), []byte(\"b\"), \"a\")\n\t// Snapshot has no files, and manifest has an edit from the previous ingest,\n\t// so this second ingest will cause manifest rotation.\n\twriteAndIngest(t, mem, d, base.MakeInternalKey([]byte(\"c\"), 0, InternalKeyKindSet), []byte(\"d\"), \"c\")\n\trequire.NoError(t, d.Close())\n\td, err = Open(\"\", opts)\n\trequire.NoError(t, err)\n\tdefer d.Close()\n\td.TestOnlyWaitForCleaning()\n\n\t// Check that the manifest has the correct LastSeqNum, equalling the highest\n\t// observed SeqNum.\n\tfilenames, err := mem.List(\"\")\n\trequire.NoError(t, err)\n\tvar manifest vfs.File\n\tfor _, filename := range filenames {\n\t\tfileType, _, ok := base.ParseFilename(mem, filename)\n\t\tif ok && fileType == fileTypeManifest {\n\t\t\tmanifest, err = mem.Open(filename)\n\t\t\trequire.NoError(t, err)\n\t\t}\n\t}\n\trequire.NotNil(t, manifest)\n\tdefer manifest.Close()\n\trr := record.NewReader(manifest, 0 /* logNum */)\n\tvar lastSeqNum base.SeqNum\n\tfor {\n\t\tr, err := rr.Next()\n\t\tif err == io.EOF {\n\t\t\tbreak\n\t\t}\n\t\trequire.NoError(t, err)\n\t\tvar ve versionEdit\n\t\terr = ve.Decode(r)\n\t\trequire.NoError(t, err)\n\t\tif ve.LastSeqNum != 0 {\n\t\t\tlastSeqNum = ve.LastSeqNum\n\t\t}\n\t}\n\t// 2 ingestions happened, so LastSeqNum should equal base.SeqNumStart + 1.\n\trequire.Equal(t, base.SeqNum(11), lastSeqNum)\n\t// logSeqNum is always one greater than the last assigned sequence number.\n\trequire.Equal(t, d.mu.versions.logSeqNum.Load(), lastSeqNum+1)\n}\n"
        },
        {
          "name": "vfs",
          "type": "tree",
          "content": null
        },
        {
          "name": "wal",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}