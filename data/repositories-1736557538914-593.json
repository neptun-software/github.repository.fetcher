{
  "metadata": {
    "timestamp": 1736557538914,
    "page": 593,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjYxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "huggingface/pytorch-image-models",
      "stars": 32856,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.03,
          "content": "*.ipynb linguist-documentation\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.31,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# PyCharm\n.idea\n\noutput/\n\n# PyTorch weights\n*.tar\n*.pth\n*.pt\n*.torch\n*.gz\nUntitled.ipynb\nTesting notebook.ipynb\n\n# Root dir exclusions\n/*.csv\n/*.yaml\n/*.json\n/*.jpg\n/*.png\n/*.zip\n/*.tar.*"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 0.3,
          "content": "message: \"If you use this software, please cite it as below.\"\ntitle: \"PyTorch Image Models\"\nversion: \"1.2.2\"\ndoi: \"10.5281/zenodo.4414861\" \nauthors:\n  - family-names: Wightman\n    given-names: Ross\nversion: 1.0.11\nyear: \"2019\"\nurl: \"https://github.com/huggingface/pytorch-image-models\"\nlicense: \"Apache 2.0\""
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 5.33,
          "content": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pledge to participate in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, caste, color, religion, or sexual\nidentity and orientation.\n\nWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n\n## Our Standards\n\nExamples of behavior that contributes to a positive environment for our\ncommunity includes:\n\n* Demonstrating empathy and kindness toward other people\n* Being respectful of differing opinions, viewpoints, and experiences\n* Giving and gracefully accepting constructive feedback\n* Accepting responsibility and apologizing to those affected by our mistakes,\n  and learning from the experience\n* Focusing on what is best not just for us as individuals, but for the overall\n  community\n\nExamples of unacceptable behavior include:\n\n* The use of sexualized language or imagery, and sexual attention or advances of\n  any kind\n* Trolling, insulting or derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or email address,\n  without their explicit permission\n* Other conduct that could reasonably be considered inappropriate in a\n  professional setting\n\n## Enforcement Responsibilities\n\nCommunity leaders are responsible for clarifying and enforcing our standards of\nacceptable behavior and will take appropriate and fair corrective action in\nresponse to any behavior that they deem inappropriate, threatening, offensive,\nor harmful.\n\nCommunity leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct, and will communicate reasons for moderation\ndecisions when appropriate.\n\n## Scope\n\nThis Code of Conduct applies within all community spaces, and also applies when\nan individual is officially representing the community in public spaces.\nExamples of representing our community include using an official e-mail address,\nposting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement at\nfeedback@huggingface.co.\nAll complaints will be reviewed and investigated promptly and fairly.\n\nAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident.\n\n## Enforcement Guidelines\n\nCommunity leaders will follow these Community Impact Guidelines in determining\nthe consequences for any action they deem in violation of this Code of Conduct:\n\n### 1. Correction\n\n**Community Impact**: Use of inappropriate language or other behavior deemed\nunprofessional or unwelcome in the community.\n\n**Consequence**: A private, written warning from community leaders, providing\nclarity around the nature of the violation and an explanation of why the\nbehavior was inappropriate. A public apology may be requested.\n\n### 2. Warning\n\n**Community Impact**: A violation through a single incident or series of\nactions.\n\n**Consequence**: A warning with consequences for continued behavior. No\ninteraction with the people involved, including unsolicited interaction with\nthose enforcing the Code of Conduct, for a specified period. This\nincludes avoiding interactions in community spaces and external channels\nlike social media. Violating these terms may lead to a temporary or permanent\nban.\n\n### 3. Temporary Ban\n\n**Community Impact**: A serious violation of community standards, including\nsustained inappropriate behavior.\n\n**Consequence**: A temporary ban from any sort of interaction or public\ncommunication with the community for a specified period of time. No public or\nprivate interaction with the people involved, including unsolicited interaction\nwith those enforcing the Code of Conduct, is allowed during this period.\nViolating these terms may lead to a permanent ban.\n\n### 4. Permanent Ban\n\n**Community Impact**: Demonstrating a pattern of violation of community\nstandards, including sustained inappropriate behavior, harassment of an\nindividual, or aggression toward or disparagement of classes of individuals.\n\n**Consequence**: A permanent ban from any public interaction within the\ncommunity.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage],\nversion 2.1, available at\n[https://www.contributor-covenant.org/version/2/1/code_of_conduct.html][v2.1].\n\nCommunity Impact Guidelines were inspired by\n[Mozilla's code of conduct enforcement ladder][Mozilla CoC].\n\nFor answers to common questions about this code of conduct, see the FAQ at\n[https://www.contributor-covenant.org/faq][FAQ]. Translations are available at\n[https://www.contributor-covenant.org/translations][translations].\n\n[homepage]: https://www.contributor-covenant.org\n[v2.1]: https://www.contributor-covenant.org/version/2/1/code_of_conduct.html\n[Mozilla CoC]: https://github.com/mozilla/diversity\n[FAQ]: https://www.contributor-covenant.org/faq\n[translations]: https://www.contributor-covenant.org/translations\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 3.92,
          "content": "*This guideline is very much a work-in-progress.*\n\nContributions to `timm` for code, documentation, tests are more than welcome!\n\nThere haven't been any formal guidelines to date so please bear with me, and feel free to add to this guide.\n\n# Coding style\n\nCode linting and auto-format (black) are not currently in place but open to consideration. In the meantime, the style to follow is (mostly) aligned with Google's guide: https://google.github.io/styleguide/pyguide.html. \n\nA few specific differences from Google style (or black)\n1. Line length is 120 char. Going over is okay in some cases (e.g. I prefer not to break URL across lines).\n2. Hanging indents are always prefered, please avoid aligning arguments with closing brackets or braces.\n\nExample, from Google guide, but this is a NO here:\n```\n   # Aligned with opening delimiter.\n   foo = long_function_name(var_one, var_two,\n                            var_three, var_four)\n   meal = (spam,\n           beans)\n\n   # Aligned with opening delimiter in a dictionary.\n   foo = {\n       'long_dictionary_key': value1 +\n                              value2,\n       ...\n   }\n```\nThis is YES:\n\n```\n   # 4-space hanging indent; nothing on first line,\n   # closing parenthesis on a new line.\n   foo = long_function_name(\n       var_one, var_two, var_three,\n       var_four\n   )\n   meal = (\n       spam,\n       beans,\n   )\n\n   # 4-space hanging indent in a dictionary.\n   foo = {\n       'long_dictionary_key':\n           long_dictionary_value,\n       ...\n   }\n```\n\nWhen there is discrepancy in a given source file (there are many origins for various bits of code and not all have been updated to what I consider current goal), please follow the style in a given file.\n\nIn general, if you add new code, formatting it with black using the following options should result in a style that is compatible with the rest of the code base:\n\n```\nblack --skip-string-normalization --line-length 120 <path-to-file>\n```\n\nAvoid formatting code that is unrelated to your PR though.\n\nPR with pure formatting / style fixes will be accepted but only in isolation from functional changes, best to ask before starting such a change.\n\n# Documentation\n\nAs with code style, docstrings style based on the Google guide: guide: https://google.github.io/styleguide/pyguide.html\n\nThe goal for the code is to eventually move to have all major functions and `__init__` methods use PEP484 type annotations.\n\nWhen type annotations are used for a function, as per the Google pyguide, they should **NOT** be duplicated in the docstrings, please leave annotations as the one source of truth re typing.\n\nThere are a LOT of gaps in current documentation relative to the functionality in timm, please, document away!\n\n# Installation\n\nCreate a Python virtual environment using Python 3.10. Inside the environment, install torch` and `torchvision` using the instructions matching your system as listed on the [PyTorch website](https://pytorch.org/).\n\nThen install the remaining dependencies:\n\n```\npython -m pip install -r requirements.txt\npython -m pip install -r requirements-dev.txt  # for testing\npython -m pip install -e .\n```\n\n## Unit tests\n\nRun the tests using:\n\n```\npytest tests/\n```\n\nSince the whole test suite takes a lot of time to run locally (a few hours), you may want to select a subset of tests relating to the changes you made by using the `-k` option of [`pytest`](https://docs.pytest.org/en/7.1.x/example/markers.html#using-k-expr-to-select-tests-based-on-their-name). Moreover, running tests in parallel (in this example 4 processes) with the `-n` option may help:\n\n```\npytest -k \"substring-to-match\" -n 4 tests/\n```\n\n## Building documentation\n\nPlease refer to [this document](https://github.com/huggingface/pytorch-image-models/tree/main/hfdocs).\n\n# Questions\n\nIf you have any questions about contribution, where / how to contribute, please ask in the [Discussions](https://github.com/huggingface/pytorch-image-models/discussions/categories/contributing) (there is a `Contributing` topic).\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.08,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"{}\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2019 Ross Wightman\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.09,
          "content": "include timm/models/_pruned/*.txt\ninclude timm/data/_info/*.txt\ninclude timm/data/_info/*.json\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 50.77,
          "content": "# PyTorch Image Models\n- [What's New](#whats-new)\n- [Introduction](#introduction)\n- [Models](#models)\n- [Features](#features)\n- [Results](#results)\n- [Getting Started (Documentation)](#getting-started-documentation)\n- [Train, Validation, Inference Scripts](#train-validation-inference-scripts)\n- [Awesome PyTorch Resources](#awesome-pytorch-resources)\n- [Licenses](#licenses)\n- [Citing](#citing)\n\n## What's New\n\n## Jan 9, 2025\n* Add support to train and validate in pure `bfloat16` or `float16`\n* `wandb` project name arg added by https://github.com/caojiaolong, use arg.experiment for name\n* Fix old issue w/ checkpoint saving not working on filesystem w/o hard-link support (e.g. FUSE fs mounts)\n* 1.0.13 release\n\n## Jan 6, 2025\n* Add `torch.utils.checkpoint.checkpoint()` wrapper in `timm.models` that defaults `use_reentrant=False`, unless `TIMM_REENTRANT_CKPT=1` is set in env.\n\n## Dec 31, 2024\n* `convnext_nano` 384x384 ImageNet-12k pretrain & fine-tune. https://huggingface.co/models?search=convnext_nano%20r384\n* Add AIM-v2 encoders from https://github.com/apple/ml-aim, see on Hub: https://huggingface.co/models?search=timm%20aimv2\n* Add PaliGemma2 encoders from https://github.com/google-research/big_vision to existing PaliGemma, see on Hub: https://huggingface.co/models?search=timm%20pali2\n* Add missing L/14 DFN2B 39B CLIP ViT, `vit_large_patch14_clip_224.dfn2b_s39b`\n* Fix existing `RmsNorm` layer & fn to match standard formulation, use PT 2.5 impl when possible. Move old impl to `SimpleNorm` layer, it's LN w/o centering or bias. There were only two `timm` models using it, and they have been updated.\n* Allow override of `cache_dir` arg for model creation\n* Pass through `trust_remote_code` for HF datasets wrapper\n* `inception_next_atto` model added by creator\n* Adan optimizer caution, and Lamb decoupled weighgt decay options\n* Some feature_info metadata fixed by https://github.com/brianhou0208\n* All OpenCLIP and JAX (CLIP, SigLIP, Pali, etc) model weights that used load time remapping were given their own HF Hub instances so that they work with `hf-hub:` based loading, and thus will work with new Transformers `TimmWrapperModel`\n\n## Nov 28, 2024\n* More optimizers\n  * Add MARS optimizer (https://arxiv.org/abs/2411.10438, https://github.com/AGI-Arena/MARS)\n  * Add LaProp optimizer (https://arxiv.org/abs/2002.04839, https://github.com/Z-T-WANG/LaProp-Optimizer)\n  * Add masking from 'Cautious Optimizers' (https://arxiv.org/abs/2411.16085, https://github.com/kyleliang919/C-Optim) to Adafactor, Adafactor Big Vision, AdamW (legacy), Adopt, Lamb, LaProp, Lion, NadamW, RMSPropTF, SGDW\n  * Cleanup some docstrings and type annotations re optimizers and factory\n* Add MobileNet-V4 Conv Medium models pretrained on in12k and fine-tuned in1k @ 384x384\n  * https://huggingface.co/timm/mobilenetv4_conv_medium.e250_r384_in12k_ft_in1k\n  * https://huggingface.co/timm/mobilenetv4_conv_medium.e250_r384_in12k\n  * https://huggingface.co/timm/mobilenetv4_conv_medium.e180_ad_r384_in12k\n  * https://huggingface.co/timm/mobilenetv4_conv_medium.e180_r384_in12k\n* Add small cs3darknet, quite good for the speed\n  * https://huggingface.co/timm/cs3darknet_focus_s.ra4_e3600_r256_in1k\n\n## Nov 12, 2024\n* Optimizer factory refactor\n  * New factory works by registering optimizers using an OptimInfo dataclass w/ some key traits\n  * Add `list_optimizers`, `get_optimizer_class`, `get_optimizer_info` to reworked `create_optimizer_v2` fn to explore optimizers, get info or class\n  * deprecate `optim.optim_factory`, move fns to `optim/_optim_factory.py` and `optim/_param_groups.py` and encourage import via `timm.optim`\n* Add Adopt (https://github.com/iShohei220/adopt) optimizer\n* Add 'Big Vision' variant of Adafactor (https://github.com/google-research/big_vision/blob/main/big_vision/optax.py) optimizer\n* Fix original Adafactor to pick better factorization dims for convolutions\n* Tweak LAMB optimizer with some improvements in torch.where functionality since original, refactor clipping a bit\n* dynamic img size support in vit, deit, eva improved to support resize from non-square patch grids, thanks https://github.com/wojtke\n* \n## Oct 31, 2024\nAdd a set of new very well trained ResNet & ResNet-V2 18/34 (basic block) weights. See https://huggingface.co/blog/rwightman/resnet-trick-or-treat\n\n## Oct 19, 2024\n* Cleanup torch amp usage to avoid cuda specific calls, merge support for Ascend (NPU) devices from [MengqingCao](https://github.com/MengqingCao) that should work now in PyTorch 2.5 w/ new device extension autoloading feature. Tested Intel Arc (XPU) in Pytorch 2.5 too and it (mostly) worked.\n\n## Oct 16, 2024\n* Fix error on importing from deprecated path `timm.models.registry`, increased priority of existing deprecation warnings to be visible\n* Port weights of InternViT-300M (https://huggingface.co/OpenGVLab/InternViT-300M-448px) to `timm` as `vit_intern300m_patch14_448`\n\n### Oct 14, 2024\n* Pre-activation (ResNetV2) version of 18/18d/34/34d ResNet model defs added by request (weights pending)\n* Release 1.0.10\n\n### Oct 11, 2024\n* MambaOut (https://github.com/yuweihao/MambaOut) model & weights added. A cheeky take on SSM vision models w/o the SSM (essentially ConvNeXt w/ gating). A mix of original weights + custom variations & weights.\n\n|model                                                                                                                |img_size|top1  |top5  |param_count|\n|---------------------------------------------------------------------------------------------------------------------|--------|------|------|-----------|\n|[mambaout_base_plus_rw.sw_e150_r384_in12k_ft_in1k](http://huggingface.co/timm/mambaout_base_plus_rw.sw_e150_r384_in12k_ft_in1k)|384     |87.506|98.428|101.66     |\n|[mambaout_base_plus_rw.sw_e150_in12k_ft_in1k](http://huggingface.co/timm/mambaout_base_plus_rw.sw_e150_in12k_ft_in1k)|288     |86.912|98.236|101.66     |\n|[mambaout_base_plus_rw.sw_e150_in12k_ft_in1k](http://huggingface.co/timm/mambaout_base_plus_rw.sw_e150_in12k_ft_in1k)|224     |86.632|98.156|101.66     |\n|[mambaout_base_tall_rw.sw_e500_in1k](http://huggingface.co/timm/mambaout_base_tall_rw.sw_e500_in1k)                  |288     |84.974|97.332|86.48      |\n|[mambaout_base_wide_rw.sw_e500_in1k](http://huggingface.co/timm/mambaout_base_wide_rw.sw_e500_in1k)                  |288     |84.962|97.208|94.45      |\n|[mambaout_base_short_rw.sw_e500_in1k](http://huggingface.co/timm/mambaout_base_short_rw.sw_e500_in1k)                |288     |84.832|97.27 |88.83      |\n|[mambaout_base.in1k](http://huggingface.co/timm/mambaout_base.in1k)                                                  |288     |84.72 |96.93 |84.81      |\n|[mambaout_small_rw.sw_e450_in1k](http://huggingface.co/timm/mambaout_small_rw.sw_e450_in1k)                          |288     |84.598|97.098|48.5       |\n|[mambaout_small.in1k](http://huggingface.co/timm/mambaout_small.in1k)                                                |288     |84.5  |96.974|48.49      |\n|[mambaout_base_wide_rw.sw_e500_in1k](http://huggingface.co/timm/mambaout_base_wide_rw.sw_e500_in1k)                  |224     |84.454|96.864|94.45      |\n|[mambaout_base_tall_rw.sw_e500_in1k](http://huggingface.co/timm/mambaout_base_tall_rw.sw_e500_in1k)                  |224     |84.434|96.958|86.48      |\n|[mambaout_base_short_rw.sw_e500_in1k](http://huggingface.co/timm/mambaout_base_short_rw.sw_e500_in1k)                |224     |84.362|96.952|88.83      |\n|[mambaout_base.in1k](http://huggingface.co/timm/mambaout_base.in1k)                                                  |224     |84.168|96.68 |84.81      |\n|[mambaout_small.in1k](http://huggingface.co/timm/mambaout_small.in1k)                                                |224     |84.086|96.63 |48.49      |\n|[mambaout_small_rw.sw_e450_in1k](http://huggingface.co/timm/mambaout_small_rw.sw_e450_in1k)                          |224     |84.024|96.752|48.5       |\n|[mambaout_tiny.in1k](http://huggingface.co/timm/mambaout_tiny.in1k)                                                  |288     |83.448|96.538|26.55      |\n|[mambaout_tiny.in1k](http://huggingface.co/timm/mambaout_tiny.in1k)                                                  |224     |82.736|96.1  |26.55      |\n|[mambaout_kobe.in1k](http://huggingface.co/timm/mambaout_kobe.in1k)                                                  |288     |81.054|95.718|9.14       |\n|[mambaout_kobe.in1k](http://huggingface.co/timm/mambaout_kobe.in1k)                                                  |224     |79.986|94.986|9.14       |\n|[mambaout_femto.in1k](http://huggingface.co/timm/mambaout_femto.in1k)                                                |288     |79.848|95.14 |7.3        |\n|[mambaout_femto.in1k](http://huggingface.co/timm/mambaout_femto.in1k)                                                |224     |78.87 |94.408|7.3        |\n\n* SigLIP SO400M ViT fine-tunes on ImageNet-1k @ 378x378, added 378x378 option for existing SigLIP 384x384 models\n  *  [vit_so400m_patch14_siglip_378.webli_ft_in1k](https://huggingface.co/timm/vit_so400m_patch14_siglip_378.webli_ft_in1k) - 89.42 top-1\n  *  [vit_so400m_patch14_siglip_gap_378.webli_ft_in1k](https://huggingface.co/timm/vit_so400m_patch14_siglip_gap_378.webli_ft_in1k) - 89.03\n* SigLIP SO400M ViT encoder from recent multi-lingual (i18n) variant, patch16 @ 256x256 (https://huggingface.co/timm/ViT-SO400M-16-SigLIP-i18n-256). OpenCLIP update pending.\n* Add two ConvNeXt 'Zepto' models & weights (one w/ overlapped stem and one w/ patch stem). Uses RMSNorm, smaller than previous 'Atto', 2.2M params.\n  * [convnext_zepto_rms_ols.ra4_e3600_r224_in1k](https://huggingface.co/timm/convnext_zepto_rms_ols.ra4_e3600_r224_in1k) - 73.20 top-1 @ 224\n  * [convnext_zepto_rms.ra4_e3600_r224_in1k](https://huggingface.co/timm/convnext_zepto_rms.ra4_e3600_r224_in1k) - 72.81 @ 224\n\n### Sept 2024\n* Add a suite of tiny test models for improved unit tests and niche low-resource applications (https://huggingface.co/blog/rwightman/timm-tiny-test)\n* Add MobileNetV4-Conv-Small (0.5x) model (https://huggingface.co/posts/rwightman/793053396198664)\n  * [mobilenetv4_conv_small_050.e3000_r224_in1k](http://hf.co/timm/mobilenetv4_conv_small_050.e3000_r224_in1k) - 65.81 top-1 @ 256, 64.76 @ 224\n* Add MobileNetV3-Large variants trained with MNV4 Small recipe\n  * [mobilenetv3_large_150d.ra4_e3600_r256_in1k](http://hf.co/timm/mobilenetv3_large_150d.ra4_e3600_r256_in1k) - 81.81 @ 320, 80.94 @ 256\n  * [mobilenetv3_large_100.ra4_e3600_r224_in1k](http://hf.co/timm/mobilenetv3_large_100.ra4_e3600_r224_in1k) - 77.16 @ 256, 76.31 @ 224\n\n\n### Aug 21, 2024\n* Updated SBB ViT models trained on ImageNet-12k and fine-tuned on ImageNet-1k, challenging quite a number of much larger, slower models\n\n| model | top1 | top5 | param_count | img_size |\n| -------------------------------------------------- | ------ | ------ | ----------- | -------- |\n| [vit_mediumd_patch16_reg4_gap_384.sbb2_e200_in12k_ft_in1k](https://huggingface.co/timm/vit_mediumd_patch16_reg4_gap_384.sbb2_e200_in12k_ft_in1k) | 87.438 | 98.256 | 64.11 | 384 |\n| [vit_mediumd_patch16_reg4_gap_256.sbb2_e200_in12k_ft_in1k](https://huggingface.co/timm/vit_mediumd_patch16_reg4_gap_256.sbb2_e200_in12k_ft_in1k) | 86.608 | 97.934 | 64.11 | 256 |\n| [vit_betwixt_patch16_reg4_gap_384.sbb2_e200_in12k_ft_in1k](https://huggingface.co/timm/vit_betwixt_patch16_reg4_gap_384.sbb2_e200_in12k_ft_in1k) | 86.594 | 98.02 | 60.4 | 384 |\n| [vit_betwixt_patch16_reg4_gap_256.sbb2_e200_in12k_ft_in1k](https://huggingface.co/timm/vit_betwixt_patch16_reg4_gap_256.sbb2_e200_in12k_ft_in1k) | 85.734 | 97.61 | 60.4 | 256 |\n* MobileNet-V1 1.25, EfficientNet-B1, & ResNet50-D weights w/ MNV4 baseline challenge recipe\n\n| model                                                                                                                    | top1   | top5   | param_count | img_size |\n|--------------------------------------------------------------------------------------------------------------------------|--------|--------|-------------|----------|\n| [resnet50d.ra4_e3600_r224_in1k](http://hf.co/timm/resnet50d.ra4_e3600_r224_in1k)                                         | 81.838 | 95.922 | 25.58       | 288      |\n| [efficientnet_b1.ra4_e3600_r240_in1k](http://hf.co/timm/efficientnet_b1.ra4_e3600_r240_in1k)                             | 81.440 | 95.700 | 7.79        | 288      |\n| [resnet50d.ra4_e3600_r224_in1k](http://hf.co/timm/resnet50d.ra4_e3600_r224_in1k)                                         | 80.952 | 95.384 | 25.58       | 224      |\n| [efficientnet_b1.ra4_e3600_r240_in1k](http://hf.co/timm/efficientnet_b1.ra4_e3600_r240_in1k)                             | 80.406 | 95.152 | 7.79        | 240      |\n| [mobilenetv1_125.ra4_e3600_r224_in1k](http://hf.co/timm/mobilenetv1_125.ra4_e3600_r224_in1k)                             | 77.600 | 93.804 | 6.27        | 256      |\n| [mobilenetv1_125.ra4_e3600_r224_in1k](http://hf.co/timm/mobilenetv1_125.ra4_e3600_r224_in1k)                             | 76.924 | 93.234 | 6.27        | 224      |\n\n* Add SAM2 (HieraDet) backbone arch & weight loading support\n* Add Hiera Small weights trained w/ abswin pos embed on in12k & fine-tuned on 1k\n\n|model                            |top1  |top5  |param_count|\n|---------------------------------|------|------|-----------|\n|hiera_small_abswin_256.sbb2_e200_in12k_ft_in1k    |84.912|97.260|35.01      |\n|hiera_small_abswin_256.sbb2_pd_e200_in12k_ft_in1k |84.560|97.106|35.01      |\n\n### Aug 8, 2024\n* Add RDNet ('DenseNets Reloaded', https://arxiv.org/abs/2403.19588), thanks [Donghyun Kim](https://github.com/dhkim0225)\n  \n### July 28, 2024\n* Add `mobilenet_edgetpu_v2_m` weights w/ `ra4` mnv4-small based recipe. 80.1% top-1 @ 224 and 80.7 @ 256.\n* Release 1.0.8\n\n### July 26, 2024\n* More MobileNet-v4 weights, ImageNet-12k pretrain w/ fine-tunes, and anti-aliased ConvLarge models\n\n| model                                                                                            |top1  |top1_err|top5  |top5_err|param_count|img_size|\n|--------------------------------------------------------------------------------------------------|------|--------|------|--------|-----------|--------|\n| [mobilenetv4_conv_aa_large.e230_r448_in12k_ft_in1k](http://hf.co/timm/mobilenetv4_conv_aa_large.e230_r448_in12k_ft_in1k)|84.99 |15.01   |97.294|2.706   |32.59      |544     |\n| [mobilenetv4_conv_aa_large.e230_r384_in12k_ft_in1k](http://hf.co/timm/mobilenetv4_conv_aa_large.e230_r384_in12k_ft_in1k)|84.772|15.228  |97.344|2.656   |32.59      |480     |\n| [mobilenetv4_conv_aa_large.e230_r448_in12k_ft_in1k](http://hf.co/timm/mobilenetv4_conv_aa_large.e230_r448_in12k_ft_in1k)|84.64 |15.36   |97.114|2.886   |32.59      |448     |\n| [mobilenetv4_conv_aa_large.e230_r384_in12k_ft_in1k](http://hf.co/timm/mobilenetv4_conv_aa_large.e230_r384_in12k_ft_in1k)|84.314|15.686  |97.102|2.898   |32.59      |384     |\n| [mobilenetv4_conv_aa_large.e600_r384_in1k](http://hf.co/timm/mobilenetv4_conv_aa_large.e600_r384_in1k)     |83.824|16.176  |96.734|3.266   |32.59      |480     |\n| [mobilenetv4_conv_aa_large.e600_r384_in1k](http://hf.co/timm/mobilenetv4_conv_aa_large.e600_r384_in1k)             |83.244|16.756  |96.392|3.608   |32.59      |384     |\n| [mobilenetv4_hybrid_medium.e200_r256_in12k_ft_in1k](http://hf.co/timm/mobilenetv4_hybrid_medium.e200_r256_in12k_ft_in1k)|82.99 |17.01   |96.67 |3.33    |11.07      |320     |\n| [mobilenetv4_hybrid_medium.e200_r256_in12k_ft_in1k](http://hf.co/timm/mobilenetv4_hybrid_medium.e200_r256_in12k_ft_in1k)|82.364|17.636  |96.256|3.744   |11.07      |256     |\n\n* Impressive MobileNet-V1 and EfficientNet-B0 baseline challenges (https://huggingface.co/blog/rwightman/mobilenet-baselines)\n  \n| model                                                                                            |top1  |top1_err|top5  |top5_err|param_count|img_size|\n|--------------------------------------------------------------------------------------------------|------|--------|------|--------|-----------|--------|\n| [efficientnet_b0.ra4_e3600_r224_in1k](http://hf.co/timm/efficientnet_b0.ra4_e3600_r224_in1k)                       |79.364|20.636  |94.754|5.246   |5.29       |256     |\n| [efficientnet_b0.ra4_e3600_r224_in1k](http://hf.co/timm/efficientnet_b0.ra4_e3600_r224_in1k)                       |78.584|21.416  |94.338|5.662   |5.29       |224     |    \n| [mobilenetv1_100h.ra4_e3600_r224_in1k](http://hf.co/timm/mobilenetv1_100h.ra4_e3600_r224_in1k)                     |76.596|23.404  |93.272|6.728   |5.28       |256     |\n| [mobilenetv1_100.ra4_e3600_r224_in1k](http://hf.co/timm/mobilenetv1_100.ra4_e3600_r224_in1k)                       |76.094|23.906  |93.004|6.996   |4.23       |256     |\n| [mobilenetv1_100h.ra4_e3600_r224_in1k](http://hf.co/timm/mobilenetv1_100h.ra4_e3600_r224_in1k)                     |75.662|24.338  |92.504|7.496   |5.28       |224     |\n| [mobilenetv1_100.ra4_e3600_r224_in1k](http://hf.co/timm/mobilenetv1_100.ra4_e3600_r224_in1k)                       |75.382|24.618  |92.312|7.688   |4.23       |224     |\n\n* Prototype of `set_input_size()` added to vit and swin v1/v2 models to allow changing image size, patch size, window size after model creation.\n* Improved support in swin for different size handling, in addition to `set_input_size`, `always_partition` and `strict_img_size` args have been added to `__init__` to allow more flexible input size constraints\n* Fix out of order indices info for intermediate 'Getter' feature wrapper, check out or range indices for same.\n* Add several `tiny` < .5M param models for testing that are actually trained on ImageNet-1k\n\n|model                       |top1  |top1_err|top5  |top5_err|param_count|img_size|crop_pct|\n|----------------------------|------|--------|------|--------|-----------|--------|--------|\n|test_efficientnet.r160_in1k |47.156|52.844  |71.726|28.274  |0.36       |192     |1.0     |\n|test_byobnet.r160_in1k      |46.698|53.302  |71.674|28.326  |0.46       |192     |1.0     |\n|test_efficientnet.r160_in1k |46.426|53.574  |70.928|29.072  |0.36       |160     |0.875   |\n|test_byobnet.r160_in1k      |45.378|54.622  |70.572|29.428  |0.46       |160     |0.875   |\n|test_vit.r160_in1k|42.0  |58.0    |68.664|31.336  |0.37       |192     |1.0     |\n|test_vit.r160_in1k|40.822|59.178  |67.212|32.788  |0.37       |160     |0.875   |\n\n* Fix vit reg token init, thanks [Promisery](https://github.com/Promisery)\n* Other misc fixes\n\n### June 24, 2024\n* 3 more MobileNetV4 hyrid weights with different MQA weight init scheme\n\n| model                                                                                            |top1  |top1_err|top5  |top5_err|param_count|img_size|\n|--------------------------------------------------------------------------------------------------|------|--------|------|--------|-----------|--------|\n| [mobilenetv4_hybrid_large.ix_e600_r384_in1k](http://hf.co/timm/mobilenetv4_hybrid_large.ix_e600_r384_in1k) |84.356|15.644  |96.892 |3.108  |37.76      |448     |\n| [mobilenetv4_hybrid_large.ix_e600_r384_in1k](http://hf.co/timm/mobilenetv4_hybrid_large.ix_e600_r384_in1k) |83.990|16.010  |96.702 |3.298  |37.76      |384     |\n| [mobilenetv4_hybrid_medium.ix_e550_r384_in1k](http://hf.co/timm/mobilenetv4_hybrid_medium.ix_e550_r384_in1k)       |83.394|16.606  |96.760|3.240   |11.07      |448     |\n| [mobilenetv4_hybrid_medium.ix_e550_r384_in1k](http://hf.co/timm/mobilenetv4_hybrid_medium.ix_e550_r384_in1k)       |82.968|17.032  |96.474|3.526   |11.07      |384     |\n| [mobilenetv4_hybrid_medium.ix_e550_r256_in1k](http://hf.co/timm/mobilenetv4_hybrid_medium.ix_e550_r256_in1k)       |82.492|17.508  |96.278|3.722   |11.07      |320     |\n| [mobilenetv4_hybrid_medium.ix_e550_r256_in1k](http://hf.co/timm/mobilenetv4_hybrid_medium.ix_e550_r256_in1k)       |81.446|18.554  |95.704|4.296   |11.07      |256     |\n* florence2 weight loading in DaViT model\n\n### June 12, 2024\n* MobileNetV4 models and initial set of `timm` trained weights added:\n\n| model                                                                                            |top1  |top1_err|top5  |top5_err|param_count|img_size|\n|--------------------------------------------------------------------------------------------------|------|--------|------|--------|-----------|--------|\n| [mobilenetv4_hybrid_large.e600_r384_in1k](http://hf.co/timm/mobilenetv4_hybrid_large.e600_r384_in1k) |84.266|15.734  |96.936 |3.064  |37.76      |448     |\n| [mobilenetv4_hybrid_large.e600_r384_in1k](http://hf.co/timm/mobilenetv4_hybrid_large.e600_r384_in1k) |83.800|16.200  |96.770 |3.230  |37.76      |384     |\n| [mobilenetv4_conv_large.e600_r384_in1k](http://hf.co/timm/mobilenetv4_conv_large.e600_r384_in1k) |83.392|16.608  |96.622 |3.378  |32.59      |448     |\n| [mobilenetv4_conv_large.e600_r384_in1k](http://hf.co/timm/mobilenetv4_conv_large.e600_r384_in1k) |82.952|17.048  |96.266 |3.734  |32.59      |384     |\n| [mobilenetv4_conv_large.e500_r256_in1k](http://hf.co/timm/mobilenetv4_conv_large.e500_r256_in1k) |82.674|17.326  |96.31 |3.69    |32.59      |320     |\n| [mobilenetv4_conv_large.e500_r256_in1k](http://hf.co/timm/mobilenetv4_conv_large.e500_r256_in1k)                   |81.862|18.138  |95.69 |4.31    |32.59      |256     |\n| [mobilenetv4_hybrid_medium.e500_r224_in1k](http://hf.co/timm/mobilenetv4_hybrid_medium.e500_r224_in1k)             |81.276|18.724  |95.742|4.258   |11.07      |256     |\n| [mobilenetv4_conv_medium.e500_r256_in1k](http://hf.co/timm/mobilenetv4_conv_medium.e500_r256_in1k)                 |80.858|19.142  |95.768|4.232   |9.72       |320     |\n| [mobilenetv4_hybrid_medium.e500_r224_in1k](http://hf.co/timm/mobilenetv4_hybrid_medium.e500_r224_in1k)             |80.442|19.558  |95.38 |4.62    |11.07      |224     |\n| [mobilenetv4_conv_blur_medium.e500_r224_in1k](http://hf.co/timm/mobilenetv4_conv_blur_medium.e500_r224_in1k)       |80.142|19.858  |95.298|4.702   |9.72       |256     |\n| [mobilenetv4_conv_medium.e500_r256_in1k](http://hf.co/timm/mobilenetv4_conv_medium.e500_r256_in1k)                 |79.928|20.072  |95.184|4.816   |9.72       |256     |\n| [mobilenetv4_conv_medium.e500_r224_in1k](http://hf.co/timm/mobilenetv4_conv_medium.e500_r224_in1k)                 |79.808|20.192  |95.186|4.814   |9.72       |256     |\n| [mobilenetv4_conv_blur_medium.e500_r224_in1k](http://hf.co/timm/mobilenetv4_conv_blur_medium.e500_r224_in1k)       |79.438|20.562  |94.932|5.068   |9.72       |224     |\n| [mobilenetv4_conv_medium.e500_r224_in1k](http://hf.co/timm/mobilenetv4_conv_medium.e500_r224_in1k)                 |79.094|20.906  |94.77 |5.23    |9.72       |224     |\n| [mobilenetv4_conv_small.e2400_r224_in1k](http://hf.co/timm/mobilenetv4_conv_small.e2400_r224_in1k)                 |74.616|25.384  |92.072|7.928   |3.77       |256     |\n| [mobilenetv4_conv_small.e1200_r224_in1k](http://hf.co/timm/mobilenetv4_conv_small.e1200_r224_in1k)                 |74.292|25.708  |92.116|7.884   |3.77       |256     |\n| [mobilenetv4_conv_small.e2400_r224_in1k](http://hf.co/timm/mobilenetv4_conv_small.e2400_r224_in1k)                 |73.756|26.244  |91.422|8.578   |3.77       |224     |\n| [mobilenetv4_conv_small.e1200_r224_in1k](http://hf.co/timm/mobilenetv4_conv_small.e1200_r224_in1k)                 |73.454|26.546  |91.34 |8.66    |3.77       |224     |\n\n* Apple MobileCLIP (https://arxiv.org/pdf/2311.17049, FastViT and ViT-B) image tower model support & weights added (part of OpenCLIP support).\n* ViTamin (https://arxiv.org/abs/2404.02132) CLIP image tower model & weights added (part of OpenCLIP support).\n* OpenAI CLIP Modified ResNet image tower modelling & weight support (via ByobNet). Refactor AttentionPool2d.\n\n### May 14, 2024\n* Support loading PaliGemma jax weights into SigLIP ViT models with average pooling.\n* Add Hiera models from Meta (https://github.com/facebookresearch/hiera).\n* Add `normalize=` flag for transorms, return non-normalized torch.Tensor with original dytpe (for `chug`)\n* Version 1.0.3 release\n\n### May 11, 2024\n* `Searching for Better ViT Baselines (For the GPU Poor)` weights and vit variants released. Exploring model shapes between Tiny and Base.\n\n| model | top1 | top5 | param_count | img_size |\n| -------------------------------------------------- | ------ | ------ | ----------- | -------- |\n| [vit_mediumd_patch16_reg4_gap_256.sbb_in12k_ft_in1k](https://huggingface.co/timm/vit_mediumd_patch16_reg4_gap_256.sbb_in12k_ft_in1k) | 86.202 | 97.874 | 64.11 | 256 |\n| [vit_betwixt_patch16_reg4_gap_256.sbb_in12k_ft_in1k](https://huggingface.co/timm/vit_betwixt_patch16_reg4_gap_256.sbb_in12k_ft_in1k)  | 85.418 | 97.48 | 60.4 | 256 |\n| [vit_mediumd_patch16_rope_reg1_gap_256.sbb_in1k](https://huggingface.co/timm/vit_mediumd_patch16_rope_reg1_gap_256.sbb_in1k)  | 84.322 | 96.812 | 63.95 | 256 |\n| [vit_betwixt_patch16_rope_reg4_gap_256.sbb_in1k](https://huggingface.co/timm/vit_betwixt_patch16_rope_reg4_gap_256.sbb_in1k)  | 83.906 | 96.684 | 60.23 | 256 |\n| [vit_base_patch16_rope_reg1_gap_256.sbb_in1k](https://huggingface.co/timm/vit_base_patch16_rope_reg1_gap_256.sbb_in1k)  | 83.866 | 96.67 | 86.43 | 256 |\n| [vit_medium_patch16_rope_reg1_gap_256.sbb_in1k](https://huggingface.co/timm/vit_medium_patch16_rope_reg1_gap_256.sbb_in1k)  | 83.81 | 96.824 | 38.74 | 256 |\n| [vit_betwixt_patch16_reg4_gap_256.sbb_in1k](https://huggingface.co/timm/vit_betwixt_patch16_reg4_gap_256.sbb_in1k)  | 83.706 | 96.616 | 60.4 | 256 |\n| [vit_betwixt_patch16_reg1_gap_256.sbb_in1k](https://huggingface.co/timm/vit_betwixt_patch16_reg1_gap_256.sbb_in1k)  | 83.628 | 96.544 | 60.4 | 256 |\n| [vit_medium_patch16_reg4_gap_256.sbb_in1k](https://huggingface.co/timm/vit_medium_patch16_reg4_gap_256.sbb_in1k)  | 83.47 | 96.622 | 38.88 | 256 |\n| [vit_medium_patch16_reg1_gap_256.sbb_in1k](https://huggingface.co/timm/vit_medium_patch16_reg1_gap_256.sbb_in1k)  | 83.462 | 96.548 | 38.88 | 256 |\n| [vit_little_patch16_reg4_gap_256.sbb_in1k](https://huggingface.co/timm/vit_little_patch16_reg4_gap_256.sbb_in1k)  | 82.514 | 96.262 | 22.52 | 256 |\n| [vit_wee_patch16_reg1_gap_256.sbb_in1k](https://huggingface.co/timm/vit_wee_patch16_reg1_gap_256.sbb_in1k)  | 80.256 | 95.360 | 13.42 | 256 |\n| [vit_pwee_patch16_reg1_gap_256.sbb_in1k](https://huggingface.co/timm/vit_pwee_patch16_reg1_gap_256.sbb_in1k)  | 80.072 | 95.136 | 15.25 | 256 |\n| [vit_mediumd_patch16_reg4_gap_256.sbb_in12k](https://huggingface.co/timm/vit_mediumd_patch16_reg4_gap_256.sbb_in12k) | N/A | N/A | 64.11 | 256 |\n| [vit_betwixt_patch16_reg4_gap_256.sbb_in12k](https://huggingface.co/timm/vit_betwixt_patch16_reg4_gap_256.sbb_in12k)  | N/A | N/A | 60.4 | 256 |\n\n* AttentionExtract helper added to extract attention maps from `timm` models. See example in https://github.com/huggingface/pytorch-image-models/discussions/1232#discussioncomment-9320949\n* `forward_intermediates()` API refined and added to more models including some ConvNets that have other extraction methods.\n* 1017 of 1047 model architectures support `features_only=True` feature extraction. Remaining 34 architectures can be supported but based on priority requests.\n* Remove torch.jit.script annotated functions including old JIT activations. Conflict with dynamo and dynamo does a much better job when used.\n\n### April 11, 2024\n* Prepping for a long overdue 1.0 release, things have been stable for a while now.\n* Significant feature that's been missing for a while, `features_only=True` support for ViT models with flat hidden states or non-std module layouts (so far covering  `'vit_*', 'twins_*', 'deit*', 'beit*', 'mvitv2*', 'eva*', 'samvit_*', 'flexivit*'`)\n* Above feature support achieved through a new `forward_intermediates()` API that can be used with a feature wrapping module or directly.\n```python\nmodel = timm.create_model('vit_base_patch16_224')\nfinal_feat, intermediates = model.forward_intermediates(input) \noutput = model.forward_head(final_feat)  # pooling + classifier head\n\nprint(final_feat.shape)\ntorch.Size([2, 197, 768])\n\nfor f in intermediates:\n    print(f.shape)\ntorch.Size([2, 768, 14, 14])\ntorch.Size([2, 768, 14, 14])\ntorch.Size([2, 768, 14, 14])\ntorch.Size([2, 768, 14, 14])\ntorch.Size([2, 768, 14, 14])\ntorch.Size([2, 768, 14, 14])\ntorch.Size([2, 768, 14, 14])\ntorch.Size([2, 768, 14, 14])\ntorch.Size([2, 768, 14, 14])\ntorch.Size([2, 768, 14, 14])\ntorch.Size([2, 768, 14, 14])\ntorch.Size([2, 768, 14, 14])\n\nprint(output.shape)\ntorch.Size([2, 1000])\n```\n\n```python\nmodel = timm.create_model('eva02_base_patch16_clip_224', pretrained=True, img_size=512, features_only=True, out_indices=(-3, -2,))\noutput = model(torch.randn(2, 3, 512, 512))\n\nfor o in output:    \n    print(o.shape)   \ntorch.Size([2, 768, 32, 32])\ntorch.Size([2, 768, 32, 32])\n```\n* TinyCLIP vision tower weights added, thx [Thien Tran](https://github.com/gau-nernst)\n\n### Feb 19, 2024\n* Next-ViT models added. Adapted from https://github.com/bytedance/Next-ViT\n* HGNet and PP-HGNetV2 models added. Adapted from https://github.com/PaddlePaddle/PaddleClas by [SeeFun](https://github.com/seefun)\n* Removed setup.py, moved to pyproject.toml based build supported by PDM\n* Add updated model EMA impl using _for_each for less overhead\n* Support device args in train script for non GPU devices\n* Other misc fixes and small additions\n* Min supported Python version increased to 3.8\n* Release 0.9.16\n\n### Jan 8, 2024\nDatasets & transform refactoring\n* HuggingFace streaming (iterable) dataset support (`--dataset hfids:org/dataset`)\n* Webdataset wrapper tweaks for improved split info fetching, can auto fetch splits from supported HF hub webdataset\n* Tested HF `datasets` and webdataset wrapper streaming from HF hub with recent `timm` ImageNet uploads to https://huggingface.co/timm\n* Make input & target column/field keys consistent across datasets and pass via args\n* Full monochrome support when using e:g: `--input-size 1 224 224` or `--in-chans 1`, sets PIL image conversion appropriately in dataset\n* Improved several alternate crop & resize transforms (ResizeKeepRatio, RandomCropOrPad, etc) for use in PixParse document AI project\n* Add SimCLR style color jitter prob along with grayscale and gaussian blur options to augmentations and args\n* Allow train without validation set (`--val-split ''`) in train script\n* Add `--bce-sum` (sum over class dim) and `--bce-pos-weight` (positive weighting) args for training as they're common BCE loss tweaks I was often hard coding \n\n### Nov 23, 2023\n* Added EfficientViT-Large models, thanks [SeeFun](https://github.com/seefun)\n* Fix Python 3.7 compat, will be dropping support for it soon\n* Other misc fixes\n* Release 0.9.12\n\n### Nov 20, 2023\n* Added significant flexibility for Hugging Face Hub based timm models via `model_args` config entry. `model_args` will be passed as kwargs through to models on creation. \n  * See example at https://huggingface.co/gaunernst/vit_base_patch16_1024_128.audiomae_as2m_ft_as20k/blob/main/config.json\n  * Usage: https://github.com/huggingface/pytorch-image-models/discussions/2035\n* Updated imagenet eval and test set csv files with latest models\n* `vision_transformer.py` typing and doc cleanup by [Lauret](https://github.com/Laurent2916)\n* 0.9.11 release\n\n### Nov 3, 2023\n* [DFN (Data Filtering Networks)](https://huggingface.co/papers/2309.17425) and [MetaCLIP](https://huggingface.co/papers/2309.16671) ViT weights added\n* DINOv2 'register' ViT model weights added (https://huggingface.co/papers/2309.16588, https://huggingface.co/papers/2304.07193)\n* Add `quickgelu` ViT variants for OpenAI, DFN, MetaCLIP weights that use it (less efficient)\n* Improved typing added to ResNet, MobileNet-v3 thanks to [Aryan](https://github.com/a-r-r-o-w)\n* ImageNet-12k fine-tuned (from LAION-2B CLIP) `convnext_xxlarge`\n* 0.9.9 release\n\n### Oct 20, 2023\n* [SigLIP](https://huggingface.co/papers/2303.15343) image tower weights supported in `vision_transformer.py`.\n  * Great potential for fine-tune and downstream feature use.\n* Experimental 'register' support in vit models as per [Vision Transformers Need Registers](https://huggingface.co/papers/2309.16588)\n* Updated RepViT with new weight release. Thanks [wangao](https://github.com/jameslahm)\n* Add patch resizing support (on pretrained weight load) to Swin models\n* 0.9.8 release pending\n\n### Sep 1, 2023\n* TinyViT added by [SeeFun](https://github.com/seefun)\n* Fix EfficientViT (MIT) to use torch.autocast so it works back to PT 1.10\n* 0.9.7 release\n\n## Introduction\n\nPy**T**orch **Im**age **M**odels (`timm`) is a collection of image models, layers, utilities, optimizers, schedulers, data-loaders / augmentations, and reference training / validation scripts that aim to pull together a wide variety of SOTA models with ability to reproduce ImageNet training results.\n\nThe work of many others is present here. I've tried to make sure all source material is acknowledged via links to github, arxiv papers, etc in the README, documentation, and code docstrings. Please let me know if I missed anything.\n\n## Features\n\n### Models\n\nAll model architecture families include variants with pretrained weights. There are specific model variants without any weights, it is NOT a bug. Help training new or better weights is always appreciated.\n\n* Aggregating Nested Transformers - https://arxiv.org/abs/2105.12723\n* BEiT - https://arxiv.org/abs/2106.08254\n* Big Transfer ResNetV2 (BiT) - https://arxiv.org/abs/1912.11370\n* Bottleneck Transformers - https://arxiv.org/abs/2101.11605\n* CaiT (Class-Attention in Image Transformers) - https://arxiv.org/abs/2103.17239\n* CoaT (Co-Scale Conv-Attentional Image Transformers) - https://arxiv.org/abs/2104.06399\n* CoAtNet (Convolution and Attention) - https://arxiv.org/abs/2106.04803\n* ConvNeXt - https://arxiv.org/abs/2201.03545\n* ConvNeXt-V2 - http://arxiv.org/abs/2301.00808\n* ConViT (Soft Convolutional Inductive Biases Vision Transformers)- https://arxiv.org/abs/2103.10697\n* CspNet (Cross-Stage Partial Networks) - https://arxiv.org/abs/1911.11929\n* DeiT - https://arxiv.org/abs/2012.12877\n* DeiT-III - https://arxiv.org/pdf/2204.07118.pdf\n* DenseNet - https://arxiv.org/abs/1608.06993\n* DLA - https://arxiv.org/abs/1707.06484\n* DPN (Dual-Path Network) - https://arxiv.org/abs/1707.01629\n* EdgeNeXt - https://arxiv.org/abs/2206.10589\n* EfficientFormer - https://arxiv.org/abs/2206.01191\n* EfficientNet (MBConvNet Family)\n    * EfficientNet NoisyStudent (B0-B7, L2) - https://arxiv.org/abs/1911.04252\n    * EfficientNet AdvProp (B0-B8) - https://arxiv.org/abs/1911.09665\n    * EfficientNet (B0-B7) - https://arxiv.org/abs/1905.11946\n    * EfficientNet-EdgeTPU (S, M, L) - https://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html\n    * EfficientNet V2 - https://arxiv.org/abs/2104.00298\n    * FBNet-C - https://arxiv.org/abs/1812.03443\n    * MixNet - https://arxiv.org/abs/1907.09595\n    * MNASNet B1, A1 (Squeeze-Excite), and Small - https://arxiv.org/abs/1807.11626\n    * MobileNet-V2 - https://arxiv.org/abs/1801.04381\n    * Single-Path NAS - https://arxiv.org/abs/1904.02877\n    * TinyNet - https://arxiv.org/abs/2010.14819\n* EfficientViT (MIT) - https://arxiv.org/abs/2205.14756\n* EfficientViT (MSRA) - https://arxiv.org/abs/2305.07027\n* EVA - https://arxiv.org/abs/2211.07636\n* EVA-02 - https://arxiv.org/abs/2303.11331\n* FastViT - https://arxiv.org/abs/2303.14189\n* FlexiViT - https://arxiv.org/abs/2212.08013\n* FocalNet (Focal Modulation Networks) - https://arxiv.org/abs/2203.11926\n* GCViT (Global Context Vision Transformer) - https://arxiv.org/abs/2206.09959\n* GhostNet - https://arxiv.org/abs/1911.11907\n* GhostNet-V2 - https://arxiv.org/abs/2211.12905\n* gMLP - https://arxiv.org/abs/2105.08050\n* GPU-Efficient Networks - https://arxiv.org/abs/2006.14090\n* Halo Nets - https://arxiv.org/abs/2103.12731\n* HGNet / HGNet-V2 - TBD\n* HRNet - https://arxiv.org/abs/1908.07919\n* InceptionNeXt - https://arxiv.org/abs/2303.16900\n* Inception-V3 - https://arxiv.org/abs/1512.00567\n* Inception-ResNet-V2 and Inception-V4 - https://arxiv.org/abs/1602.07261\n* Lambda Networks - https://arxiv.org/abs/2102.08602\n* LeViT (Vision Transformer in ConvNet's Clothing) - https://arxiv.org/abs/2104.01136\n* MambaOut - https://arxiv.org/abs/2405.07992\n* MaxViT (Multi-Axis Vision Transformer) - https://arxiv.org/abs/2204.01697\n* MetaFormer (PoolFormer-v2, ConvFormer, CAFormer) - https://arxiv.org/abs/2210.13452\n* MLP-Mixer - https://arxiv.org/abs/2105.01601\n* MobileCLIP - https://arxiv.org/abs/2311.17049\n* MobileNet-V3 (MBConvNet w/ Efficient Head) - https://arxiv.org/abs/1905.02244\n  * FBNet-V3 - https://arxiv.org/abs/2006.02049\n  * HardCoRe-NAS - https://arxiv.org/abs/2102.11646\n  * LCNet - https://arxiv.org/abs/2109.15099\n* MobileNetV4 - https://arxiv.org/abs/2404.10518\n* MobileOne - https://arxiv.org/abs/2206.04040\n* MobileViT - https://arxiv.org/abs/2110.02178\n* MobileViT-V2 - https://arxiv.org/abs/2206.02680\n* MViT-V2 (Improved Multiscale Vision Transformer) - https://arxiv.org/abs/2112.01526\n* NASNet-A - https://arxiv.org/abs/1707.07012\n* NesT - https://arxiv.org/abs/2105.12723\n* Next-ViT - https://arxiv.org/abs/2207.05501\n* NFNet-F - https://arxiv.org/abs/2102.06171\n* NF-RegNet / NF-ResNet - https://arxiv.org/abs/2101.08692\n* PNasNet - https://arxiv.org/abs/1712.00559\n* PoolFormer (MetaFormer) - https://arxiv.org/abs/2111.11418\n* Pooling-based Vision Transformer (PiT) - https://arxiv.org/abs/2103.16302\n* PVT-V2 (Improved Pyramid Vision Transformer) - https://arxiv.org/abs/2106.13797\n* RDNet (DenseNets Reloaded) - https://arxiv.org/abs/2403.19588\n* RegNet - https://arxiv.org/abs/2003.13678\n* RegNetZ - https://arxiv.org/abs/2103.06877\n* RepVGG - https://arxiv.org/abs/2101.03697\n* RepGhostNet - https://arxiv.org/abs/2211.06088\n* RepViT - https://arxiv.org/abs/2307.09283\n* ResMLP - https://arxiv.org/abs/2105.03404\n* ResNet/ResNeXt\n    * ResNet (v1b/v1.5) - https://arxiv.org/abs/1512.03385\n    * ResNeXt - https://arxiv.org/abs/1611.05431\n    * 'Bag of Tricks' / Gluon C, D, E, S variations - https://arxiv.org/abs/1812.01187\n    * Weakly-supervised (WSL) Instagram pretrained / ImageNet tuned ResNeXt101 - https://arxiv.org/abs/1805.00932\n    * Semi-supervised (SSL) / Semi-weakly Supervised (SWSL) ResNet/ResNeXts - https://arxiv.org/abs/1905.00546\n    * ECA-Net (ECAResNet) - https://arxiv.org/abs/1910.03151v4\n    * Squeeze-and-Excitation Networks (SEResNet) - https://arxiv.org/abs/1709.01507\n    * ResNet-RS - https://arxiv.org/abs/2103.07579\n* Res2Net - https://arxiv.org/abs/1904.01169\n* ResNeSt - https://arxiv.org/abs/2004.08955\n* ReXNet - https://arxiv.org/abs/2007.00992\n* SelecSLS - https://arxiv.org/abs/1907.00837\n* Selective Kernel Networks - https://arxiv.org/abs/1903.06586\n* Sequencer2D - https://arxiv.org/abs/2205.01972\n* Swin S3 (AutoFormerV2) - https://arxiv.org/abs/2111.14725\n* Swin Transformer - https://arxiv.org/abs/2103.14030\n* Swin Transformer V2 - https://arxiv.org/abs/2111.09883\n* Transformer-iN-Transformer (TNT) - https://arxiv.org/abs/2103.00112\n* TResNet - https://arxiv.org/abs/2003.13630\n* Twins (Spatial Attention in Vision Transformers) - https://arxiv.org/pdf/2104.13840.pdf\n* Visformer - https://arxiv.org/abs/2104.12533\n* Vision Transformer - https://arxiv.org/abs/2010.11929\n* ViTamin - https://arxiv.org/abs/2404.02132\n* VOLO (Vision Outlooker) - https://arxiv.org/abs/2106.13112\n* VovNet V2 and V1 - https://arxiv.org/abs/1911.06667\n* Xception - https://arxiv.org/abs/1610.02357\n* Xception (Modified Aligned, Gluon) - https://arxiv.org/abs/1802.02611\n* Xception (Modified Aligned, TF) - https://arxiv.org/abs/1802.02611\n* XCiT (Cross-Covariance Image Transformers) - https://arxiv.org/abs/2106.09681\n\n### Optimizers\nTo see full list of optimizers w/ descriptions: `timm.optim.list_optimizers(with_description=True)`\n\nIncluded optimizers available via `timm.optim.create_optimizer_v2` factory method:\n* `adabelief` an implementation of AdaBelief adapted from https://github.com/juntang-zhuang/Adabelief-Optimizer - https://arxiv.org/abs/2010.07468\n* `adafactor` adapted from [FAIRSeq impl](https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py) - https://arxiv.org/abs/1804.04235\n* `adafactorbv` adapted from [Big Vision](https://github.com/google-research/big_vision/blob/main/big_vision/optax.py) - https://arxiv.org/abs/2106.04560\n* `adahessian` by [David Samuel](https://github.com/davda54/ada-hessian) - https://arxiv.org/abs/2006.00719\n* `adamp` and `sgdp` by [Naver ClovAI](https://github.com/clovaai) - https://arxiv.org/abs/2006.08217\n* `adan` an implementation of Adan adapted from https://github.com/sail-sg/Adan - https://arxiv.org/abs/2208.06677\n* `adopt` ADOPT adapted from https://github.com/iShohei220/adopt - https://arxiv.org/abs/2411.02853\n* `lamb` an implementation of Lamb and LambC (w/ trust-clipping) cleaned up and modified to support use with XLA - https://arxiv.org/abs/1904.00962\n* `laprop` optimizer from https://github.com/Z-T-WANG/LaProp-Optimizer - https://arxiv.org/abs/2002.04839\n* `lars` an implementation of LARS and LARC (w/ trust-clipping) - https://arxiv.org/abs/1708.03888\n* `lion` and implementation of Lion adapted from https://github.com/google/automl/tree/master/lion - https://arxiv.org/abs/2302.06675\n* `lookahead` adapted from impl by [Liam](https://github.com/alphadl/lookahead.pytorch) - https://arxiv.org/abs/1907.08610\n* `madgrad` an implementation of MADGRAD adapted from https://github.com/facebookresearch/madgrad - https://arxiv.org/abs/2101.11075\n* `mars` MARS optimizer from https://github.com/AGI-Arena/MARS - https://arxiv.org/abs/2411.10438\n* `nadam` an implementation of Adam w/ Nesterov momentum\n* `nadamw` an implementation of AdamW (Adam w/ decoupled weight-decay) w/ Nesterov momentum. A simplified impl based on https://github.com/mlcommons/algorithmic-efficiency\n* `novograd` by [Masashi Kimura](https://github.com/convergence-lab/novograd) - https://arxiv.org/abs/1905.11286\n* `radam` by [Liyuan Liu](https://github.com/LiyuanLucasLiu/RAdam) - https://arxiv.org/abs/1908.03265\n* `rmsprop_tf` adapted from PyTorch RMSProp by myself. Reproduces much improved Tensorflow RMSProp behaviour\n* `sgdw` and implementation of SGD w/ decoupled weight-decay\n* `fused<name>` optimizers by name with [NVIDIA Apex](https://github.com/NVIDIA/apex/tree/master/apex/optimizers) installed\n* `bnb<name>` optimizers by name with [BitsAndBytes](https://github.com/TimDettmers/bitsandbytes) installed\n* `cadamw`, `clion`, and more 'Cautious' optimizers from https://github.com/kyleliang919/C-Optim - https://arxiv.org/abs/2411.16085\n* `adam`, `adamw`, `rmsprop`, `adadelta`, `adagrad`, and `sgd` pass through to `torch.optim` implementations\n\n### Augmentations\n* Random Erasing from [Zhun Zhong](https://github.com/zhunzhong07/Random-Erasing/blob/master/transforms.py) - https://arxiv.org/abs/1708.04896)\n* Mixup - https://arxiv.org/abs/1710.09412\n* CutMix - https://arxiv.org/abs/1905.04899\n* AutoAugment (https://arxiv.org/abs/1805.09501) and RandAugment (https://arxiv.org/abs/1909.13719) ImageNet configurations modeled after impl for EfficientNet training (https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/autoaugment.py)\n* AugMix w/ JSD loss, JSD w/ clean + augmented mixing support works with AutoAugment and RandAugment as well - https://arxiv.org/abs/1912.02781\n* SplitBachNorm - allows splitting batch norm layers between clean and augmented (auxiliary batch norm) data\n\n### Regularization\n* DropPath aka \"Stochastic Depth\" - https://arxiv.org/abs/1603.09382\n* DropBlock - https://arxiv.org/abs/1810.12890\n* Blur Pooling - https://arxiv.org/abs/1904.11486\n\n### Other\n\nSeveral (less common) features that I often utilize in my projects are included. Many of their additions are the reason why I maintain my own set of models, instead of using others' via PIP:\n\n* All models have a common default configuration interface and API for\n    * accessing/changing the classifier - `get_classifier` and `reset_classifier`\n    * doing a forward pass on just the features - `forward_features` (see [documentation](https://huggingface.co/docs/timm/feature_extraction))\n    * these makes it easy to write consistent network wrappers that work with any of the models\n* All models support multi-scale feature map extraction (feature pyramids) via create_model (see [documentation](https://huggingface.co/docs/timm/feature_extraction))\n    * `create_model(name, features_only=True, out_indices=..., output_stride=...)`\n    * `out_indices` creation arg specifies which feature maps to return, these indices are 0 based and generally correspond to the `C(i + 1)` feature level.\n    * `output_stride` creation arg controls output stride of the network by using dilated convolutions. Most networks are stride 32 by default. Not all networks support this.\n    * feature map channel counts, reduction level (stride) can be queried AFTER model creation via the `.feature_info` member\n* All models have a consistent pretrained weight loader that adapts last linear if necessary, and from 3 to 1 channel input if desired\n* High performance [reference training, validation, and inference scripts](https://huggingface.co/docs/timm/training_script) that work in several process/GPU modes:\n    * NVIDIA DDP w/ a single GPU per process, multiple processes with APEX present (AMP mixed-precision optional)\n    * PyTorch DistributedDataParallel w/ multi-gpu, single process (AMP disabled as it crashes when enabled)\n    * PyTorch w/ single GPU single process (AMP optional)\n* A dynamic global pool implementation that allows selecting from average pooling, max pooling, average + max, or concat([average, max]) at model creation. All global pooling is adaptive average by default and compatible with pretrained weights.\n* A 'Test Time Pool' wrapper that can wrap any of the included models and usually provides improved performance doing inference with input images larger than the training size. Idea adapted from original DPN implementation when I ported (https://github.com/cypw/DPNs)\n* Learning rate schedulers\n  * Ideas adopted from\n     * [AllenNLP schedulers](https://github.com/allenai/allennlp/tree/master/allennlp/training/learning_rate_schedulers)\n     * [FAIRseq lr_scheduler](https://github.com/pytorch/fairseq/tree/master/fairseq/optim/lr_scheduler)\n     * SGDR: Stochastic Gradient Descent with Warm Restarts (https://arxiv.org/abs/1608.03983)\n  * Schedulers include `step`, `cosine` w/ restarts, `tanh` w/ restarts, `plateau`\n* Space-to-Depth by [mrT23](https://github.com/mrT23/TResNet/blob/master/src/models/tresnet/layers/space_to_depth.py) (https://arxiv.org/abs/1801.04590) -- original paper?\n* Adaptive Gradient Clipping (https://arxiv.org/abs/2102.06171, https://github.com/deepmind/deepmind-research/tree/master/nfnets)\n* An extensive selection of channel and/or spatial attention modules:\n    * Bottleneck Transformer - https://arxiv.org/abs/2101.11605\n    * CBAM - https://arxiv.org/abs/1807.06521\n    * Effective Squeeze-Excitation (ESE) - https://arxiv.org/abs/1911.06667\n    * Efficient Channel Attention (ECA) - https://arxiv.org/abs/1910.03151\n    * Gather-Excite (GE) - https://arxiv.org/abs/1810.12348\n    * Global Context (GC) - https://arxiv.org/abs/1904.11492\n    * Halo - https://arxiv.org/abs/2103.12731\n    * Involution - https://arxiv.org/abs/2103.06255\n    * Lambda Layer - https://arxiv.org/abs/2102.08602\n    * Non-Local (NL) -  https://arxiv.org/abs/1711.07971\n    * Squeeze-and-Excitation (SE) - https://arxiv.org/abs/1709.01507\n    * Selective Kernel (SK) - (https://arxiv.org/abs/1903.06586\n    * Split (SPLAT) - https://arxiv.org/abs/2004.08955\n    * Shifted Window (SWIN) - https://arxiv.org/abs/2103.14030\n\n## Results\n\nModel validation results can be found in the [results tables](results/README.md)\n\n## Getting Started (Documentation)\n\nThe official documentation can be found at https://huggingface.co/docs/hub/timm. Documentation contributions are welcome.\n\n[Getting Started with PyTorch Image Models (timm): A Practitioners Guide](https://towardsdatascience.com/getting-started-with-pytorch-image-models-timm-a-practitioners-guide-4e77b4bf9055) by [Chris Hughes](https://github.com/Chris-hughes10) is an extensive blog post covering many aspects of `timm` in detail.\n\n[timmdocs](http://timm.fast.ai/) is an alternate set of documentation for `timm`. A big thanks to [Aman Arora](https://github.com/amaarora) for his efforts creating timmdocs.\n\n[paperswithcode](https://paperswithcode.com/lib/timm) is a good resource for browsing the models within `timm`.\n\n## Train, Validation, Inference Scripts\n\nThe root folder of the repository contains reference train, validation, and inference scripts that work with the included models and other features of this repository. They are adaptable for other datasets and use cases with a little hacking. See [documentation](https://huggingface.co/docs/timm/training_script).\n\n## Awesome PyTorch Resources\n\nOne of the greatest assets of PyTorch is the community and their contributions. A few of my favourite resources that pair well with the models and components here are listed below.\n\n### Object Detection, Instance and Semantic Segmentation\n* Detectron2 - https://github.com/facebookresearch/detectron2\n* Segmentation Models (Semantic) - https://github.com/qubvel/segmentation_models.pytorch\n* EfficientDet (Obj Det, Semantic soon) - https://github.com/rwightman/efficientdet-pytorch\n\n### Computer Vision / Image Augmentation\n* Albumentations - https://github.com/albumentations-team/albumentations\n* Kornia - https://github.com/kornia/kornia\n\n### Knowledge Distillation\n* RepDistiller - https://github.com/HobbitLong/RepDistiller\n* torchdistill - https://github.com/yoshitomo-matsubara/torchdistill\n\n### Metric Learning\n* PyTorch Metric Learning - https://github.com/KevinMusgrave/pytorch-metric-learning\n\n### Training / Frameworks\n* fastai - https://github.com/fastai/fastai\n\n## Licenses\n\n### Code\nThe code here is licensed Apache 2.0. I've taken care to make sure any third party code included or adapted has compatible (permissive) licenses such as MIT, BSD, etc. I've made an effort to avoid any GPL / LGPL conflicts. That said, it is your responsibility to ensure you comply with licenses here and conditions of any dependent licenses. Where applicable, I've linked the sources/references for various components in docstrings. If you think I've missed anything please create an issue.\n\n### Pretrained Weights\nSo far all of the pretrained weights available here are pretrained on ImageNet with a select few that have some additional pretraining (see extra note below). ImageNet was released for non-commercial research purposes only (https://image-net.org/download). It's not clear what the implications of that are for the use of pretrained weights from that dataset. Any models I have trained with ImageNet are done for research purposes and one should assume that the original dataset license applies to the weights. It's best to seek legal advice if you intend to use the pretrained weights in a commercial product.\n\n#### Pretrained on more than ImageNet\nSeveral weights included or references here were pretrained with proprietary datasets that I do not have access to. These include the Facebook WSL, SSL, SWSL ResNe(Xt) and the Google Noisy Student EfficientNet models. The Facebook models have an explicit non-commercial license (CC-BY-NC 4.0, https://github.com/facebookresearch/semi-supervised-ImageNet1K-models, https://github.com/facebookresearch/WSL-Images). The Google models do not appear to have any restriction beyond the Apache 2.0 license (and ImageNet concerns). In either case, you should contact Facebook or Google with any questions.\n\n## Citing\n\n### BibTeX\n\n```bibtex\n@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/rwightman/pytorch-image-models}}\n}\n```\n\n### Latest DOI\n\n[![DOI](https://zenodo.org/badge/168799526.svg)](https://zenodo.org/badge/latestdoi/168799526)\n"
        },
        {
          "name": "UPGRADING.md",
          "type": "blob",
          "size": 2.46,
          "content": "# Upgrading from previous versions\n\nI generally try to maintain code interface and especially model weight compatibility across many `timm` versions. Sometimes there are exceptions.\n\n## Checkpoint remapping\n\nPretrained weight remapping is handled by `checkpoint_filter_fn` in a model implementation module. This remaps old pretrained checkpoints to new, and also 3rd party (original) checkpoints to `timm` format if the model was modified when brought into `timm`.\n\nThe `checkpoint_filter_fn` is automatically called when loading pretrained weights via `pretrained=True`, but they can be called manually if you call the fn directly with the current model instance and old state dict.\n\n## Upgrading from 0.6 and earlier\n\nMany changes were made since the 0.6.x stable releases. They were previewed in 0.8.x dev releases but not everyone transitioned.\n* `timm.models.layers` moved to `timm.layers`:\n  * `from timm.models.layers import name` will still work via deprecation mapping (but please transition to `timm.layers`).\n  * `import timm.models.layers.module` or `from timm.models.layers.module import name` needs to be changed now.\n* Builder, helper, non-model modules in `timm.models` have a `_` prefix added, ie `timm.models.helpers` -> `timm.models._helpers`, there are temporary deprecation mapping files but those will be removed.\n* All models now support `architecture.pretrained_tag` naming (ex `resnet50.rsb_a1`).\n  * The pretrained_tag is the specific weight variant (different head) for the architecture.\n  * Using only `architecture` defaults to the first weights in the default_cfgs for that model architecture.\n  * In adding pretrained tags, many model names that existed to differentiate were renamed to use the tag  (ex: `vit_base_patch16_224_in21k` -> `vit_base_patch16_224.augreg_in21k`). There are deprecation mappings for these.\n* A number of models had their checkpoints remapped to match architecture changes needed to better support `features_only=True`, there are `checkpoint_filter_fn` methods in any model module that was remapped. These can be passed to `timm.models.load_checkpoint(..., filter_fn=timm.models.swin_transformer_v2.checkpoint_filter_fn)` to remap your existing checkpoint.\n* The Hugging Face Hub (https://huggingface.co/timm) is now the primary source for `timm` weights. Model cards include link to papers, original source, license. \n* Previous 0.6.x can be cloned from [0.6.x](https://github.com/rwightman/pytorch-image-models/tree/0.6.x) branch or installed via pip with version.\n"
        },
        {
          "name": "avg_checkpoints.py",
          "type": "blob",
          "size": 5.79,
          "content": "#!/usr/bin/env python3\n\"\"\" Checkpoint Averaging Script\n\nThis script averages all model weights for checkpoints in specified path that match\nthe specified filter wildcard. All checkpoints must be from the exact same model.\n\nFor any hope of decent results, the checkpoints should be from the same or child\n(via resumes) training session. This can be viewed as similar to maintaining running\nEMA (exponential moving average) of the model weights or performing SWA (stochastic\nweight averaging), but post-training.\n\nHacked together by / Copyright 2020 Ross Wightman (https://github.com/rwightman)\n\"\"\"\nimport torch\nimport argparse\nimport os\nimport glob\nimport hashlib\nfrom timm.models import load_state_dict\ntry:\n    import safetensors.torch\n    _has_safetensors = True\nexcept ImportError:\n    _has_safetensors = False\n\nDEFAULT_OUTPUT = \"./averaged.pth\"\nDEFAULT_SAFE_OUTPUT = \"./averaged.safetensors\"\n\nparser = argparse.ArgumentParser(description='PyTorch Checkpoint Averager')\nparser.add_argument('--input', default='', type=str, metavar='PATH',\n                    help='path to base input folder containing checkpoints')\nparser.add_argument('--filter', default='*.pth.tar', type=str, metavar='WILDCARD',\n                    help='checkpoint filter (path wildcard)')\nparser.add_argument('--output', default=DEFAULT_OUTPUT, type=str, metavar='PATH',\n                    help=f'Output filename. Defaults to {DEFAULT_SAFE_OUTPUT} when passing --safetensors.')\nparser.add_argument('--no-use-ema', dest='no_use_ema', action='store_true',\n                    help='Force not using ema version of weights (if present)')\nparser.add_argument('--no-sort', dest='no_sort', action='store_true',\n                    help='Do not sort and select by checkpoint metric, also makes \"n\" argument irrelevant')\nparser.add_argument('-n', type=int, default=10, metavar='N',\n                    help='Number of checkpoints to average')\nparser.add_argument('--safetensors', action='store_true',\n                    help='Save weights using safetensors instead of the default torch way (pickle).')\n\n\ndef checkpoint_metric(checkpoint_path):\n    if not checkpoint_path or not os.path.isfile(checkpoint_path):\n        return {}\n    print(\"=> Extracting metric from checkpoint '{}'\".format(checkpoint_path))\n    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n    metric = None\n    if 'metric' in checkpoint:\n        metric = checkpoint['metric']\n    elif 'metrics' in checkpoint and 'metric_name' in checkpoint:\n        metrics = checkpoint['metrics']\n        print(metrics)\n        metric = metrics[checkpoint['metric_name']]\n    return metric\n\n\ndef main():\n    args = parser.parse_args()\n    # by default use the EMA weights (if present)\n    args.use_ema = not args.no_use_ema\n    # by default sort by checkpoint metric (if present) and avg top n checkpoints\n    args.sort = not args.no_sort\n\n    if args.safetensors and args.output == DEFAULT_OUTPUT:\n        # Default path changes if using safetensors\n        args.output = DEFAULT_SAFE_OUTPUT\n\n    output, output_ext = os.path.splitext(args.output)\n    if not output_ext:\n        output_ext = ('.safetensors' if args.safetensors else '.pth')\n    output = output + output_ext\n\n    if args.safetensors and not output_ext == \".safetensors\":\n        print(\n            \"Warning: saving weights as safetensors but output file extension is not \"\n            f\"set to '.safetensors': {args.output}\"\n        )\n\n    if os.path.exists(output):\n        print(\"Error: Output filename ({}) already exists.\".format(output))\n        exit(1)\n\n    pattern = args.input\n    if not args.input.endswith(os.path.sep) and not args.filter.startswith(os.path.sep):\n        pattern += os.path.sep\n    pattern += args.filter\n    checkpoints = glob.glob(pattern, recursive=True)\n\n    if args.sort:\n        checkpoint_metrics = []\n        for c in checkpoints:\n            metric = checkpoint_metric(c)\n            if metric is not None:\n                checkpoint_metrics.append((metric, c))\n        checkpoint_metrics = list(sorted(checkpoint_metrics))\n        checkpoint_metrics = checkpoint_metrics[-args.n:]\n        if checkpoint_metrics:\n            print(\"Selected checkpoints:\")\n            [print(m, c) for m, c in checkpoint_metrics]\n        avg_checkpoints = [c for m, c in checkpoint_metrics]\n    else:\n        avg_checkpoints = checkpoints\n        if avg_checkpoints:\n            print(\"Selected checkpoints:\")\n            [print(c) for c in checkpoints]\n\n    if not avg_checkpoints:\n        print('Error: No checkpoints found to average.')\n        exit(1)\n\n    avg_state_dict = {}\n    avg_counts = {}\n    for c in avg_checkpoints:\n        new_state_dict = load_state_dict(c, args.use_ema)\n        if not new_state_dict:\n            print(f\"Error: Checkpoint ({c}) doesn't exist\")\n            continue\n        for k, v in new_state_dict.items():\n            if k not in avg_state_dict:\n                avg_state_dict[k] = v.clone().to(dtype=torch.float64)\n                avg_counts[k] = 1\n            else:\n                avg_state_dict[k] += v.to(dtype=torch.float64)\n                avg_counts[k] += 1\n\n    for k, v in avg_state_dict.items():\n        v.div_(avg_counts[k])\n\n    # float32 overflow seems unlikely based on weights seen to date, but who knows\n    float32_info = torch.finfo(torch.float32)\n    final_state_dict = {}\n    for k, v in avg_state_dict.items():\n        v = v.clamp(float32_info.min, float32_info.max)\n        final_state_dict[k] = v.to(dtype=torch.float32)\n\n    if args.safetensors:\n        assert _has_safetensors, \"`pip install safetensors` to use .safetensors\"\n        safetensors.torch.save_file(final_state_dict, output)\n    else:\n        torch.save(final_state_dict, output)\n\n    with open(output, 'rb') as f:\n        sha_hash = hashlib.sha256(f.read()).hexdigest()\n    print(f\"=> Saved state_dict to '{output}, SHA256: {sha_hash}'\")\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "benchmark.py",
          "type": "blob",
          "size": 27.81,
          "content": "#!/usr/bin/env python3\n\"\"\" Model Benchmark Script\n\nAn inference and train step benchmark script for timm models.\n\nHacked together by Ross Wightman (https://github.com/rwightman)\n\"\"\"\nimport argparse\nimport csv\nimport json\nimport logging\nimport time\nfrom collections import OrderedDict\nfrom contextlib import suppress\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\n\nfrom timm.data import resolve_data_config\nfrom timm.layers import set_fast_norm\nfrom timm.models import create_model, is_model, list_models\nfrom timm.optim import create_optimizer_v2\nfrom timm.utils import setup_default_logging, set_jit_fuser, decay_batch_step, check_batch_size_retry, ParseKwargs,\\\n    reparameterize_model\n\nhas_apex = False\ntry:\n    from apex import amp\n    has_apex = True\nexcept ImportError:\n    pass\n\ntry:\n    from deepspeed.profiling.flops_profiler import get_model_profile\n    has_deepspeed_profiling = True\nexcept ImportError as e:\n    has_deepspeed_profiling = False\n\ntry:\n    from fvcore.nn import FlopCountAnalysis, flop_count_str, ActivationCountAnalysis\n    has_fvcore_profiling = True\nexcept ImportError as e:\n    FlopCountAnalysis = None\n    has_fvcore_profiling = False\n\ntry:\n    from functorch.compile import memory_efficient_fusion\n    has_functorch = True\nexcept ImportError as e:\n    has_functorch = False\n\nhas_compile = hasattr(torch, 'compile')\n\nif torch.cuda.is_available():\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.benchmark = True\n_logger = logging.getLogger('validate')\n\n\nparser = argparse.ArgumentParser(description='PyTorch Benchmark')\n\n# benchmark specific args\nparser.add_argument('--model-list', metavar='NAME', default='',\n                    help='txt file based list of model names to benchmark')\nparser.add_argument('--bench', default='both', type=str,\n                    help=\"Benchmark mode. One of 'inference', 'train', 'both'. Defaults to 'both'\")\nparser.add_argument('--detail', action='store_true', default=False,\n                    help='Provide train fwd/bwd/opt breakdown detail if True. Defaults to False')\nparser.add_argument('--no-retry', action='store_true', default=False,\n                    help='Do not decay batch size and retry on error.')\nparser.add_argument('--results-file', default='', type=str,\n                    help='Output csv file for validation results (summary)')\nparser.add_argument('--results-format', default='csv', type=str,\n                    help='Format for results file one of (csv, json) (default: csv).')\nparser.add_argument('--num-warm-iter', default=10, type=int,\n                    help='Number of warmup iterations (default: 10)')\nparser.add_argument('--num-bench-iter', default=40, type=int,\n                    help='Number of benchmark iterations (default: 40)')\nparser.add_argument('--device', default='cuda', type=str,\n                    help=\"device to run benchmark on\")\n\n# common inference / train args\nparser.add_argument('--model', '-m', metavar='NAME', default='resnet50',\n                    help='model architecture (default: resnet50)')\nparser.add_argument('-b', '--batch-size', default=256, type=int,\n                    metavar='N', help='mini-batch size (default: 256)')\nparser.add_argument('--img-size', default=None, type=int,\n                    metavar='N', help='Input image dimension, uses model default if empty')\nparser.add_argument('--input-size', default=None, nargs=3, type=int,\n                    metavar='N N N', help='Input all image dimensions (d h w, e.g. --input-size 3 224 224), uses model default if empty')\nparser.add_argument('--use-train-size', action='store_true', default=False,\n                    help='Run inference at train size, not test-input-size if it exists.')\nparser.add_argument('--num-classes', type=int, default=None,\n                    help='Number classes in dataset')\nparser.add_argument('--gp', default=None, type=str, metavar='POOL',\n                    help='Global pool type, one of (fast, avg, max, avgmax, avgmaxc). Model default if None.')\nparser.add_argument('--channels-last', action='store_true', default=False,\n                    help='Use channels_last memory layout')\nparser.add_argument('--grad-checkpointing', action='store_true', default=False,\n                    help='Enable gradient checkpointing through model blocks/stages')\nparser.add_argument('--amp', action='store_true', default=False,\n                    help='use PyTorch Native AMP for mixed precision training. Overrides --precision arg.')\nparser.add_argument('--amp-dtype', default='float16', type=str,\n                    help='lower precision AMP dtype (default: float16). Overrides --precision arg if args.amp True.')\nparser.add_argument('--precision', default='float32', type=str,\n                    help='Numeric precision. One of (amp, float32, float16, bfloat16, tf32)')\nparser.add_argument('--fuser', default='', type=str,\n                    help=\"Select jit fuser. One of ('', 'te', 'old', 'nvfuser')\")\nparser.add_argument('--fast-norm', default=False, action='store_true',\n                    help='enable experimental fast-norm')\nparser.add_argument('--reparam', default=False, action='store_true',\n                    help='Reparameterize model')\nparser.add_argument('--model-kwargs', nargs='*', default={}, action=ParseKwargs)\nparser.add_argument('--torchcompile-mode', type=str, default=None,\n                    help=\"torch.compile mode (default: None).\")\n\n# codegen (model compilation) options\nscripting_group = parser.add_mutually_exclusive_group()\nscripting_group.add_argument('--torchscript', dest='torchscript', action='store_true',\n                             help='convert model torchscript for inference')\nscripting_group.add_argument('--torchcompile', nargs='?', type=str, default=None, const='inductor',\n                             help=\"Enable compilation w/ specified backend (default: inductor).\")\nscripting_group.add_argument('--aot-autograd', default=False, action='store_true',\n                             help=\"Enable AOT Autograd optimization.\")\n\n# train optimizer parameters\nparser.add_argument('--opt', default='sgd', type=str, metavar='OPTIMIZER',\n                    help='Optimizer (default: \"sgd\"')\nparser.add_argument('--opt-eps', default=None, type=float, metavar='EPSILON',\n                    help='Optimizer Epsilon (default: None, use opt default)')\nparser.add_argument('--opt-betas', default=None, type=float, nargs='+', metavar='BETA',\n                    help='Optimizer Betas (default: None, use opt default)')\nparser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n                    help='Optimizer momentum (default: 0.9)')\nparser.add_argument('--weight-decay', type=float, default=0.0001,\n                    help='weight decay (default: 0.0001)')\nparser.add_argument('--clip-grad', type=float, default=None, metavar='NORM',\n                    help='Clip gradient norm (default: None, no clipping)')\nparser.add_argument('--clip-mode', type=str, default='norm',\n                    help='Gradient clipping mode. One of (\"norm\", \"value\", \"agc\")')\n\n\n# model regularization / loss params that impact model or loss fn\nparser.add_argument('--smoothing', type=float, default=0.1,\n                    help='Label smoothing (default: 0.1)')\nparser.add_argument('--drop', type=float, default=0.0, metavar='PCT',\n                    help='Dropout rate (default: 0.)')\nparser.add_argument('--drop-path', type=float, default=None, metavar='PCT',\n                    help='Drop path rate (default: None)')\nparser.add_argument('--drop-block', type=float, default=None, metavar='PCT',\n                    help='Drop block rate (default: None)')\n\n\ndef timestamp(sync=False):\n    return time.perf_counter()\n\n\ndef cuda_timestamp(sync=False, device=None):\n    if sync:\n        torch.cuda.synchronize(device=device)\n    return time.perf_counter()\n\n\ndef count_params(model: nn.Module):\n    return sum([m.numel() for m in model.parameters()])\n\n\ndef resolve_precision(precision: str):\n    assert precision in ('amp', 'amp_bfloat16', 'float16', 'bfloat16', 'float32')\n    amp_dtype = None  # amp disabled\n    model_dtype = torch.float32\n    data_dtype = torch.float32\n    if precision == 'amp':\n        amp_dtype = torch.float16\n    elif precision == 'amp_bfloat16':\n        amp_dtype = torch.bfloat16\n    elif precision == 'float16':\n        model_dtype = torch.float16\n        data_dtype = torch.float16\n    elif precision == 'bfloat16':\n        model_dtype = torch.bfloat16\n        data_dtype = torch.bfloat16\n    return amp_dtype, model_dtype, data_dtype\n\n\ndef profile_deepspeed(model, input_size=(3, 224, 224), batch_size=1, detailed=False):\n    _, macs, _ = get_model_profile(\n        model=model,\n        input_shape=(batch_size,) + input_size,  # input shape/resolution\n        print_profile=detailed,  # prints the model graph with the measured profile attached to each module\n        detailed=detailed,  # print the detailed profile\n        warm_up=10,  # the number of warm-ups before measuring the time of each module\n        as_string=False,  # print raw numbers (e.g. 1000) or as human-readable strings (e.g. 1k)\n        output_file=None,  # path to the output file. If None, the profiler prints to stdout.\n        ignore_modules=None)  # the list of modules to ignore in the profiling\n    return macs, 0  # no activation count in DS\n\n\ndef profile_fvcore(model, input_size=(3, 224, 224), batch_size=1, detailed=False, force_cpu=False):\n    if force_cpu:\n        model = model.to('cpu')\n    device, dtype = next(model.parameters()).device, next(model.parameters()).dtype\n    example_input = torch.ones((batch_size,) + input_size, device=device, dtype=dtype)\n    fca = FlopCountAnalysis(model, example_input)\n    aca = ActivationCountAnalysis(model, example_input)\n    if detailed:\n        fcs = flop_count_str(fca)\n        print(fcs)\n    return fca.total(), aca.total()\n\n\nclass BenchmarkRunner:\n    def __init__(\n            self,\n            model_name,\n            detail=False,\n            device='cuda',\n            torchscript=False,\n            torchcompile=None,\n            torchcompile_mode=None,\n            aot_autograd=False,\n            reparam=False,\n            precision='float32',\n            fuser='',\n            num_warm_iter=10,\n            num_bench_iter=50,\n            use_train_size=False,\n            **kwargs\n    ):\n        self.model_name = model_name\n        self.detail = detail\n        self.device = device\n        self.amp_dtype, self.model_dtype, self.data_dtype = resolve_precision(precision)\n        self.channels_last = kwargs.pop('channels_last', False)\n        if self.amp_dtype is not None:\n            self.amp_autocast = partial(torch.amp.autocast, device_type=device, dtype=self.amp_dtype)\n        else:\n            self.amp_autocast = suppress\n\n        if fuser:\n            set_jit_fuser(fuser)\n        self.model = create_model(\n            model_name,\n            num_classes=kwargs.pop('num_classes', None),\n            in_chans=3,\n            global_pool=kwargs.pop('gp', 'fast'),\n            scriptable=torchscript,\n            drop_rate=kwargs.pop('drop', 0.),\n            drop_path_rate=kwargs.pop('drop_path', None),\n            drop_block_rate=kwargs.pop('drop_block', None),\n            **kwargs.pop('model_kwargs', {}),\n        )\n        if reparam:\n            self.model = reparameterize_model(self.model)\n        self.model.to(\n            device=self.device,\n            dtype=self.model_dtype,\n            memory_format=torch.channels_last if self.channels_last else None,\n        )\n        self.num_classes = self.model.num_classes\n        self.param_count = count_params(self.model)\n        _logger.info('Model %s created, param count: %d' % (model_name, self.param_count))\n\n        data_config = resolve_data_config(kwargs, model=self.model, use_test_size=not use_train_size)\n        self.input_size = data_config['input_size']\n        self.batch_size = kwargs.pop('batch_size', 256)\n\n        self.compiled = False\n        if torchscript:\n            self.model = torch.jit.script(self.model)\n            self.compiled = True\n        elif torchcompile:\n            assert has_compile, 'A version of torch w/ torch.compile() is required, possibly a nightly.'\n            torch._dynamo.reset()\n            self.model = torch.compile(self.model, backend=torchcompile, mode=torchcompile_mode)\n            self.compiled = True\n        elif aot_autograd:\n            assert has_functorch, \"functorch is needed for --aot-autograd\"\n            self.model = memory_efficient_fusion(self.model)\n            self.compiled = True\n\n        self.example_inputs = None\n        self.num_warm_iter = num_warm_iter\n        self.num_bench_iter = num_bench_iter\n        self.log_freq = num_bench_iter // 5\n        if 'cuda' in self.device:\n            self.time_fn = partial(cuda_timestamp, device=self.device)\n        else:\n            self.time_fn = timestamp\n\n    def _init_input(self):\n        self.example_inputs = torch.randn(\n            (self.batch_size,) + self.input_size, device=self.device, dtype=self.data_dtype)\n        if self.channels_last:\n            self.example_inputs = self.example_inputs.contiguous(memory_format=torch.channels_last)\n\n\nclass InferenceBenchmarkRunner(BenchmarkRunner):\n\n    def __init__(\n            self,\n            model_name,\n            device='cuda',\n            torchscript=False,\n            **kwargs\n    ):\n        super().__init__(model_name=model_name, device=device, torchscript=torchscript, **kwargs)\n        self.model.eval()\n\n    def run(self):\n        def _step():\n            t_step_start = self.time_fn()\n            with self.amp_autocast():\n                output = self.model(self.example_inputs)\n            t_step_end = self.time_fn(True)\n            return t_step_end - t_step_start\n\n        _logger.info(\n            f'Running inference benchmark on {self.model_name} for {self.num_bench_iter} steps w/ '\n            f'input size {self.input_size} and batch size {self.batch_size}.')\n\n        with torch.no_grad():\n            self._init_input()\n\n            for _ in range(self.num_warm_iter):\n                _step()\n\n            total_step = 0.\n            num_samples = 0\n            t_run_start = self.time_fn()\n            for i in range(self.num_bench_iter):\n                delta_fwd = _step()\n                total_step += delta_fwd\n                num_samples += self.batch_size\n                num_steps = i + 1\n                if num_steps % self.log_freq == 0:\n                    _logger.info(\n                        f\"Infer [{num_steps}/{self.num_bench_iter}].\"\n                        f\" {num_samples / total_step:0.2f} samples/sec.\"\n                        f\" {1000 * total_step / num_steps:0.3f} ms/step.\")\n            t_run_end = self.time_fn(True)\n            t_run_elapsed = t_run_end - t_run_start\n\n        results = dict(\n            samples_per_sec=round(num_samples / t_run_elapsed, 2),\n            step_time=round(1000 * total_step / self.num_bench_iter, 3),\n            batch_size=self.batch_size,\n            img_size=self.input_size[-1],\n            param_count=round(self.param_count / 1e6, 2),\n        )\n\n        retries = 0 if self.compiled else 2  # skip profiling if model is scripted\n        while retries:\n            retries -= 1\n            try:\n                if has_deepspeed_profiling:\n                    macs, _ = profile_deepspeed(self.model, self.input_size)\n                    results['gmacs'] = round(macs / 1e9, 2)\n                elif has_fvcore_profiling:\n                    macs, activations = profile_fvcore(self.model, self.input_size, force_cpu=not retries)\n                    results['gmacs'] = round(macs / 1e9, 2)\n                    results['macts'] = round(activations / 1e6, 2)\n            except RuntimeError as e:\n                pass\n\n        _logger.info(\n            f\"Inference benchmark of {self.model_name} done. \"\n            f\"{results['samples_per_sec']:.2f} samples/sec, {results['step_time']:.2f} ms/step\")\n\n        return results\n\n\nclass TrainBenchmarkRunner(BenchmarkRunner):\n\n    def __init__(\n            self,\n            model_name,\n            device='cuda',\n            torchscript=False,\n            **kwargs\n    ):\n        super().__init__(model_name=model_name, device=device, torchscript=torchscript, **kwargs)\n        self.model.train()\n\n        self.loss = nn.CrossEntropyLoss().to(self.device)\n        self.target_shape = tuple()\n\n        self.optimizer = create_optimizer_v2(\n            self.model,\n            opt=kwargs.pop('opt', 'sgd'),\n            lr=kwargs.pop('lr', 1e-4))\n\n        if kwargs.pop('grad_checkpointing', False):\n            self.model.set_grad_checkpointing()\n\n    def _gen_target(self, batch_size):\n        return torch.empty(\n            (batch_size,) + self.target_shape, device=self.device, dtype=torch.long).random_(self.num_classes)\n\n    def run(self):\n        def _step(detail=False):\n            self.optimizer.zero_grad()  # can this be ignored?\n            t_start = self.time_fn()\n            t_fwd_end = t_start\n            t_bwd_end = t_start\n            with self.amp_autocast():\n                output = self.model(self.example_inputs)\n                if isinstance(output, tuple):\n                    output = output[0]\n                if detail:\n                    t_fwd_end = self.time_fn(True)\n                target = self._gen_target(output.shape[0])\n                self.loss(output, target).backward()\n                if detail:\n                    t_bwd_end = self.time_fn(True)\n            self.optimizer.step()\n            t_end = self.time_fn(True)\n            if detail:\n                delta_fwd = t_fwd_end - t_start\n                delta_bwd = t_bwd_end - t_fwd_end\n                delta_opt = t_end - t_bwd_end\n                return delta_fwd, delta_bwd, delta_opt\n            else:\n                delta_step = t_end - t_start\n                return delta_step\n\n        _logger.info(\n            f'Running train benchmark on {self.model_name} for {self.num_bench_iter} steps w/ '\n            f'input size {self.input_size} and batch size {self.batch_size}.')\n\n        self._init_input()\n\n        for _ in range(self.num_warm_iter):\n            _step()\n\n        t_run_start = self.time_fn()\n        if self.detail:\n            total_fwd = 0.\n            total_bwd = 0.\n            total_opt = 0.\n            num_samples = 0\n            for i in range(self.num_bench_iter):\n                delta_fwd, delta_bwd, delta_opt = _step(True)\n                num_samples += self.batch_size\n                total_fwd += delta_fwd\n                total_bwd += delta_bwd\n                total_opt += delta_opt\n                num_steps = (i + 1)\n                if num_steps % self.log_freq == 0:\n                    total_step = total_fwd + total_bwd + total_opt\n                    _logger.info(\n                        f\"Train [{num_steps}/{self.num_bench_iter}].\"\n                        f\" {num_samples / total_step:0.2f} samples/sec.\"\n                        f\" {1000 * total_fwd / num_steps:0.3f} ms/step fwd,\"\n                        f\" {1000 * total_bwd / num_steps:0.3f} ms/step bwd,\"\n                        f\" {1000 * total_opt / num_steps:0.3f} ms/step opt.\"\n                    )\n            total_step = total_fwd + total_bwd + total_opt\n            t_run_elapsed = self.time_fn() - t_run_start\n            results = dict(\n                samples_per_sec=round(num_samples / t_run_elapsed, 2),\n                step_time=round(1000 * total_step / self.num_bench_iter, 3),\n                fwd_time=round(1000 * total_fwd / self.num_bench_iter, 3),\n                bwd_time=round(1000 * total_bwd / self.num_bench_iter, 3),\n                opt_time=round(1000 * total_opt / self.num_bench_iter, 3),\n                batch_size=self.batch_size,\n                img_size=self.input_size[-1],\n                param_count=round(self.param_count / 1e6, 2),\n            )\n        else:\n            total_step = 0.\n            num_samples = 0\n            for i in range(self.num_bench_iter):\n                delta_step = _step(False)\n                num_samples += self.batch_size\n                total_step += delta_step\n                num_steps = (i + 1)\n                if num_steps % self.log_freq == 0:\n                    _logger.info(\n                        f\"Train [{num_steps}/{self.num_bench_iter}].\"\n                        f\" {num_samples / total_step:0.2f} samples/sec.\"\n                        f\" {1000 * total_step / num_steps:0.3f} ms/step.\")\n            t_run_elapsed = self.time_fn() - t_run_start\n            results = dict(\n                samples_per_sec=round(num_samples / t_run_elapsed, 2),\n                step_time=round(1000 * total_step / self.num_bench_iter, 3),\n                batch_size=self.batch_size,\n                img_size=self.input_size[-1],\n                param_count=round(self.param_count / 1e6, 2),\n            )\n\n        _logger.info(\n            f\"Train benchmark of {self.model_name} done. \"\n            f\"{results['samples_per_sec']:.2f} samples/sec, {results['step_time']:.2f} ms/sample\")\n\n        return results\n\n\nclass ProfileRunner(BenchmarkRunner):\n\n    def __init__(self, model_name, device='cuda', profiler='', **kwargs):\n        super().__init__(model_name=model_name, device=device, **kwargs)\n        if not profiler:\n            if has_deepspeed_profiling:\n                profiler = 'deepspeed'\n            elif has_fvcore_profiling:\n                profiler = 'fvcore'\n        assert profiler, \"One of deepspeed or fvcore needs to be installed for profiling to work.\"\n        self.profiler = profiler\n        self.model.eval()\n\n    def run(self):\n        _logger.info(\n            f'Running profiler on {self.model_name} w/ '\n            f'input size {self.input_size} and batch size {self.batch_size}.')\n\n        macs = 0\n        activations = 0\n        if self.profiler == 'deepspeed':\n            macs, _ = profile_deepspeed(self.model, self.input_size, batch_size=self.batch_size, detailed=True)\n        elif self.profiler == 'fvcore':\n            macs, activations = profile_fvcore(self.model, self.input_size, batch_size=self.batch_size, detailed=True)\n\n        results = dict(\n            gmacs=round(macs / 1e9, 2),\n            macts=round(activations / 1e6, 2),\n            batch_size=self.batch_size,\n            img_size=self.input_size[-1],\n            param_count=round(self.param_count / 1e6, 2),\n        )\n\n        _logger.info(\n            f\"Profile of {self.model_name} done. \"\n            f\"{results['gmacs']:.2f} GMACs, {results['param_count']:.2f} M params.\")\n\n        return results\n\n\ndef _try_run(\n        model_name,\n        bench_fn,\n        bench_kwargs,\n        initial_batch_size,\n        no_batch_size_retry=False\n):\n    batch_size = initial_batch_size\n    results = dict()\n    error_str = 'Unknown'\n    while batch_size:\n        try:\n            torch.cuda.empty_cache()\n            bench = bench_fn(model_name=model_name, batch_size=batch_size, **bench_kwargs)\n            results = bench.run()\n            return results\n        except RuntimeError as e:\n            error_str = str(e)\n            _logger.error(f'\"{error_str}\" while running benchmark.')\n            if not check_batch_size_retry(error_str):\n                _logger.error(f'Unrecoverable error encountered while benchmarking {model_name}, skipping.')\n                break\n            if no_batch_size_retry:\n                break\n        batch_size = decay_batch_step(batch_size)\n        _logger.warning(f'Reducing batch size to {batch_size} for retry.')\n    results['error'] = error_str\n    return results\n\n\ndef benchmark(args):\n    if args.amp:\n        _logger.warning(\"Overriding precision to 'amp' since --amp flag set.\")\n        args.precision = 'amp' if args.amp_dtype == 'float16' else '_'.join(['amp', args.amp_dtype])\n    _logger.info(f'Benchmarking in {args.precision} precision. '\n                 f'{\"NHWC\" if args.channels_last else \"NCHW\"} layout. '\n                 f'torchscript {\"enabled\" if args.torchscript else \"disabled\"}')\n\n    bench_kwargs = vars(args).copy()\n    bench_kwargs.pop('amp')\n    model = bench_kwargs.pop('model')\n    batch_size = bench_kwargs.pop('batch_size')\n\n    bench_fns = (InferenceBenchmarkRunner,)\n    prefixes = ('infer',)\n    if args.bench == 'both':\n        bench_fns = (\n            InferenceBenchmarkRunner,\n            TrainBenchmarkRunner\n        )\n        prefixes = ('infer', 'train')\n    elif args.bench == 'train':\n        bench_fns = TrainBenchmarkRunner,\n        prefixes = 'train',\n    elif args.bench.startswith('profile'):\n        # specific profiler used if included in bench mode string, otherwise default to deepspeed, fallback to fvcore\n        if 'deepspeed' in args.bench:\n            assert has_deepspeed_profiling, \"deepspeed must be installed to use deepspeed flop counter\"\n            bench_kwargs['profiler'] = 'deepspeed'\n        elif 'fvcore' in args.bench:\n            assert has_fvcore_profiling, \"fvcore must be installed to use fvcore flop counter\"\n            bench_kwargs['profiler'] = 'fvcore'\n        bench_fns = ProfileRunner,\n        batch_size = 1\n\n    model_results = OrderedDict(model=model)\n    for prefix, bench_fn in zip(prefixes, bench_fns):\n        run_results = _try_run(\n            model,\n            bench_fn,\n            bench_kwargs=bench_kwargs,\n            initial_batch_size=batch_size,\n            no_batch_size_retry=args.no_retry,\n        )\n        if prefix and 'error' not in run_results:\n            run_results = {'_'.join([prefix, k]): v for k, v in run_results.items()}\n        model_results.update(run_results)\n        if 'error' in run_results:\n            break\n    if 'error' not in model_results:\n        param_count = model_results.pop('infer_param_count', model_results.pop('train_param_count', 0))\n        model_results.setdefault('param_count', param_count)\n        model_results.pop('train_param_count', 0)\n    return model_results\n\n\ndef main():\n    setup_default_logging()\n    args = parser.parse_args()\n    model_cfgs = []\n    model_names = []\n\n    if args.fast_norm:\n        set_fast_norm()\n\n    if args.model_list:\n        args.model = ''\n        with open(args.model_list) as f:\n            model_names = [line.rstrip() for line in f]\n        model_cfgs = [(n, None) for n in model_names]\n    elif args.model == 'all':\n        # validate all models in a list of names with pretrained checkpoints\n        args.pretrained = True\n        model_names = list_models(pretrained=True, exclude_filters=['*in21k'])\n        model_cfgs = [(n, None) for n in model_names]\n    elif not is_model(args.model):\n        # model name doesn't exist, try as wildcard filter\n        model_names = list_models(args.model)\n        model_cfgs = [(n, None) for n in model_names]\n\n    if len(model_cfgs):\n        _logger.info('Running bulk validation on these pretrained models: {}'.format(', '.join(model_names)))\n        results = []\n        try:\n            for m, _ in model_cfgs:\n                if not m:\n                    continue\n                args.model = m\n                r = benchmark(args)\n                if r:\n                    results.append(r)\n                time.sleep(10)\n        except KeyboardInterrupt as e:\n            pass\n        sort_key = 'infer_samples_per_sec'\n        if 'train' in args.bench:\n            sort_key = 'train_samples_per_sec'\n        elif 'profile' in args.bench:\n            sort_key = 'infer_gmacs'\n        results = filter(lambda x: sort_key in x, results)\n        results = sorted(results, key=lambda x: x[sort_key], reverse=True)\n    else:\n        results = benchmark(args)\n\n    if args.results_file:\n        write_results(args.results_file, results, format=args.results_format)\n\n    # output results in JSON to stdout w/ delimiter for runner script\n    print(f'--result\\n{json.dumps(results, indent=4)}')\n\n\ndef write_results(results_file, results, format='csv'):\n    with open(results_file, mode='w') as cf:\n        if format == 'json':\n            json.dump(results, cf, indent=4)\n        else:\n            if not isinstance(results, (list, tuple)):\n                results = [results]\n            if not results:\n                return\n            dw = csv.DictWriter(cf, fieldnames=results[0].keys())\n            dw.writeheader()\n            for r in results:\n                dw.writerow(r)\n            cf.flush()\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "bulk_runner.py",
          "type": "blob",
          "size": 8.41,
          "content": "#!/usr/bin/env python3\n\"\"\" Bulk Model Script Runner\n\nRun validation or benchmark script in separate process for each model\n\nBenchmark all 'vit*' models:\npython bulk_runner.py  --model-list 'vit*' --results-file vit_bench.csv benchmark.py --amp -b 512\n\nValidate all models:\npython bulk_runner.py  --model-list all --results-file val.csv --pretrained validate.py --data-dir /imagenet/validation/ --amp -b 512 --retry\n\nHacked together by Ross Wightman (https://github.com/rwightman)\n\"\"\"\nimport argparse\nimport os\nimport sys\nimport csv\nimport json\nimport subprocess\nimport time\nfrom typing import Callable, List, Tuple, Union\n\n\nfrom timm.models import is_model, list_models, get_pretrained_cfg, get_arch_pretrained_cfgs\n\n\nparser = argparse.ArgumentParser(description='Per-model process launcher')\n\n# model and results args\nparser.add_argument(\n    '--model-list', metavar='NAME', default='',\n    help='txt file based list of model names to benchmark')\nparser.add_argument(\n    '--results-file', default='', type=str, metavar='FILENAME',\n    help='Output csv file for validation results (summary)')\nparser.add_argument(\n    '--sort-key', default='', type=str, metavar='COL',\n    help='Specify sort key for results csv')\nparser.add_argument(\n    \"--pretrained\", action='store_true',\n    help=\"only run models with pretrained weights\")\n\nparser.add_argument(\n    \"--delay\",\n    type=float,\n    default=0,\n    help=\"Interval, in seconds, to delay between model invocations.\",\n)\nparser.add_argument(\n    \"--start_method\", type=str, default=\"spawn\", choices=[\"spawn\", \"fork\", \"forkserver\"],\n    help=\"Multiprocessing start method to use when creating workers.\",\n)\nparser.add_argument(\n    \"--no_python\",\n    help=\"Skip prepending the script with 'python' - just execute it directly. Useful \"\n         \"when the script is not a Python script.\",\n)\nparser.add_argument(\n    \"-m\",\n    \"--module\",\n    help=\"Change each process to interpret the launch script as a Python module, executing \"\n         \"with the same behavior as 'python -m'.\",\n)\n\n# positional\nparser.add_argument(\n    \"script\", type=str,\n    help=\"Full path to the program/script to be launched for each model config.\",\n)\nparser.add_argument(\"script_args\", nargs=argparse.REMAINDER)\n\n\ndef cmd_from_args(args) -> Tuple[Union[Callable, str], List[str]]:\n    # If ``args`` not passed, defaults to ``sys.argv[:1]``\n    with_python = not args.no_python\n    cmd: Union[Callable, str]\n    cmd_args = []\n    if with_python:\n        cmd = os.getenv(\"PYTHON_EXEC\", sys.executable)\n        cmd_args.append(\"-u\")\n        if args.module:\n            cmd_args.append(\"-m\")\n        cmd_args.append(args.script)\n    else:\n        if args.module:\n            raise ValueError(\n                \"Don't use both the '--no_python' flag\"\n                \" and the '--module' flag at the same time.\"\n            )\n        cmd = args.script\n    cmd_args.extend(args.script_args)\n\n    return cmd, cmd_args\n\n\ndef _get_model_cfgs(\n        model_names,\n        num_classes=None,\n        expand_train_test=False,\n        include_crop=True,\n        expand_arch=False,\n):\n    model_cfgs = set()\n\n    for name in model_names:\n        if expand_arch:\n            pt_cfgs = get_arch_pretrained_cfgs(name).values()\n        else:\n            pt_cfg = get_pretrained_cfg(name)\n            pt_cfgs = [pt_cfg] if pt_cfg is not None else []\n\n        for cfg in pt_cfgs:\n            if cfg.input_size is None:\n                continue\n            if num_classes is not None and getattr(cfg, 'num_classes', 0) != num_classes:\n                continue\n\n            # Add main configuration\n            size = cfg.input_size[-1]\n            if include_crop:\n                model_cfgs.add((name, size, cfg.crop_pct))\n            else:\n                model_cfgs.add((name, size))\n\n            # Add test configuration if required\n            if expand_train_test and cfg.test_input_size is not None:\n                test_size = cfg.test_input_size[-1]\n                if include_crop:\n                    test_crop = cfg.test_crop_pct or cfg.crop_pct\n                    model_cfgs.add((name, test_size, test_crop))\n                else:\n                    model_cfgs.add((name, test_size))\n\n    # Format the output\n    if include_crop:\n        return [(n, {'img-size': r, 'crop-pct': cp}) for n, r, cp in sorted(model_cfgs)]\n    else:\n        return [(n, {'img-size': r}) for n, r in sorted(model_cfgs)]\n\n\ndef main():\n    args = parser.parse_args()\n    cmd, cmd_args = cmd_from_args(args)\n\n    model_cfgs = []\n    if args.model_list == 'all':\n        model_names = list_models(\n            pretrained=args.pretrained,  # only include models w/ pretrained checkpoints if set\n        )\n        model_cfgs = [(n, None) for n in model_names]\n    elif args.model_list == 'all_in1k':\n        model_names = list_models(pretrained=True)\n        model_cfgs = _get_model_cfgs(model_names, num_classes=1000, expand_train_test=True)\n    elif args.model_list == 'all_res':\n        model_names = list_models()\n        model_cfgs = _get_model_cfgs(model_names, expand_train_test=True, include_crop=False, expand_arch=True)\n    elif not is_model(args.model_list):\n        # model name doesn't exist, try as wildcard filter\n        model_names = list_models(args.model_list)\n        model_cfgs = [(n, None) for n in model_names]\n\n    if not model_cfgs and os.path.exists(args.model_list):\n        with open(args.model_list) as f:\n            model_names = [line.rstrip() for line in f]\n            model_cfgs = _get_model_cfgs(\n                model_names,\n                #num_classes=1000,\n                expand_train_test=True,\n                #include_crop=False,\n            )\n\n    if len(model_cfgs):\n        results_file = args.results_file or './results.csv'\n        results = []\n        errors = []\n        model_strings = '\\n'.join([f'{x[0]}, {x[1]}' for x in model_cfgs])\n        print(f\"Running script on these models:\\n {model_strings}\")\n        if not args.sort_key:\n            if 'benchmark' in args.script:\n                if any(['train' in a for a in args.script_args]):\n                    sort_key = 'train_samples_per_sec'\n                else:\n                    sort_key = 'infer_samples_per_sec'\n            else:\n                sort_key = 'top1'\n        else:\n            sort_key = args.sort_key\n        print(f'Script: {args.script}, Args: {args.script_args}, Sort key: {sort_key}')\n\n        try:\n            for m, ax in model_cfgs:\n                if not m:\n                    continue\n                args_str = (cmd, *[str(e) for e in cmd_args], '--model', m)\n                if ax is not None:\n                    extra_args = [(f'--{k}', str(v)) for k, v in ax.items()]\n                    extra_args = [i for t in extra_args for i in t]\n                    args_str += tuple(extra_args)\n                try:\n                    o = subprocess.check_output(args=args_str).decode('utf-8').split('--result')[-1]\n                    r = json.loads(o)\n                    results.append(r)\n                except Exception as e:\n                    # FIXME batch_size retry loop is currently done in either validation.py or benchmark.py\n                    # for further robustness (but more overhead), we may want to manage that by looping here...\n                    errors.append(dict(model=m, error=str(e)))\n                if args.delay:\n                    time.sleep(args.delay)\n        except KeyboardInterrupt as e:\n            pass\n\n        errors.extend(list(filter(lambda x: 'error' in x, results)))\n        if errors:\n            print(f'{len(errors)} models had errors during run.')\n            for e in errors:\n                if 'model' in e:\n                    print(f\"\\t {e['model']} ({e.get('error', 'Unknown')})\")\n                else:\n                    print(e)\n\n        results = list(filter(lambda x: 'error' not in x, results))\n\n        no_sortkey = list(filter(lambda x: sort_key not in x, results))\n        if no_sortkey:\n            print(f'{len(no_sortkey)} results missing sort key, skipping sort.')\n        else:\n            results = sorted(results, key=lambda x: x[sort_key], reverse=True)\n\n        if len(results):\n            print(f'{len(results)} models run successfully. Saving results to {results_file}.')\n            write_results(results_file, results)\n\n\ndef write_results(results_file, results):\n    with open(results_file, mode='w') as cf:\n        dw = csv.DictWriter(cf, fieldnames=results[0].keys())\n        dw.writeheader()\n        for r in results:\n            dw.writerow(r)\n        cf.flush()\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "clean_checkpoint.py",
          "type": "blob",
          "size": 4.12,
          "content": "#!/usr/bin/env python3\n\"\"\" Checkpoint Cleaning Script\n\nTakes training checkpoints with GPU tensors, optimizer state, extra dict keys, etc.\nand outputs a CPU  tensor checkpoint with only the `state_dict` along with SHA256\ncalculation for model zoo compatibility.\n\nHacked together by / Copyright 2020 Ross Wightman (https://github.com/rwightman)\n\"\"\"\nimport torch\nimport argparse\nimport os\nimport hashlib\nimport shutil\nimport tempfile\nfrom timm.models import load_state_dict\ntry:\n    import safetensors.torch\n    _has_safetensors = True\nexcept ImportError:\n    _has_safetensors = False\n\nparser = argparse.ArgumentParser(description='PyTorch Checkpoint Cleaner')\nparser.add_argument('--checkpoint', default='', type=str, metavar='PATH',\n                    help='path to latest checkpoint (default: none)')\nparser.add_argument('--output', default='', type=str, metavar='PATH',\n                    help='output path')\nparser.add_argument('--no-use-ema', dest='no_use_ema', action='store_true',\n                    help='use ema version of weights if present')\nparser.add_argument('--no-hash', dest='no_hash', action='store_true',\n                    help='no hash in output filename')\nparser.add_argument('--clean-aux-bn', dest='clean_aux_bn', action='store_true',\n                    help='remove auxiliary batch norm layers (from SplitBN training) from checkpoint')\nparser.add_argument('--safetensors', action='store_true',\n                    help='Save weights using safetensors instead of the default torch way (pickle).')\n\n\ndef main():\n    args = parser.parse_args()\n\n    if os.path.exists(args.output):\n        print(\"Error: Output filename ({}) already exists.\".format(args.output))\n        exit(1)\n\n    clean_checkpoint(\n        args.checkpoint,\n        args.output,\n        not args.no_use_ema,\n        args.no_hash,\n        args.clean_aux_bn,\n        safe_serialization=args.safetensors,\n    )\n\n\ndef clean_checkpoint(\n        checkpoint,\n        output,\n        use_ema=True,\n        no_hash=False,\n        clean_aux_bn=False,\n        safe_serialization: bool=False,\n):\n    # Load an existing checkpoint to CPU, strip everything but the state_dict and re-save\n    if checkpoint and os.path.isfile(checkpoint):\n        print(\"=> Loading checkpoint '{}'\".format(checkpoint))\n        state_dict = load_state_dict(checkpoint, use_ema=use_ema)\n        new_state_dict = {}\n        for k, v in state_dict.items():\n            if clean_aux_bn and 'aux_bn' in k:\n                # If all aux_bn keys are removed, the SplitBN layers will end up as normal and\n                # load with the unmodified model using BatchNorm2d.\n                continue\n            name = k[7:] if k.startswith('module.') else k\n            new_state_dict[name] = v\n        print(\"=> Loaded state_dict from '{}'\".format(checkpoint))\n\n        ext = ''\n        if output:\n            checkpoint_root, checkpoint_base = os.path.split(output)\n            checkpoint_base, ext = os.path.splitext(checkpoint_base)\n        else:\n            checkpoint_root = ''\n            checkpoint_base = os.path.split(checkpoint)[1]\n            checkpoint_base = os.path.splitext(checkpoint_base)[0]\n\n        temp_filename = '__' + checkpoint_base\n        if safe_serialization:\n            assert _has_safetensors, \"`pip install safetensors` to use .safetensors\"\n            safetensors.torch.save_file(new_state_dict, temp_filename)\n        else:\n            torch.save(new_state_dict, temp_filename)\n\n        with open(temp_filename, 'rb') as f:\n            sha_hash = hashlib.sha256(f.read()).hexdigest()\n\n        if ext:\n            final_ext = ext\n        else:\n            final_ext = ('.safetensors' if safe_serialization else '.pth')\n\n        if no_hash:\n            final_filename = checkpoint_base + final_ext\n        else:\n            final_filename = '-'.join([checkpoint_base, sha_hash[:8]]) + final_ext\n\n        shutil.move(temp_filename, os.path.join(checkpoint_root, final_filename))\n        print(\"=> Saved state_dict to '{}, SHA256: {}'\".format(final_filename, sha_hash))\n        return final_filename\n    else:\n        print(\"Error: Checkpoint ({}) doesn't exist\".format(checkpoint))\n        return ''\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "convert",
          "type": "tree",
          "content": null
        },
        {
          "name": "distributed_train.sh",
          "type": "blob",
          "size": 0.08,
          "content": "#!/bin/bash\nNUM_PROC=$1\nshift\ntorchrun --nproc_per_node=$NUM_PROC train.py \"$@\"\n\n"
        },
        {
          "name": "hfdocs",
          "type": "tree",
          "content": null
        },
        {
          "name": "hubconf.py",
          "type": "blob",
          "size": 0.09,
          "content": "dependencies = ['torch']\nimport timm\nglobals().update(timm.models._registry._model_entrypoints)\n"
        },
        {
          "name": "inference.py",
          "type": "blob",
          "size": 15.55,
          "content": "#!/usr/bin/env python3\n\"\"\"PyTorch Inference Script\n\nAn example inference script that outputs top-k class ids for images in a folder into a csv.\n\nHacked together by / Copyright 2020 Ross Wightman (https://github.com/rwightman)\n\"\"\"\nimport argparse\nimport json\nimport logging\nimport os\nimport time\nfrom contextlib import suppress\nfrom functools import partial\n\nimport numpy as np\nimport pandas as pd\nimport torch\n\nfrom timm.data import create_dataset, create_loader, resolve_data_config, ImageNetInfo, infer_imagenet_subset\nfrom timm.layers import apply_test_time_pool\nfrom timm.models import create_model\nfrom timm.utils import AverageMeter, setup_default_logging, set_jit_fuser, ParseKwargs\n\ntry:\n    from apex import amp\n    has_apex = True\nexcept ImportError:\n    has_apex = False\n\ntry:\n    from functorch.compile import memory_efficient_fusion\n    has_functorch = True\nexcept ImportError as e:\n    has_functorch = False\n\nhas_compile = hasattr(torch, 'compile')\n\n\n_FMT_EXT = {\n    'json': '.json',\n    'json-record': '.json',\n    'json-split': '.json',\n    'parquet': '.parquet',\n    'csv': '.csv',\n}\n\ntorch.backends.cudnn.benchmark = True\n_logger = logging.getLogger('inference')\n\n\nparser = argparse.ArgumentParser(description='PyTorch ImageNet Inference')\nparser.add_argument('data', nargs='?', metavar='DIR', const=None,\n                    help='path to dataset (*deprecated*, use --data-dir)')\nparser.add_argument('--data-dir', metavar='DIR',\n                    help='path to dataset (root dir)')\nparser.add_argument('--dataset', metavar='NAME', default='',\n                    help='dataset type + name (\"<type>/<name>\") (default: ImageFolder or ImageTar if empty)')\nparser.add_argument('--split', metavar='NAME', default='validation',\n                    help='dataset split (default: validation)')\nparser.add_argument('--model', '-m', metavar='MODEL', default='resnet50',\n                    help='model architecture (default: resnet50)')\nparser.add_argument('-j', '--workers', default=2, type=int, metavar='N',\n                    help='number of data loading workers (default: 2)')\nparser.add_argument('-b', '--batch-size', default=256, type=int,\n                    metavar='N', help='mini-batch size (default: 256)')\nparser.add_argument('--img-size', default=None, type=int,\n                    metavar='N', help='Input image dimension, uses model default if empty')\nparser.add_argument('--in-chans', type=int, default=None, metavar='N',\n                    help='Image input channels (default: None => 3)')\nparser.add_argument('--input-size', default=None, nargs=3, type=int,\n                    metavar='N N N', help='Input all image dimensions (d h w, e.g. --input-size 3 224 224), uses model default if empty')\nparser.add_argument('--use-train-size', action='store_true', default=False,\n                    help='force use of train input size, even when test size is specified in pretrained cfg')\nparser.add_argument('--crop-pct', default=None, type=float,\n                    metavar='N', help='Input image center crop pct')\nparser.add_argument('--crop-mode', default=None, type=str,\n                    metavar='N', help='Input image crop mode (squash, border, center). Model default if None.')\nparser.add_argument('--mean', type=float, nargs='+', default=None, metavar='MEAN',\n                    help='Override mean pixel value of dataset')\nparser.add_argument('--std', type=float,  nargs='+', default=None, metavar='STD',\n                    help='Override std deviation of of dataset')\nparser.add_argument('--interpolation', default='', type=str, metavar='NAME',\n                    help='Image resize interpolation type (overrides model)')\nparser.add_argument('--num-classes', type=int, default=None,\n                    help='Number classes in dataset')\nparser.add_argument('--class-map', default='', type=str, metavar='FILENAME',\n                    help='path to class to idx mapping file (default: \"\")')\nparser.add_argument('--log-freq', default=10, type=int,\n                    metavar='N', help='batch logging frequency (default: 10)')\nparser.add_argument('--checkpoint', default='', type=str, metavar='PATH',\n                    help='path to latest checkpoint (default: none)')\nparser.add_argument('--pretrained', dest='pretrained', action='store_true',\n                    help='use pre-trained model')\nparser.add_argument('--num-gpu', type=int, default=1,\n                    help='Number of GPUS to use')\nparser.add_argument('--test-pool', dest='test_pool', action='store_true',\n                    help='enable test time pool')\nparser.add_argument('--channels-last', action='store_true', default=False,\n                    help='Use channels_last memory layout')\nparser.add_argument('--device', default='cuda', type=str,\n                    help=\"Device (accelerator) to use.\")\nparser.add_argument('--amp', action='store_true', default=False,\n                    help='use Native AMP for mixed precision training')\nparser.add_argument('--amp-dtype', default='float16', type=str,\n                    help='lower precision AMP dtype (default: float16)')\nparser.add_argument('--fuser', default='', type=str,\n                    help=\"Select jit fuser. One of ('', 'te', 'old', 'nvfuser')\")\nparser.add_argument('--model-kwargs', nargs='*', default={}, action=ParseKwargs)\nparser.add_argument('--torchcompile-mode', type=str, default=None,\n                    help=\"torch.compile mode (default: None).\")\n\nscripting_group = parser.add_mutually_exclusive_group()\nscripting_group.add_argument('--torchscript', default=False, action='store_true',\n                             help='torch.jit.script the full model')\nscripting_group.add_argument('--torchcompile', nargs='?', type=str, default=None, const='inductor',\n                             help=\"Enable compilation w/ specified backend (default: inductor).\")\nscripting_group.add_argument('--aot-autograd', default=False, action='store_true',\n                             help=\"Enable AOT Autograd support.\")\n\nparser.add_argument('--results-dir', type=str, default=None,\n                    help='folder for output results')\nparser.add_argument('--results-file', type=str, default=None,\n                    help='results filename (relative to results-dir)')\nparser.add_argument('--results-format', type=str, nargs='+', default=['csv'],\n                    help='results format (one of \"csv\", \"json\", \"json-split\", \"parquet\")')\nparser.add_argument('--results-separate-col', action='store_true', default=False,\n                    help='separate output columns per result index.')\nparser.add_argument('--topk', default=1, type=int,\n                    metavar='N', help='Top-k to output to CSV')\nparser.add_argument('--fullname', action='store_true', default=False,\n                    help='use full sample name in output (not just basename).')\nparser.add_argument('--filename-col', type=str, default='filename',\n                    help='name for filename / sample name column')\nparser.add_argument('--index-col', type=str, default='index',\n                    help='name for output indices column(s)')\nparser.add_argument('--label-col', type=str, default='label',\n                    help='name for output indices column(s)')\nparser.add_argument('--output-col', type=str, default=None,\n                    help='name for logit/probs output column(s)')\nparser.add_argument('--output-type', type=str, default='prob',\n                    help='output type colum (\"prob\" for probabilities, \"logit\" for raw logits)')\nparser.add_argument('--label-type', type=str, default='description',\n                    help='type of label to output, one of  \"none\", \"name\", \"description\", \"detailed\"')\nparser.add_argument('--include-index', action='store_true', default=False,\n                    help='include the class index in results')\nparser.add_argument('--exclude-output', action='store_true', default=False,\n                    help='exclude logits/probs from results, just indices. topk must be set !=0.')\n\n\ndef main():\n    setup_default_logging()\n    args = parser.parse_args()\n    # might as well try to do something useful...\n    args.pretrained = args.pretrained or not args.checkpoint\n\n    if torch.cuda.is_available():\n        torch.backends.cuda.matmul.allow_tf32 = True\n        torch.backends.cudnn.benchmark = True\n\n    device = torch.device(args.device)\n\n    # resolve AMP arguments based on PyTorch / Apex availability\n    amp_autocast = suppress\n    if args.amp:\n        assert args.amp_dtype in ('float16', 'bfloat16')\n        amp_dtype = torch.bfloat16 if args.amp_dtype == 'bfloat16' else torch.float16\n        amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)\n        _logger.info('Running inference in mixed precision with native PyTorch AMP.')\n    else:\n        _logger.info('Running inference in float32. AMP not enabled.')\n\n    if args.fuser:\n        set_jit_fuser(args.fuser)\n\n    # create model\n    in_chans = 3\n    if args.in_chans is not None:\n        in_chans = args.in_chans\n    elif args.input_size is not None:\n        in_chans = args.input_size[0]\n\n    model = create_model(\n        args.model,\n        num_classes=args.num_classes,\n        in_chans=in_chans,\n        pretrained=args.pretrained,\n        checkpoint_path=args.checkpoint,\n        **args.model_kwargs,\n    )\n    if args.num_classes is None:\n        assert hasattr(model, 'num_classes'), 'Model must have `num_classes` attr if not set on cmd line/config.'\n        args.num_classes = model.num_classes\n\n    _logger.info(\n        f'Model {args.model} created, param count: {sum([m.numel() for m in model.parameters()])}')\n\n    data_config = resolve_data_config(vars(args), model=model)\n    test_time_pool = False\n    if args.test_pool:\n        model, test_time_pool = apply_test_time_pool(model, data_config)\n\n    model = model.to(device)\n    model.eval()\n    if args.channels_last:\n        model = model.to(memory_format=torch.channels_last)\n\n    if args.torchscript:\n        model = torch.jit.script(model)\n    elif args.torchcompile:\n        assert has_compile, 'A version of torch w/ torch.compile() is required for --compile, possibly a nightly.'\n        torch._dynamo.reset()\n        model = torch.compile(model, backend=args.torchcompile, mode=args.torchcompile_mode)\n    elif args.aot_autograd:\n        assert has_functorch, \"functorch is needed for --aot-autograd\"\n        model = memory_efficient_fusion(model)\n\n    if args.num_gpu > 1:\n        model = torch.nn.DataParallel(model, device_ids=list(range(args.num_gpu)))\n\n    root_dir = args.data or args.data_dir\n    dataset = create_dataset(\n        root=root_dir,\n        name=args.dataset,\n        split=args.split,\n        class_map=args.class_map,\n    )\n\n    if test_time_pool:\n        data_config['crop_pct'] = 1.0\n\n    workers = 1 if 'tfds' in args.dataset or 'wds' in args.dataset else args.workers\n    loader = create_loader(\n        dataset,\n        batch_size=args.batch_size,\n        use_prefetcher=True,\n        num_workers=workers,\n        device=device,\n        **data_config,\n    )\n\n    to_label = None\n    if args.label_type in ('name', 'description', 'detail'):\n        imagenet_subset = infer_imagenet_subset(model)\n        if imagenet_subset is not None:\n            dataset_info = ImageNetInfo(imagenet_subset)\n            if args.label_type == 'name':\n                to_label = lambda x: dataset_info.index_to_label_name(x)\n            elif args.label_type == 'detail':\n                to_label = lambda x: dataset_info.index_to_description(x, detailed=True)\n            else:\n                to_label = lambda x: dataset_info.index_to_description(x)\n            to_label = np.vectorize(to_label)\n        else:\n            _logger.error(\"Cannot deduce ImageNet subset from model, no labelling will be performed.\")\n\n    top_k = min(args.topk, args.num_classes)\n    batch_time = AverageMeter()\n    end = time.time()\n    all_indices = []\n    all_labels = []\n    all_outputs = []\n    use_probs = args.output_type == 'prob'\n    with torch.no_grad():\n        for batch_idx, (input, _) in enumerate(loader):\n\n            with amp_autocast():\n                output = model(input)\n\n            if use_probs:\n                output = output.softmax(-1)\n\n            if top_k:\n                output, indices = output.topk(top_k)\n                np_indices = indices.cpu().numpy()\n                if args.include_index:\n                    all_indices.append(np_indices)\n                if to_label is not None:\n                    np_labels = to_label(np_indices)\n                    all_labels.append(np_labels)\n\n            all_outputs.append(output.cpu().numpy())\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if batch_idx % args.log_freq == 0:\n                _logger.info('Predict: [{0}/{1}] Time {batch_time.val:.3f} ({batch_time.avg:.3f})'.format(\n                    batch_idx, len(loader), batch_time=batch_time))\n\n    all_indices = np.concatenate(all_indices, axis=0) if all_indices else None\n    all_labels = np.concatenate(all_labels, axis=0) if all_labels else None\n    all_outputs = np.concatenate(all_outputs, axis=0).astype(np.float32)\n    filenames = loader.dataset.filenames(basename=not args.fullname)\n\n    output_col = args.output_col or ('prob' if use_probs else 'logit')\n    data_dict = {args.filename_col: filenames}\n    if args.results_separate_col and all_outputs.shape[-1] > 1:\n        if all_indices is not None:\n            for i in range(all_indices.shape[-1]):\n                data_dict[f'{args.index_col}_{i}'] = all_indices[:, i]\n        if all_labels is not None:\n            for i in range(all_labels.shape[-1]):\n                data_dict[f'{args.label_col}_{i}'] = all_labels[:, i]\n        for i in range(all_outputs.shape[-1]):\n            data_dict[f'{output_col}_{i}'] = all_outputs[:, i]\n    else:\n        if all_indices is not None:\n            if all_indices.shape[-1] == 1:\n                all_indices = all_indices.squeeze(-1)\n            data_dict[args.index_col] = list(all_indices)\n        if all_labels is not None:\n            if all_labels.shape[-1] == 1:\n                all_labels = all_labels.squeeze(-1)\n            data_dict[args.label_col] = list(all_labels)\n        if all_outputs.shape[-1] == 1:\n            all_outputs = all_outputs.squeeze(-1)\n        data_dict[output_col] = list(all_outputs)\n\n    df = pd.DataFrame(data=data_dict)\n\n    results_filename = args.results_file\n    if results_filename:\n        filename_no_ext, ext = os.path.splitext(results_filename)\n        if ext and ext in _FMT_EXT.values():\n            # if filename provided with one of expected ext,\n            # remove it as it will be added back\n            results_filename = filename_no_ext\n    else:\n        # base default filename on model name + img-size\n        img_size = data_config[\"input_size\"][1]\n        results_filename = f'{args.model}-{img_size}'\n\n    if args.results_dir:\n        results_filename = os.path.join(args.results_dir, results_filename)\n\n    for fmt in args.results_format:\n        save_results(df, results_filename, fmt)\n\n    print(f'--result')\n    print(df.set_index(args.filename_col).to_json(orient='index', indent=4))\n\n\ndef save_results(df, results_filename, results_format='csv', filename_col='filename'):\n    results_filename += _FMT_EXT[results_format]\n    if results_format == 'parquet':\n        df.set_index(filename_col).to_parquet(results_filename)\n    elif results_format == 'json':\n        df.set_index(filename_col).to_json(results_filename, indent=4, orient='index')\n    elif results_format == 'json-records':\n        df.to_json(results_filename, lines=True, orient='records')\n    elif results_format == 'json-split':\n        df.to_json(results_filename, indent=4, orient='split', index=False)\n    else:\n        df.to_csv(results_filename, index=False)\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "onnx_export.py",
          "type": "blob",
          "size": 4.56,
          "content": "\"\"\" ONNX export script\n\nExport PyTorch models as ONNX graphs.\n\nThis export script originally started as an adaptation of code snippets found at\nhttps://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html\n\nThe default parameters work with PyTorch 1.6 and ONNX 1.7 and produce an optimal ONNX graph\nfor hosting in the ONNX runtime (see onnx_validate.py). To export an ONNX model compatible\nwith caffe2 (see caffe2_benchmark.py and caffe2_validate.py), the --keep-init and --aten-fallback\nflags are currently required.\n\nOlder versions of PyTorch/ONNX (tested PyTorch 1.4, ONNX 1.5) do not need extra flags for\ncaffe2 compatibility, but they produce a model that isn't as fast running on ONNX runtime.\n\nMost new release of PyTorch and ONNX cause some sort of breakage in the export / usage of ONNX models.\nPlease do your research and search ONNX and PyTorch issue tracker before asking me. Thanks.\n\nCopyright 2020 Ross Wightman\n\"\"\"\nimport argparse\n\nimport timm\nfrom timm.utils.model import reparameterize_model\nfrom timm.utils.onnx import onnx_export\n\nparser = argparse.ArgumentParser(description='PyTorch ImageNet Validation')\nparser.add_argument('output', metavar='ONNX_FILE',\n                    help='output model filename')\nparser.add_argument('--model', '-m', metavar='MODEL', default='mobilenetv3_large_100',\n                    help='model architecture (default: mobilenetv3_large_100)')\nparser.add_argument('--opset', type=int, default=None,\n                    help='ONNX opset to use (default: 10)')\nparser.add_argument('--keep-init', action='store_true', default=False,\n                    help='Keep initializers as input. Needed for Caffe2 compatible export in newer PyTorch/ONNX.')\nparser.add_argument('--aten-fallback', action='store_true', default=False,\n                    help='Fallback to ATEN ops. Helps fix AdaptiveAvgPool issue with Caffe2 in newer PyTorch/ONNX.')\nparser.add_argument('--dynamic-size', action='store_true', default=False,\n                    help='Export model width dynamic width/height. Not recommended for \"tf\" models with SAME padding.')\nparser.add_argument('--check-forward', action='store_true', default=False,\n                    help='Do a full check of torch vs onnx forward after export.')\nparser.add_argument('-b', '--batch-size', default=1, type=int,\n                    metavar='N', help='mini-batch size (default: 1)')\nparser.add_argument('--img-size', default=None, type=int,\n                    metavar='N', help='Input image dimension, uses model default if empty')\nparser.add_argument('--mean', type=float, nargs='+', default=None, metavar='MEAN',\n                    help='Override mean pixel value of dataset')\nparser.add_argument('--std', type=float,  nargs='+', default=None, metavar='STD',\n                    help='Override std deviation of of dataset')\nparser.add_argument('--num-classes', type=int, default=1000,\n                    help='Number classes in dataset')\nparser.add_argument('--checkpoint', default='', type=str, metavar='PATH',\n                    help='path to checkpoint (default: none)')\nparser.add_argument('--reparam', default=False, action='store_true',\n                    help='Reparameterize model')\nparser.add_argument('--training', default=False, action='store_true',\n                    help='Export in training mode (default is eval)')\nparser.add_argument('--verbose', default=False, action='store_true',\n                    help='Extra stdout output')\nparser.add_argument('--dynamo', default=False, action='store_true',\n                    help='Use torch dynamo export.')\n\ndef main():\n    args = parser.parse_args()\n\n    args.pretrained = True\n    if args.checkpoint:\n        args.pretrained = False\n\n    print(\"==> Creating PyTorch {} model\".format(args.model))\n    # NOTE exportable=True flag disables autofn/jit scripted activations and uses Conv2dSameExport layers\n    # for models using SAME padding\n    model = timm.create_model(\n        args.model,\n        num_classes=args.num_classes,\n        in_chans=3,\n        pretrained=args.pretrained,\n        checkpoint_path=args.checkpoint,\n        exportable=True,\n    )\n\n    if args.reparam:\n        model = reparameterize_model(model)\n\n    onnx_export(\n        model,\n        args.output,\n        opset=args.opset,\n        dynamic_size=args.dynamic_size,\n        aten_fallback=args.aten_fallback,\n        keep_initializers=args.keep_init,\n        check_forward=args.check_forward,\n        training=args.training,\n        verbose=args.verbose,\n        use_dynamo=args.dynamo,\n        input_size=(3, args.img_size, args.img_size),\n        batch_size=args.batch_size,\n    )\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "onnx_validate.py",
          "type": "blob",
          "size": 4.44,
          "content": "\"\"\" ONNX-runtime validation script\n\nThis script was created to verify accuracy and performance of exported ONNX\nmodels running with the onnxruntime. It utilizes the PyTorch dataloader/processing\npipeline for a fair comparison against the originals.\n\nCopyright 2020 Ross Wightman\n\"\"\"\nimport argparse\nimport numpy as np\nimport onnxruntime\nfrom timm.data import create_loader, resolve_data_config, create_dataset\nfrom timm.utils import AverageMeter\nimport time\n\nparser = argparse.ArgumentParser(description='ONNX Validation')\nparser.add_argument('data', metavar='DIR',\n                    help='path to dataset')\nparser.add_argument('--onnx-input', default='', type=str, metavar='PATH',\n                    help='path to onnx model/weights file')\nparser.add_argument('--onnx-output-opt', default='', type=str, metavar='PATH',\n                    help='path to output optimized onnx graph')\nparser.add_argument('--profile', action='store_true', default=False,\n                    help='Enable profiler output.')\nparser.add_argument('-j', '--workers', default=2, type=int, metavar='N',\n                    help='number of data loading workers (default: 2)')\nparser.add_argument('-b', '--batch-size', default=256, type=int,\n                    metavar='N', help='mini-batch size (default: 256)')\nparser.add_argument('--img-size', default=None, type=int,\n                    metavar='N', help='Input image dimension, uses model default if empty')\nparser.add_argument('--mean', type=float, nargs='+', default=None, metavar='MEAN',\n                    help='Override mean pixel value of dataset')\nparser.add_argument('--std', type=float,  nargs='+', default=None, metavar='STD',\n                    help='Override std deviation of of dataset')\nparser.add_argument('--crop-pct', type=float, default=None, metavar='PCT',\n                    help='Override default crop pct of 0.875')\nparser.add_argument('--interpolation', default='', type=str, metavar='NAME',\n                    help='Image resize interpolation type (overrides model)')\nparser.add_argument('--print-freq', '-p', default=10, type=int,\n                    metavar='N', help='print frequency (default: 10)')\n\n\ndef main():\n    args = parser.parse_args()\n    args.gpu_id = 0\n\n    # Set graph optimization level\n    sess_options = onnxruntime.SessionOptions()\n    sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL\n    if args.profile:\n        sess_options.enable_profiling = True\n    if args.onnx_output_opt:\n        sess_options.optimized_model_filepath = args.onnx_output_opt\n\n    session = onnxruntime.InferenceSession(args.onnx_input, sess_options)\n\n    data_config = resolve_data_config(vars(args))\n    loader = create_loader(\n        create_dataset('', args.data),\n        input_size=data_config['input_size'],\n        batch_size=args.batch_size,\n        use_prefetcher=False,\n        interpolation=data_config['interpolation'],\n        mean=data_config['mean'],\n        std=data_config['std'],\n        num_workers=args.workers,\n        crop_pct=data_config['crop_pct']\n    )\n\n    input_name = session.get_inputs()[0].name\n\n    batch_time = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n    end = time.time()\n    for i, (input, target) in enumerate(loader):\n        # run the net and return prediction\n        output = session.run([], {input_name: input.data.numpy()})\n        output = output[0]\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy_np(output, target.numpy())\n        top1.update(prec1.item(), input.size(0))\n        top5.update(prec5.item(), input.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % args.print_freq == 0:\n            print(\n                f'Test: [{i}/{len(loader)}]\\t'\n                f'Time {batch_time.val:.3f} ({batch_time.avg:.3f}, {input.size(0) / batch_time.avg:.3f}/s, '\n                f'{100 * batch_time.avg / input.size(0):.3f} ms/sample) \\t'\n                f'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n                f'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'\n            )\n\n    print(f' * Prec@1 {top1.avg:.3f} ({100-top1.avg:.3f}) Prec@5 {top5.avg:.3f} ({100.-top5.avg:.3f})')\n\n\ndef accuracy_np(output, target):\n    max_indices = np.argsort(output, axis=1)[:, ::-1]\n    top5 = 100 * np.equal(max_indices[:, :5], target[:, np.newaxis]).sum(axis=1).mean()\n    top1 = 100 * np.equal(max_indices[:, 0], target).mean()\n    return top1, top5\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 2.06,
          "content": "[build-system]\nrequires = [\"pdm-backend\"]\nbuild-backend = \"pdm.backend\"\n\n[project]\nname = \"timm\"\nauthors = [\n    {name = \"Ross Wightman\", email = \"ross@huggingface.co\"},\n]\ndescription = \"PyTorch Image Models\"\nreadme = \"README.md\"\nrequires-python = \">=3.8\"\nkeywords = [\"pytorch\", \"image-classification\"]\nlicense = {text = \"Apache-2.0\"}\nclassifiers = [\n        'Development Status :: 5 - Production/Stable',\n        'Intended Audience :: Education',\n        'Intended Audience :: Science/Research',\n        'License :: OSI Approved :: Apache Software License',\n        'Programming Language :: Python :: 3.8',\n        'Programming Language :: Python :: 3.9',\n        'Programming Language :: Python :: 3.10',\n        'Programming Language :: Python :: 3.11',\n        'Programming Language :: Python :: 3.12',\n        'Topic :: Scientific/Engineering',\n        'Topic :: Scientific/Engineering :: Artificial Intelligence',\n        'Topic :: Software Development',\n        'Topic :: Software Development :: Libraries',\n        'Topic :: Software Development :: Libraries :: Python Modules',\n]\ndependencies = [\n        'torch',\n        'torchvision',\n        'pyyaml',\n        'huggingface_hub',\n        'safetensors',\n]\ndynamic = [\"version\"]\n\n[project.urls]\nhomepage = \"https://github.com/huggingface/pytorch-image-models\"\ndocumentation = \"https://huggingface.co/docs/timm/en/index\"\nrepository = \"https://github.com/huggingface/pytorch-image-models\"\n\n[tool.pdm.dev-dependencies]\ntest = [\n        'pytest',\n        'pytest-timeout',\n        'pytest-xdist',\n        'pytest-forked',\n        'expecttest',\n]\n\n[tool.pdm.version]\nsource = \"file\"\npath = \"timm/version.py\"\n\n[tool.pytest.ini_options]\ntestpaths = ['tests']\nmarkers = [\n    \"base: marker for model tests using the basic setup\",\n    \"cfg: marker for model tests checking the config\",\n    \"torchscript: marker for model tests using torchscript\",\n    \"features: marker for model tests checking feature extraction\",\n    \"fxforward: marker for model tests using torch fx (only forward)\",\n    \"fxbackward: marker for model tests using torch fx (only backward)\",\n]"
        },
        {
          "name": "requirements-dev.txt",
          "type": "blob",
          "size": 0.06,
          "content": "pytest\npytest-timeout\npytest-xdist\npytest-forked\nexpecttest\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.07,
          "content": "torch>=1.7\ntorchvision\npyyaml\nhuggingface_hub\nsafetensors>=0.2\nnumpy<2.0\n"
        },
        {
          "name": "results",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.16,
          "content": "[dist_conda]\n\nconda_name_differences = 'torch:pytorch'\nchannels = pytorch\nnoarch = True\n\n[metadata]\n\nurl = \"https://github.com/huggingface/pytorch-image-models\""
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "timm",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 56.67,
          "content": "#!/usr/bin/env python3\n\"\"\" ImageNet Training Script\n\nThis is intended to be a lean and easily modifiable ImageNet training script that reproduces ImageNet\ntraining results with some of the latest networks and training techniques. It favours canonical PyTorch\nand standard Python style over trying to be able to 'do it all.' That said, it offers quite a few speed\nand training result improvements over the usual PyTorch example scripts. Repurpose as you see fit.\n\nThis script was started from an early version of the PyTorch ImageNet example\n(https://github.com/pytorch/examples/tree/master/imagenet)\n\nNVIDIA CUDA specific speedups adopted from NVIDIA Apex examples\n(https://github.com/NVIDIA/apex/tree/master/examples/imagenet)\n\nHacked together by / Copyright 2020 Ross Wightman (https://github.com/rwightman)\n\"\"\"\nimport argparse\nimport copy\nimport importlib\nimport json\nimport logging\nimport os\nimport time\nfrom collections import OrderedDict\nfrom contextlib import suppress\nfrom datetime import datetime\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport torchvision.utils\nimport yaml\nfrom torch.nn.parallel import DistributedDataParallel as NativeDDP\n\nfrom timm import utils\nfrom timm.data import create_dataset, create_loader, resolve_data_config, Mixup, FastCollateMixup, AugMixDataset\nfrom timm.layers import convert_splitbn_model, convert_sync_batchnorm, set_fast_norm\nfrom timm.loss import JsdCrossEntropy, SoftTargetCrossEntropy, BinaryCrossEntropy, LabelSmoothingCrossEntropy\nfrom timm.models import create_model, safe_model_name, resume_checkpoint, load_checkpoint, model_parameters\nfrom timm.optim import create_optimizer_v2, optimizer_kwargs\nfrom timm.scheduler import create_scheduler_v2, scheduler_kwargs\nfrom timm.utils import ApexScaler, NativeScaler\n\ntry:\n    from apex import amp\n    from apex.parallel import DistributedDataParallel as ApexDDP\n    from apex.parallel import convert_syncbn_model\n    has_apex = True\nexcept ImportError:\n    has_apex = False\n\n\ntry:\n    import wandb\n    has_wandb = True\nexcept ImportError:\n    has_wandb = False\n\ntry:\n    from functorch.compile import memory_efficient_fusion\n    has_functorch = True\nexcept ImportError as e:\n    has_functorch = False\n\nhas_compile = hasattr(torch, 'compile')\n\n\n_logger = logging.getLogger('train')\n\n# The first arg parser parses out only the --config argument, this argument is used to\n# load a yaml file containing key-values that override the defaults for the main parser below\nconfig_parser = parser = argparse.ArgumentParser(description='Training Config', add_help=False)\nparser.add_argument('-c', '--config', default='', type=str, metavar='FILE',\n                    help='YAML config file specifying default arguments')\n\n\nparser = argparse.ArgumentParser(description='PyTorch ImageNet Training')\n\n# Dataset parameters\ngroup = parser.add_argument_group('Dataset parameters')\n# Keep this argument outside the dataset group because it is positional.\nparser.add_argument('data', nargs='?', metavar='DIR', const=None,\n                    help='path to dataset (positional is *deprecated*, use --data-dir)')\nparser.add_argument('--data-dir', metavar='DIR',\n                    help='path to dataset (root dir)')\nparser.add_argument('--dataset', metavar='NAME', default='',\n                    help='dataset type + name (\"<type>/<name>\") (default: ImageFolder or ImageTar if empty)')\ngroup.add_argument('--train-split', metavar='NAME', default='train',\n                   help='dataset train split (default: train)')\ngroup.add_argument('--val-split', metavar='NAME', default='validation',\n                   help='dataset validation split (default: validation)')\nparser.add_argument('--train-num-samples', default=None, type=int,\n                    metavar='N', help='Manually specify num samples in train split, for IterableDatasets.')\nparser.add_argument('--val-num-samples', default=None, type=int,\n                    metavar='N', help='Manually specify num samples in validation split, for IterableDatasets.')\ngroup.add_argument('--dataset-download', action='store_true', default=False,\n                   help='Allow download of dataset for torch/ and tfds/ datasets that support it.')\ngroup.add_argument('--class-map', default='', type=str, metavar='FILENAME',\n                   help='path to class to idx mapping file (default: \"\")')\ngroup.add_argument('--input-img-mode', default=None, type=str,\n                   help='Dataset image conversion mode for input images.')\ngroup.add_argument('--input-key', default=None, type=str,\n                   help='Dataset key for input images.')\ngroup.add_argument('--target-key', default=None, type=str,\n                   help='Dataset key for target labels.')\ngroup.add_argument('--dataset-trust-remote-code', action='store_true', default=False,\n                   help='Allow huggingface dataset import to execute code downloaded from the dataset\\'s repo.')\n\n# Model parameters\ngroup = parser.add_argument_group('Model parameters')\ngroup.add_argument('--model', default='resnet50', type=str, metavar='MODEL',\n                   help='Name of model to train (default: \"resnet50\")')\ngroup.add_argument('--pretrained', action='store_true', default=False,\n                   help='Start with pretrained version of specified network (if avail)')\ngroup.add_argument('--pretrained-path', default=None, type=str,\n                   help='Load this checkpoint as if they were the pretrained weights (with adaptation).')\ngroup.add_argument('--initial-checkpoint', default='', type=str, metavar='PATH',\n                   help='Load this checkpoint into model after initialization (default: none)')\ngroup.add_argument('--resume', default='', type=str, metavar='PATH',\n                   help='Resume full model and optimizer state from checkpoint (default: none)')\ngroup.add_argument('--no-resume-opt', action='store_true', default=False,\n                   help='prevent resume of optimizer state when resuming model')\ngroup.add_argument('--num-classes', type=int, default=None, metavar='N',\n                   help='number of label classes (Model default if None)')\ngroup.add_argument('--gp', default=None, type=str, metavar='POOL',\n                   help='Global pool type, one of (fast, avg, max, avgmax, avgmaxc). Model default if None.')\ngroup.add_argument('--img-size', type=int, default=None, metavar='N',\n                   help='Image size (default: None => model default)')\ngroup.add_argument('--in-chans', type=int, default=None, metavar='N',\n                   help='Image input channels (default: None => 3)')\ngroup.add_argument('--input-size', default=None, nargs=3, type=int,\n                   metavar='N N N',\n                   help='Input all image dimensions (d h w, e.g. --input-size 3 224 224), uses model default if empty')\ngroup.add_argument('--crop-pct', default=None, type=float,\n                   metavar='N', help='Input image center crop percent (for validation only)')\ngroup.add_argument('--mean', type=float, nargs='+', default=None, metavar='MEAN',\n                   help='Override mean pixel value of dataset')\ngroup.add_argument('--std', type=float, nargs='+', default=None, metavar='STD',\n                   help='Override std deviation of dataset')\ngroup.add_argument('--interpolation', default='', type=str, metavar='NAME',\n                   help='Image resize interpolation type (overrides model)')\ngroup.add_argument('-b', '--batch-size', type=int, default=128, metavar='N',\n                   help='Input batch size for training (default: 128)')\ngroup.add_argument('-vb', '--validation-batch-size', type=int, default=None, metavar='N',\n                   help='Validation batch size override (default: None)')\ngroup.add_argument('--channels-last', action='store_true', default=False,\n                   help='Use channels_last memory layout')\ngroup.add_argument('--fuser', default='', type=str,\n                   help=\"Select jit fuser. One of ('', 'te', 'old', 'nvfuser')\")\ngroup.add_argument('--grad-accum-steps', type=int, default=1, metavar='N',\n                   help='The number of steps to accumulate gradients (default: 1)')\ngroup.add_argument('--grad-checkpointing', action='store_true', default=False,\n                   help='Enable gradient checkpointing through model blocks/stages')\ngroup.add_argument('--fast-norm', default=False, action='store_true',\n                   help='enable experimental fast-norm')\ngroup.add_argument('--model-kwargs', nargs='*', default={}, action=utils.ParseKwargs)\ngroup.add_argument('--head-init-scale', default=None, type=float,\n                   help='Head initialization scale')\ngroup.add_argument('--head-init-bias', default=None, type=float,\n                   help='Head initialization bias value')\ngroup.add_argument('--torchcompile-mode', type=str, default=None,\n                    help=\"torch.compile mode (default: None).\")\n\n# scripting / codegen\nscripting_group = group.add_mutually_exclusive_group()\nscripting_group.add_argument('--torchscript', dest='torchscript', action='store_true',\n                             help='torch.jit.script the full model')\nscripting_group.add_argument('--torchcompile', nargs='?', type=str, default=None, const='inductor',\n                             help=\"Enable compilation w/ specified backend (default: inductor).\")\n\n# Device & distributed\ngroup = parser.add_argument_group('Device parameters')\ngroup.add_argument('--device', default='cuda', type=str,\n                    help=\"Device (accelerator) to use.\")\ngroup.add_argument('--amp', action='store_true', default=False,\n                   help='use NVIDIA Apex AMP or Native AMP for mixed precision training')\ngroup.add_argument('--amp-dtype', default='float16', type=str,\n                   help='lower precision AMP dtype (default: float16)')\ngroup.add_argument('--amp-impl', default='native', type=str,\n                   help='AMP impl to use, \"native\" or \"apex\" (default: native)')\ngroup.add_argument('--model-dtype', default=None, type=str,\n                   help='Model dtype override (non-AMP) (default: float32)')\ngroup.add_argument('--no-ddp-bb', action='store_true', default=False,\n                   help='Force broadcast buffers for native DDP to off.')\ngroup.add_argument('--synchronize-step', action='store_true', default=False,\n                   help='torch.cuda.synchronize() end of each step')\ngroup.add_argument(\"--local_rank\", default=0, type=int)\nparser.add_argument('--device-modules', default=None, type=str, nargs='+',\n                    help=\"Python imports for device backend modules.\")\n\n# Optimizer parameters\ngroup = parser.add_argument_group('Optimizer parameters')\ngroup.add_argument('--opt', default='sgd', type=str, metavar='OPTIMIZER',\n                   help='Optimizer (default: \"sgd\")')\ngroup.add_argument('--opt-eps', default=None, type=float, metavar='EPSILON',\n                   help='Optimizer Epsilon (default: None, use opt default)')\ngroup.add_argument('--opt-betas', default=None, type=float, nargs='+', metavar='BETA',\n                   help='Optimizer Betas (default: None, use opt default)')\ngroup.add_argument('--momentum', type=float, default=0.9, metavar='M',\n                   help='Optimizer momentum (default: 0.9)')\ngroup.add_argument('--weight-decay', type=float, default=2e-5,\n                   help='weight decay (default: 2e-5)')\ngroup.add_argument('--clip-grad', type=float, default=None, metavar='NORM',\n                   help='Clip gradient norm (default: None, no clipping)')\ngroup.add_argument('--clip-mode', type=str, default='norm',\n                   help='Gradient clipping mode. One of (\"norm\", \"value\", \"agc\")')\ngroup.add_argument('--layer-decay', type=float, default=None,\n                   help='layer-wise learning rate decay (default: None)')\ngroup.add_argument('--opt-kwargs', nargs='*', default={}, action=utils.ParseKwargs)\n\n# Learning rate schedule parameters\ngroup = parser.add_argument_group('Learning rate schedule parameters')\ngroup.add_argument('--sched', type=str, default='cosine', metavar='SCHEDULER',\n                   help='LR scheduler (default: \"cosine\"')\ngroup.add_argument('--sched-on-updates', action='store_true', default=False,\n                   help='Apply LR scheduler step on update instead of epoch end.')\ngroup.add_argument('--lr', type=float, default=None, metavar='LR',\n                   help='learning rate, overrides lr-base if set (default: None)')\ngroup.add_argument('--lr-base', type=float, default=0.1, metavar='LR',\n                   help='base learning rate: lr = lr_base * global_batch_size / base_size')\ngroup.add_argument('--lr-base-size', type=int, default=256, metavar='DIV',\n                   help='base learning rate batch size (divisor, default: 256).')\ngroup.add_argument('--lr-base-scale', type=str, default='', metavar='SCALE',\n                   help='base learning rate vs batch_size scaling (\"linear\", \"sqrt\", based on opt if empty)')\ngroup.add_argument('--lr-noise', type=float, nargs='+', default=None, metavar='pct, pct',\n                   help='learning rate noise on/off epoch percentages')\ngroup.add_argument('--lr-noise-pct', type=float, default=0.67, metavar='PERCENT',\n                   help='learning rate noise limit percent (default: 0.67)')\ngroup.add_argument('--lr-noise-std', type=float, default=1.0, metavar='STDDEV',\n                   help='learning rate noise std-dev (default: 1.0)')\ngroup.add_argument('--lr-cycle-mul', type=float, default=1.0, metavar='MULT',\n                   help='learning rate cycle len multiplier (default: 1.0)')\ngroup.add_argument('--lr-cycle-decay', type=float, default=0.5, metavar='MULT',\n                   help='amount to decay each learning rate cycle (default: 0.5)')\ngroup.add_argument('--lr-cycle-limit', type=int, default=1, metavar='N',\n                   help='learning rate cycle limit, cycles enabled if > 1')\ngroup.add_argument('--lr-k-decay', type=float, default=1.0,\n                   help='learning rate k-decay for cosine/poly (default: 1.0)')\ngroup.add_argument('--warmup-lr', type=float, default=1e-5, metavar='LR',\n                   help='warmup learning rate (default: 1e-5)')\ngroup.add_argument('--min-lr', type=float, default=0, metavar='LR',\n                   help='lower lr bound for cyclic schedulers that hit 0 (default: 0)')\ngroup.add_argument('--epochs', type=int, default=300, metavar='N',\n                   help='number of epochs to train (default: 300)')\ngroup.add_argument('--epoch-repeats', type=float, default=0., metavar='N',\n                   help='epoch repeat multiplier (number of times to repeat dataset epoch per train epoch).')\ngroup.add_argument('--start-epoch', default=None, type=int, metavar='N',\n                   help='manual epoch number (useful on restarts)')\ngroup.add_argument('--decay-milestones', default=[90, 180, 270], type=int, nargs='+', metavar=\"MILESTONES\",\n                   help='list of decay epoch indices for multistep lr. must be increasing')\ngroup.add_argument('--decay-epochs', type=float, default=90, metavar='N',\n                   help='epoch interval to decay LR')\ngroup.add_argument('--warmup-epochs', type=int, default=5, metavar='N',\n                   help='epochs to warmup LR, if scheduler supports')\ngroup.add_argument('--warmup-prefix', action='store_true', default=False,\n                   help='Exclude warmup period from decay schedule.'),\ngroup.add_argument('--cooldown-epochs', type=int, default=0, metavar='N',\n                   help='epochs to cooldown LR at min_lr, after cyclic schedule ends')\ngroup.add_argument('--patience-epochs', type=int, default=10, metavar='N',\n                   help='patience epochs for Plateau LR scheduler (default: 10)')\ngroup.add_argument('--decay-rate', '--dr', type=float, default=0.1, metavar='RATE',\n                   help='LR decay rate (default: 0.1)')\n\n# Augmentation & regularization parameters\ngroup = parser.add_argument_group('Augmentation and regularization parameters')\ngroup.add_argument('--no-aug', action='store_true', default=False,\n                   help='Disable all training augmentation, override other train aug args')\ngroup.add_argument('--train-crop-mode', type=str, default=None,\n                   help='Crop-mode in train'),\ngroup.add_argument('--scale', type=float, nargs='+', default=[0.08, 1.0], metavar='PCT',\n                   help='Random resize scale (default: 0.08 1.0)')\ngroup.add_argument('--ratio', type=float, nargs='+', default=[3. / 4., 4. / 3.], metavar='RATIO',\n                   help='Random resize aspect ratio (default: 0.75 1.33)')\ngroup.add_argument('--hflip', type=float, default=0.5,\n                   help='Horizontal flip training aug probability')\ngroup.add_argument('--vflip', type=float, default=0.,\n                   help='Vertical flip training aug probability')\ngroup.add_argument('--color-jitter', type=float, default=0.4, metavar='PCT',\n                   help='Color jitter factor (default: 0.4)')\ngroup.add_argument('--color-jitter-prob', type=float, default=None, metavar='PCT',\n                   help='Probability of applying any color jitter.')\ngroup.add_argument('--grayscale-prob', type=float, default=None, metavar='PCT',\n                   help='Probability of applying random grayscale conversion.')\ngroup.add_argument('--gaussian-blur-prob', type=float, default=None, metavar='PCT',\n                   help='Probability of applying gaussian blur.')\ngroup.add_argument('--aa', type=str, default=None, metavar='NAME',\n                   help='Use AutoAugment policy. \"v0\" or \"original\". (default: None)'),\ngroup.add_argument('--aug-repeats', type=float, default=0,\n                   help='Number of augmentation repetitions (distributed training only) (default: 0)')\ngroup.add_argument('--aug-splits', type=int, default=0,\n                   help='Number of augmentation splits (default: 0, valid: 0 or >=2)')\ngroup.add_argument('--jsd-loss', action='store_true', default=False,\n                   help='Enable Jensen-Shannon Divergence + CE loss. Use with `--aug-splits`.')\ngroup.add_argument('--bce-loss', action='store_true', default=False,\n                   help='Enable BCE loss w/ Mixup/CutMix use.')\ngroup.add_argument('--bce-sum', action='store_true', default=False,\n                   help='Sum over classes when using BCE loss.')\ngroup.add_argument('--bce-target-thresh', type=float, default=None,\n                   help='Threshold for binarizing softened BCE targets (default: None, disabled).')\ngroup.add_argument('--bce-pos-weight', type=float, default=None,\n                   help='Positive weighting for BCE loss.')\ngroup.add_argument('--reprob', type=float, default=0., metavar='PCT',\n                   help='Random erase prob (default: 0.)')\ngroup.add_argument('--remode', type=str, default='pixel',\n                   help='Random erase mode (default: \"pixel\")')\ngroup.add_argument('--recount', type=int, default=1,\n                   help='Random erase count (default: 1)')\ngroup.add_argument('--resplit', action='store_true', default=False,\n                   help='Do not random erase first (clean) augmentation split')\ngroup.add_argument('--mixup', type=float, default=0.0,\n                   help='mixup alpha, mixup enabled if > 0. (default: 0.)')\ngroup.add_argument('--cutmix', type=float, default=0.0,\n                   help='cutmix alpha, cutmix enabled if > 0. (default: 0.)')\ngroup.add_argument('--cutmix-minmax', type=float, nargs='+', default=None,\n                   help='cutmix min/max ratio, overrides alpha and enables cutmix if set (default: None)')\ngroup.add_argument('--mixup-prob', type=float, default=1.0,\n                   help='Probability of performing mixup or cutmix when either/both is enabled')\ngroup.add_argument('--mixup-switch-prob', type=float, default=0.5,\n                   help='Probability of switching to cutmix when both mixup and cutmix enabled')\ngroup.add_argument('--mixup-mode', type=str, default='batch',\n                   help='How to apply mixup/cutmix params. Per \"batch\", \"pair\", or \"elem\"')\ngroup.add_argument('--mixup-off-epoch', default=0, type=int, metavar='N',\n                   help='Turn off mixup after this epoch, disabled if 0 (default: 0)')\ngroup.add_argument('--smoothing', type=float, default=0.1,\n                   help='Label smoothing (default: 0.1)')\ngroup.add_argument('--train-interpolation', type=str, default='random',\n                   help='Training interpolation (random, bilinear, bicubic default: \"random\")')\ngroup.add_argument('--drop', type=float, default=0.0, metavar='PCT',\n                   help='Dropout rate (default: 0.)')\ngroup.add_argument('--drop-connect', type=float, default=None, metavar='PCT',\n                   help='Drop connect rate, DEPRECATED, use drop-path (default: None)')\ngroup.add_argument('--drop-path', type=float, default=None, metavar='PCT',\n                   help='Drop path rate (default: None)')\ngroup.add_argument('--drop-block', type=float, default=None, metavar='PCT',\n                   help='Drop block rate (default: None)')\n\n# Batch norm parameters (only works with gen_efficientnet based models currently)\ngroup = parser.add_argument_group('Batch norm parameters', 'Only works with gen_efficientnet based models currently.')\ngroup.add_argument('--bn-momentum', type=float, default=None,\n                   help='BatchNorm momentum override (if not None)')\ngroup.add_argument('--bn-eps', type=float, default=None,\n                   help='BatchNorm epsilon override (if not None)')\ngroup.add_argument('--sync-bn', action='store_true',\n                   help='Enable NVIDIA Apex or Torch synchronized BatchNorm.')\ngroup.add_argument('--dist-bn', type=str, default='reduce',\n                   help='Distribute BatchNorm stats between nodes after each epoch (\"broadcast\", \"reduce\", or \"\")')\ngroup.add_argument('--split-bn', action='store_true',\n                   help='Enable separate BN layers per augmentation split.')\n\n# Model Exponential Moving Average\ngroup = parser.add_argument_group('Model exponential moving average parameters')\ngroup.add_argument('--model-ema', action='store_true', default=False,\n                   help='Enable tracking moving average of model weights.')\ngroup.add_argument('--model-ema-force-cpu', action='store_true', default=False,\n                   help='Force ema to be tracked on CPU, rank=0 node only. Disables EMA validation.')\ngroup.add_argument('--model-ema-decay', type=float, default=0.9998,\n                   help='Decay factor for model weights moving average (default: 0.9998)')\ngroup.add_argument('--model-ema-warmup', action='store_true',\n                   help='Enable warmup for model EMA decay.')\n\n# Misc\ngroup = parser.add_argument_group('Miscellaneous parameters')\ngroup.add_argument('--seed', type=int, default=42, metavar='S',\n                   help='random seed (default: 42)')\ngroup.add_argument('--worker-seeding', type=str, default='all',\n                   help='worker seed mode (default: all)')\ngroup.add_argument('--log-interval', type=int, default=50, metavar='N',\n                   help='how many batches to wait before logging training status')\ngroup.add_argument('--recovery-interval', type=int, default=0, metavar='N',\n                   help='how many batches to wait before writing recovery checkpoint')\ngroup.add_argument('--checkpoint-hist', type=int, default=10, metavar='N',\n                   help='number of checkpoints to keep (default: 10)')\ngroup.add_argument('-j', '--workers', type=int, default=4, metavar='N',\n                   help='how many training processes to use (default: 4)')\ngroup.add_argument('--save-images', action='store_true', default=False,\n                   help='save images of input batches every log interval for debugging')\ngroup.add_argument('--pin-mem', action='store_true', default=False,\n                   help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\ngroup.add_argument('--no-prefetcher', action='store_true', default=False,\n                   help='disable fast prefetcher')\ngroup.add_argument('--output', default='', type=str, metavar='PATH',\n                   help='path to output folder (default: none, current dir)')\ngroup.add_argument('--experiment', default='', type=str, metavar='NAME',\n                   help='name of train experiment, name of sub-folder for output')\ngroup.add_argument('--eval-metric', default='top1', type=str, metavar='EVAL_METRIC',\n                   help='Best metric (default: \"top1\"')\ngroup.add_argument('--tta', type=int, default=0, metavar='N',\n                   help='Test/inference time augmentation (oversampling) factor. 0=None (default: 0)')\ngroup.add_argument('--use-multi-epochs-loader', action='store_true', default=False,\n                   help='use the multi-epochs-loader to save time at the beginning of every epoch')\ngroup.add_argument('--log-wandb', action='store_true', default=False,\n                   help='log training and validation metrics to wandb')\ngroup.add_argument('--wandb-project', default=None, type=str,\n                   help='wandb project name')\ngroup.add_argument('--wandb-tags', default=[], type=str, nargs='+',\n                   help='wandb tags')\ngroup.add_argument('--wandb-resume-id', default='', type=str, metavar='ID',\n                   help='If resuming a run, the id of the run in wandb')\n\n\ndef _parse_args():\n    # Do we have a config file to parse?\n    args_config, remaining = config_parser.parse_known_args()\n    if args_config.config:\n        with open(args_config.config, 'r') as f:\n            cfg = yaml.safe_load(f)\n            parser.set_defaults(**cfg)\n\n    # The main arg parser parses the rest of the args, the usual\n    # defaults will have been overridden if config file specified.\n    args = parser.parse_args(remaining)\n\n    # Cache the args as a text string to save them in the output dir later\n    args_text = yaml.safe_dump(args.__dict__, default_flow_style=False)\n    return args, args_text\n\n\ndef main():\n    utils.setup_default_logging()\n    args, args_text = _parse_args()\n\n    if args.device_modules:\n        for module in args.device_modules:\n            importlib.import_module(module)\n\n    if torch.cuda.is_available():\n        torch.backends.cuda.matmul.allow_tf32 = True\n        torch.backends.cudnn.benchmark = True\n\n    args.prefetcher = not args.no_prefetcher\n    args.grad_accum_steps = max(1, args.grad_accum_steps)\n    device = utils.init_distributed_device(args)\n    if args.distributed:\n        _logger.info(\n            'Training in distributed mode with multiple processes, 1 device per process.'\n            f'Process {args.rank}, total {args.world_size}, device {args.device}.')\n    else:\n        _logger.info(f'Training with a single process on 1 device ({args.device}).')\n    assert args.rank >= 0\n\n    model_dtype = None\n    if args.model_dtype:\n        assert args.model_dtype in ('float32', 'float16', 'bfloat16')\n        model_dtype = getattr(torch, args.model_dtype)\n        if model_dtype == torch.float16:\n            _logger.warning('float16 is not recommended for training, for half precision bfloat16 is recommended.')\n\n    # resolve AMP arguments based on PyTorch / Apex availability\n    use_amp = None\n    amp_dtype = torch.float16\n    if args.amp:\n        assert model_dtype is None or model_dtype == torch.float32, 'float32 model dtype must be used with AMP'\n        if args.amp_impl == 'apex':\n            assert has_apex, 'AMP impl specified as APEX but APEX is not installed.'\n            use_amp = 'apex'\n            assert args.amp_dtype == 'float16'\n        else:\n            use_amp = 'native'\n            assert args.amp_dtype in ('float16', 'bfloat16')\n        if args.amp_dtype == 'bfloat16':\n            amp_dtype = torch.bfloat16\n\n    utils.random_seed(args.seed, args.rank)\n\n    if args.fuser:\n        utils.set_jit_fuser(args.fuser)\n    if args.fast_norm:\n        set_fast_norm()\n\n    in_chans = 3\n    if args.in_chans is not None:\n        in_chans = args.in_chans\n    elif args.input_size is not None:\n        in_chans = args.input_size[0]\n\n    factory_kwargs = {}\n    if args.pretrained_path:\n        # merge with pretrained_cfg of model, 'file' has priority over 'url' and 'hf_hub'.\n        factory_kwargs['pretrained_cfg_overlay'] = dict(\n            file=args.pretrained_path,\n            num_classes=-1,  # force head adaptation\n        )\n\n    model = create_model(\n        args.model,\n        pretrained=args.pretrained,\n        in_chans=in_chans,\n        num_classes=args.num_classes,\n        drop_rate=args.drop,\n        drop_path_rate=args.drop_path,\n        drop_block_rate=args.drop_block,\n        global_pool=args.gp,\n        bn_momentum=args.bn_momentum,\n        bn_eps=args.bn_eps,\n        scriptable=args.torchscript,\n        checkpoint_path=args.initial_checkpoint,\n        **factory_kwargs,\n        **args.model_kwargs,\n    )\n    if args.head_init_scale is not None:\n        with torch.no_grad():\n            model.get_classifier().weight.mul_(args.head_init_scale)\n            model.get_classifier().bias.mul_(args.head_init_scale)\n    if args.head_init_bias is not None:\n        nn.init.constant_(model.get_classifier().bias, args.head_init_bias)\n\n    if args.num_classes is None:\n        assert hasattr(model, 'num_classes'), 'Model must have `num_classes` attr if not set on cmd line/config.'\n        args.num_classes = model.num_classes  # FIXME handle model default vs config num_classes more elegantly\n\n    if args.grad_checkpointing:\n        model.set_grad_checkpointing(enable=True)\n\n    if utils.is_primary(args):\n        _logger.info(\n            f'Model {safe_model_name(args.model)} created, param count:{sum([m.numel() for m in model.parameters()])}')\n\n    data_config = resolve_data_config(vars(args), model=model, verbose=utils.is_primary(args))\n\n    # setup augmentation batch splits for contrastive loss or split bn\n    num_aug_splits = 0\n    if args.aug_splits > 0:\n        assert args.aug_splits > 1, 'A split of 1 makes no sense'\n        num_aug_splits = args.aug_splits\n\n    # enable split bn (separate bn stats per batch-portion)\n    if args.split_bn:\n        assert num_aug_splits > 1 or args.resplit\n        model = convert_splitbn_model(model, max(num_aug_splits, 2))\n\n    # move model to GPU, enable channels last layout if set\n    model.to(device=device, dtype=model_dtype)  # FIXME move model device & dtype into create_model\n    if args.channels_last:\n        model.to(memory_format=torch.channels_last)\n\n    # setup synchronized BatchNorm for distributed training\n    if args.distributed and args.sync_bn:\n        args.dist_bn = ''  # disable dist_bn when sync BN active\n        assert not args.split_bn\n        if has_apex and use_amp == 'apex':\n            # Apex SyncBN used with Apex AMP\n            # WARNING this won't currently work with models using BatchNormAct2d\n            model = convert_syncbn_model(model)\n        else:\n            model = convert_sync_batchnorm(model)\n        if utils.is_primary(args):\n            _logger.info(\n                'Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using '\n                'zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.')\n\n    if args.torchscript:\n        assert not args.torchcompile\n        assert not use_amp == 'apex', 'Cannot use APEX AMP with torchscripted model'\n        assert not args.sync_bn, 'Cannot use SyncBatchNorm with torchscripted model'\n        model = torch.jit.script(model)\n\n    if not args.lr:\n        global_batch_size = args.batch_size * args.world_size * args.grad_accum_steps\n        batch_ratio = global_batch_size / args.lr_base_size\n        if not args.lr_base_scale:\n            on = args.opt.lower()\n            args.lr_base_scale = 'sqrt' if any([o in on for o in ('ada', 'lamb')]) else 'linear'\n        if args.lr_base_scale == 'sqrt':\n            batch_ratio = batch_ratio ** 0.5\n        args.lr = args.lr_base * batch_ratio\n        if utils.is_primary(args):\n            _logger.info(\n                f'Learning rate ({args.lr}) calculated from base learning rate ({args.lr_base}) '\n                f'and effective global batch size ({global_batch_size}) with {args.lr_base_scale} scaling.')\n\n    optimizer = create_optimizer_v2(\n        model,\n        **optimizer_kwargs(cfg=args),\n        **args.opt_kwargs,\n    )\n    if utils.is_primary(args):\n        defaults = copy.deepcopy(optimizer.defaults)\n        defaults['weight_decay'] = args.weight_decay  # this isn't stored in optimizer.defaults\n        defaults = ', '.join([f'{k}: {v}' for k, v in defaults.items()])\n        logging.info(\n            f'Created {type(optimizer).__name__} ({args.opt}) optimizer: {defaults}'\n        )\n\n    # setup automatic mixed-precision (AMP) loss scaling and op casting\n    amp_autocast = suppress  # do nothing\n    loss_scaler = None\n    if use_amp == 'apex':\n        assert device.type == 'cuda'\n        model, optimizer = amp.initialize(model, optimizer, opt_level='O1')\n        loss_scaler = ApexScaler()\n        if utils.is_primary(args):\n            _logger.info('Using NVIDIA APEX AMP. Training in mixed precision.')\n    elif use_amp == 'native':\n        amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)\n        if device.type in ('cuda',) and amp_dtype == torch.float16:\n            # loss scaler only used for float16 (half) dtype, bfloat16 does not need it\n            loss_scaler = NativeScaler(device=device.type)\n        if utils.is_primary(args):\n            _logger.info('Using native Torch AMP. Training in mixed precision.')\n    else:\n        if utils.is_primary(args):\n            _logger.info(f'AMP not enabled. Training in {model_dtype or torch.float32}.')\n\n    # optionally resume from a checkpoint\n    resume_epoch = None\n    if args.resume:\n        resume_epoch = resume_checkpoint(\n            model,\n            args.resume,\n            optimizer=None if args.no_resume_opt else optimizer,\n            loss_scaler=None if args.no_resume_opt else loss_scaler,\n            log_info=utils.is_primary(args),\n        )\n\n    # setup exponential moving average of model weights, SWA could be used here too\n    model_ema = None\n    if args.model_ema:\n        # Important to create EMA model after cuda(), DP wrapper, and AMP but before DDP wrapper\n        model_ema = utils.ModelEmaV3(\n            model,\n            decay=args.model_ema_decay,\n            use_warmup=args.model_ema_warmup,\n            device='cpu' if args.model_ema_force_cpu else None,\n        )\n        if args.resume:\n            load_checkpoint(model_ema.module, args.resume, use_ema=True)\n        if args.torchcompile:\n            model_ema = torch.compile(model_ema, backend=args.torchcompile)\n\n    # setup distributed training\n    if args.distributed:\n        if has_apex and use_amp == 'apex':\n            # Apex DDP preferred unless native amp is activated\n            if utils.is_primary(args):\n                _logger.info(\"Using NVIDIA APEX DistributedDataParallel.\")\n            model = ApexDDP(model, delay_allreduce=True)\n        else:\n            if utils.is_primary(args):\n                _logger.info(\"Using native Torch DistributedDataParallel.\")\n            model = NativeDDP(model, device_ids=[device], broadcast_buffers=not args.no_ddp_bb)\n        # NOTE: EMA model does not need to be wrapped by DDP\n\n    if args.torchcompile:\n        # torch compile should be done after DDP\n        assert has_compile, 'A version of torch w/ torch.compile() is required for --compile, possibly a nightly.'\n        model = torch.compile(model, backend=args.torchcompile, mode=args.torchcompile_mode)\n\n    # create the train and eval datasets\n    if args.data and not args.data_dir:\n        args.data_dir = args.data\n    if args.input_img_mode is None:\n        input_img_mode = 'RGB' if data_config['input_size'][0] == 3 else 'L'\n    else:\n        input_img_mode = args.input_img_mode\n\n    dataset_train = create_dataset(\n        args.dataset,\n        root=args.data_dir,\n        split=args.train_split,\n        is_training=True,\n        class_map=args.class_map,\n        download=args.dataset_download,\n        batch_size=args.batch_size,\n        seed=args.seed,\n        repeats=args.epoch_repeats,\n        input_img_mode=input_img_mode,\n        input_key=args.input_key,\n        target_key=args.target_key,\n        num_samples=args.train_num_samples,\n        trust_remote_code=args.dataset_trust_remote_code,\n    )\n\n    if args.val_split:\n        dataset_eval = create_dataset(\n            args.dataset,\n            root=args.data_dir,\n            split=args.val_split,\n            is_training=False,\n            class_map=args.class_map,\n            download=args.dataset_download,\n            batch_size=args.batch_size,\n            input_img_mode=input_img_mode,\n            input_key=args.input_key,\n            target_key=args.target_key,\n            num_samples=args.val_num_samples,\n            trust_remote_code=args.dataset_trust_remote_code,\n        )\n\n    # setup mixup / cutmix\n    collate_fn = None\n    mixup_fn = None\n    mixup_active = args.mixup > 0 or args.cutmix > 0. or args.cutmix_minmax is not None\n    if mixup_active:\n        mixup_args = dict(\n            mixup_alpha=args.mixup,\n            cutmix_alpha=args.cutmix,\n            cutmix_minmax=args.cutmix_minmax,\n            prob=args.mixup_prob,\n            switch_prob=args.mixup_switch_prob,\n            mode=args.mixup_mode,\n            label_smoothing=args.smoothing,\n            num_classes=args.num_classes\n        )\n        if args.prefetcher:\n            assert not num_aug_splits  # collate conflict (need to support de-interleaving in collate mixup)\n            collate_fn = FastCollateMixup(**mixup_args)\n        else:\n            mixup_fn = Mixup(**mixup_args)\n\n    # wrap dataset in AugMix helper\n    if num_aug_splits > 1:\n        dataset_train = AugMixDataset(dataset_train, num_splits=num_aug_splits)\n\n    # create data loaders w/ augmentation pipeline\n    train_interpolation = args.train_interpolation\n    if args.no_aug or not train_interpolation:\n        train_interpolation = data_config['interpolation']\n    loader_train = create_loader(\n        dataset_train,\n        input_size=data_config['input_size'],\n        batch_size=args.batch_size,\n        is_training=True,\n        no_aug=args.no_aug,\n        re_prob=args.reprob,\n        re_mode=args.remode,\n        re_count=args.recount,\n        re_split=args.resplit,\n        train_crop_mode=args.train_crop_mode,\n        scale=args.scale,\n        ratio=args.ratio,\n        hflip=args.hflip,\n        vflip=args.vflip,\n        color_jitter=args.color_jitter,\n        color_jitter_prob=args.color_jitter_prob,\n        grayscale_prob=args.grayscale_prob,\n        gaussian_blur_prob=args.gaussian_blur_prob,\n        auto_augment=args.aa,\n        num_aug_repeats=args.aug_repeats,\n        num_aug_splits=num_aug_splits,\n        interpolation=train_interpolation,\n        mean=data_config['mean'],\n        std=data_config['std'],\n        num_workers=args.workers,\n        distributed=args.distributed,\n        collate_fn=collate_fn,\n        pin_memory=args.pin_mem,\n        img_dtype=model_dtype or torch.float32,\n        device=device,\n        use_prefetcher=args.prefetcher,\n        use_multi_epochs_loader=args.use_multi_epochs_loader,\n        worker_seeding=args.worker_seeding,\n    )\n\n    loader_eval = None\n    if args.val_split:\n        eval_workers = args.workers\n        if args.distributed and ('tfds' in args.dataset or 'wds' in args.dataset):\n            # FIXME reduces validation padding issues when using TFDS, WDS w/ workers and distributed training\n            eval_workers = min(2, args.workers)\n        loader_eval = create_loader(\n            dataset_eval,\n            input_size=data_config['input_size'],\n            batch_size=args.validation_batch_size or args.batch_size,\n            is_training=False,\n            interpolation=data_config['interpolation'],\n            mean=data_config['mean'],\n            std=data_config['std'],\n            num_workers=eval_workers,\n            distributed=args.distributed,\n            crop_pct=data_config['crop_pct'],\n            pin_memory=args.pin_mem,\n            img_dtype=model_dtype or torch.float32,\n            device=device,\n            use_prefetcher=args.prefetcher,\n        )\n\n    # setup loss function\n    if args.jsd_loss:\n        assert num_aug_splits > 1  # JSD only valid with aug splits set\n        train_loss_fn = JsdCrossEntropy(num_splits=num_aug_splits, smoothing=args.smoothing)\n    elif mixup_active:\n        # smoothing is handled with mixup target transform which outputs sparse, soft targets\n        if args.bce_loss:\n            train_loss_fn = BinaryCrossEntropy(\n                target_threshold=args.bce_target_thresh,\n                sum_classes=args.bce_sum,\n                pos_weight=args.bce_pos_weight,\n            )\n        else:\n            train_loss_fn = SoftTargetCrossEntropy()\n    elif args.smoothing:\n        if args.bce_loss:\n            train_loss_fn = BinaryCrossEntropy(\n                smoothing=args.smoothing,\n                target_threshold=args.bce_target_thresh,\n                sum_classes=args.bce_sum,\n                pos_weight=args.bce_pos_weight,\n            )\n        else:\n            train_loss_fn = LabelSmoothingCrossEntropy(smoothing=args.smoothing)\n    else:\n        train_loss_fn = nn.CrossEntropyLoss()\n    train_loss_fn = train_loss_fn.to(device=device)\n    validate_loss_fn = nn.CrossEntropyLoss().to(device=device)\n\n    # setup checkpoint saver and eval metric tracking\n    eval_metric = args.eval_metric if loader_eval is not None else 'loss'\n    decreasing_metric = eval_metric == 'loss'\n    best_metric = None\n    best_epoch = None\n    saver = None\n    output_dir = None\n    if utils.is_primary(args):\n        if args.experiment:\n            exp_name = args.experiment\n        else:\n            exp_name = '-'.join([\n                datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n                safe_model_name(args.model),\n                str(data_config['input_size'][-1])\n            ])\n        output_dir = utils.get_outdir(args.output if args.output else './output/train', exp_name)\n        saver = utils.CheckpointSaver(\n            model=model,\n            optimizer=optimizer,\n            args=args,\n            model_ema=model_ema,\n            amp_scaler=loss_scaler,\n            checkpoint_dir=output_dir,\n            recovery_dir=output_dir,\n            decreasing=decreasing_metric,\n            max_history=args.checkpoint_hist\n        )\n        with open(os.path.join(output_dir, 'args.yaml'), 'w') as f:\n            f.write(args_text)\n\n        if args.log_wandb:\n            if has_wandb:\n                assert not args.wandb_resume_id or args.resume\n                wandb.init(\n                    project=args.wandb_project,\n                    name=exp_name,\n                    config=args,\n                    tags=args.wandb_tags,\n                    resume=\"must\" if args.wandb_resume_id else None,\n                    id=args.wandb_resume_id if args.wandb_resume_id else None,\n                )\n            else:\n                _logger.warning(\n                    \"You've requested to log metrics to wandb but package not found. \"\n                    \"Metrics not being logged to wandb, try `pip install wandb`\")\n\n    # setup learning rate schedule and starting epoch\n    updates_per_epoch = (len(loader_train) + args.grad_accum_steps - 1) // args.grad_accum_steps\n    lr_scheduler, num_epochs = create_scheduler_v2(\n        optimizer,\n        **scheduler_kwargs(args, decreasing_metric=decreasing_metric),\n        updates_per_epoch=updates_per_epoch,\n    )\n    start_epoch = 0\n    if args.start_epoch is not None:\n        # a specified start_epoch will always override the resume epoch\n        start_epoch = args.start_epoch\n    elif resume_epoch is not None:\n        start_epoch = resume_epoch\n    if lr_scheduler is not None and start_epoch > 0:\n        if args.sched_on_updates:\n            lr_scheduler.step_update(start_epoch * updates_per_epoch)\n        else:\n            lr_scheduler.step(start_epoch)\n\n    if utils.is_primary(args):\n        if args.warmup_prefix:\n            sched_explain = '(warmup_epochs + epochs + cooldown_epochs). Warmup added to total when warmup_prefix=True'\n        else:\n            sched_explain = '(epochs + cooldown_epochs). Warmup within epochs when warmup_prefix=False'\n        _logger.info(\n            f'Scheduled epochs: {num_epochs} {sched_explain}. '\n            f'LR stepped per {\"epoch\" if lr_scheduler.t_in_epochs else \"update\"}.')\n\n    results = []\n    try:\n        for epoch in range(start_epoch, num_epochs):\n            if hasattr(dataset_train, 'set_epoch'):\n                dataset_train.set_epoch(epoch)\n            elif args.distributed and hasattr(loader_train.sampler, 'set_epoch'):\n                loader_train.sampler.set_epoch(epoch)\n\n            train_metrics = train_one_epoch(\n                epoch,\n                model,\n                loader_train,\n                optimizer,\n                train_loss_fn,\n                args,\n                lr_scheduler=lr_scheduler,\n                saver=saver,\n                output_dir=output_dir,\n                amp_autocast=amp_autocast,\n                loss_scaler=loss_scaler,\n                model_dtype=model_dtype,\n                model_ema=model_ema,\n                mixup_fn=mixup_fn,\n                num_updates_total=num_epochs * updates_per_epoch,\n            )\n\n            if args.distributed and args.dist_bn in ('broadcast', 'reduce'):\n                if utils.is_primary(args):\n                    _logger.info(\"Distributing BatchNorm running means and vars\")\n                utils.distribute_bn(model, args.world_size, args.dist_bn == 'reduce')\n\n            if loader_eval is not None:\n                eval_metrics = validate(\n                    model,\n                    loader_eval,\n                    validate_loss_fn,\n                    args,\n                    device=device,\n                    amp_autocast=amp_autocast,\n                    model_dtype=model_dtype,\n                )\n\n                if model_ema is not None and not args.model_ema_force_cpu:\n                    if args.distributed and args.dist_bn in ('broadcast', 'reduce'):\n                        utils.distribute_bn(model_ema, args.world_size, args.dist_bn == 'reduce')\n\n                    ema_eval_metrics = validate(\n                        model_ema,\n                        loader_eval,\n                        validate_loss_fn,\n                        args,\n                        device=device,\n                        amp_autocast=amp_autocast,\n                        log_suffix=' (EMA)',\n                    )\n                    eval_metrics = ema_eval_metrics\n            else:\n                eval_metrics = None\n\n            if output_dir is not None:\n                lrs = [param_group['lr'] for param_group in optimizer.param_groups]\n                utils.update_summary(\n                    epoch,\n                    train_metrics,\n                    eval_metrics,\n                    filename=os.path.join(output_dir, 'summary.csv'),\n                    lr=sum(lrs) / len(lrs),\n                    write_header=best_metric is None,\n                    log_wandb=args.log_wandb and has_wandb,\n                )\n\n            if eval_metrics is not None:\n                latest_metric = eval_metrics[eval_metric]\n            else:\n                latest_metric = train_metrics[eval_metric]\n\n            if saver is not None:\n                # save proper checkpoint with eval metric\n                best_metric, best_epoch = saver.save_checkpoint(epoch, metric=latest_metric)\n\n            if lr_scheduler is not None:\n                # step LR for next epoch\n                lr_scheduler.step(epoch + 1, latest_metric)\n\n            latest_results = {\n                'epoch': epoch,\n                'train': train_metrics,\n            }\n            if eval_metrics is not None:\n                latest_results['validation'] = eval_metrics\n            results.append(latest_results)\n\n    except KeyboardInterrupt:\n        pass\n\n    if best_metric is not None:\n        # log best metric as tracked by checkpoint saver\n        _logger.info('*** Best metric: {0} (epoch {1})'.format(best_metric, best_epoch))\n\n    if utils.is_primary(args):\n        # for parsable results display, dump top-10 summaries to avoid excess console spam\n        display_results = sorted(\n            results,\n            key=lambda x: x.get('validation', x.get('train')).get(eval_metric, 0),\n            reverse=decreasing_metric,\n        )\n        print(f'--result\\n{json.dumps(display_results[-10:], indent=4)}')\n\n\ndef train_one_epoch(\n        epoch,\n        model,\n        loader,\n        optimizer,\n        loss_fn,\n        args,\n        device=torch.device('cuda'),\n        lr_scheduler=None,\n        saver=None,\n        output_dir=None,\n        amp_autocast=suppress,\n        loss_scaler=None,\n        model_dtype=None,\n        model_ema=None,\n        mixup_fn=None,\n        num_updates_total=None,\n):\n    if args.mixup_off_epoch and epoch >= args.mixup_off_epoch:\n        if args.prefetcher and loader.mixup_enabled:\n            loader.mixup_enabled = False\n        elif mixup_fn is not None:\n            mixup_fn.mixup_enabled = False\n\n    second_order = hasattr(optimizer, 'is_second_order') and optimizer.is_second_order\n    has_no_sync = hasattr(model, \"no_sync\")\n    update_time_m = utils.AverageMeter()\n    data_time_m = utils.AverageMeter()\n    losses_m = utils.AverageMeter()\n\n    model.train()\n\n    accum_steps = args.grad_accum_steps\n    last_accum_steps = len(loader) % accum_steps\n    updates_per_epoch = (len(loader) + accum_steps - 1) // accum_steps\n    num_updates = epoch * updates_per_epoch\n    last_batch_idx = len(loader) - 1\n    last_batch_idx_to_accum = len(loader) - last_accum_steps\n\n    data_start_time = update_start_time = time.time()\n    optimizer.zero_grad()\n    update_sample_count = 0\n    for batch_idx, (input, target) in enumerate(loader):\n        last_batch = batch_idx == last_batch_idx\n        need_update = last_batch or (batch_idx + 1) % accum_steps == 0\n        update_idx = batch_idx // accum_steps\n        if batch_idx >= last_batch_idx_to_accum:\n            accum_steps = last_accum_steps\n\n        if not args.prefetcher:\n            input, target = input.to(device=device, dtype=model_dtype), target.to(device=device)\n            if mixup_fn is not None:\n                input, target = mixup_fn(input, target)\n        if args.channels_last:\n            input = input.contiguous(memory_format=torch.channels_last)\n\n        # multiply by accum steps to get equivalent for full update\n        data_time_m.update(accum_steps * (time.time() - data_start_time))\n\n        def _forward():\n            with amp_autocast():\n                output = model(input)\n                loss = loss_fn(output, target)\n            if accum_steps > 1:\n                loss /= accum_steps\n            return loss\n\n        def _backward(_loss):\n            if loss_scaler is not None:\n                loss_scaler(\n                    _loss,\n                    optimizer,\n                    clip_grad=args.clip_grad,\n                    clip_mode=args.clip_mode,\n                    parameters=model_parameters(model, exclude_head='agc' in args.clip_mode),\n                    create_graph=second_order,\n                    need_update=need_update,\n                )\n            else:\n                _loss.backward(create_graph=second_order)\n                if need_update:\n                    if args.clip_grad is not None:\n                        utils.dispatch_clip_grad(\n                            model_parameters(model, exclude_head='agc' in args.clip_mode),\n                            value=args.clip_grad,\n                            mode=args.clip_mode,\n                        )\n                    optimizer.step()\n\n        if has_no_sync and not need_update:\n            with model.no_sync():\n                loss = _forward()\n                _backward(loss)\n        else:\n            loss = _forward()\n            _backward(loss)\n\n        losses_m.update(loss.item() * accum_steps, input.size(0))\n        update_sample_count += input.size(0)\n\n        if not need_update:\n            data_start_time = time.time()\n            continue\n\n        num_updates += 1\n        optimizer.zero_grad()\n        if model_ema is not None:\n            model_ema.update(model, step=num_updates)\n\n        if args.synchronize_step:\n            if device.type == 'cuda':\n                torch.cuda.synchronize()\n            elif device.type == 'npu':\n                torch.npu.synchronize()\n        time_now = time.time()\n        update_time_m.update(time.time() - update_start_time)\n        update_start_time = time_now\n\n        if update_idx % args.log_interval == 0:\n            lrl = [param_group['lr'] for param_group in optimizer.param_groups]\n            lr = sum(lrl) / len(lrl)\n\n            loss_avg, loss_now = losses_m.avg, losses_m.val\n            if args.distributed:\n                # synchronize current step and avg loss, each process keeps its own running avg\n                loss_avg = utils.reduce_tensor(loss.new([loss_avg]), args.world_size).item()\n                loss_now = utils.reduce_tensor(loss.new([loss_now]), args.world_size).item()\n                update_sample_count *= args.world_size\n\n            if utils.is_primary(args):\n                _logger.info(\n                    f'Train: {epoch} [{update_idx:>4d}/{updates_per_epoch} '\n                    f'({100. * (update_idx + 1) / updates_per_epoch:>3.0f}%)]  '\n                    f'Loss: {loss_now:#.3g} ({loss_avg:#.3g})  '\n                    f'Time: {update_time_m.val:.3f}s, {update_sample_count / update_time_m.val:>7.2f}/s  '\n                    f'({update_time_m.avg:.3f}s, {update_sample_count / update_time_m.avg:>7.2f}/s)  '\n                    f'LR: {lr:.3e}  '\n                    f'Data: {data_time_m.val:.3f} ({data_time_m.avg:.3f})'\n                )\n\n                if args.save_images and output_dir:\n                    torchvision.utils.save_image(\n                        input,\n                        os.path.join(output_dir, 'train-batch-%d.jpg' % batch_idx),\n                        padding=0,\n                        normalize=True\n                    )\n\n        if saver is not None and args.recovery_interval and (\n                (update_idx + 1) % args.recovery_interval == 0):\n            saver.save_recovery(epoch, batch_idx=update_idx)\n\n        if lr_scheduler is not None:\n            lr_scheduler.step_update(num_updates=num_updates, metric=losses_m.avg)\n\n        update_sample_count = 0\n        data_start_time = time.time()\n        # end for\n\n    if hasattr(optimizer, 'sync_lookahead'):\n        optimizer.sync_lookahead()\n\n    loss_avg = losses_m.avg\n    if args.distributed:\n        # synchronize avg loss, each process keeps its own running avg\n        loss_avg = torch.tensor([loss_avg], device=device, dtype=torch.float32)\n        loss_avg = utils.reduce_tensor(loss_avg, args.world_size).item()\n    return OrderedDict([('loss', loss_avg)])\n\n\ndef validate(\n        model,\n        loader,\n        loss_fn,\n        args,\n        device=torch.device('cuda'),\n        amp_autocast=suppress,\n        model_dtype=None,\n        log_suffix=''\n):\n    batch_time_m = utils.AverageMeter()\n    losses_m = utils.AverageMeter()\n    top1_m = utils.AverageMeter()\n    top5_m = utils.AverageMeter()\n\n    model.eval()\n\n    end = time.time()\n    last_idx = len(loader) - 1\n    with torch.no_grad():\n        for batch_idx, (input, target) in enumerate(loader):\n            last_batch = batch_idx == last_idx\n            if not args.prefetcher:\n                input = input.to(device=device, dtype=model_dtype)\n                target = target.to(device=device)\n            if args.channels_last:\n                input = input.contiguous(memory_format=torch.channels_last)\n\n            with amp_autocast():\n                output = model(input)\n                if isinstance(output, (tuple, list)):\n                    output = output[0]\n\n                # augmentation reduction\n                reduce_factor = args.tta\n                if reduce_factor > 1:\n                    output = output.unfold(0, reduce_factor, reduce_factor).mean(dim=2)\n                    target = target[0:target.size(0):reduce_factor]\n\n                loss = loss_fn(output, target)\n            acc1, acc5 = utils.accuracy(output, target, topk=(1, 5))\n\n            if args.distributed:\n                reduced_loss = utils.reduce_tensor(loss.data, args.world_size)\n                acc1 = utils.reduce_tensor(acc1, args.world_size)\n                acc5 = utils.reduce_tensor(acc5, args.world_size)\n            else:\n                reduced_loss = loss.data\n\n            if device.type == 'cuda':\n                torch.cuda.synchronize()\n            elif device.type == \"npu\":\n                torch.npu.synchronize()\n\n            losses_m.update(reduced_loss.item(), input.size(0))\n            top1_m.update(acc1.item(), output.size(0))\n            top5_m.update(acc5.item(), output.size(0))\n\n            batch_time_m.update(time.time() - end)\n            end = time.time()\n            if utils.is_primary(args) and (last_batch or batch_idx % args.log_interval == 0):\n                log_name = 'Test' + log_suffix\n                _logger.info(\n                    f'{log_name}: [{batch_idx:>4d}/{last_idx}]  '\n                    f'Time: {batch_time_m.val:.3f} ({batch_time_m.avg:.3f})  '\n                    f'Loss: {losses_m.val:>7.3f} ({losses_m.avg:>6.3f})  '\n                    f'Acc@1: {top1_m.val:>7.3f} ({top1_m.avg:>7.3f})  '\n                    f'Acc@5: {top5_m.val:>7.3f} ({top5_m.avg:>7.3f})'\n                )\n\n    metrics = OrderedDict([('loss', losses_m.avg), ('top1', top1_m.avg), ('top5', top5_m.avg)])\n\n    return metrics\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "validate.py",
          "type": "blob",
          "size": 21.47,
          "content": "#!/usr/bin/env python3\n\"\"\" ImageNet Validation Script\n\nThis is intended to be a lean and easily modifiable ImageNet validation script for evaluating pretrained\nmodels or training checkpoints against ImageNet or similarly organized image datasets. It prioritizes\ncanonical PyTorch, standard Python style, and good performance. Repurpose as you see fit.\n\nHacked together by Ross Wightman (https://github.com/rwightman)\n\"\"\"\nimport argparse\nimport csv\nimport glob\nimport json\nimport logging\nimport os\nimport time\nfrom collections import OrderedDict\nfrom contextlib import suppress\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\n\nfrom timm.data import create_dataset, create_loader, resolve_data_config, RealLabelsImagenet\nfrom timm.layers import apply_test_time_pool, set_fast_norm\nfrom timm.models import create_model, load_checkpoint, is_model, list_models\nfrom timm.utils import accuracy, AverageMeter, natural_key, setup_default_logging, set_jit_fuser, \\\n    decay_batch_step, check_batch_size_retry, ParseKwargs, reparameterize_model\n\ntry:\n    from apex import amp\n    has_apex = True\nexcept ImportError:\n    has_apex = False\n\ntry:\n    from functorch.compile import memory_efficient_fusion\n    has_functorch = True\nexcept ImportError as e:\n    has_functorch = False\n\nhas_compile = hasattr(torch, 'compile')\n\n_logger = logging.getLogger('validate')\n\n\nparser = argparse.ArgumentParser(description='PyTorch ImageNet Validation')\nparser.add_argument('data', nargs='?', metavar='DIR', const=None,\n                    help='path to dataset (*deprecated*, use --data-dir)')\nparser.add_argument('--data-dir', metavar='DIR',\n                    help='path to dataset (root dir)')\nparser.add_argument('--dataset', metavar='NAME', default='',\n                    help='dataset type + name (\"<type>/<name>\") (default: ImageFolder or ImageTar if empty)')\nparser.add_argument('--split', metavar='NAME', default='validation',\n                    help='dataset split (default: validation)')\nparser.add_argument('--num-samples', default=None, type=int,\n                    metavar='N', help='Manually specify num samples in dataset split, for IterableDatasets.')\nparser.add_argument('--dataset-download', action='store_true', default=False,\n                    help='Allow download of dataset for torch/ and tfds/ datasets that support it.')\nparser.add_argument('--class-map', default='', type=str, metavar='FILENAME',\n                    help='path to class to idx mapping file (default: \"\")')\nparser.add_argument('--input-key', default=None, type=str,\n                   help='Dataset key for input images.')\nparser.add_argument('--input-img-mode', default=None, type=str,\n                   help='Dataset image conversion mode for input images.')\nparser.add_argument('--target-key', default=None, type=str,\n                   help='Dataset key for target labels.')\nparser.add_argument('--dataset-trust-remote-code', action='store_true', default=False,\n                   help='Allow huggingface dataset import to execute code downloaded from the dataset\\'s repo.')\n\nparser.add_argument('--model', '-m', metavar='NAME', default='dpn92',\n                    help='model architecture (default: dpn92)')\nparser.add_argument('--pretrained', dest='pretrained', action='store_true',\n                    help='use pre-trained model')\nparser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n                    help='number of data loading workers (default: 4)')\nparser.add_argument('-b', '--batch-size', default=256, type=int,\n                    metavar='N', help='mini-batch size (default: 256)')\nparser.add_argument('--img-size', default=None, type=int,\n                    metavar='N', help='Input image dimension, uses model default if empty')\nparser.add_argument('--in-chans', type=int, default=None, metavar='N',\n                    help='Image input channels (default: None => 3)')\nparser.add_argument('--input-size', default=None, nargs=3, type=int,\n                    metavar='N N N', help='Input all image dimensions (d h w, e.g. --input-size 3 224 224), uses model default if empty')\nparser.add_argument('--use-train-size', action='store_true', default=False,\n                    help='force use of train input size, even when test size is specified in pretrained cfg')\nparser.add_argument('--crop-pct', default=None, type=float,\n                    metavar='N', help='Input image center crop pct')\nparser.add_argument('--crop-mode', default=None, type=str,\n                    metavar='N', help='Input image crop mode (squash, border, center). Model default if None.')\nparser.add_argument('--crop-border-pixels', type=int, default=None,\n                    help='Crop pixels from image border.')\nparser.add_argument('--mean', type=float, nargs='+', default=None, metavar='MEAN',\n                    help='Override mean pixel value of dataset')\nparser.add_argument('--std', type=float,  nargs='+', default=None, metavar='STD',\n                    help='Override std deviation of of dataset')\nparser.add_argument('--interpolation', default='', type=str, metavar='NAME',\n                    help='Image resize interpolation type (overrides model)')\nparser.add_argument('--num-classes', type=int, default=None,\n                    help='Number classes in dataset')\nparser.add_argument('--gp', default=None, type=str, metavar='POOL',\n                    help='Global pool type, one of (fast, avg, max, avgmax, avgmaxc). Model default if None.')\nparser.add_argument('--log-freq', default=10, type=int,\n                    metavar='N', help='batch logging frequency (default: 10)')\nparser.add_argument('--checkpoint', default='', type=str, metavar='PATH',\n                    help='path to latest checkpoint (default: none)')\nparser.add_argument('--num-gpu', type=int, default=1,\n                    help='Number of GPUS to use')\nparser.add_argument('--test-pool', dest='test_pool', action='store_true',\n                    help='enable test time pool')\nparser.add_argument('--no-prefetcher', action='store_true', default=False,\n                    help='disable fast prefetcher')\nparser.add_argument('--pin-mem', action='store_true', default=False,\n                    help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\nparser.add_argument('--channels-last', action='store_true', default=False,\n                    help='Use channels_last memory layout')\nparser.add_argument('--device', default='cuda', type=str,\n                    help=\"Device (accelerator) to use.\")\nparser.add_argument('--amp', action='store_true', default=False,\n                    help='use NVIDIA Apex AMP or Native AMP for mixed precision training')\nparser.add_argument('--amp-dtype', default='float16', type=str,\n                    help='lower precision AMP dtype (default: float16)')\nparser.add_argument('--amp-impl', default='native', type=str,\n                    help='AMP impl to use, \"native\" or \"apex\" (default: native)')\nparser.add_argument('--model-dtype', default=None, type=str,\n                   help='Model dtype override (non-AMP) (default: float32)')\nparser.add_argument('--tf-preprocessing', action='store_true', default=False,\n                    help='Use Tensorflow preprocessing pipeline (require CPU TF installed')\nparser.add_argument('--use-ema', dest='use_ema', action='store_true',\n                    help='use ema version of weights if present')\nparser.add_argument('--fuser', default='', type=str,\n                    help=\"Select jit fuser. One of ('', 'te', 'old', 'nvfuser')\")\nparser.add_argument('--fast-norm', default=False, action='store_true',\n                    help='enable experimental fast-norm')\nparser.add_argument('--reparam', default=False, action='store_true',\n                    help='Reparameterize model')\nparser.add_argument('--model-kwargs', nargs='*', default={}, action=ParseKwargs)\nparser.add_argument('--torchcompile-mode', type=str, default=None,\n                    help=\"torch.compile mode (default: None).\")\n\nscripting_group = parser.add_mutually_exclusive_group()\nscripting_group.add_argument('--torchscript', default=False, action='store_true',\n                             help='torch.jit.script the full model')\nscripting_group.add_argument('--torchcompile', nargs='?', type=str, default=None, const='inductor',\n                             help=\"Enable compilation w/ specified backend (default: inductor).\")\nscripting_group.add_argument('--aot-autograd', default=False, action='store_true',\n                             help=\"Enable AOT Autograd support.\")\n\nparser.add_argument('--results-file', default='', type=str, metavar='FILENAME',\n                    help='Output csv file for validation results (summary)')\nparser.add_argument('--results-format', default='csv', type=str,\n                    help='Format for results file one of (csv, json) (default: csv).')\nparser.add_argument('--real-labels', default='', type=str, metavar='FILENAME',\n                    help='Real labels JSON file for imagenet evaluation')\nparser.add_argument('--valid-labels', default='', type=str, metavar='FILENAME',\n                    help='Valid label indices txt file for validation of partial label space')\nparser.add_argument('--retry', default=False, action='store_true',\n                    help='Enable batch size decay & retry for single model validation')\n\n\ndef validate(args):\n    # might as well try to validate something\n    args.pretrained = args.pretrained or not args.checkpoint\n    args.prefetcher = not args.no_prefetcher\n\n    if torch.cuda.is_available():\n        torch.backends.cuda.matmul.allow_tf32 = True\n        torch.backends.cudnn.benchmark = True\n\n    device = torch.device(args.device)\n\n    model_dtype = None\n    if args.model_dtype:\n        assert args.model_dtype in ('float32', 'float16', 'bfloat16')\n        model_dtype = getattr(torch, args.model_dtype)\n\n    # resolve AMP arguments based on PyTorch / Apex availability\n    use_amp = None\n    amp_autocast = suppress\n    if args.amp:\n        assert model_dtype is None or model_dtype == torch.float32, 'float32 model dtype must be used with AMP'\n        if args.amp_impl == 'apex':\n            assert has_apex, 'AMP impl specified as APEX but APEX is not installed.'\n            assert args.amp_dtype == 'float16'\n            use_amp = 'apex'\n            _logger.info('Validating in mixed precision with NVIDIA APEX AMP.')\n        else:\n            assert args.amp_dtype in ('float16', 'bfloat16')\n            use_amp = 'native'\n            amp_dtype = torch.bfloat16 if args.amp_dtype == 'bfloat16' else torch.float16\n            amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)\n            _logger.info('Validating in mixed precision with native PyTorch AMP.')\n    else:\n        _logger.info(f'Validating in {model_dtype or torch.float32}. AMP not enabled.')\n\n    if args.fuser:\n        set_jit_fuser(args.fuser)\n\n    if args.fast_norm:\n        set_fast_norm()\n\n    # create model\n    in_chans = 3\n    if args.in_chans is not None:\n        in_chans = args.in_chans\n    elif args.input_size is not None:\n        in_chans = args.input_size[0]\n\n    model = create_model(\n        args.model,\n        pretrained=args.pretrained,\n        num_classes=args.num_classes,\n        in_chans=in_chans,\n        global_pool=args.gp,\n        scriptable=args.torchscript,\n        **args.model_kwargs,\n    )\n    if args.num_classes is None:\n        assert hasattr(model, 'num_classes'), 'Model must have `num_classes` attr if not set on cmd line/config.'\n        args.num_classes = model.num_classes\n\n    if args.checkpoint:\n        load_checkpoint(model, args.checkpoint, args.use_ema)\n\n    if args.reparam:\n        model = reparameterize_model(model)\n\n    param_count = sum([m.numel() for m in model.parameters()])\n    _logger.info('Model %s created, param count: %d' % (args.model, param_count))\n\n    data_config = resolve_data_config(\n        vars(args),\n        model=model,\n        use_test_size=not args.use_train_size,\n        verbose=True,\n    )\n    test_time_pool = False\n    if args.test_pool:\n        model, test_time_pool = apply_test_time_pool(model, data_config)\n\n    model = model.to(device=device, dtype=model_dtype)  # FIXME move model device & dtype into create_model\n    if args.channels_last:\n        model = model.to(memory_format=torch.channels_last)\n\n    if args.torchscript:\n        assert not use_amp == 'apex', 'Cannot use APEX AMP with torchscripted model'\n        model = torch.jit.script(model)\n    elif args.torchcompile:\n        assert has_compile, 'A version of torch w/ torch.compile() is required for --compile, possibly a nightly.'\n        torch._dynamo.reset()\n        model = torch.compile(model, backend=args.torchcompile, mode=args.torchcompile_mode)\n    elif args.aot_autograd:\n        assert has_functorch, \"functorch is needed for --aot-autograd\"\n        model = memory_efficient_fusion(model)\n\n    if use_amp == 'apex':\n        model = amp.initialize(model, opt_level='O1')\n\n    if args.num_gpu > 1:\n        model = torch.nn.DataParallel(model, device_ids=list(range(args.num_gpu)))\n\n    criterion = nn.CrossEntropyLoss().to(device)\n\n    root_dir = args.data or args.data_dir\n    if args.input_img_mode is None:\n        input_img_mode = 'RGB' if data_config['input_size'][0] == 3 else 'L'\n    else:\n        input_img_mode = args.input_img_mode\n    dataset = create_dataset(\n        root=root_dir,\n        name=args.dataset,\n        split=args.split,\n        download=args.dataset_download,\n        load_bytes=args.tf_preprocessing,\n        class_map=args.class_map,\n        num_samples=args.num_samples,\n        input_key=args.input_key,\n        input_img_mode=input_img_mode,\n        target_key=args.target_key,\n        trust_remote_code=args.dataset_trust_remote_code,\n    )\n\n    if args.valid_labels:\n        with open(args.valid_labels, 'r') as f:\n            valid_labels = [int(line.rstrip()) for line in f]\n    else:\n        valid_labels = None\n\n    if args.real_labels:\n        real_labels = RealLabelsImagenet(dataset.filenames(basename=True), real_json=args.real_labels)\n    else:\n        real_labels = None\n\n    crop_pct = 1.0 if test_time_pool else data_config['crop_pct']\n    loader = create_loader(\n        dataset,\n        input_size=data_config['input_size'],\n        batch_size=args.batch_size,\n        use_prefetcher=args.prefetcher,\n        interpolation=data_config['interpolation'],\n        mean=data_config['mean'],\n        std=data_config['std'],\n        num_workers=args.workers,\n        crop_pct=crop_pct,\n        crop_mode=data_config['crop_mode'],\n        crop_border_pixels=args.crop_border_pixels,\n        pin_memory=args.pin_mem,\n        device=device,\n        img_dtype=model_dtype or torch.float32,\n        tf_preprocessing=args.tf_preprocessing,\n    )\n\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    model.eval()\n    with torch.no_grad():\n        # warmup, reduce variability of first batch time, especially for comparing torchscript vs non\n        input = torch.randn((args.batch_size,) + tuple(data_config['input_size'])).to(device=device, dtype=model_dtype)\n        if args.channels_last:\n            input = input.contiguous(memory_format=torch.channels_last)\n        with amp_autocast():\n            model(input)\n\n        end = time.time()\n        for batch_idx, (input, target) in enumerate(loader):\n            if args.no_prefetcher:\n                target = target.to(device=device)\n                input = input.to(device=device, dtype=model_dtype)\n            if args.channels_last:\n                input = input.contiguous(memory_format=torch.channels_last)\n\n            # compute output\n            with amp_autocast():\n                output = model(input)\n\n                if valid_labels is not None:\n                    output = output[:, valid_labels]\n                loss = criterion(output, target)\n\n            if real_labels is not None:\n                real_labels.add_result(output)\n\n            # measure accuracy and record loss\n            acc1, acc5 = accuracy(output.detach(), target, topk=(1, 5))\n            losses.update(loss.item(), input.size(0))\n            top1.update(acc1.item(), input.size(0))\n            top5.update(acc5.item(), input.size(0))\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if batch_idx % args.log_freq == 0:\n                _logger.info(\n                    'Test: [{0:>4d}/{1}]  '\n                    'Time: {batch_time.val:.3f}s ({batch_time.avg:.3f}s, {rate_avg:>7.2f}/s)  '\n                    'Loss: {loss.val:>7.4f} ({loss.avg:>6.4f})  '\n                    'Acc@1: {top1.val:>7.3f} ({top1.avg:>7.3f})  '\n                    'Acc@5: {top5.val:>7.3f} ({top5.avg:>7.3f})'.format(\n                        batch_idx,\n                        len(loader),\n                        batch_time=batch_time,\n                        rate_avg=input.size(0) / batch_time.avg,\n                        loss=losses,\n                        top1=top1,\n                        top5=top5\n                    )\n                )\n\n    if real_labels is not None:\n        # real labels mode replaces topk values at the end\n        top1a, top5a = real_labels.get_accuracy(k=1), real_labels.get_accuracy(k=5)\n    else:\n        top1a, top5a = top1.avg, top5.avg\n    results = OrderedDict(\n        model=args.model,\n        top1=round(top1a, 4), top1_err=round(100 - top1a, 4),\n        top5=round(top5a, 4), top5_err=round(100 - top5a, 4),\n        param_count=round(param_count / 1e6, 2),\n        img_size=data_config['input_size'][-1],\n        crop_pct=crop_pct,\n        interpolation=data_config['interpolation'],\n    )\n\n    _logger.info(' * Acc@1 {:.3f} ({:.3f}) Acc@5 {:.3f} ({:.3f})'.format(\n       results['top1'], results['top1_err'], results['top5'], results['top5_err']))\n\n    return results\n\n\ndef _try_run(args, initial_batch_size):\n    batch_size = initial_batch_size\n    results = OrderedDict()\n    error_str = 'Unknown'\n    while batch_size:\n        args.batch_size = batch_size * args.num_gpu  # multiply by num-gpu for DataParallel case\n        try:\n            if 'cuda' in args.device and torch.cuda.is_available():\n                torch.cuda.empty_cache()\n            elif \"npu\" in args.device and torch.npu.is_available():\n                torch.npu.empty_cache()\n            results = validate(args)\n            return results\n        except RuntimeError as e:\n            error_str = str(e)\n            _logger.error(f'\"{error_str}\" while running validation.')\n            if not check_batch_size_retry(error_str):\n                break\n        batch_size = decay_batch_step(batch_size)\n        _logger.warning(f'Reducing batch size to {batch_size} for retry.')\n    results['error'] = error_str\n    _logger.error(f'{args.model} failed to validate ({error_str}).')\n    return results\n\n\n_NON_IN1K_FILTERS = ['*_in21k', '*_in22k', '*in12k', '*_dino', '*fcmae', '*seer']\n\n\ndef main():\n    setup_default_logging()\n    args = parser.parse_args()\n    model_cfgs = []\n    model_names = []\n    if os.path.isdir(args.checkpoint):\n        # validate all checkpoints in a path with same model\n        checkpoints = glob.glob(args.checkpoint + '/*.pth.tar')\n        checkpoints += glob.glob(args.checkpoint + '/*.pth')\n        model_names = list_models(args.model)\n        model_cfgs = [(args.model, c) for c in sorted(checkpoints, key=natural_key)]\n    else:\n        if args.model == 'all':\n            # validate all models in a list of names with pretrained checkpoints\n            args.pretrained = True\n            model_names = list_models(\n                pretrained=True,\n                exclude_filters=_NON_IN1K_FILTERS,\n            )\n            model_cfgs = [(n, '') for n in model_names]\n        elif not is_model(args.model):\n            # model name doesn't exist, try as wildcard filter\n            model_names = list_models(\n                args.model,\n                pretrained=True,\n            )\n            model_cfgs = [(n, '') for n in model_names]\n\n        if not model_cfgs and os.path.isfile(args.model):\n            with open(args.model) as f:\n                model_names = [line.rstrip() for line in f]\n            model_cfgs = [(n, None) for n in model_names if n]\n\n    if len(model_cfgs):\n        _logger.info('Running bulk validation on these pretrained models: {}'.format(', '.join(model_names)))\n        results = []\n        try:\n            initial_batch_size = args.batch_size\n            for m, c in model_cfgs:\n                args.model = m\n                args.checkpoint = c\n                r = _try_run(args, initial_batch_size)\n                if 'error' in r:\n                    continue\n                if args.checkpoint:\n                    r['checkpoint'] = args.checkpoint\n                results.append(r)\n        except KeyboardInterrupt as e:\n            pass\n        results = sorted(results, key=lambda x: x['top1'], reverse=True)\n    else:\n        if args.retry:\n            results = _try_run(args, args.batch_size)\n        else:\n            results = validate(args)\n\n    if args.results_file:\n        write_results(args.results_file, results, format=args.results_format)\n\n    # output results in JSON to stdout w/ delimiter for runner script\n    print(f'--result\\n{json.dumps(results, indent=4)}')\n\n\ndef write_results(results_file, results, format='csv'):\n    with open(results_file, mode='w') as cf:\n        if format == 'json':\n            json.dump(results, cf, indent=4)\n        else:\n            if not isinstance(results, (list, tuple)):\n                results = [results]\n            if not results:\n                return\n            dw = csv.DictWriter(cf, fieldnames=results[0].keys())\n            dw.writeheader()\n            for r in results:\n                dw.writerow(r)\n            cf.flush()\n\n\n\nif __name__ == '__main__':\n    main()\n"
        }
      ]
    }
  ]
}