{
  "metadata": {
    "timestamp": 1736557455166,
    "page": 475,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjUwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "microsoft/DeepSpeed",
      "stars": 36177,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".clang-format",
          "type": "blob",
          "size": 4.4,
          "content": "---\n# Refer to the following link for the explanation of each params:\n#   http://releases.llvm.org/8.0.0/tools/clang/docs/ClangFormatStyleOptions.html\nLanguage: Cpp\n# BasedOnStyle: Google\nAccessModifierOffset: -4\nAlignAfterOpenBracket: Align\nAlignConsecutiveAssignments: false\nAlignConsecutiveDeclarations: false\nAlignEscapedNewlines: Left\nAlignOperands: true\nAlignTrailingComments: true\nAllowAllParametersOfDeclarationOnNextLine: false\nAllowShortBlocksOnASingleLine: true\nAllowShortCaseLabelsOnASingleLine: true\nAllowShortFunctionsOnASingleLine: All\nAllowShortIfStatementsOnASingleLine: true\nAllowShortLoopsOnASingleLine: true\n# This is deprecated\nAlwaysBreakAfterDefinitionReturnType: None\nAlwaysBreakAfterReturnType: None\nAlwaysBreakBeforeMultilineStrings: true\nAlwaysBreakTemplateDeclarations: true\nBinPackArguments:  false\nBinPackParameters: false\nBraceWrapping:\n  AfterClass:            false\n  AfterControlStatement: false\n  AfterEnum:             false\n  AfterFunction:         false\n  AfterNamespace:        false\n  AfterObjCDeclaration:  false\n  AfterStruct:           false\n  AfterUnion:            false\n  AfterExternBlock:      false\n  BeforeCatch:           false\n  BeforeElse:            false\n  IndentBraces:          false\n  # disabling the below splits, else, they'll just add to the vertical length of source files!\n  SplitEmptyFunction: false\n  SplitEmptyRecord: false\n  SplitEmptyNamespace: false\nBreakBeforeBinaryOperators: None\nBreakBeforeBraces: WebKit\nBreakBeforeInheritanceComma: false\nBreakInheritanceList: BeforeColon\nBreakBeforeTernaryOperators: true\nBreakConstructorInitializersBeforeComma: false\nBreakConstructorInitializers: BeforeColon\nBreakAfterJavaFieldAnnotations: false\nBreakStringLiterals: true\nColumnLimit: 100\nCommentPragmas: '^ IWYU pragma:'\nCompactNamespaces: false\nConstructorInitializerAllOnOneLineOrOnePerLine: true\n# Kept the below 2 to be the same as `IndentWidth` to keep everything uniform\nConstructorInitializerIndentWidth: 4\nContinuationIndentWidth: 4\nCpp11BracedListStyle: true\nDerivePointerAlignment: false\nDisableFormat: false\nExperimentalAutoDetectBinPacking: false\nFixNamespaceComments: true\nForEachMacros:\n  - foreach\n  - Q_FOREACH\n  - BOOST_FOREACH\nIncludeBlocks: Preserve\nIncludeCategories:\n  - Regex:           '^<ext/.*\\.h>'\n    Priority:        2\n  - Regex:           '^<.*\\.h>'\n    Priority:        1\n  - Regex:           '^<.*'\n    Priority:        2\n  - Regex:           '.*'\n    Priority:        3\nIncludeIsMainRegex: '([-_](test|unittest))?$'\nIndentCaseLabels: true\nIndentPPDirectives: None\nIndentWidth:     4\nIndentWrappedFunctionNames: false\nJavaScriptQuotes: Leave\nJavaScriptWrapImports: true\nKeepEmptyLinesAtTheStartOfBlocks: false\nMacroBlockBegin: ''\nMacroBlockEnd:   ''\nMaxEmptyLinesToKeep: 1\nNamespaceIndentation: None\nObjCBinPackProtocolList: Never\nObjCBlockIndentWidth: 4\nObjCSpaceAfterProperty: false\nObjCSpaceBeforeProtocolList: true\nPenaltyBreakAssignment: 4\nPenaltyBreakBeforeFirstCallParameter: 1\nPenaltyBreakComment: 300\nPenaltyBreakFirstLessLess: 120\nPenaltyBreakString: 1000\nPenaltyBreakTemplateDeclaration: 10\nPenaltyExcessCharacter: 1000000\nPenaltyReturnTypeOnItsOwnLine: 200\nPointerAlignment: Left\nRawStringFormats:\n  - Language: Cpp\n    Delimiters:\n      - cc\n      - CC\n      - cpp\n      - Cpp\n      - CPP\n      - 'c++'\n      - 'C++'\n    CanonicalDelimiter: ''\n  - Language: TextProto\n    Delimiters:\n      - pb\n      - PB\n      - proto\n      - PROTO\n    EnclosingFunctions:\n      - EqualsProto\n      - EquivToProto\n      - PARSE_PARTIAL_TEXT_PROTO\n      - PARSE_TEST_PROTO\n      - PARSE_TEXT_PROTO\n      - ParseTextOrDie\n      - ParseTextProtoOrDie\n    CanonicalDelimiter: ''\n    BasedOnStyle: google\n# Enabling comment reflow causes doxygen comments to be messed up in their formats!\nReflowComments: true\nSortIncludes: true\nSortUsingDeclarations: true\nSpaceAfterCStyleCast: false\nSpaceAfterTemplateKeyword: true\nSpaceBeforeAssignmentOperators: true\nSpaceBeforeCpp11BracedList: false\nSpaceBeforeCtorInitializerColon: true\nSpaceBeforeInheritanceColon: true\nSpaceBeforeParens: ControlStatements\nSpaceBeforeRangeBasedForLoopColon: true\nSpaceInEmptyParentheses: false\nSpacesBeforeTrailingComments: 2\nSpacesInAngles: false\nSpacesInContainerLiterals: true\nSpacesInCStyleCastParentheses: false\nSpacesInParentheses: false\nSpacesInSquareBrackets: false\nStandard: Cpp11\nStatementMacros:\n  - Q_UNUSED\n  - QT_REQUIRE_VERSION\n# Be consistent with indent-width, even for people who use tab for indentation!\nTabWidth: 4\nUseTab: Never\n"
        },
        {
          "name": ".flake8",
          "type": "blob",
          "size": 0.1,
          "content": "[flake8]\nignore = E,F403,F405,F541,F841,W\nselect = E9,F,W6\nper-file-ignores =\n    __init__.py:F401\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1,
          "content": "## Ignore Python compiled files\n*.pyc\n\n## Ignore IDE-specific files and directories\n# JetBrains IDE settings\n.idea/\n# Visual Studio Code settings\n.vscode/\n# Theia IDE settings\n.theia/\n\n## Ignore temporary and backup files\n# General backup files\n*~\n# Vim swap files\n*.swp\n\n## Ignore log files\n*.log\n\n## Ignore a specific generated file\ndeepspeed/git_version_info_installed.py\n\n## Ignore Python bytecode cache\n__pycache__\n\n## Build + installation data\n# Build artifacts\nbuild/\n# Distribution files\ndist/\n# Compiled shared objects\n*.so\n# Deepspeed package info\ndeepspeed.egg-info/\n# Build information\nbuild.txt\n\n## Website generated files\n# Jekyll generated site\ndocs/_site/\n# Generated documentation\ndocs/build\ndocs/code-docs/source/_build\ndocs/code-docs/_build\ndocs/code-docs/build\n# SASS cache\n.sass-cache/\n# Jekyll cache\n.jekyll-cache/\n.jekyll-metadata\n\n## Testing data\n# Saved checkpoints for testing\ntests/unit/saved_checkpoint/\n\n# HIP files created during AMD compilation\n*_hip.cpp\n*_hip.h\n*.hip\n*.cuh\n*hip_layers.h\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0,
          "content": null
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 3.38,
          "content": "repos:\n-   repo: meta\n    hooks:\n    -   id: check-hooks-apply\n    -   id: check-useless-excludes\n\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v5.0.0\n    hooks:\n    -   id: check-case-conflict\n    -   id: check-json\n    -   id: check-symlinks\n    -   id: check-yaml\n    -   id: destroyed-symlinks\n    -   id: end-of-file-fixer\n        exclude: docs/CNAME\n    -   id: fix-byte-order-marker\n    -   id: fix-encoding-pragma\n        args: [--remove]\n    -   id: mixed-line-ending\n        args: [--fix=lf]\n    -   id: requirements-txt-fixer\n    -   id: trailing-whitespace\n\n-   repo: https://github.com/google/yapf\n    rev: v0.40.0\n    hooks:\n    -   id: yapf\n\n-   repo: https://gitlab.com/daverona/pre-commit/cpp\n    rev: 0.8.0\n    hooks:\n    -   id: clang-format  # formatter of C/C++ code based on a style guide: LLVM, Google, Chromium, Mozilla, and WebKit available\n        args: []\n\n-   repo: local\n    hooks:\n    -   id: check-torchdist\n        name: check-torchdist\n        entry: ./scripts/check-torchdist.py\n        language: python\n        exclude: ^(deepspeed/comm/|docs/|benchmarks/|scripts/check-torchdist.py|deepspeed/moe/sharded_moe.py|deepspeed/runtime/comm/coalesced_collectives.py|deepspeed/elasticity/elastic_agent.py|deepspeed/launcher/launch.py|tests/unit/comm/test_dist.py)\n        # Specific deepspeed/ files are excluded for now until we wrap ProcessGroup in deepspeed.comm\n\n-   repo: local\n    hooks:\n    -   id: check-license\n        name: check-license\n        entry: ./scripts/check-license.py\n        language: python\n        files: \\.(py|c|cpp|cu|cc|h|hpp|cuh|hip|tr)$\n        exclude: ^(deepspeed/inference/v2/kernels/ragged_ops/blocked_flash|deepspeed/inference/v2/kernels/cutlass_ops/grouped_gemm)\n\n-   repo: https://github.com/codespell-project/codespell\n    rev: v2.1.0\n    hooks:\n    -   id: codespell\n        args: [\n            # Do not check files that are automatically generated\n            '--skip=docs/Gemfile.lock,tests/unit/gpt2-merges.txt,tests/unit/gpt2-vocab.json',\n            '--ignore-regex=\\\\n',  # Do not count the 'n' in an escaped newline as part of a word\n            '--ignore-words-list=youn,unsupport,noe,cann',  # Word used in error messages that need rewording\n            --check-filenames,\n            --check-hidden\n        ]\n\n-   repo: https://github.com/pycqa/flake8\n    rev: 5.0.4\n    hooks:\n    -   id: flake8\n        args: ['--config=.flake8']\n\n-   repo: local\n    hooks:\n    -   id: check-torchcuda\n        name: check-torchcuda\n        entry: ./scripts/check-torchcuda.py\n        language: python\n        exclude: ^(.github/workflows/|scripts/check-torchcuda.py|docs/_tutorials/accelerator-abstraction-interface.md|docs/_tutorials/deepnvme.md|accelerator/cuda_accelerator.py|deepspeed/inference/engine.py|deepspeed/model_implementations/transformers/clip_encoder.py|deepspeed/model_implementations/diffusers/vae.py|deepspeed/model_implementations/diffusers/unet.py|op_builder/spatial_inference.py|op_builder/transformer_inference.py|op_builder/builder.py|setup.py|tests/unit/ops/sparse_attention/test_sparse_attention.py)\n        # Specific deepspeed/ files are excluded for now until we wrap ProcessGroup in deepspeed.comm\n\n-   repo: local\n    hooks:\n    -   id: check-extraindexurl\n        name: check-extraindexurl\n        entry: ./scripts/check-extraindexurl.py\n        language: python\n        files: \\.(yml|yaml|sh|py)$\n        exclude: ^(scripts/check-extraindexurl.py)\n"
        },
        {
          "name": ".pylintrc",
          "type": "blob",
          "size": 17.46,
          "content": "[MASTER]\n\n# A comma-separated list of package or module names from where C extensions may\n# be loaded. Extensions are loading into the active Python interpreter and may\n# run arbitrary code.\nextension-pkg-whitelist=\n\n# Add files or directories to the blacklist. They should be base names, not\n# paths.\nignore=CVS\n\n# Add files or directories matching the regex patterns to the blacklist. The\n# regex matches against base names, not paths.\nignore-patterns=\n\n# Python code to execute, usually for sys.path manipulation such as\n# pygtk.require().\n#init-hook=\n\n# Use multiple processes to speed up Pylint. Specifying 0 will auto-detect the\n# number of processors available to use.\njobs=1\n\n# Control the amount of potential inferred values when inferring a single\n# object. This can help the performance when dealing with large functions or\n# complex, nested conditions.\nlimit-inference-results=100\n\n# List of plugins (as comma separated values of python module names) to load,\n# usually to register additional checkers.\nload-plugins=\n\n# Pickle collected data for later comparisons.\npersistent=yes\n\n# Specify a configuration file.\n#rcfile=\n\n# When enabled, pylint would attempt to guess common misconfiguration and emit\n# user-friendly hints instead of false-positive error messages.\nsuggestion-mode=yes\n\n# Allow loading of arbitrary C extensions. Extensions are imported into the\n# active Python interpreter and may run arbitrary code.\nunsafe-load-any-extension=no\n\n\n[MESSAGES CONTROL]\n\n# Only show warnings with the listed confidence levels. Leave empty to show\n# all. Valid levels: HIGH, INFERENCE, INFERENCE_FAILURE, UNDEFINED.\nconfidence=\n\n# Disable the message, report, category or checker with the given id(s). You\n# can either give multiple identifiers separated by comma (,) or put this\n# option multiple times (only on the command line, not in the configuration\n# file where it should appear only once). You can also use \"--disable=all\" to\n# disable everything first and then re-enable specific checks. For example, if\n# you want to run only the similarities checker, you can use \"--disable=all\n# --enable=similarities\". If you want to run only the classes checker, but have\n# no Warning level messages displayed, use \"--disable=all --enable=classes\n# --disable=W\".\ndisable=print-statement,\n        parameter-unpacking,\n        unpacking-in-except,\n        old-raise-syntax,\n        backtick,\n        long-suffix,\n        old-ne-operator,\n        old-octal-literal,\n        import-star-module-level,\n        non-ascii-bytes-literal,\n        raw-checker-failed,\n        bad-inline-option,\n        locally-disabled,\n        file-ignored,\n        suppressed-message,\n        useless-suppression,\n        deprecated-pragma,\n        use-symbolic-message-instead,\n        apply-builtin,\n        basestring-builtin,\n        buffer-builtin,\n        cmp-builtin,\n        coerce-builtin,\n        execfile-builtin,\n        file-builtin,\n        long-builtin,\n        raw_input-builtin,\n        reduce-builtin,\n        standarderror-builtin,\n        unicode-builtin,\n        xrange-builtin,\n        coerce-method,\n        delslice-method,\n        getslice-method,\n        setslice-method,\n        no-absolute-import,\n        old-division,\n        dict-iter-method,\n        dict-view-method,\n        next-method-called,\n        metaclass-assignment,\n        indexing-exception,\n        raising-string,\n        reload-builtin,\n        oct-method,\n        hex-method,\n        nonzero-method,\n        cmp-method,\n        input-builtin,\n        round-builtin,\n        intern-builtin,\n        unichr-builtin,\n        map-builtin-not-iterating,\n        zip-builtin-not-iterating,\n        range-builtin-not-iterating,\n        filter-builtin-not-iterating,\n        using-cmp-argument,\n        eq-without-hash,\n        div-method,\n        idiv-method,\n        rdiv-method,\n        exception-message-attribute,\n        invalid-str-codec,\n        sys-max-int,\n        bad-python3-import,\n        deprecated-string-function,\n        deprecated-str-translate-call,\n        deprecated-itertools-function,\n        deprecated-types-field,\n        next-method-defined,\n        dict-items-not-iterating,\n        dict-keys-not-iterating,\n        dict-values-not-iterating,\n        deprecated-operator-function,\n        deprecated-urllib-function,\n        xreadlines-attribute,\n        deprecated-sys-function,\n        exception-escape,\n        comprehension-escape\n\n# Enable the message, report, category or checker with the given id(s). You can\n# either give multiple identifier separated by comma (,) or put this option\n# multiple time (only on the command line, not in the configuration file where\n# it should appear only once). See also the \"--disable\" option for examples.\nenable=c-extension-no-member\n\n\n[REPORTS]\n\n# Python expression which should return a score less than or equal to 10. You\n# have access to the variables 'error', 'warning', 'refactor', and 'convention'\n# which contain the number of messages in each category, as well as 'statement'\n# which is the total number of statements analyzed. This score is used by the\n# global evaluation report (RP0004).\nevaluation=10.0 - ((float(5 * error + warning + refactor + convention) / statement) * 10)\n\n# Template used to display messages. This is a python new-style format string\n# used to format the message information. See doc for all details.\n#msg-template=\n\n# Set the output format. Available formats are text, parseable, colorized, json\n# and msvs (visual studio). You can also give a reporter class, e.g.\n# mypackage.mymodule.MyReporterClass.\noutput-format=text\n\n# Tells whether to display a full report or only the messages.\nreports=no\n\n# Activate the evaluation score.\nscore=yes\n\n\n[REFACTORING]\n\n# Maximum number of nested blocks for function / method body\nmax-nested-blocks=5\n\n# Complete name of functions that never returns. When checking for\n# inconsistent-return-statements if a never returning function is called then\n# it will be considered as an explicit return statement and no message will be\n# printed.\nnever-returning-functions=sys.exit\n\n\n[BASIC]\n\n# Naming style matching correct argument names.\nargument-naming-style=snake_case\n\n# Regular expression matching correct argument names. Overrides argument-\n# naming-style.\n#argument-rgx=\n\n# Naming style matching correct attribute names.\nattr-naming-style=snake_case\n\n# Regular expression matching correct attribute names. Overrides attr-naming-\n# style.\n#attr-rgx=\n\n# Bad variable names which should always be refused, separated by a comma.\nbad-names=foo,\n          bar,\n          baz,\n          toto,\n          tutu,\n          tata\n\n# Naming style matching correct class attribute names.\nclass-attribute-naming-style=any\n\n# Regular expression matching correct class attribute names. Overrides class-\n# attribute-naming-style.\n#class-attribute-rgx=\n\n# Naming style matching correct class names.\nclass-naming-style=PascalCase\n\n# Regular expression matching correct class names. Overrides class-naming-\n# style.\n#class-rgx=\n\n# Naming style matching correct constant names.\nconst-naming-style=UPPER_CASE\n\n# Regular expression matching correct constant names. Overrides const-naming-\n# style.\n#const-rgx=\n\n# Minimum line length for functions/classes that require docstrings, shorter\n# ones are exempt.\ndocstring-min-length=-1\n\n# Naming style matching correct function names.\nfunction-naming-style=snake_case\n\n# Regular expression matching correct function names. Overrides function-\n# naming-style.\n#function-rgx=\n\n# Good variable names which should always be accepted, separated by a comma.\ngood-names=i,\n           j,\n           k,\n           ex,\n           Run,\n           _\n\n# Include a hint for the correct naming format with invalid-name.\ninclude-naming-hint=no\n\n# Naming style matching correct inline iteration names.\ninlinevar-naming-style=any\n\n# Regular expression matching correct inline iteration names. Overrides\n# inlinevar-naming-style.\n#inlinevar-rgx=\n\n# Naming style matching correct method names.\nmethod-naming-style=snake_case\n\n# Regular expression matching correct method names. Overrides method-naming-\n# style.\n#method-rgx=\n\n# Naming style matching correct module names.\nmodule-naming-style=snake_case\n\n# Regular expression matching correct module names. Overrides module-naming-\n# style.\n#module-rgx=\n\n# Colon-delimited sets of names that determine each other's naming style when\n# the name regexes allow several styles.\nname-group=\n\n# Regular expression which should only match function or class names that do\n# not require a docstring.\nno-docstring-rgx=^_\n\n# List of decorators that produce properties, such as abc.abstractproperty. Add\n# to this list to register other decorators that produce valid properties.\n# These decorators are taken in consideration only for invalid-name.\nproperty-classes=abc.abstractproperty\n\n# Naming style matching correct variable names.\nvariable-naming-style=snake_case\n\n# Regular expression matching correct variable names. Overrides variable-\n# naming-style.\n#variable-rgx=\n\n\n[LOGGING]\n\n# Format style used to check logging format string. `old` means using %\n# formatting, `new` is for `{}` formatting,and `fstr` is for f-strings.\nlogging-format-style=old\n\n# Logging modules to check that the string format arguments are in logging\n# function parameter format.\nlogging-modules=logging\n\n\n[TYPECHECK]\n\n# List of decorators that produce context managers, such as\n# contextlib.contextmanager. Add to this list to register other decorators that\n# produce valid context managers.\ncontextmanager-decorators=contextlib.contextmanager\n\n# List of members which are set dynamically and missed by pylint inference\n# system, and so shouldn't trigger E1101 when accessed. Python regular\n# expressions are accepted.\ngenerated-members=\n\n# Tells whether missing members accessed in mixin class should be ignored. A\n# mixin class is detected if its name ends with \"mixin\" (case insensitive).\nignore-mixin-members=yes\n\n# Tells whether to warn about missing members when the owner of the attribute\n# is inferred to be None.\nignore-none=yes\n\n# This flag controls whether pylint should warn about no-member and similar\n# checks whenever an opaque object is returned when inferring. The inference\n# can return multiple potential results while evaluating a Python object, but\n# some branches might not be evaluated, which results in partial inference. In\n# that case, it might be useful to still emit no-member and other checks for\n# the rest of the inferred objects.\nignore-on-opaque-inference=yes\n\n# List of class names for which member attributes should not be checked (useful\n# for classes with dynamically set attributes). This supports the use of\n# qualified names.\nignored-classes=optparse.Values,thread._local,_thread._local\n\n# List of module names for which member attributes should not be checked\n# (useful for modules/projects where namespaces are manipulated during runtime\n# and thus existing member attributes cannot be deduced by static analysis). It\n# supports qualified module names, as well as Unix pattern matching.\nignored-modules=\n\n# Show a hint with possible names when a member name was not found. The aspect\n# of finding the hint is based on edit distance.\nmissing-member-hint=yes\n\n# The minimum edit distance a name should have in order to be considered a\n# similar match for a missing member name.\nmissing-member-hint-distance=1\n\n# The total number of similar names that should be taken in consideration when\n# showing a hint for a missing member.\nmissing-member-max-choices=1\n\n# List of decorators that change the signature of a decorated function.\nsignature-mutators=\n\n\n[SIMILARITIES]\n\n# Ignore comments when computing similarities.\nignore-comments=yes\n\n# Ignore docstrings when computing similarities.\nignore-docstrings=yes\n\n# Ignore imports when computing similarities.\nignore-imports=no\n\n# Minimum lines number of a similarity.\nmin-similarity-lines=4\n\n\n[STRING]\n\n# This flag controls whether the implicit-str-concat-in-sequence should\n# generate a warning on implicit string concatenation in sequences defined over\n# several lines.\ncheck-str-concat-over-line-jumps=no\n\n\n[VARIABLES]\n\n# List of additional names supposed to be defined in builtins. Remember that\n# you should avoid defining new builtins when possible.\nadditional-builtins=\n\n# Tells whether unused global variables should be treated as a violation.\nallow-global-unused-variables=yes\n\n# List of strings which can identify a callback function by name. A callback\n# name must start or end with one of those strings.\ncallbacks=cb_,\n          _cb\n\n# A regular expression matching the name of dummy variables (i.e. expected to\n# not be used).\ndummy-variables-rgx=_+$|(_[a-zA-Z0-9_]*[a-zA-Z0-9]+?$)|dummy|^ignored_|^unused_\n\n# Argument names that match this expression will be ignored. Default to name\n# with leading underscore.\nignored-argument-names=_.*|^ignored_|^unused_\n\n# Tells whether we should check for unused import in __init__ files.\ninit-import=no\n\n# List of qualified module names which can have objects that can redefine\n# builtins.\nredefining-builtins-modules=six.moves,past.builtins,future.builtins,builtins,io\n\n\n[FORMAT]\n\n# Expected format of line ending, e.g. empty (any line ending), LF or CRLF.\nexpected-line-ending-format=\n\n# Regexp for a line that is allowed to be longer than the limit.\nignore-long-lines=^\\s*(# )?<?https?://\\S+>?$\n\n# Number of spaces of indent required inside a hanging or continued line.\nindent-after-paren=4\n\n# String used as indentation unit. This is usually \"    \" (4 spaces) or \"\\t\" (1\n# tab).\nindent-string='    '\n\n# Maximum number of characters on a single line.\nmax-line-length=90\n\n# Maximum number of lines in a module.\nmax-module-lines=1000\n\n# List of optional constructs for which whitespace checking is disabled. `dict-\n# separator` is used to allow tabulation in dicts, etc.: {1  : 1,\\n222: 2}.\n# `trailing-comma` allows a space between comma and closing bracket: (a, ).\n# `empty-line` allows space-only lines.\nno-space-check=trailing-comma,\n               dict-separator\n\n# Allow the body of a class to be on the same line as the declaration if body\n# contains single statement.\nsingle-line-class-stmt=no\n\n# Allow the body of an if to be on the same line as the test if there is no\n# else.\nsingle-line-if-stmt=no\n\n\n[MISCELLANEOUS]\n\n# List of note tags to take in consideration, separated by a comma.\nnotes=FIXME,\n      XXX,\n      TODO\n\n\n[SPELLING]\n\n# Limits count of emitted suggestions for spelling mistakes.\nmax-spelling-suggestions=4\n\n# Spelling dictionary name. Available dictionaries: none. To make it work,\n# install the python-enchant package.\nspelling-dict=\n\n# List of comma separated words that should not be checked.\nspelling-ignore-words=\n\n# A path to a file that contains the private dictionary; one word per line.\nspelling-private-dict-file=\n\n# Tells whether to store unknown words to the private dictionary (see the\n# --spelling-private-dict-file option) instead of raising a message.\nspelling-store-unknown-words=no\n\n\n[CLASSES]\n\n# List of method names used to declare (i.e. assign) instance attributes.\ndefining-attr-methods=__init__,\n                      __new__,\n                      setUp,\n                      __post_init__\n\n# List of member names, which should be excluded from the protected access\n# warning.\nexclude-protected=_asdict,\n                  _fields,\n                  _replace,\n                  _source,\n                  _make\n\n# List of valid names for the first argument in a class method.\nvalid-classmethod-first-arg=cls\n\n# List of valid names for the first argument in a metaclass class method.\nvalid-metaclass-classmethod-first-arg=cls\n\n\n[DESIGN]\n\n# Maximum number of arguments for function / method.\nmax-args=10\n\n# Maximum number of attributes for a class (see R0902).\nmax-attributes=20\n\n# Maximum number of boolean expressions in an if statement (see R0916).\nmax-bool-expr=5\n\n# Maximum number of branch for function / method body.\nmax-branches=12\n\n# Maximum number of locals for function / method body.\nmax-locals=15\n\n# Maximum number of parents for a class (see R0901).\nmax-parents=7\n\n# Maximum number of public methods for a class (see R0904).\nmax-public-methods=20\n\n# Maximum number of return / yield for function / method body.\nmax-returns=6\n\n# Maximum number of statements in function / method body.\nmax-statements=50\n\n# Minimum number of public methods for a class (see R0903).\nmin-public-methods=2\n\n\n[IMPORTS]\n\n# List of modules that can be imported at any level, not just the top level\n# one.\nallow-any-import-level=\n\n# Allow wildcard imports from modules that define __all__.\nallow-wildcard-with-all=no\n\n# Analyse import fallback blocks. This can be used to support both Python 2 and\n# 3 compatible code, which means that the block might have code that exists\n# only in one or another interpreter, leading to false positives when analysed.\nanalyse-fallback-blocks=no\n\n# Deprecated modules which should not be used, separated by a comma.\ndeprecated-modules=optparse,tkinter.tix\n\n# Create a graph of external dependencies in the given file (report RP0402 must\n# not be disabled).\next-import-graph=\n\n# Create a graph of every (i.e. internal and external) dependencies in the\n# given file (report RP0402 must not be disabled).\nimport-graph=\n\n# Create a graph of internal dependencies in the given file (report RP0402 must\n# not be disabled).\nint-import-graph=\n\n# Force import order to recognize a module as part of the standard\n# compatibility libraries.\nknown-standard-library=\n\n# Force import order to recognize a module as part of a third party library.\nknown-third-party=enchant\n\n# Couples of modules and preferred modules, separated by a comma.\npreferred-modules=\n\n\n[EXCEPTIONS]\n\n# Exceptions that will emit a warning when being caught. Defaults to\n# \"BaseException, Exception\".\novergeneral-exceptions=BaseException,\n                       Exception\n"
        },
        {
          "name": ".readthedocs.yml",
          "type": "blob",
          "size": 0.45,
          "content": "# Required\nversion: 2\nbuild:\n  os: \"ubuntu-22.04\"\n  tools:\n    python: \"3.8\"\n\n# Build documentation in the docs/ directory with Sphinx\nsphinx:\n  configuration: docs/code-docs/source/conf.py\n  fail_on_warning: false\n\n# Optionally build your docs in additional formats such as PDF\nformats:\n  - pdf\n\n# Optionally set the version of Python and requirements required to build your docs\npython:\n  install:\n    - requirements: requirements/requirements-readthedocs.txt\n"
        },
        {
          "name": ".style.yapf",
          "type": "blob",
          "size": 0.07,
          "content": "[style]\nSPLIT_ALL_COMMA_SEPARATED_VALUES = false\nCOLUMN_LIMIT = 119\n"
        },
        {
          "name": "CODEOWNERS",
          "type": "blob",
          "size": 1.91,
          "content": "# This file is used to subscribe for notifications for PRs\n# related to specific file paths, does not necessarily mean\n# approval is required from these people before merging.\n#\n# Learn more about CODEOWNERS syntax here:\n# https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-code-owners\n\n\n# top-level repo folders\n/.github/ @loadams\n/azure/ @loadams\n/benchmarks/ @guanhuawang @tjruwase\n/bin/ @loadams\n/csrc/ @tjruwase\n/deepspeed/ @loadams @tjruwase\n/docker/ @loadams @guanhuawang\n/docs/ @loadams @tjruwase\n/examples/ @jomayeri @tohtana\n/op_builder/ @loadams @tjruwase @jomayeri\n/release/ @loadams @jomayeri\n/requirements/ @loadams\n/scripts/ @loadams @tjruwase\n/tests/ @tjruwase @loadams @tohtana\n\n# deepspeed\n/deepspeed/autotuning/ @loadams\n/deepspeed/checkpoint/ @tjruwase\n/deepspeed/comm/ @guanhuawang\n/deepspeed/compression/ @tjruwase\n/deepspeed/elasticity/ @tjruwase\n/deepspeed/launcher/ @loadams\n/deepspeed/module_inject/ @hwchen2017 @loadams\n/deepspeed/moe/ @tohtana\n/deepspeed/monitor/ @tjruwase\n/deepspeed/nebula/ @tjruwase\n/deepspeed/nvme/ @tjruwase @jomayeri\n/deepspeed/ops/ @tohtana\n/deepspeed/pipe/ @tohtana @loadams\n/deepspeed/profiling/ @loadams\n/deepspeed/sequence/ @tohtana\n/deepspeed/utils/ @tjruwase @tohtana\n\n# inference\n/deepspeed/inference/ @hwchen2017 @tohtana\n/deepspeed/model_implementations/@tohtana @loadams\n\n# training\n/deepspeed/runtime/ @tjruwase @tohtana\n/deepspeed/runtime/activation_checkpointing/ @tjruwase\n/deepspeed/runtime/checkpoint_engine/ @tjruwase\n/deepspeed/runtime/comm/ @guanhuawang\n/deepspeed/runtime/compression/ @tjruwase\n/deepspeed/runtime/data_pipeline/ @tjruwase\n/deepspeed/runtime/domino/ @guanhuawang @hwchen2017\n/deepspeed/runtime/fp16/ @tjruwase @tohtana\n/deepspeed/runtime/fp16/onebit/ @tjruwase\n/deepspeed/runtime/pipe/ @loadams @tohtana\n/deepspeed/runtime/swap_tensor/ @tjruwase @jomayeri\n/deepspeed/runtime/zero/ @tjruwase @tohtana\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 0.43,
          "content": "# Microsoft Open Source Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n\nResources:\n\n- [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/)\n- [Microsoft Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\n- Contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with questions or concerns\n"
        },
        {
          "name": "COMMITTERS.md",
          "type": "blob",
          "size": 0.63,
          "content": "# DeepSpeed TSC Committers #\n\n| Name | GitHub ID | Affiliation\n|--- | ---- | --- |\n| Olatunji Ruwase | [tjruwase](https://github.com/tjruwase)     | Microsoft |\n| Logan Adams     | [loadams](https://github.com/loadams)      | Microsoft |\n| Masahiro Tanaka | [tohtana](https://github.com/tohtana)      | Microsoft |\n| Jeff Rasley     | [jeffra](https://github.com/jeffra)       | SnowFlake  |\n| Minjia Zhang    | [minjiazhang](https://github.com/minjiazhang)  | UIUC  |\n| Ashwin Aji      | [ashwinma](https://github.com/ashwinma)        | AMD   |\n| Sam Foreman     | [saforem2](https://github.com/saforem2)        | Argonne National Laboratory |\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 5.82,
          "content": "# Contributing\nDeepSpeed welcomes your contributions!\n\n## Prerequisites\nDeepSpeed uses [pre-commit](https://pre-commit.com/) to ensure that formatting is\nconsistent across DeepSpeed. First, ensure that `pre-commit` is installed from either\ninstalling DeepSpeed or `pip install pre-commit`. Next, the pre-commit hooks must be\ninstalled once before commits can be made:\n```bash\npre-commit install\n```\n\nAfterwards, our suite of formatting tests run automatically before each `git commit`. You\ncan also run these manually:\n```bash\npre-commit run --all-files\n```\nIf a formatting test fails, it will fix the modified code in place and abort\nthe `git commit`. After looking over the changes, you can `git add <modified files>`\nand then repeat the previous `git commit` command.\n\n\n## Testing\nDeepSpeed tracks two types of tests: unit tests and more costly model convergence tests.\nThe model convergence tests train\n[DeepSpeedExamples](https://github.com/microsoft/DeepSpeedExamples/) and measure\nend-to-end convergence and related metrics. Unit tests are found in `tests/unit/` and\nthe model convergence tests are found in `tests/model/`.\n\n### Unit Tests\n[PyTest](https://docs.pytest.org/en/latest/) is used to execute tests. PyTest can be\ninstalled from PyPI via `pip install pytest`. Simply invoke `pytest --forked` to run the\nunit tests:\n```bash\npytest --forked tests/unit/\n```\nYou can also provide the `-v` flag to `pytest` to see additional information about the\ntests. Note that [pytest-forked](https://github.com/pytest-dev/pytest-forked) and the\n`--forked` flag are required to test CUDA functionality in distributed tests.\n\n### Model Tests\nTo execute model tests, first [install DeepSpeed](#installation). The\n[DeepSpeedExamples](https://github.com/microsoft/DeepSpeedExamples/) repository is cloned\nas part of this process. Next, execute the model test driver:\n```bash\ncd tests/model/\npytest run_sanity_check.py\n```\nNote that the `--forked` flag is not necessary for the model tests.\n\n## Contributor License Agreement\nThis project welcomes contributions and suggestions. Most contributions require you to\nagree to a Contributor License Agreement (CLA) declaring that you have the right to, and\nactually do, grant us the rights to use your contribution. For details, visit\nhttps://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need\nto provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply\nfollow the instructions provided by the bot. You will only need to do this once across\nall repos using our CLA.\n\n## Code of Conduct\nThis project has adopted the [Microsoft Open Source Code of\nConduct](https://opensource.microsoft.com/codeofconduct/). For more information see the\n[Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact\n[opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or\ncomments.\n\n## New Feature Contribution Guidelines\nUnlike bug fix or improving existing feature (where users usually directly submit a PR and we review it), adding a new feature to DeepSpeed requires several steps: (1) proposal and discussion, (2) implementation and verification, (3) release and maintenance. This general guideline applies to all new feature contributions. Core DeepSpeed team member contributions may complete step 1 internally.\n\n### Step 1: proposal and discussion\nWe ask users to first post your intended feature in an issue. This issue needs to include:\n\n* A description of the proposed feature.\n* A motivation of why it will be useful to DeepSpeed users.\n* A rough design of how you implement the feature inside DeepSpeed.\n* (Important) Results or planned experiments to demonstrate the effectiveness and correctness of the feature.\n  * If this is a general feature applicable to different tasks, we require testing it on at least one CV task (e.g., [CIFAR](https://www.deepspeed.ai/tutorials/cifar-10/)) and one NLP task (e.g., [SQuAD](https://www.deepspeed.ai/tutorials/bert-finetuning/)). If this is a feature for one kind of task only, it is fine to just test on the specific task.\n  * If the feature only affects performance and does not affect training convergence, we require testing on a fraction of training to demonstrate that the training/validation loss are consistent with baseline, and that the performance is better than baseline.\n  * If the feature does affect training convergence, we require testing the whole training to demonstrate that the feature achieves better/on-par final model quality and training performance compared to baseline.\n\nBased on the issue we shall discuss the merit of the new feature and decide whether accept or decline the proposal. Once accepted and after we confirm the design and implementation plan, we are ready for step 2.\n\n### Step 2: implementation and verification\nContributor will go ahead and implement the feature, and the DeepSpeed team will provide guidance/helps as needed. The required deliverables include:\n\n* A PR to [microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed) including (1) the feature implementation (2) unit tests (3) documentation (4) tutorial\n* A PR to [microsoft/DeepSpeedExamples](https://github.com/microsoft/DeepSpeedExamples) or [microsoft/Megatron-DeepSpeed](https://github.com/microsoft/Megatron-DeepSpeed) including the examples of how to use the feature (this is related to the planned testing experiments in proposal)\n* In the implementation (code, documentation, tutorial), we require the feature author to record their GitHub username as a contact method for future questions/maintenance.\n\nAfter receiving the PRs, we will review them and merge them after necessary tests/fixes.\n\n### Step 3: release and maintenance\nAfter the PRs are merged, we will announce the feature on our website (with credit to the feature author). We ask the feature author to commit to the maintenance of the feature.\n"
        },
        {
          "name": "GOVERNANCE.md",
          "type": "blob",
          "size": 10.45,
          "content": "\n# DeepSpeed Project Charter and Governance\n\nThis charter sets forth the responsibilities and procedures for technical contribution to, and oversight of, the DeepSpeed open source project. All contributors (including committers, maintainers, and other technical positions) and other participants in the Project (collectively, \"Collaborators\") must comply with the terms of this Charter.\n\n## Mission and Scope of the Project\n\nThe mission of the Project is to DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.\n\nThe scope of the Project includes collaborative development under the Project License (as defined herein) supporting the mission, including documentation, testing, integration, and the creation of other artifacts that aid the development, deployment, operation, or adoption of the open source project.\n\n## Technical Steering Committee\n\n1. The Technical Steering Committee (the \"TSC\") will be responsible for all technical oversight of the open source Project.\n\n2. The TSC voting members are initially the Project's Committers. At the inception of the project, the Committers of the Project will be as set forth within the \"CONTRIBUTING\" file within the Project's code repository. The TSC may choose an alternative approach for determining the voting members of the TSC, and any such alternative approach will be documented in the CONTRIBUTING file. Any meetings of the Technical Steering Committee are intended to be open to the public, and can be conducted electronically, via teleconference, or in person.\n\n3. TSC projects generally will involve Contributors and Committers. The TSC may adopt or modify roles so long as the roles are documented in the CONTRIBUTING file. Unless otherwise documented:\n\n\t- **Contributors** include anyone in the technical community that contributes code, documentation, or other technical artifacts to the Project.\n\t- **Committers** are Contributors who have earned the ability to modify (\"commit\") source code, documentation, or other technical artifacts in a project's repository.\n\n\t-  A Contributor may become a Committer by a majority approval of the existing Committers. A Committer may be removed by a majority approval of the other existing Committers.\n\n4. Participation in the Project through becoming a Contributor and Committer is open to anyone so long as they abide by the terms of this Charter.\n\n5. The TSC may:\n\t- Establish workflow procedures for the submission, approval, and closure/archiving of projects.\n\t- Set requirements for the promotion of Contributors to Committer status, as applicable.\n\t- Amend, adjust, refine and/or eliminate the roles of Contributors and Committers, and create new roles, and publicly document any TSC roles, as it sees fit.\n\n6. The TSC may elect a TSC Chair, who will preside over meetings of the TSC and will serve until their resignation or replacement by the TSC. The TSC Chair, or any other TSC member so designated by the TSC, will serve as the primary communication contact between the Project and AI & Data, a directed fund of The Linux Foundation.\n\n7. Responsibilities:  The TSC will be responsible for all aspects of oversight relating to the Project, which may include:\n\n\t- Coordinating the technical direction of the Project.\n\t- Approving project or system proposals (including, but not limited to, incubation, deprecation, and changes to a sub-project's scope).\n\t- Organizing sub-projects and removing sub-projects.\n\t- Creating sub-committees or working groups to focus on cross-project technical issues and requirements.\n\t- Appointing representatives to work with other open source or open standards communities.\n\t- Establishing community norms, workflows, issuing releases, and security issue reporting policies.\n\t- Approving and implementing policies and processes for contributing (to be published in the CONTRIBUTING file) and coordinating with the series manager of the Project (as provided for in the Series Agreement, the \"Series Manager\") to resolve matters or concerns that may arise as set forth in Section 7 of this Charter.\n\t- Discussions, seeking consensus, and where necessary, voting on technical matters relating to the code base that affect multiple projects.\n\t- Coordinating any marketing, events, or communications regarding the Project.\n\n## TSC Voting\n\n1. While the Project aims to operate as a consensus-based community, if any TSC decision requires a vote to move the Project forward, the voting members of the TSC will vote on a one vote per voting member basis.\n\n2. Quorum for TSC meetings requires at least fifty percent of all voting members of the TSC to be present. The TSC may continue to meet if quorum is not met but will be prevented from making any decisions at the meeting.\n\n3. Except as provided in Section 7.c. and 8.a, decisions by vote at a meeting require a majority vote of those in attendance, provided quorum is met. Decisions made by electronic vote without a meeting require a majority vote of all voting members of the TSC.\n\n4. In the event a vote cannot be resolved by the TSC, any voting member of the TSC may refer the matter to the Series Manager for assistance in reaching a resolution.\n\n## Compliance with Policies\n\n1. This Charter is subject to the Series Agreement for the Project and the Operating Agreement of LF Projects. Contributors will comply with the policies of LF Projects as may be adopted and amended by LF Projects, including, without limitation, the policies listed at https://lfprojects.org/policies/.\n\n2. The TSC may adopt a code of conduct (\"CoC\") for the Project, which is subject to approval by the Series Manager. In the event that a Project-specific CoC has not been approved, the LF Projects Code of Conduct listed at https://lfprojects.org/policies will apply for all Collaborators in the Project.\n\n3. When amending or adopting any policy applicable to the Project, LF Projects will publish such policy, as to be amended or adopted, on its website at least 30 days prior to such policy taking effect; provided, however, that in the case of any amendment of the Trademark Policy or Terms of Use of LF Projects, any such amendment is effective upon publication on LF Project's website.\n\n4. All Collaborators must allow open participation from any individual or organization meeting the requirements for contributing under this Charter and any policies adopted for all Collaborators by the TSC, regardless of competitive interests. Put another way, the Project community must not seek to exclude any participant based on any criteria, requirement, or reason other than those that are reasonable and applied on a non-discriminatory basis to all Collaborators in the Project community.\n\n5. The Project will operate in a transparent, open, collaborative, and ethical manner at all times. The output of all Project discussions, proposals, timelines, decisions, and status should be made open and easily visible to all. Any potential violations of this requirement should be reported immediately to the Series Manager.\n\n## Community Assets\n\n1. LF Projects will hold title to all trade or service marks used by the Project (\"Project Trademarks\"), whether based on common law or registered rights. Project Trademarks will be transferred and assigned to LF Projects to hold on behalf of the Project. Any use of any Project Trademarks by Collaborators in the Project will be in accordance with the license from LF Projects and inure to the benefit of LF Projects.\n\n2. The Project will, as permitted and in accordance with such license from LF Projects, develop and own all Project GitHub and social media accounts, and domain name registrations created by the Project community.\n\n3. Under no circumstances will LF Projects be expected or required to undertake any action on behalf of the Project that is inconsistent with the tax-exempt status or purpose, as applicable, of the Joint Development Foundation or LF Projects, LLC.\n\n## General Rules and Operations\n\nThe Project will:\n\n1. Engage in the work of the Project in a professional manner consistent with maintaining a cohesive community, while also maintaining the goodwill and esteem of LF Projects, Joint Development Foundation, and other partner organizations in the open source community.\n2. Respect the rights of all trademark owners, including any branding and trademark usage guidelines.\n\n## Intellectual Property Policy\n\n1. Collaborators acknowledge that the copyright in all new contributions will be retained by the copyright holder as independent works of authorship and that no contributor or copyright holder will be required to assign copyrights to the Project.\n\n2. Except as described in Section 7.c., all contributions to the Project are subject to the following:\n\n    - All new inbound code contributions to the Project must be made using Apache License, Version 2.0 available at http://www.apache.org/licenses/LICENSE-2.0 (the \"Project License\").\n\t- All new inbound code contributions must also be accompanied by a Developer Certificate of Origin (http://developercertificate.org) sign-off in the source code system that is submitted through a TSC-approved contribution process which will bind the authorized contributor and, if not self-employed, their employer to the applicable license.\n\t- All outbound code will be made available under the Project License.\n\t- Documentation will be received and made available by the Project under the Creative Commons Attribution 4.0 International License (available at http://creativecommons.org/licenses/by/4.0/).\n\t- The Project may seek to integrate and contribute back to other open source projects (\"Upstream Projects\"). In such cases, the Project will conform to all license requirements of the Upstream Projects, including dependencies, leveraged by the Project. Upstream Project code contributions not stored within the Project's main code repository will comply with the contribution process and license terms for the applicable Upstream Project.\n\n3. The TSC may approve the use of an alternative license or licenses for inbound or outbound contributions on an exception basis. To request an exception, please describe the contribution, the alternative open source license(s), and the justification for using an alternative open source license for the Project. License exceptions must be approved by a two-thirds vote of the entire TSC.\n\n4. Contributed files should contain license information, such as SPDX short form identifiers, indicating the open source license or licenses pertaining to the file.\n\n## Amendments\n\n1. This charter may be amended by a two-thirds vote of the entire TSC and is subject to approval by LF Projects.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.09,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.41,
          "content": "include *.txt README.md\ninclude deepspeed/inference/v2/kernels/ragged_ops/libs/*.so\ninclude deepspeed/inference/v2/kernels/cutlass_ops/libs/*.so\nrecursive-include requirements *.txt\nrecursive-include deepspeed *.cpp *.h *.hpp *.cu *.hip *.tr *.cuh *.cc *.json\nrecursive-include csrc *.cpp *.h *.hpp *.cu *.tr *.cuh *.cc\nrecursive-include op_builder *.py\nrecursive-include benchmarks *.py\nrecursive-include accelerator *.py\n"
        },
        {
          "name": "MANIFEST_win.in",
          "type": "blob",
          "size": 0.23,
          "content": "include *.txt README.md\nrecursive-include requirements *.txt\n\n# this is for Windows only\nrecursive-include deepspeed *.tr\nrecursive-exclude deepspeed/ops/csrc *.cpp *.h *.cu *.cuh *.cc\nprune csrc\nprune op_builder\nprune accelerator\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 38.57,
          "content": "[![License Apache 2.0](https://badgen.net/badge/license/apache2.0/blue)](https://github.com/Microsoft/DeepSpeed/blob/master/LICENSE)\n[![PyPI version](https://badge.fury.io/py/deepspeed.svg)](https://pypi.org/project/deepspeed/)\n[![Downloads](https://static.pepy.tech/badge/deepspeed)](https://pepy.tech/project/deepspeed)\n[![Build](https://badgen.net/badge/build/check-status/blue)](#build-pipeline-status)\n[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/9530/badge)](https://www.bestpractices.dev/projects/9530)\n[![Twitter](https://img.shields.io/twitter/follow/MSFTDeepSpeed)](https://twitter.com/intent/follow?screen_name=MSFTDeepSpeed)\n[![Japanese Twitter](https://img.shields.io/badge/%E6%97%A5%E6%9C%AC%E8%AA%9ETwitter-%40MSFTDeepSpeedJP-blue)](https://twitter.com/MSFTDeepSpeedJP)\n[![Chinese Zhihu](https://img.shields.io/badge/%E7%9F%A5%E4%B9%8E-%E5%BE%AE%E8%BD%AFDeepSpeed-blue)](https://www.zhihu.com/people/deepspeed)\n\n\n<div align=\"center\">\n <img src=\"docs/assets/images/DeepSpeed_light.svg#gh-light-mode-only\" width=\"400px\">\n <img src=\"docs/assets/images/DeepSpeed_dark_transparent.svg#gh-dark-mode-only\" width=\"400px\">\n</div>\n\n## Latest News\n<b> <span style=\"color:orange\" > DeepSpeed empowers ChatGPT-like model training with a single click, offering 15x speedup over SOTA RLHF systems with unprecedented cost reduction at all scales; [learn how](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat)</span>.</b>\n\n* [2024/12] [Ulysses-Offload: Democratizing Long Context LLM Training ](https://github.com/microsoft/DeepSpeed/blob/master/blogs/ulysses-offload/README.md)\n* [2024/12] [DeepSpeed-Domino: Communication-Free LLM Training Engine](https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-domino/README.md)\n* [2024/08] [DeepSpeed on Windows](https://github.com/microsoft/DeepSpeed/tree/master/blogs/windows/08-2024/README.md) [[](https://github.com/microsoft/DeepSpeed/tree/master/blogs/windows/08-2024/japanese/README.md)]\n* [2024/08] [DeepNVMe: Improving DL Applications through I/O Optimizations](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-gds/README.md) [[](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-gds/japanese/README.md)]\n* [2024/07] [DeepSpeed Universal Checkpointing: Efficient and Flexible Checkpointing for Large Scale Distributed Training](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-ucp/README.md) [[](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-ucp/chinese/README.md)] [[](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-ucp/japanese/README.md)]\n* [2024/03] [DeepSpeed-FP6:The power of FP6-Centric Serving for Large Language Models](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fp6/03-05-2024) [[English](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fp6/03-05-2024/README.md)] [[](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fp6/03-05-2024/README-Chinese.md)]\n* [2024/01] [DeepSpeed-FastGen: Introducing Mixtral, Phi-2, and Falcon support with major performance and feature enhancements.](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen/2024-01-19)\n* [2023/11] [Llama 2 Inference on 4th Gen Intel Xeon Scalable Processor with DeepSpeed](https://github.com/microsoft/DeepSpeed/tree/master/blogs/intel-inference) [[Intel version]](https://www.intel.com/content/www/us/en/developer/articles/technical/xllama-2-on-xeon-scalable-processor-with-deepspeed.html)\n* [2023/11] [DeepSpeed ZeRO-Offload++: 6x Higher Training Throughput via Collaborative CPU/GPU Twin-Flow](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-offloadpp)\n* [2023/11] [DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen) [[English](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen)] [[](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen/chinese/README.md)] [[](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen/japanese/README.md)]\n* [2023/10] [DeepSpeed-VisualChat: Improve Your Chat Experience with Multi-Round Multi-Image Inputs](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-visualchat/10-03-2023/README.md) [[English](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-visualchat/10-03-2023/README.md)] [[](https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-visualchat/10-03-2023/README-Chinese.md)] [[](https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-visualchat/10-03-2023/README-Japanese.md)]\n* [2023/09] Announcing the DeepSpeed4Science Initiative: Enabling large-scale scientific discovery through sophisticated AI system technologies [[Tutorials](https://www.deepspeed.ai/deepspeed4science/)] [[White paper](https://arxiv.org/abs/2310.04610)] [[Blog](https://www.microsoft.com/en-us/research/blog/announcing-the-deepspeed4science-initiative-enabling-large-scale-scientific-discovery-through-sophisticated-ai-system-technologies/)] [[](https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed4science/chinese/README.md)] [[](https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed4science/japanese/README.md)]\n\n\n<!-- NOTE: we must use html for news items otherwise links will be broken in the 'more news' section -->\n<details>\n <summary>More news</summary>\n <ul>\n  <li>[2023/08] <a href=\"https://github.com/microsoft/DeepSpeedExamples/blob/master/inference/huggingface/zero_inference/README.md\">DeepSpeed ZeRO-Inference: 20x faster inference through weight quantization and KV cache offloading</a></li>\n\n  <li>[2023/08] <a href=\"https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat/ds-chat-release-8-31/README.md\">DeepSpeed-Chat: Llama/Llama-2 system support, efficiency boost, and training stability improvements</a></li>\n\n  <li>[2023/08] <a href=\"https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-ulysses\">DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models</a> [<a href=\"https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-ulysses/chinese/README.md\"></a>] [<a href=\"https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-ulysses/japanese/README.md\"></a>]</li>\n\n  <li>[2023/06] <a href=\"https://www.microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/\">ZeRO++: A leap in speed for LLM and chat model training with 4X less communication</a> [<a href=\"https://www.microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/\">English</a>] [<a href=\"https://github.com/microsoft/DeepSpeed/blob/master/blogs/zeropp/chinese/README.md\"></a>] [<a href=\"https://github.com/microsoft/DeepSpeed/blob/master/blogs/zeropp/japanese/README.md\"></a>]</li>\n </ul>\n</details>\n\n---\n\n# Extreme Speed and Scale for DL Training and Inference\n\n***[DeepSpeed](https://www.deepspeed.ai/) enables world's most powerful language models like [MT-530B](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/) and [BLOOM](https://huggingface.co/blog/bloom-megatron-deepspeed)***. It is an easy-to-use deep learning optimization software suite that powers unprecedented scale and speed for both training and inference. With DeepSpeed you can:\n\n* Train/Inference dense or sparse models with billions or trillions of parameters\n* Achieve excellent system throughput and efficiently scale to thousands of GPUs\n* Train/Inference on resource constrained GPU systems\n* Achieve unprecedented low latency and high throughput for inference\n* Achieve extreme compression for an unparalleled inference latency and model size reduction with low costs\n\n---\n\n# DeepSpeed's four innovation pillars\n\n<img src=\"docs/assets/images/DeepSpeed-pillars.png\" width=\"800px\">\n\n\n## DeepSpeed-Training\n\nDeepSpeed offers a confluence of system innovations, that has made large scale DL training effective, and efficient, greatly improved ease of use, and redefined the DL training landscape in terms of scale that is possible. These innovations such as ZeRO, 3D-Parallelism, DeepSpeed-MoE, ZeRO-Infinity, etc. fall under the training pillar. Learn more: [DeepSpeed-Training](https://www.deepspeed.ai/training/)\n\n## DeepSpeed-Inference\n\nDeepSpeed brings together innovations in parallelism technology such as tensor, pipeline, expert and ZeRO-parallelism, and combines them with high performance custom inference kernels, communication optimizations and heterogeneous memory technologies to enable inference at an unprecedented scale, while achieving unparalleled latency, throughput and cost reduction. This systematic composition of system technologies for inference falls under the inference pillar. Learn more: [DeepSpeed-Inference](https://www.deepspeed.ai/inference)\n\n\n## DeepSpeed-Compression\n\nTo further increase the inference efficiency, DeepSpeed offers easy-to-use and flexible-to-compose compression techniques for researchers and practitioners to compress their models while delivering faster speed, smaller model size, and significantly reduced compression cost. Moreover, SoTA innovations on compression like ZeroQuant and XTC are included under the compression pillar. Learn more: [DeepSpeed-Compression](https://www.deepspeed.ai/compression)\n\n## DeepSpeed4Science\n\nIn line with Microsoft's mission to solve humanity's most pressing challenges, the DeepSpeed team at Microsoft is responding to this opportunity by launching a new initiative called *DeepSpeed4Science*, aiming to build unique capabilities through AI system technology innovations to help domain experts to unlock today's biggest science mysteries. Learn more: [tutorials](https://www.deepspeed.ai/deepspeed4science/)\n\n---\n\n# DeepSpeed Software Suite\n\n## DeepSpeed Library\n\n   The [DeepSpeed](https://github.com/microsoft/deepspeed) library (this repository) implements and packages the innovations and technologies in DeepSpeed Training, Inference and Compression Pillars into a single easy-to-use, open-sourced repository. It allows for easy composition of multitude of features within a single training, inference or compression pipeline. The DeepSpeed Library is heavily adopted by the DL community, and has been used to enable some of the most powerful models (see [DeepSpeed Adoption](#deepspeed-adoption)).\n\n## Model Implementations for Inference (MII)\n\n   [Model Implementations for Inference (MII)](https://github.com/microsoft/deepspeed-mii) is an open-sourced repository for making low-latency and high-throughput inference accessible to all data scientists by alleviating the need to apply complex system optimization techniques themselves. Out-of-box, MII offers support for thousands of widely used DL models, optimized using DeepSpeed-Inference, that can be deployed with a few lines of code, while achieving significant latency reduction compared to their vanilla open-sourced versions.\n\n## DeepSpeed on Azure\n\n   DeepSpeed users are diverse and have access to different environments. We recommend to try DeepSpeed on Azure as it is the simplest and easiest method. The recommended method to try DeepSpeed on Azure is through AzureML [recipes](https://github.com/Azure/azureml-examples/tree/main/v1/python-sdk/workflows/train/deepspeed). The job submission and data preparation scripts have been made available [here](https://github.com/microsoft/Megatron-DeepSpeed/tree/main/examples_deepspeed/azureml). For more details on how to use DeepSpeed on Azure, please follow the [Azure tutorial](https://www.deepspeed.ai/tutorials/azure/).\n\n---\n\n# DeepSpeed Adoption\n\nDeepSpeed is an important part of Microsofts new\n[AI at Scale](https://www.microsoft.com/en-us/research/project/ai-at-scale/)\ninitiative to enable next-generation AI capabilities at scale, where you can find more\ninformation [here](https://innovation.microsoft.com/en-us/exploring-ai-at-scale).\n\nDeepSpeed has been used to train many different large-scale models, below is a list of several examples that we are aware of (if you'd like to include your model please submit a PR):\n\n  * [Megatron-Turing NLG (530B)](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/)\n  * [Jurassic-1 (178B)](https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf)\n  * [BLOOM (176B)](https://huggingface.co/blog/bloom-megatron-deepspeed)\n  * [GLM (130B)](https://github.com/THUDM/GLM-130B)\n  * [xTrimoPGLM (100B)](https://www.biorxiv.org/content/10.1101/2023.07.05.547496v2)\n  * [YaLM (100B)](https://github.com/yandex/YaLM-100B)\n  * [GPT-NeoX (20B)](https://github.com/EleutherAI/gpt-neox)\n  * [AlexaTM (20B)](https://www.amazon.science/blog/20b-parameter-alexa-model-sets-new-marks-in-few-shot-learning)\n  * [Turing NLG (17B)](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/)\n  * [METRO-LM (5.4B)](https://arxiv.org/pdf/2204.06644.pdf)\n\nDeepSpeed has been integrated with several different popular open-source DL frameworks such as:\n\n|                                                                                                | Documentation                                |\n| ---------------------------------------------------------------------------------------------- | -------------------------------------------- |\n<img src=\"docs/assets/images/transformers-light.png#gh-light-mode-only\" width=\"250px\"><img src=\"docs/assets/images/transformers-dark.png#gh-dark-mode-only\" width=\"250px\"> | [Transformers with DeepSpeed](https://huggingface.co/docs/transformers/deepspeed) |\n| <img src=\"docs/assets/images/accelerate-light.png#gh-light-mode-only\" width=\"250px\"><img src=\"docs/assets/images/accelerate-dark.png#gh-dark-mode-only\" width=\"250px\"> | [Accelerate with DeepSpeed](https://huggingface.co/docs/accelerate/usage_guides/deepspeed) |\n| <img src=\"docs/assets/images/lightning-light.svg#gh-light-mode-only\" width=\"200px\"><img src=\"docs/assets/images/lightning-dark.svg#gh-dark-mode-only\" width=\"200px\"> | [Lightning with DeepSpeed](https://lightning.ai/docs/pytorch/stable/advanced/model_parallel.html#deepspeed) |\n| <img src=\"docs/assets/images/mosaicml.svg\" width=\"200px\"> | [MosaicML with DeepSpeed](https://docs.mosaicml.com/projects/composer/en/latest/trainer/using_the_trainer.html?highlight=deepspeed#deepspeed-integration) |\n| <img src=\"docs/assets/images/determined.svg\" width=\"225px\"> | [Determined with DeepSpeed](https://docs.determined.ai/latest/training/apis-howto/deepspeed/overview.html) |\n| <img src=\"https://user-images.githubusercontent.com/58739961/187154444-fce76639-ac8d-429b-9354-c6fac64b7ef8.jpg\" width=150> | [MMEngine with DeepSpeed](https://mmengine.readthedocs.io/en/latest/common_usage/large_model_training.html#deepspeed) |\n\n---\n\n# Build Pipeline Status\n\n| Description | Status |\n| ----------- | ------ |\n| NVIDIA | [![nv-torch110-p40](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch110-p40.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch110-p40.yml) [![nv-torch110-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch110-v100.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch110-v100.yml) [![nv-torch-latest-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-latest-v100.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-latest-v100.yml) [![nv-h100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-h100.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-h100.yml) [![nv-inference](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-inference.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-inference.yml) [![nv-nightly](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-nightly.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-nightly.yml) |\n| AMD | [![amd-mi200](https://github.com/microsoft/DeepSpeed/actions/workflows/amd-mi200.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/amd-mi200.yml) |\n| CPU | [![torch-latest-cpu](https://github.com/microsoft/DeepSpeed/actions/workflows/cpu-torch-latest.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/cpu-torch-latest.yml) [![cpu-inference](https://github.com/microsoft/DeepSpeed/actions/workflows/cpu-inference.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/cpu-inference.yml) |\n| Intel Gaudi | [![hpu-gaudi2](https://github.com/microsoft/DeepSpeed/actions/workflows/hpu-gaudi2.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/hpu-gaudi2.yml) |\n| Intel XPU | [![xpu-max1100](https://github.com/microsoft/DeepSpeed/actions/workflows/xpu-max1100.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/xpu-max1100.yml) |\n| PyTorch Nightly | [![nv-torch-nightly-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-nightly-v100.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-torch-nightly-v100.yml) |\n| Integrations | [![nv-transformers-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-transformers-v100.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-transformers-v100.yml) [![nv-lightning-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-lightning-v100.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-lightning-v100.yml) [![nv-accelerate-v100](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-accelerate-v100.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-accelerate-v100.yml) [![nv-mii](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-mii.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-mii.yml) [![nv-ds-chat](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-ds-chat.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-ds-chat.yml) [![nv-sd](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-sd.yml/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/nv-sd.yml) |\n| Misc | [![Formatting](https://github.com/microsoft/DeepSpeed/actions/workflows/formatting.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/formatting.yml) [![pages-build-deployment](https://github.com/microsoft/DeepSpeed/actions/workflows/pages/pages-build-deployment/badge.svg)](https://github.com/microsoft/DeepSpeed/actions/workflows/pages/pages-build-deployment) [![Documentation Status](https://readthedocs.org/projects/deepspeed/badge/?version=latest)](https://deepspeed.readthedocs.io/en/latest/?badge=latest)[![python](https://github.com/microsoft/DeepSpeed/actions/workflows/python.yml/badge.svg?branch=master)](https://github.com/microsoft/DeepSpeed/actions/workflows/python.yml) |\n| Huawei Ascend NPU | [![Huawei Ascend NPU](https://github.com/Ascend/Ascend-CI/actions/workflows/deepspeed.yaml/badge.svg?branch=main)](https://github.com/Ascend/Ascend-CI/actions/workflows/deepspeed.yaml) |\n\n# Installation\n\nThe quickest way to get started with DeepSpeed is via pip, this will install\nthe latest release of DeepSpeed which is not tied to specific PyTorch or CUDA\nversions. DeepSpeed includes several C++/CUDA extensions that we commonly refer\nto as our 'ops'.  By default, all of these extensions/ops will be built\njust-in-time (JIT) using [torch's JIT C++ extension loader that relies on\nninja](https://pytorch.org/docs/stable/cpp_extension.html) to build and\ndynamically link them at runtime.\n\n## Requirements\n* [PyTorch](https://pytorch.org/) must be installed _before_ installing DeepSpeed.\n* For full feature support we recommend a version of PyTorch that is >= 1.9 and ideally the latest PyTorch stable release.\n* A CUDA or ROCm compiler such as [nvcc](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/#introduction) or [hipcc](https://github.com/ROCm-Developer-Tools/HIPCC) used to compile C++/CUDA/HIP extensions.\n* Specific GPUs we develop and test against are listed below, this doesn't mean your GPU will not work if it doesn't fall into this category it's just DeepSpeed is most well tested on the following:\n  * NVIDIA: Pascal, Volta, Ampere, and Hopper architectures\n  * AMD: MI100 and MI200\n\n## Contributed HW support\n* DeepSpeed now support various HW accelerators.\n\n| Contributor | Hardware                            | Accelerator Name | Contributor validated | Upstream validated |\n|-------------|-------------------------------------|------------------| --------------------- |--------------------|\n| Huawei      | Huawei Ascend NPU                   | npu              | Yes | No                 |\n| Intel       | Intel(R) Gaudi(R) 2 AI accelerator  | hpu              | Yes | Yes                |\n| Intel       | Intel(R) Xeon(R) Processors         | cpu              | Yes | Yes                |\n| Intel       | Intel(R) Data Center GPU Max series | xpu              | Yes | Yes                |\n\n## PyPI\nWe regularly push releases to [PyPI](https://pypi.org/project/deepspeed/) and encourage users to install from there in most cases.\n\n```bash\npip install deepspeed\n```\n\nAfter installation, you can validate your install and see which extensions/ops\nyour machine is compatible with via the DeepSpeed environment report.\n\n```bash\nds_report\n```\n\nIf you would like to pre-install any of the DeepSpeed extensions/ops (instead\nof JIT compiling) or install pre-compiled ops via PyPI please see our [advanced\ninstallation instructions](https://www.deepspeed.ai/tutorials/advanced-install/).\n\n## Windows\nWindows support is partially supported with DeepSpeed. On Windows you can build wheel with following steps, currently only inference mode is supported.\n1. Install pytorch, such as pytorch 1.8 + cuda 11.1\n2. Install visual cpp build tools, such as VS2019 C++ x64/x86 build tools\n3. Launch cmd console with Administrator privilege for creating required symlink folders\n4. Run `python setup.py bdist_wheel` to build wheel in `dist` folder\n\n# Features\n\nPlease checkout [DeepSpeed-Training](https://www.deepspeed.ai/training), [DeepSpeed-Inference](https://www.deepspeed.ai/inference) and [DeepSpeed-Compression](https://www.deepspeed.ai/compression) pages for full set of features offered along each of these three pillars.\n\n# Further Reading\n\nAll DeepSpeed documentation, tutorials, and blogs can be found on our website: [deepspeed.ai](https://www.deepspeed.ai/)\n\n\n|                                                                                                | Description                                  |\n| ---------------------------------------------------------------------------------------------- | -------------------------------------------- |\n| [Getting Started](https://www.deepspeed.ai/getting-started/)                                   |  First steps with DeepSpeed                  |\n| [DeepSpeed JSON Configuration](https://www.deepspeed.ai/docs/config-json/)                     |  Configuring DeepSpeed                       |\n| [API Documentation](https://deepspeed.readthedocs.io/en/latest/)                               |  Generated DeepSpeed API documentation       |\n| [Tutorials](https://www.deepspeed.ai/tutorials/)                                               |  Tutorials                                   |\n| [Blogs](https://www.deepspeed.ai/posts/)                                                       |  Blogs                                   |\n\n\n# Contributing\nDeepSpeed welcomes your contributions! Please see our\n[contributing](CONTRIBUTING.md) guide for more details on formatting, testing,\netc.<br/>\nThanks so much to all of our amazing contributors!\n\n<a href=\"https://github.com/microsoft/DeepSpeed/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=microsoft/DeepSpeed&r=\"  width=\"800px\"/>\n</a>\n\n## Contributor License Agreement\nThis project welcomes contributions and suggestions. Most contributions require you to\nagree to a Contributor License Agreement (CLA) declaring that you have the right to, and\nactually do, grant us the rights to use your contribution. For details, visit\nhttps://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need\nto provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply\nfollow the instructions provided by the bot. You will only need to do this once across\nall repos using our CLA.\n\n## Code of Conduct\nThis project has adopted the [Microsoft Open Source Code of\nConduct](https://opensource.microsoft.com/codeofconduct/). For more information see the\n[Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact\n[opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n# Publications\n1. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He. (2019) ZeRO: memory optimizations toward training trillion parameter models. [arXiv:1910.02054](https://arxiv.org/abs/1910.02054) and [In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC '20)](https://dl.acm.org/doi/10.5555/3433701.3433727).\n2. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. (2020) DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters. [In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD '20, Tutorial)](https://dl.acm.org/doi/10.1145/3394486.3406703).\n3. Minjia Zhang, Yuxiong He. (2020) Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping. [arXiv:2010.13369](https://arxiv.org/abs/2010.13369) and [NeurIPS 2020](https://proceedings.neurips.cc/paper/2020/hash/a1140a3d0df1c81e24ae954d935e8926-Abstract.html).\n4. Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, Yuxiong He. (2021) ZeRO-Offload: Democratizing Billion-Scale Model Training. [arXiv:2101.06840](https://arxiv.org/abs/2101.06840) and [USENIX ATC 2021](https://www.usenix.org/conference/atc21/presentation/ren-jie). [[paper]](https://arxiv.org/abs/2101.06840) [[slides]](https://www.usenix.org/system/files/atc21_slides_ren-jie.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/)\n5. Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, Yuxiong He. (2021) 1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed. [arXiv:2102.02888](https://arxiv.org/abs/2102.02888) and [ICML 2021](http://proceedings.mlr.press/v139/tang21a.html).\n6. Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, Yuxiong He. (2021) ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning. [arXiv:2104.07857](https://arxiv.org/abs/2104.07857) and [SC 2021](https://dl.acm.org/doi/abs/10.1145/3458817.3476205). [[paper]](https://arxiv.org/abs/2104.07857) [[slides]](docs/assets/files/SC21-ZeRO-Infinity.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/)\n7. Conglong Li, Ammar Ahmad Awan, Hanlin Tang, Samyam Rajbhandari, Yuxiong He. (2021) 1-bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMB's Convergence Speed. [arXiv:2104.06069](https://arxiv.org/abs/2104.06069) and [HiPC 2022](https://hipc.org/advance-program/).\n8. Conglong Li, Minjia Zhang, Yuxiong He. (2021) The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training GPT Models. [arXiv:2108.06084](https://arxiv.org/abs/2108.06084) and [NeurIPS 2022](https://openreview.net/forum?id=JpZ5du_Kdh).\n9. Yucheng Lu, Conglong Li, Minjia Zhang, Christopher De Sa, Yuxiong He. (2022) Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam. [arXiv:2202.06009](https://arxiv.org/abs/2202.06009).\n10. Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, Yuxiong He. (2022) DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale [arXiv:2201.05596](https://arxiv.org/abs/2201.05596) and [ICML 2022](https://proceedings.mlr.press/v162/rajbhandari22a.html). [[pdf]](https://arxiv.org/abs/2201.05596) [[slides]](docs/assets/files/ICML-5mins.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/)\n11. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, Bryan Catanzaro. (2022) Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model [arXiv:2201.11990](https://arxiv.org/abs/2201.11990).\n12. Xiaoxia Wu, Zhewei Yao, Minjia Zhang, Conglong Li, Yuxiong He. (2022) Extreme Compression for Pre-trained Transformers Made Simple and Efficient. [arXiv:2206.01859](https://arxiv.org/abs/2206.01859) and [NeurIPS 2022](https://openreview.net/forum?id=xNeAhc2CNAl).\n13. Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, Yuxiong He. (2022) ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers. [arXiv:2206.01861](https://arxiv.org/abs/2206.01861) and [NeurIPS 2022](https://openreview.net/forum?id=f-fVCElZ-G1) [[slides]](docs/assets/files/zeroquant_series.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-compression-a-composable-library-for-extreme-compression-and-zero-cost-quantization/)\n14. Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, Yuxiong He. (2022) DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale. [arXiv:2207.00032](https://arxiv.org/abs/2207.00032) and [SC 2022](https://dl.acm.org/doi/abs/10.5555/3571885.3571946). [[paper]](https://arxiv.org/abs/2207.00032) [[slides]](docs/assets/files/sc22-ds-inference.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-accelerating-large-scale-model-inference-and-training-via-system-optimizations-and-compression/)\n15. Zhewei Yao, Xiaoxia Wu, Conglong Li, Connor Holmes, Minjia Zhang, Cheng Li, Yuxiong He. (2022) Random-LTD: Random and Layerwise Token Dropping Brings Efficient Training for Large-scale Transformers. [arXiv:2211.11586](https://arxiv.org/abs/2211.11586).\n16. Conglong Li, Zhewei Yao, Xiaoxia Wu, Minjia Zhang, Yuxiong He. (2022) DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing. [arXiv:2212.03597](https://arxiv.org/abs/2212.03597) [ENLSP2023 Workshop at NeurIPS2023](https://neurips2023-enlsp.github.io/)\n17. Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, Yuxiong He. (2023) Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases. [arXiv:2301.12017](https://arxiv.org/abs/2301.12017) and [ICML2023](https://icml.cc/Conferences/2023).\n18. Syed Zawad, Cheng Li, Zhewei Yao, Elton Zheng, Yuxiong He, Feng Yan. (2023) DySR: Adaptive Super-Resolution via Algorithm and System Co-design. [ICLR:2023](https://openreview.net/forum?id=Pgtn4l6eKjv).\n19. Sheng Shen, Zhewei Yao, Chunyuan Li, Trevor Darrell, Kurt Keutzer, Yuxiong He. (2023) Scaling Vision-Language Models with Sparse Mixture of Experts. [arXiv:2303.07226](https://arxiv.org/abs/2303.07226) and [Finding at EMNLP2023](https://2023.emnlp.org/).\n20. Quentin Anthony, Ammar Ahmad Awan, Jeff Rasley, Yuxiong He, Aamir Shafi, Mustafa Abduljabbar, Hari Subramoni, Dhabaleswar Panda. (2023) MCR-DL: Mix-and-Match Communication Runtime for Deep Learning [arXiv:2303.08374](https://arxiv.org/abs/2303.08374) and will appear at IPDPS 2023.\n21. Siddharth Singh, Olatunji Ruwase, Ammar Ahmad Awan, Samyam Rajbhandari, Yuxiong He, Abhinav Bhatele. (2023) A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training [arXiv:2303.06318](https://arxiv.org/abs/2303.06318) and will appear at ICS 2023.\n22. Guanhua Wang, Heyang Qin, Sam Ade Jacobs, Xiaoxia Wu, Connor Holmes, Zhewei Yao, Samyam Rajbhandari, Olatunji Ruwase, Feng Yan, Lei Yang, Yuxiong He. (2023) ZeRO++: Extremely Efficient Collective Communication for Giant Model Training [arXiv:2306.10209](https://arxiv.org/abs/2306.10209) and [ML for Sys Workshop at NeurIPS2023](http://mlforsystems.org/) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/)\n23. Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, Yuxiong He. (2023) ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation [arXiv:2303.08302](https://arxiv.org/abs/2303.08302) and [ENLSP2023 Workshop at NeurIPS2023](https://neurips2023-enlsp.github.io/) [[slides]](docs/assets/files/zeroquant_series.pdf)\n24. Pareesa Ameneh Golnari, Zhewei Yao, Yuxiong He. (2023) Selective Guidance: Are All the Denoising Steps of Guided Diffusion Important? [arXiv:2305.09847](https://arxiv.org/abs/2305.09847)\n25. Zhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes, Zhongzhu Zhou, Michael Wyatt, Molly Smith, Lev Kurilenko, Heyang Qin, Masahiro Tanaka, Shuai Che, Shuaiwen Leon Song, Yuxiong He. (2023) DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales [arXiv:2308.01320](https://arxiv.org/abs/2308.01320).\n26. Xiaoxia Wu, Zhewei Yao, Yuxiong He. (2023) ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats [arXiv:2307.09782](https://arxiv.org/abs/2307.09782) and [ENLSP2023 Workshop at NeurIPS2023](https://neurips2023-enlsp.github.io/) [[slides]](docs/assets/files/zeroquant_series.pdf)\n27. Zhewei Yao, Xiaoxia Wu, Conglong Li, Minjia Zhang, Heyang Qin, Olatunji Ruwase, Ammar Ahmad Awan, Samyam Rajbhandari, Yuxiong He. (2023) DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention [arXiv:2309.14327](https://arxiv.org/pdf/2309.14327.pdf)\n28. Shuaiwen Leon Song, Bonnie Kruft, Minjia Zhang, Conglong Li, Shiyang Chen, Chengming Zhang, Masahiro Tanaka, Xiaoxia Wu, Jeff Rasley, Ammar Ahmad Awan, Connor Holmes, Martin Cai, Adam Ghanem, Zhongzhu Zhou, Yuxiong He, et al. (2023) DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery through Sophisticated AI System Technologies [arXiv:2310.04610](https://arxiv.org/abs/2310.04610) [[blog]](https://www.microsoft.com/en-us/research/blog/announcing-the-deepspeed4science-initiative-enabling-large-scale-scientific-discovery-through-sophisticated-ai-system-technologies/)\n29. Zhewei Yao, Reza Yazdani Aminabadi, Stephen Youn, Xiaoxia Wu, Elton Zheng, Yuxiong He. (2023) ZeroQuant-HERO: Hardware-Enhanced Robust Optimized Post-Training Quantization Framework for W8A8 Transformers [arXiv:2310.17723](https://arxiv.org/abs/2310.17723)\n\n30. Xiaoxia Wu, Haojun Xia, Stephen Youn, Zhen Zheng, Shiyang Chen, Arash Bakhtiari, Michael Wyatt, Reza Yazdani Aminabadi, Yuxiong He, Olatunji Ruwase, Leon Song, Zhewei Yao (2023) ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks [arXiv:2312.08583](https://arxiv.org/abs/2312.08583)\n\n31. Haojun Xia, Zhen Zheng, Xiaoxia Wu, Shiyang Chen, Zhewei Yao, Stephen Youn, Arash Bakhtiari, Michael Wyatt, Donglin Zhuang, Zhongzhu Zhou, Olatunji Ruwase, Yuxiong He, Shuaiwen Leon Song. (2024) FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design  [arXiv:2401.14112](https://arxiv.org/abs/2401.14112)\n32. Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Reza Yazdani Aminadabi, Shuaiwen Leon Song, Samyam Rajbhandari, Yuxiong He. (2024) [System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models](https://dl.acm.org/doi/10.1145/3662158.3662806)\n33. Xinyu Lian, Sam Ade Jacobs, Lev Kurilenko, Masahiro Tanaka, Stas Bekman, Olatunji Ruwase, Minjia Zhang. (2024) Universal Checkpointing: Efficient and Flexible Checkpointing for Large Scale Distributed Training [arXiv:2406.18820](https://arxiv.org/abs/2406.18820)\n\n\n\n\n# Videos\n1. DeepSpeed KDD 2020 Tutorial\n    1. [Overview](https://www.youtube.com/watch?v=CaseqC45DNc&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=29)\n    2. [ZeRO + large model training](https://www.youtube.com/watch?v=y4_bCiAsIAk&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=28)\n    3. [17B T-NLG demo](https://www.youtube.com/watch?v=9V-ZbP92drg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=27)\n    4. [Fastest BERT training + RScan tuning](https://www.youtube.com/watch?v=o1K-ZG9F6u0&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=26)\n    5. DeepSpeed hands on deep dive: [part 1](https://www.youtube.com/watch?v=_NOk-mBwDYg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=92), [part 2](https://www.youtube.com/watch?v=sG6_c4VXLww&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=94), [part 3](https://www.youtube.com/watch?v=k9yPkBTayos&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=93)\n    6. [FAQ](https://www.youtube.com/watch?v=nsHu6vEgPew&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=24)\n2. Microsoft Research Webinar\n    * Registration is free and all videos are available on-demand.\n    * [ZeRO & Fastest BERT: Increasing the scale and speed of deep learning training in DeepSpeed](https://note.microsoft.com/MSR-Webinar-DeepSpeed-Registration-On-Demand.html).\n3. [DeepSpeed on AzureML](https://youtu.be/yBVXR8G8Bg8)\n4. [Large Model Training and Inference with DeepSpeed // Samyam Rajbhandari // LLMs in Prod Conference](https://www.youtube.com/watch?v=cntxC3g22oU) [[slides]](docs/assets/files/presentation-mlops.pdf)\n5. Community Tutorials\n    * [DeepSpeed: All the tricks to scale to gigantic models (Mark Saroufim)](https://www.youtube.com/watch?v=pDGI668pNg0)\n    * [Turing-NLG, DeepSpeed and the ZeRO optimizer (Yannic Kilcher)](https://www.youtube.com/watch?v=tC01FRB0M7w)\n    * [Ultimate Guide To Scaling ML Models (The AI Epiphany)](https://www.youtube.com/watch?v=hc0u4avAkuM)\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 2.93,
          "content": "<!-- BEGIN MICROSOFT SECURITY.MD V0.0.3 BLOCK -->\n\n## Security\n\nMicrosoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include [Microsoft](https://github.com/Microsoft), [Azure](https://github.com/Azure), [DotNet](https://github.com/dotnet), [AspNet](https://github.com/aspnet), [Xamarin](https://github.com/xamarin), and [our GitHub organizations](https://opensource.microsoft.com/).\n\nIf you believe you have found a security vulnerability in any Microsoft-owned repository that meets Microsoft's [Microsoft's definition of a security vulnerability](https://docs.microsoft.com/en-us/previous-versions/tn-archive/cc751383(v=technet.10)) of a security vulnerability, please report it to us as described below.\n\n## Reporting Security Issues\n\n**Please do not report security vulnerabilities through public GitHub issues.**\n\nInstead, please report them to the Microsoft Security Response Center (MSRC) at [https://msrc.microsoft.com/create-report](https://msrc.microsoft.com/create-report).\n\nIf you prefer to submit without logging in, send email to [secure@microsoft.com](mailto:secure@microsoft.com).  If possible, encrypt your message with our PGP key; please download it from the [Microsoft Security Response Center PGP Key page](https://www.microsoft.com/en-us/msrc/pgp-key-msrc).\n\nYou should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Additional information can be found at [microsoft.com/msrc](https://www.microsoft.com/msrc).\n\nPlease include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue:\n\n  * Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.)\n  * Full paths of source file(s) related to the manifestation of the issue\n  * The location of the affected source code (tag/branch/commit or direct URL)\n  * Any special configuration required to reproduce the issue\n  * Step-by-step instructions to reproduce the issue\n  * Proof-of-concept or exploit code (if possible)\n  * Impact of the issue, including how an attacker might exploit the issue\n\nThis information will help us triage your report more quickly.\n\nIf you are reporting for a bug bounty, more complete reports can contribute to a higher bounty award. Please visit our [Microsoft Bug Bounty Program](https://microsoft.com/msrc/bounty) page for more details about our active programs.\n\n## Preferred Languages\n\nWe prefer all communications to be in English.\n\n## Policy\n\nMicrosoft follows the principle of [Coordinated Vulnerability Disclosure](https://www.microsoft.com/en-us/msrc/cvd).\n\n<!-- END MICROSOFT SECURITY.MD BLOCK -->\n\n---\n\nPlease see [PyTorch's Security Policy](https://github.com/pytorch/pytorch/blob/main/SECURITY.md) for more information and recommendations on how to securely interact with models.\n"
        },
        {
          "name": "accelerator",
          "type": "tree",
          "content": null
        },
        {
          "name": "azure",
          "type": "tree",
          "content": null
        },
        {
          "name": "benchmarks",
          "type": "tree",
          "content": null
        },
        {
          "name": "bin",
          "type": "tree",
          "content": null
        },
        {
          "name": "blogs",
          "type": "tree",
          "content": null
        },
        {
          "name": "build_win.bat",
          "type": "blob",
          "size": 0.25,
          "content": "@echo off\n\nset CUDA_HOME=%CUDA_PATH%\nset DISTUTILS_USE_SDK=1\n\nset DS_BUILD_AIO=0\nset DS_BUILD_CUTLASS_OPS=0\nset DS_BUILD_EVOFORMER_ATTN=0\nset DS_BUILD_FP_QUANTIZER=0\nset DS_BUILD_RAGGED_DEVICE_OPS=0\nset DS_BUILD_SPARSE_ATTN=0\n\npython setup.py bdist_wheel\n\n:end\n"
        },
        {
          "name": "csrc",
          "type": "tree",
          "content": null
        },
        {
          "name": "deepspeed",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "environment.yml",
          "type": "blob",
          "size": 0.33,
          "content": "channels:\n  - nvidia/label/cuda-11.8.0\n  - pytorch # or pytorch-nightly\n  - conda-forge\ndependencies:\n  - pytorch\n  - torchvision\n  - torchaudio\n  - cuda\n  - pytorch-cuda=11.8\n  - compilers\n  - sysroot_linux-64==2.17\n  - gcc==11.4\n  - ninja\n  - py-cpuinfo\n  - libaio\n  - ca-certificates\n  - certifi\n  - openssl\n  - python=3.10\n  - pydantic\n"
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "install.sh",
          "type": "blob",
          "size": 4.17,
          "content": "#!/usr/bin/env bash\n\nset -e\nerr_report() {\n    echo \"Error on line $1\"\n    echo \"Fail to install deepspeed\"\n}\ntrap 'err_report $LINENO' ERR\n\nusage() {\n  echo \"\"\"\nUsage: install.sh [options...]\n\nBy default will install deepspeed and all third party dependencies across all machines listed in\nhostfile (hostfile: /job/hostfile). If no hostfile exists, will only install locally\n\n[optional]\n    -l, --local_only        Install only on local machine\n    -s, --pip_sudo          Run pip install with sudo (default: no sudo)\n    -r, --allow_sudo        Allow script to be run by root (probably don't want this, instead use --pip_sudo)\n    -n, --no_clean          Do not clean prior build state, by default prior build files are removed before building wheels\n    -m, --pip_mirror        Use the specified pip mirror (default: the default pip mirror)\n    -H, --hostfile          Path to MPI-style hostfile (default: /job/hostfile)\n    -e, --examples          Checkout deepspeed example submodule (no install)\n    -v, --verbose           Verbose logging\n    -h, --help              This help text\n  \"\"\"\n}\n\nds_only=0\nlocal_only=0\npip_sudo=0\nentire_dlts_job=1\nhostfile=/job/hostfile\npip_mirror=\"\"\nskip_requirements=0\nallow_sudo=0\nno_clean=0\nverbose=0\nexamples=0\n\nwhile [[ $# -gt 0 ]]\ndo\nkey=\"$1\"\ncase $key in\n    -l|--local_only)\n    local_only=1;\n    shift\n    ;;\n    -s|--pip_sudo)\n    pip_sudo=1;\n    shift\n    ;;\n    -m|--pip_mirror)\n    pip_mirror=$2;\n    shift\n    shift\n    ;;\n    -v|--verbose)\n    verbose=1;\n    shift\n    ;;\n    -r|--allow_sudo)\n    allow_sudo=1;\n    shift\n    ;;\n    -n|--no_clean)\n    no_clean=1;\n    shift\n    ;;\n    -H|--hostfile)\n    hostfile=$2\n    if [ ! -f $2 ]; then\n        echo \"User-provided hostfile does not exist at $hostfile, exiting\"\n        exit 1\n    fi\n    shift\n    shift\n    ;;\n    -e|--examples)\n    examples=1\n    shift\n    ;;\n    -h|--help)\n    usage\n    exit 0\n    ;;\n    *)\n    echo \"Unknown argument(s)\"\n    usage\n    exit 1\n    shift\n    ;;\nesac\ndone\n\nuser=`whoami`\nif [ \"$allow_sudo\" == \"0\" ]; then\n    if [ \"$user\" == \"root\" ]; then\n        echo \"WARNING: running as root, if you want to install DeepSpeed with sudo please use -s/--pip_sudo instead\"\n        usage\n        exit 1\n    fi\nfi\n\nif [ \"$examples\" == \"1\" ]; then\n    git submodule update --init --recursive\n    exit 0\nfi\n\nif [ \"$verbose\" == \"1\" ]; then\n    VERBOSE=\"-v\"\n    PIP_VERBOSE=\"\"\nelse\n    VERBOSE=\"\"\n    PIP_VERBOSE=\"--disable-pip-version-check\"\nfi\n\nrm_if_exist() {\n    echo \"Attempting to remove $1\"\n    if [ -f $1 ]; then\n        rm $VERBOSE $1\n    elif [ -d $1 ]; then\n        rm -rf $VERBOSE $1\n    fi\n}\n\nif [ \"$no_clean\" == \"0\" ]; then\n    # remove deepspeed build files\n    rm_if_exist deepspeed/git_version_info_installed.py\n    rm_if_exist dist\n    rm_if_exist build\n    rm_if_exist deepspeed.egg-info\nfi\n\nif [ \"$pip_sudo\" == \"1\" ]; then\n    PIP_SUDO=\"sudo -H\"\nelse\n    PIP_SUDO=\"\"\nfi\n\nif [ \"$pip_mirror\" != \"\" ]; then\n    PIP_INSTALL=\"pip install $VERBOSE $PIP_VERBOSE -i $pip_mirror\"\nelse\n    PIP_INSTALL=\"pip install $VERBOSE $PIP_VERBOSE\"\nfi\n\n\nif [ ! -f $hostfile ]; then\n    echo \"No hostfile exists at $hostfile, installing locally\"\n    local_only=1\nfi\n\necho \"Building deepspeed wheel\"\npython setup.py $VERBOSE bdist_wheel\n\nif [ \"$local_only\" == \"1\" ]; then\n    echo \"Installing deepspeed\"\n#    $PIP_SUDO pip uninstall -y deepspeed\n    $PIP_SUDO $PIP_INSTALL dist/deepspeed*.whl\n    ds_report\nelse\n    local_path=`pwd`\n    if [ -f $hostfile ]; then\n        hosts=`cat $hostfile | awk '{print $1}' | paste -sd \",\" -`;\n    else\n        echo \"hostfile not found, cannot proceed\"\n        exit 1\n    fi\n    export PDSH_RCMD_TYPE=ssh\n    tmp_wheel_path=\"/tmp/deepspeed_wheels\"\n\n    pdsh -w $hosts \"if [ -d $tmp_wheel_path ]; then rm $tmp_wheel_path/*; else mkdir -pv $tmp_wheel_path; fi\"\n    pdcp -w $hosts requirements/requirements.txt ${tmp_wheel_path}/\n\n    echo \"Installing deepspeed\"\n    pdsh -w $hosts \"$PIP_SUDO pip uninstall -y deepspeed\"\n    pdcp -w $hosts dist/deepspeed*.whl $tmp_wheel_path/\n    pdsh -w $hosts \"$PIP_SUDO $PIP_INSTALL $tmp_wheel_path/deepspeed*.whl\"\n    pdsh -w $hosts \"ds_report\"\n    pdsh -w $hosts \"if [ -d $tmp_wheel_path ]; then rm $tmp_wheel_path/*.whl; rm $tmp_wheel_path/*.txt; rmdir $tmp_wheel_path; fi\"\nfi\n"
        },
        {
          "name": "op_builder",
          "type": "tree",
          "content": null
        },
        {
          "name": "release",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.11,
          "content": "[options.entry_points]\npytest_randomly.random_seeder =\n    deepspeed = deepspeed.runtime.utils:set_random_seed\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 12.23,
          "content": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\nDeepSpeed library\n\nTo build wheel on Windows:\n1. Install pytorch, such as pytorch 1.12 + cuda 11.6.\n2. Install visual cpp build tool.\n3. Include cuda toolkit.\n4. Launch cmd console with Administrator privilege for creating required symlink folders.\n\n\nCreate a new wheel via the following command:\nbuild_win.bat\n\nThe wheel will be located at: dist/*.whl\n\"\"\"\n\nimport pathlib\nimport os\nimport shutil\nimport sys\nimport subprocess\nfrom setuptools import setup, find_packages\nfrom setuptools.command import egg_info\nimport time\nimport typing\nimport shlex\n\ntorch_available = True\ntry:\n    import torch\nexcept ImportError:\n    torch_available = False\n    print('[WARNING] Unable to import torch, pre-compiling ops will be disabled. ' \\\n        'Please visit https://pytorch.org/ to see how to properly install torch on your system.')\n\nfrom op_builder import get_default_compute_capabilities, OpBuilder\nfrom op_builder.all_ops import ALL_OPS, accelerator_name\nfrom op_builder.builder import installed_cuda_version\n\nfrom accelerator import get_accelerator\n\n# Fetch rocm state.\nis_rocm_pytorch = OpBuilder.is_rocm_pytorch()\nrocm_version = OpBuilder.installed_rocm_version()\n\nRED_START = '\\033[31m'\nRED_END = '\\033[0m'\nERROR = f\"{RED_START} [ERROR] {RED_END}\"\n\n\ndef abort(msg):\n    print(f\"{ERROR} {msg}\")\n    assert False, msg\n\n\ndef fetch_requirements(path):\n    with open(path, 'r') as fd:\n        return [r.strip() for r in fd.readlines()]\n\n\ndef is_env_set(key):\n    \"\"\"\n    Checks if an environment variable is set and not \"\".\n    \"\"\"\n    return bool(os.environ.get(key, None))\n\n\ndef get_env_if_set(key, default: typing.Any = \"\"):\n    \"\"\"\n    Returns an environment variable if it is set and not \"\",\n    otherwise returns a default value. In contrast, the fallback\n    parameter of os.environ.get() is skipped if the variable is set to \"\".\n    \"\"\"\n    return os.environ.get(key, None) or default\n\n\ninstall_requires = fetch_requirements('requirements/requirements.txt')\nextras_require = {\n    '1bit': [],  # add cupy based on cuda/rocm version\n    '1bit_mpi': fetch_requirements('requirements/requirements-1bit-mpi.txt'),\n    'readthedocs': fetch_requirements('requirements/requirements-readthedocs.txt'),\n    'dev': fetch_requirements('requirements/requirements-dev.txt'),\n    'autotuning': fetch_requirements('requirements/requirements-autotuning.txt'),\n    'autotuning_ml': fetch_requirements('requirements/requirements-autotuning-ml.txt'),\n    'sparse_attn': fetch_requirements('requirements/requirements-sparse_attn.txt'),\n    'sparse': fetch_requirements('requirements/requirements-sparse_pruning.txt'),\n    'inf': fetch_requirements('requirements/requirements-inf.txt'),\n    'sd': fetch_requirements('requirements/requirements-sd.txt'),\n    'triton': fetch_requirements('requirements/requirements-triton.txt'),\n}\n\n# Only install pynvml on nvidia gpus.\nif torch_available and get_accelerator().device_name() == 'cuda' and not is_rocm_pytorch:\n    install_requires.append('nvidia-ml-py')\n\n# Add specific cupy version to both onebit extension variants.\nif torch_available and get_accelerator().device_name() == 'cuda':\n    cupy = None\n    if is_rocm_pytorch:\n        rocm_major, rocm_minor = rocm_version\n        # XXX cupy support for rocm 5 is not available yet.\n        if rocm_major <= 4:\n            cupy = f\"cupy-rocm-{rocm_major}-{rocm_minor}\"\n    else:\n        cuda_major_ver, cuda_minor_ver = installed_cuda_version()\n        if (cuda_major_ver < 11) or ((cuda_major_ver == 11) and (cuda_minor_ver < 3)):\n            cupy = f\"cupy-cuda{cuda_major_ver}{cuda_minor_ver}\"\n        else:\n            cupy = f\"cupy-cuda{cuda_major_ver}x\"\n\n    if cupy:\n        extras_require['1bit'].append(cupy)\n        extras_require['1bit_mpi'].append(cupy)\n\n# Make an [all] extra that installs all needed dependencies.\nall_extras = set()\nfor extra in extras_require.items():\n    for req in extra[1]:\n        all_extras.add(req)\nextras_require['all'] = list(all_extras)\n\ncmdclass = {}\n\n# For any pre-installed ops force disable ninja.\nif torch_available:\n    use_ninja = is_env_set(\"DS_ENABLE_NINJA\")\n    cmdclass['build_ext'] = get_accelerator().build_extension().with_options(use_ninja=use_ninja)\n\nif torch_available:\n    TORCH_MAJOR = torch.__version__.split('.')[0]\n    TORCH_MINOR = torch.__version__.split('.')[1]\nelse:\n    TORCH_MAJOR = \"0\"\n    TORCH_MINOR = \"0\"\n\nif torch_available and not get_accelerator().device_name() == 'cuda':\n    # Fix to allow docker builds, similar to https://github.com/NVIDIA/apex/issues/486.\n    print(\"[WARNING] Torch did not find cuda available, if cross-compiling or running with cpu only \"\n          \"you can ignore this message. Adding compute capability for Pascal, Volta, and Turing \"\n          \"(compute capabilities 6.0, 6.1, 6.2)\")\n    if not is_env_set(\"TORCH_CUDA_ARCH_LIST\"):\n        os.environ[\"TORCH_CUDA_ARCH_LIST\"] = get_default_compute_capabilities()\n\next_modules = []\n\n# Default to pre-install kernels to false so we rely on JIT on Linux, opposite on Windows.\nBUILD_OP_PLATFORM = 1 if sys.platform == \"win32\" else 0\nBUILD_OP_DEFAULT = int(get_env_if_set('DS_BUILD_OPS', BUILD_OP_PLATFORM))\nprint(f\"DS_BUILD_OPS={BUILD_OP_DEFAULT}\")\n\nif BUILD_OP_DEFAULT:\n    assert torch_available, \"Unable to pre-compile ops without torch installed. Please install torch before attempting to pre-compile ops.\"\n\n\ndef command_exists(cmd):\n    if sys.platform == \"win32\":\n        safe_cmd = shlex.split(f'{cmd}')\n        result = subprocess.Popen(safe_cmd, stdout=subprocess.PIPE)\n        return result.wait() == 1\n    else:\n        safe_cmd = shlex.split(f\"bash -c type {cmd}\")\n        result = subprocess.Popen(safe_cmd, stdout=subprocess.PIPE)\n        return result.wait() == 0\n\n\ndef op_envvar(op_name):\n    assert hasattr(ALL_OPS[op_name], 'BUILD_VAR'), \\\n        f\"{op_name} is missing BUILD_VAR field\"\n    return ALL_OPS[op_name].BUILD_VAR\n\n\ndef op_enabled(op_name):\n    env_var = op_envvar(op_name)\n    return int(get_env_if_set(env_var, BUILD_OP_DEFAULT))\n\n\ninstall_ops = dict.fromkeys(ALL_OPS.keys(), False)\nfor op_name, builder in ALL_OPS.items():\n    op_compatible = builder.is_compatible()\n\n    # If op is requested but not available, throw an error.\n    if op_enabled(op_name) and not op_compatible:\n        env_var = op_envvar(op_name)\n        if not is_env_set(env_var):\n            builder.warning(f\"Skip pre-compile of incompatible {op_name}; One can disable {op_name} with {env_var}=0\")\n        continue\n\n    # If op is compatible but install is not enabled (JIT mode).\n    if is_rocm_pytorch and op_compatible and not op_enabled(op_name):\n        builder.hipify_extension()\n\n    # If op install enabled, add builder to extensions.\n    if op_enabled(op_name) and op_compatible:\n        assert torch_available, f\"Unable to pre-compile {op_name}, please first install torch\"\n        install_ops[op_name] = op_enabled(op_name)\n        ext_modules.append(builder.builder())\n\nprint(f'Install Ops={install_ops}')\n\n# Write out version/git info.\ngit_hash_cmd = shlex.split(\"bash -c \\\"git rev-parse --short HEAD\\\"\")\ngit_branch_cmd = shlex.split(\"bash -c \\\"git rev-parse --abbrev-ref HEAD\\\"\")\nif command_exists('git') and not is_env_set('DS_BUILD_STRING'):\n    try:\n        result = subprocess.check_output(git_hash_cmd)\n        git_hash = result.decode('utf-8').strip()\n        result = subprocess.check_output(git_branch_cmd)\n        git_branch = result.decode('utf-8').strip()\n    except subprocess.CalledProcessError:\n        git_hash = \"unknown\"\n        git_branch = \"unknown\"\nelse:\n    git_hash = \"unknown\"\n    git_branch = \"unknown\"\n\nif sys.platform == \"win32\":\n    shutil.rmtree('.\\\\deepspeed\\\\ops\\\\csrc', ignore_errors=True)\n    pathlib.Path('.\\\\deepspeed\\\\ops\\\\csrc').unlink(missing_ok=True)\n    shutil.copytree('.\\\\csrc', '.\\\\deepspeed\\\\ops\\\\csrc', dirs_exist_ok=True)\n    shutil.rmtree('.\\\\deepspeed\\\\ops\\\\op_builder', ignore_errors=True)\n    pathlib.Path('.\\\\deepspeed\\\\ops\\\\op_builder').unlink(missing_ok=True)\n    shutil.copytree('.\\\\op_builder', '.\\\\deepspeed\\\\ops\\\\op_builder', dirs_exist_ok=True)\n    shutil.rmtree('.\\\\deepspeed\\\\accelerator', ignore_errors=True)\n    pathlib.Path('.\\\\deepspeed\\\\accelerator').unlink(missing_ok=True)\n    shutil.copytree('.\\\\accelerator', '.\\\\deepspeed\\\\accelerator', dirs_exist_ok=True)\n    egg_info.manifest_maker.template = 'MANIFEST_win.in'\n\n# Parse the DeepSpeed version string from version.txt.\nversion_str = open('version.txt', 'r').read().strip()\n\n# Build specifiers like .devX can be added at install time. Otherwise, add the git hash.\n# Example: DS_BUILD_STRING=\".dev20201022\" python setup.py sdist bdist_wheel.\n\n# Building wheel for distribution, update version file.\nif is_env_set('DS_BUILD_STRING'):\n    # Build string env specified, probably building for distribution.\n    with open('build.txt', 'w') as fd:\n        fd.write(os.environ['DS_BUILD_STRING'])\n    version_str += os.environ['DS_BUILD_STRING']\nelif os.path.isfile('build.txt'):\n    # build.txt exists, probably installing from distribution.\n    with open('build.txt', 'r') as fd:\n        version_str += fd.read().strip()\nelse:\n    # None of the above, probably installing from source.\n    version_str += f'+{git_hash}'\n\ntorch_version = \".\".join([TORCH_MAJOR, TORCH_MINOR])\nbf16_support = False\n# Set cuda_version to 0.0 if cpu-only.\ncuda_version = \"0.0\"\nnccl_version = \"0.0\"\n# Set hip_version to 0.0 if cpu-only.\nhip_version = \"0.0\"\nif torch_available and torch.version.cuda is not None:\n    cuda_version = \".\".join(torch.version.cuda.split('.')[:2])\n    if sys.platform != \"win32\":\n        if isinstance(torch.cuda.nccl.version(), int):\n            # This will break if minor version > 9.\n            nccl_version = \".\".join(str(torch.cuda.nccl.version())[:2])\n        else:\n            nccl_version = \".\".join(map(str, torch.cuda.nccl.version()[:2]))\n    if hasattr(torch.cuda, 'is_bf16_supported') and torch.cuda.is_available():\n        bf16_support = torch.cuda.is_bf16_supported()\nif torch_available and hasattr(torch.version, 'hip') and torch.version.hip is not None:\n    hip_version = \".\".join(torch.version.hip.split('.')[:2])\ntorch_info = {\n    \"version\": torch_version,\n    \"bf16_support\": bf16_support,\n    \"cuda_version\": cuda_version,\n    \"nccl_version\": nccl_version,\n    \"hip_version\": hip_version\n}\n\nprint(f\"version={version_str}, git_hash={git_hash}, git_branch={git_branch}\")\nwith open('deepspeed/git_version_info_installed.py', 'w') as fd:\n    fd.write(f\"version='{version_str}'\\n\")\n    fd.write(f\"git_hash='{git_hash}'\\n\")\n    fd.write(f\"git_branch='{git_branch}'\\n\")\n    fd.write(f\"installed_ops={install_ops}\\n\")\n    fd.write(f\"accelerator_name='{accelerator_name}'\\n\")\n    fd.write(f\"torch_info={torch_info}\\n\")\n\nprint(f'install_requires={install_requires}')\nprint(f'ext_modules={ext_modules}')\n\n# Parse README.md to make long_description for PyPI page.\nthisdir = os.path.abspath(os.path.dirname(__file__))\nwith open(os.path.join(thisdir, 'README.md'), encoding='utf-8') as fin:\n    readme_text = fin.read()\n\nif sys.platform == \"win32\":\n    scripts = ['bin/deepspeed.bat', 'bin/ds', 'bin/ds_report.bat', 'bin/ds_report']\nelse:\n    scripts = [\n        'bin/deepspeed', 'bin/deepspeed.pt', 'bin/ds', 'bin/ds_ssh', 'bin/ds_report', 'bin/ds_bench', 'bin/dsr',\n        'bin/ds_elastic', 'bin/ds_nvme_tune', 'bin/ds_io'\n    ]\n\nstart_time = time.time()\n\nsetup(name='deepspeed',\n      version=version_str,\n      description='DeepSpeed library',\n      long_description=readme_text,\n      long_description_content_type='text/markdown',\n      author='DeepSpeed Team',\n      author_email='deepspeed-info@microsoft.com',\n      url='http://deepspeed.ai',\n      project_urls={\n          'Documentation': 'https://deepspeed.readthedocs.io',\n          'Source': 'https://github.com/microsoft/DeepSpeed',\n      },\n      install_requires=install_requires,\n      extras_require=extras_require,\n      packages=find_packages(include=['deepspeed', 'deepspeed.*']),\n      include_package_data=True,\n      scripts=scripts,\n      classifiers=[\n          'Programming Language :: Python :: 3.8', 'Programming Language :: Python :: 3.9',\n          'Programming Language :: Python :: 3.10', 'Programming Language :: Python :: 3.11',\n          'Programming Language :: Python :: 3.12'\n      ],\n      license='Apache Software License 2.0',\n      ext_modules=ext_modules,\n      cmdclass=cmdclass)\n\nend_time = time.time()\nprint(f'deepspeed build time = {end_time - start_time} secs')\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "version.txt",
          "type": "blob",
          "size": 0.01,
          "content": "0.16.3\n"
        }
      ]
    }
  ]
}