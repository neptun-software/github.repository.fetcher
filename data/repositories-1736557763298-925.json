{
  "metadata": {
    "timestamp": 1736557763298,
    "page": 925,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjk1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "facebookresearch/fastText",
      "stars": 26013,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.11,
          "content": ".*.swp\n*.o\n*.bin\n*.vec\n*.bc\n.DS_Store\ndata\nfasttext\nresult\nwebsite/node_modules/\npackage-lock.json\nnode_modules/\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 2.19,
          "content": "#\n# Copyright (c) 2016-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\ncmake_minimum_required(VERSION 2.8.9)\nproject(fasttext)\n\nset(CMAKE_CXX_STANDARD 17)\n\n# The version number.\nset (fasttext_VERSION_MAJOR 0)\nset (fasttext_VERSION_MINOR 1)\n\ninclude_directories(fasttext)\n\nset(CMAKE_CXX_FLAGS \" -pthread -std=c++17 -funroll-loops -O3 -march=native\")\n\nset(HEADER_FILES\n    src/args.h\n    src/autotune.h\n    src/densematrix.h\n    src/dictionary.h\n    src/fasttext.h\n    src/loss.h\n    src/matrix.h\n    src/meter.h\n    src/model.h\n    src/productquantizer.h\n    src/quantmatrix.h\n    src/real.h\n    src/utils.h\n    src/vector.h)\n\nset(SOURCE_FILES\n    src/args.cc\n    src/autotune.cc\n    src/densematrix.cc\n    src/dictionary.cc\n    src/fasttext.cc\n    src/loss.cc\n    src/main.cc\n    src/matrix.cc\n    src/meter.cc\n    src/model.cc\n    src/productquantizer.cc\n    src/quantmatrix.cc\n    src/utils.cc\n    src/vector.cc)\n\n\nif (NOT MSVC)\n  include(GNUInstallDirs)\n  configure_file(\"fasttext.pc.in\" \"fasttext.pc\" @ONLY)\n  install(FILES \"${CMAKE_BINARY_DIR}/fasttext.pc\" DESTINATION ${CMAKE_INSTALL_LIBDIR}/pkgconfig)\nendif()\n\nadd_library(fasttext-shared SHARED ${SOURCE_FILES} ${HEADER_FILES})\nadd_library(fasttext-static STATIC ${SOURCE_FILES} ${HEADER_FILES})\nadd_library(fasttext-static_pic STATIC ${SOURCE_FILES} ${HEADER_FILES})\nset_target_properties(fasttext-shared PROPERTIES OUTPUT_NAME fasttext\n  SOVERSION \"${fasttext_VERSION_MAJOR}\")\nset_target_properties(fasttext-static PROPERTIES OUTPUT_NAME fasttext)\nset_target_properties(fasttext-static_pic PROPERTIES OUTPUT_NAME fasttext_pic\n  POSITION_INDEPENDENT_CODE True)\nadd_executable(fasttext-bin src/main.cc)\ntarget_link_libraries(fasttext-bin pthread fasttext-static)\nset_target_properties(fasttext-bin PROPERTIES PUBLIC_HEADER \"${HEADER_FILES}\" OUTPUT_NAME fasttext)\ninstall (TARGETS fasttext-shared\n    LIBRARY DESTINATION lib)\ninstall (TARGETS fasttext-static\n    ARCHIVE DESTINATION lib)\ninstall (TARGETS fasttext-static_pic\n    ARCHIVE DESTINATION lib)\ninstall (TARGETS fasttext-bin\n    RUNTIME DESTINATION bin\n PUBLIC_HEADER DESTINATION include/fasttext)\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.28,
          "content": "# Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to make participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\n  advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\n  address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies within all project spaces, and it also applies when\nan individual is representing the project or its community in public spaces.\nExamples of representing a project or community include using an official\nproject e-mail address, posting via an official social media account, or acting\nas an appointed representative at an online or offline event. Representation of\na project may be further defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at <opensource-conduct@fb.com>. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq\n\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 2.01,
          "content": "# Contributing to fastText\nWe want to make contributing to this project as easy and transparent as possible.\n\n## Issues\nWe use GitHub issues to track public bugs. Please ensure your description is clear and has sufficient instructions to be able to reproduce the issue.\n\n### Reproducing issues\nPlease make sure that the issue you mention is not a result of one of the existing third-party libraries. For example, please do not post an issue if you encountered an error within a third-party Python library. We can only help you with errors which can be directly reproduced either with our C++ code or the corresponding Python bindings. If you do find an error, please post detailed steps to reproduce it. If we can't reproduce your error, we can't help you fix it.\n\n## Pull Requests\nPlease post an Issue before submitting a pull request. This might save you some time as it is possible we can't support your contribution, albeit we try our best to accomodate your (planned) work and highly appreciate your time. Generally, it is best to have a pull request emerge from an issue rather than the other way around.\n\nTo create a pull request:\n\n1. Fork the repo and create your branch from `master`.\n2. If you've added code that should be tested, add tests.\n3. If you've changed APIs, update the documentation.\n4. Ensure the test suite passes.\n5. Make sure your code lints.\n6. If you haven't already, complete the Contributor License Agreement (\"CLA\").\n\n## Tests\nFirst, you will need to make sure you have the required data. For that, please have a look at the fetch_test_data.sh script under tests. Next run the tests using the runtests.py script passing a path to the directory containing the datasets.\n\n## Contributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\n\nComplete your CLA here: <https://code.facebook.com/cla>\n\n## License\nBy contributing to fastText, you agree that your contributions will be licensed under its MIT license.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.05,
          "content": "MIT License\n\nCopyright (c) 2016-present, Facebook, Inc.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.09,
          "content": "include LICENSE\ninclude PATENTS\n\nrecursive-include python *.md *.rst\nrecursive-include src *.h\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 4.12,
          "content": "#\n# Copyright (c) 2016-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nCXX = c++\nCXXFLAGS = -pthread -std=c++17 -march=native\nOBJS = args.o autotune.o matrix.o dictionary.o loss.o productquantizer.o densematrix.o quantmatrix.o vector.o model.o utils.o meter.o fasttext.o\nINCLUDES = -I.\n\nopt: CXXFLAGS += -O3 -funroll-loops -DNDEBUG\nopt: fasttext\n\ncoverage: CXXFLAGS += -O0 -fno-inline -fprofile-arcs --coverage\ncoverage: fasttext\n\ndebug: CXXFLAGS += -g -O0 -fno-inline\ndebug: fasttext\n\nwasm: webassembly/fasttext_wasm.js\n\nwasmdebug: export EMCC_DEBUG=1\nwasmdebug: webassembly/fasttext_wasm.js\n\n\nargs.o: src/args.cc src/args.h\n\t$(CXX) $(CXXFLAGS) -c src/args.cc\n\nautotune.o: src/autotune.cc src/autotune.h\n\t$(CXX) $(CXXFLAGS) -c src/autotune.cc\n\nmatrix.o: src/matrix.cc src/matrix.h\n\t$(CXX) $(CXXFLAGS) -c src/matrix.cc\n\ndictionary.o: src/dictionary.cc src/dictionary.h src/args.h\n\t$(CXX) $(CXXFLAGS) -c src/dictionary.cc\n\nloss.o: src/loss.cc src/loss.h src/matrix.h src/real.h\n\t$(CXX) $(CXXFLAGS) -c src/loss.cc\n\nproductquantizer.o: src/productquantizer.cc src/productquantizer.h src/utils.h\n\t$(CXX) $(CXXFLAGS) -c src/productquantizer.cc\n\ndensematrix.o: src/densematrix.cc src/densematrix.h src/utils.h src/matrix.h\n\t$(CXX) $(CXXFLAGS) -c src/densematrix.cc\n\nquantmatrix.o: src/quantmatrix.cc src/quantmatrix.h src/utils.h src/matrix.h\n\t$(CXX) $(CXXFLAGS) -c src/quantmatrix.cc\n\nvector.o: src/vector.cc src/vector.h src/utils.h\n\t$(CXX) $(CXXFLAGS) -c src/vector.cc\n\nmodel.o: src/model.cc src/model.h src/args.h\n\t$(CXX) $(CXXFLAGS) -c src/model.cc\n\nutils.o: src/utils.cc src/utils.h\n\t$(CXX) $(CXXFLAGS) -c src/utils.cc\n\nmeter.o: src/meter.cc src/meter.h\n\t$(CXX) $(CXXFLAGS) -c src/meter.cc\n\nfasttext.o: src/fasttext.cc src/*.h\n\t$(CXX) $(CXXFLAGS) -c src/fasttext.cc\n\nfasttext: $(OBJS) src/fasttext.cc src/main.cc\n\t$(CXX) $(CXXFLAGS) $(OBJS) src/main.cc -o fasttext\n\nclean:\n\trm -rf *.o *.gcno *.gcda fasttext *.bc webassembly/fasttext_wasm.js webassembly/fasttext_wasm.wasm\n\n\nEMCXX = em++\nEMCXXFLAGS = --bind --std=c++11 -s WASM=1 -s ALLOW_MEMORY_GROWTH=1 -s \"EXTRA_EXPORTED_RUNTIME_METHODS=['addOnPostRun', 'FS']\" -s \"DISABLE_EXCEPTION_CATCHING=0\" -s \"EXCEPTION_DEBUG=1\" -s \"FORCE_FILESYSTEM=1\" -s \"MODULARIZE=1\" -s \"EXPORT_ES6=1\" -s 'EXPORT_NAME=\"FastTextModule\"' -Isrc/\nEMOBJS = args.bc autotune.bc matrix.bc dictionary.bc loss.bc productquantizer.bc densematrix.bc quantmatrix.bc vector.bc model.bc utils.bc meter.bc fasttext.bc main.bc\n\n\nmain.bc: webassembly/fasttext_wasm.cc\n\t$(EMCXX) $(EMCXXFLAGS)  webassembly/fasttext_wasm.cc -o main.bc\n\nargs.bc: src/args.cc src/args.h\n\t$(EMCXX) $(EMCXXFLAGS)  src/args.cc -o args.bc\n\nautotune.bc: src/autotune.cc src/autotune.h\n\t$(EMCXX) $(EMCXXFLAGS)  src/autotune.cc -o autotune.bc\n\nmatrix.bc: src/matrix.cc src/matrix.h\n\t$(EMCXX) $(EMCXXFLAGS) src/matrix.cc -o matrix.bc\n\ndictionary.bc: src/dictionary.cc src/dictionary.h src/args.h\n\t$(EMCXX) $(EMCXXFLAGS)  src/dictionary.cc -o dictionary.bc\n\nloss.bc: src/loss.cc src/loss.h src/matrix.h src/real.h\n\t$(EMCXX) $(EMCXXFLAGS) src/loss.cc -o loss.bc\n\nproductquantizer.bc: src/productquantizer.cc src/productquantizer.h src/utils.h\n\t$(EMCXX) $(EMCXXFLAGS)  src/productquantizer.cc -o productquantizer.bc\n\ndensematrix.bc: src/densematrix.cc src/densematrix.h src/utils.h src/matrix.h\n\t$(EMCXX) $(EMCXXFLAGS) src/densematrix.cc -o densematrix.bc\n\nquantmatrix.bc: src/quantmatrix.cc src/quantmatrix.h src/utils.h src/matrix.h\n\t$(EMCXX) $(EMCXXFLAGS) src/quantmatrix.cc -o quantmatrix.bc\n\nvector.bc: src/vector.cc src/vector.h src/utils.h\n\t$(EMCXX) $(EMCXXFLAGS)  src/vector.cc -o vector.bc\n\nmodel.bc: src/model.cc src/model.h src/args.h\n\t$(EMCXX) $(EMCXXFLAGS)  src/model.cc -o model.bc\n\nutils.bc: src/utils.cc src/utils.h\n\t$(EMCXX) $(EMCXXFLAGS)  src/utils.cc -o utils.bc\n\nmeter.bc: src/meter.cc src/meter.h\n\t$(EMCXX) $(EMCXXFLAGS)  src/meter.cc -o meter.bc\n\nfasttext.bc: src/fasttext.cc src/*.h\n\t$(EMCXX) $(EMCXXFLAGS)  src/fasttext.cc -o fasttext.bc\n\nwebassembly/fasttext_wasm.js: $(EMOBJS) webassembly/fasttext_wasm.cc Makefile\n\t$(EMCXX) $(EMCXXFLAGS) $(EMOBJS) -o webassembly/fasttext_wasm.js\n\n\n"
        },
        {
          "name": "PACKAGE",
          "type": "blob",
          "size": 0.13,
          "content": "load(\"@fbcode_macros//build_defs:package_local_utils.bzl\", \"package_local_utils\")\n\npackage_local_utils.set_clang_version(15, True)\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 13.29,
          "content": "# fastText\n[fastText](https://fasttext.cc/) is a library for efficient learning of word representations and sentence classification.\n\n[![CircleCI](https://circleci.com/gh/facebookresearch/fastText/tree/master.svg?style=svg)](https://circleci.com/gh/facebookresearch/fastText/tree/master)\n\n## Table of contents\n\n* [Resources](#resources)\n   * [Models](#models)\n   * [Supplementary data](#supplementary-data)\n   * [FAQ](#faq)\n   * [Cheatsheet](#cheatsheet)\n* [Requirements](#requirements)\n* [Building fastText](#building-fasttext)\n   * [Getting the source code](#getting-the-source-code)\n   * [Building fastText using make (preferred)](#building-fasttext-using-make-preferred)\n   * [Building fastText using cmake](#building-fasttext-using-cmake)\n   * [Building fastText for Python](#building-fasttext-for-python)\n* [Example use cases](#example-use-cases)\n   * [Word representation learning](#word-representation-learning)\n   * [Obtaining word vectors for out-of-vocabulary words](#obtaining-word-vectors-for-out-of-vocabulary-words)\n   * [Text classification](#text-classification)\n* [Full documentation](#full-documentation)\n* [References](#references)\n   * [Enriching Word Vectors with Subword Information](#enriching-word-vectors-with-subword-information)\n   * [Bag of Tricks for Efficient Text Classification](#bag-of-tricks-for-efficient-text-classification)\n   * [FastText.zip: Compressing text classification models](#fasttextzip-compressing-text-classification-models)\n* [Join the fastText community](#join-the-fasttext-community)\n* [License](#license)\n\n## Resources\n\n### Models\n- Recent state-of-the-art [English word vectors](https://fasttext.cc/docs/en/english-vectors.html).\n- Word vectors for [157 languages trained on Wikipedia and Crawl](https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md).\n- Models for [language identification](https://fasttext.cc/docs/en/language-identification.html#content) and [various supervised tasks](https://fasttext.cc/docs/en/supervised-models.html#content).\n\n### Supplementary data\n- The preprocessed [YFCC100M data](https://fasttext.cc/docs/en/dataset.html#content) used in [2].\n\n### FAQ\n\nYou can find [answers to frequently asked questions](https://fasttext.cc/docs/en/faqs.html#content) on our [website](https://fasttext.cc/).\n\n### Cheatsheet\n\nWe also provide a [cheatsheet](https://fasttext.cc/docs/en/cheatsheet.html#content) full of useful one-liners.\n\n## Requirements\n\nWe are continuously building and testing our library, CLI and Python bindings under various docker images using [circleci](https://circleci.com/).\n\nGenerally, **fastText** builds on modern Mac OS and Linux distributions.\nSince it uses some C++11 features, it requires a compiler with good C++11 support.\nThese include :\n\n* (g++-4.7.2 or newer) or (clang-3.3 or newer)\n\nCompilation is carried out using a Makefile, so you will need to have a working **make**.\nIf you want to use **cmake** you need at least version 2.8.9.\n\nOne of the oldest distributions we successfully built and tested the CLI under is [Debian jessie](https://www.debian.org/releases/jessie/).\n\nFor the word-similarity evaluation script you will need:\n\n* Python 2.6 or newer\n* NumPy & SciPy\n\nFor the python bindings (see the subdirectory python) you will need:\n\n* Python version 2.7 or >=3.4\n* NumPy & SciPy\n* [pybind11](https://github.com/pybind/pybind11)\n\nOne of the oldest distributions we successfully built and tested the Python bindings under is [Debian jessie](https://www.debian.org/releases/jessie/).\n\nIf these requirements make it impossible for you to use fastText, please open an issue and we will try to accommodate you.\n\n## Building fastText\n\nWe discuss building the latest stable version of fastText.\n\n### Getting the source code\n\nYou can find our [latest stable release](https://github.com/facebookresearch/fastText/releases/latest) in the usual place.\n\nThere is also the master branch that contains all of our most recent work, but comes along with all the usual caveats of an unstable branch. You might want to use this if you are a developer or power-user.\n\n### Building fastText using make (preferred)\n\n```\n$ wget https://github.com/facebookresearch/fastText/archive/v0.9.2.zip\n$ unzip v0.9.2.zip\n$ cd fastText-0.9.2\n$ make\n```\n\nThis will produce object files for all the classes as well as the main binary `fasttext`.\nIf you do not plan on using the default system-wide compiler, update the two macros defined at the beginning of the Makefile (CC and INCLUDES).\n\n### Building fastText using cmake\n\nFor now this is not part of a release, so you will need to clone the master branch.\n\n```\n$ git clone https://github.com/facebookresearch/fastText.git\n$ cd fastText\n$ mkdir build && cd build && cmake ..\n$ make && make install\n```\n\nThis will create the fasttext binary and also all relevant libraries (shared, static, PIC).\n\n### Building fastText for Python\n\nFor now this is not part of a release, so you will need to clone the master branch.\n\n```\n$ git clone https://github.com/facebookresearch/fastText.git\n$ cd fastText\n$ pip install .\n```\n\nFor further information and introduction see python/README.md\n\n## Example use cases\n\nThis library has two main use cases: word representation learning and text classification.\nThese were described in the two papers [1](#enriching-word-vectors-with-subword-information) and [2](#bag-of-tricks-for-efficient-text-classification).\n\n### Word representation learning\n\nIn order to learn word vectors, as described in [1](#enriching-word-vectors-with-subword-information), do:\n\n```\n$ ./fasttext skipgram -input data.txt -output model\n```\n\nwhere `data.txt` is a training file containing `UTF-8` encoded text.\nBy default the word vectors will take into account character n-grams from 3 to 6 characters.\nAt the end of optimization the program will save two files: `model.bin` and `model.vec`.\n`model.vec` is a text file containing the word vectors, one per line.\n`model.bin` is a binary file containing the parameters of the model along with the dictionary and all hyper parameters.\nThe binary file can be used later to compute word vectors or to restart the optimization.\n\n### Obtaining word vectors for out-of-vocabulary words\n\nThe previously trained model can be used to compute word vectors for out-of-vocabulary words.\nProvided you have a text file `queries.txt` containing words for which you want to compute vectors, use the following command:\n\n```\n$ ./fasttext print-word-vectors model.bin < queries.txt\n```\n\nThis will output word vectors to the standard output, one vector per line.\nThis can also be used with pipes:\n\n```\n$ cat queries.txt | ./fasttext print-word-vectors model.bin\n```\n\nSee the provided scripts for an example. For instance, running:\n\n```\n$ ./word-vector-example.sh\n```\n\nwill compile the code, download data, compute word vectors and evaluate them on the rare words similarity dataset RW [Thang et al. 2013].\n\n### Text classification\n\nThis library can also be used to train supervised text classifiers, for instance for sentiment analysis.\nIn order to train a text classifier using the method described in [2](#bag-of-tricks-for-efficient-text-classification), use:\n\n```\n$ ./fasttext supervised -input train.txt -output model\n```\n\nwhere `train.txt` is a text file containing a training sentence per line along with the labels.\nBy default, we assume that labels are words that are prefixed by the string `__label__`.\nThis will output two files: `model.bin` and `model.vec`.\nOnce the model was trained, you can evaluate it by computing the precision and recall at k (P@k and R@k) on a test set using:\n\n```\n$ ./fasttext test model.bin test.txt k\n```\n\nThe argument `k` is optional, and is equal to `1` by default.\n\nIn order to obtain the k most likely labels for a piece of text, use:\n\n```\n$ ./fasttext predict model.bin test.txt k\n```\n\nor use `predict-prob` to also get the probability for each label\n\n```\n$ ./fasttext predict-prob model.bin test.txt k\n```\n\nwhere `test.txt` contains a piece of text to classify per line.\nDoing so will print to the standard output the k most likely labels for each line.\nThe argument `k` is optional, and equal to `1` by default.\nSee `classification-example.sh` for an example use case.\nIn order to reproduce results from the paper [2](#bag-of-tricks-for-efficient-text-classification), run `classification-results.sh`, this will download all the datasets and reproduce the results from Table 1.\n\nIf you want to compute vector representations of sentences or paragraphs, please use:\n\n```\n$ ./fasttext print-sentence-vectors model.bin < text.txt\n```\n\nThis assumes that the `text.txt` file contains the paragraphs that you want to get vectors for.\nThe program will output one vector representation per line in the file.\n\nYou can also quantize a supervised model to reduce its memory usage with the following command:\n\n```\n$ ./fasttext quantize -output model\n```\nThis will create a `.ftz` file with a smaller memory footprint. All the standard functionality, like `test` or `predict` work the same way on the quantized models:\n```\n$ ./fasttext test model.ftz test.txt\n```\nThe quantization procedure follows the steps described in [3](#fasttextzip-compressing-text-classification-models). You can\nrun the script `quantization-example.sh` for an example.\n\n\n## Full documentation\n\nInvoke a command without arguments to list available arguments and their default values:\n\n```\n$ ./fasttext supervised\nEmpty input or output path.\n\nThe following arguments are mandatory:\n  -input              training file path\n  -output             output file path\n\nThe following arguments are optional:\n  -verbose            verbosity level [2]\n\nThe following arguments for the dictionary are optional:\n  -minCount           minimal number of word occurrences [1]\n  -minCountLabel      minimal number of label occurrences [0]\n  -wordNgrams         max length of word ngram [1]\n  -bucket             number of buckets [2000000]\n  -minn               min length of char ngram [0]\n  -maxn               max length of char ngram [0]\n  -t                  sampling threshold [0.0001]\n  -label              labels prefix [__label__]\n\nThe following arguments for training are optional:\n  -lr                 learning rate [0.1]\n  -lrUpdateRate       change the rate of updates for the learning rate [100]\n  -dim                size of word vectors [100]\n  -ws                 size of the context window [5]\n  -epoch              number of epochs [5]\n  -neg                number of negatives sampled [5]\n  -loss               loss function {ns, hs, softmax} [softmax]\n  -thread             number of threads [12]\n  -pretrainedVectors  pretrained word vectors for supervised learning []\n  -saveOutput         whether output params should be saved [0]\n\nThe following arguments for quantization are optional:\n  -cutoff             number of words and ngrams to retain [0]\n  -retrain            finetune embeddings if a cutoff is applied [0]\n  -qnorm              quantizing the norm separately [0]\n  -qout               quantizing the classifier [0]\n  -dsub               size of each sub-vector [2]\n```\n\nDefaults may vary by mode. (Word-representation modes `skipgram` and `cbow` use a default `-minCount` of 5.)\n\n## References\n\nPlease cite [1](#enriching-word-vectors-with-subword-information) if using this code for learning word representations or [2](#bag-of-tricks-for-efficient-text-classification) if using for text classification.\n\n### Enriching Word Vectors with Subword Information\n\n[1] P. Bojanowski\\*, E. Grave\\*, A. Joulin, T. Mikolov, [*Enriching Word Vectors with Subword Information*](https://arxiv.org/abs/1607.04606)\n\n```\n@article{bojanowski2017enriching,\n  title={Enriching Word Vectors with Subword Information},\n  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},\n  journal={Transactions of the Association for Computational Linguistics},\n  volume={5},\n  year={2017},\n  issn={2307-387X},\n  pages={135--146}\n}\n```\n\n### Bag of Tricks for Efficient Text Classification\n\n[2] A. Joulin, E. Grave, P. Bojanowski, T. Mikolov, [*Bag of Tricks for Efficient Text Classification*](https://arxiv.org/abs/1607.01759)\n\n```\n@InProceedings{joulin2017bag,\n  title={Bag of Tricks for Efficient Text Classification},\n  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},\n  booktitle={Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},\n  month={April},\n  year={2017},\n  publisher={Association for Computational Linguistics},\n  pages={427--431},\n}\n```\n\n### FastText.zip: Compressing text classification models\n\n[3] A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. Jégou, T. Mikolov, [*FastText.zip: Compressing text classification models*](https://arxiv.org/abs/1612.03651)\n\n```\n@article{joulin2016fasttext,\n  title={FastText.zip: Compressing text classification models},\n  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Douze, Matthijs and J{\\'e}gou, H{\\'e}rve and Mikolov, Tomas},\n  journal={arXiv preprint arXiv:1612.03651},\n  year={2016}\n}\n```\n\n(\\* These authors contributed equally.)\n\n\n## Join the fastText community\n\n* Facebook page: https://www.facebook.com/groups/1174547215919768\n* Google group: https://groups.google.com/forum/#!forum/fasttext-library\n* Contact: [egrave@fb.com](mailto:egrave@fb.com), [bojanowski@fb.com](mailto:bojanowski@fb.com), [ajoulin@fb.com](mailto:ajoulin@fb.com), [tmikolov@fb.com](mailto:tmikolov@fb.com)\n\nSee the CONTRIBUTING file for information about how to help out.\n\n## License\n\nfastText is MIT-licensed.\n"
        },
        {
          "name": "alignment",
          "type": "tree",
          "content": null
        },
        {
          "name": "classification-example.sh",
          "type": "blob",
          "size": 1.39,
          "content": "#!/usr/bin/env bash\n#\n# Copyright (c) 2016-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nmyshuf() {\n  perl -MList::Util=shuffle -e 'print shuffle(<>);' \"$@\";\n}\n\nnormalize_text() {\n  tr '[:upper:]' '[:lower:]' | sed -e 's/^/__label__/g' | \\\n    sed -e \"s/'/ ' /g\" -e 's/\"//g' -e 's/\\./ \\. /g' -e 's/<br \\/>/ /g' \\\n        -e 's/,/ , /g' -e 's/(/ ( /g' -e 's/)/ ) /g' -e 's/\\!/ \\! /g' \\\n        -e 's/\\?/ \\? /g' -e 's/\\;/ /g' -e 's/\\:/ /g' | tr -s \" \" | myshuf\n}\n\nRESULTDIR=result\nDATADIR=data\n\nmkdir -p \"${RESULTDIR}\"\nmkdir -p \"${DATADIR}\"\n\nif [ ! -f \"${DATADIR}/dbpedia.train\" ]\nthen\n  wget -c \"https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbQ2Vic1kxMmZZQ1k\" -O \"${DATADIR}/dbpedia_csv.tar.gz\"\n  tar -xzvf \"${DATADIR}/dbpedia_csv.tar.gz\" -C \"${DATADIR}\"\n  cat \"${DATADIR}/dbpedia_csv/train.csv\" | normalize_text > \"${DATADIR}/dbpedia.train\"\n  cat \"${DATADIR}/dbpedia_csv/test.csv\" | normalize_text > \"${DATADIR}/dbpedia.test\"\nfi\n\nmake\n\n./fasttext supervised -input \"${DATADIR}/dbpedia.train\" -output \"${RESULTDIR}/dbpedia\" -dim 10 -lr 0.1 -wordNgrams 2 -minCount 1 -bucket 10000000 -epoch 5 -thread 4\n\n./fasttext test \"${RESULTDIR}/dbpedia.bin\" \"${DATADIR}/dbpedia.test\"\n\n./fasttext predict \"${RESULTDIR}/dbpedia.bin\" \"${DATADIR}/dbpedia.test\" > \"${RESULTDIR}/dbpedia.test.predict\"\n"
        },
        {
          "name": "classification-results.sh",
          "type": "blob",
          "size": 3.08,
          "content": "#!/usr/bin/env bash\n#\n# Copyright (c) 2016-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\n# This script produces the results from Table 1 in the following paper:\n# Bag of Tricks for Efficient Text Classification, arXiv 1607.01759, 2016\n\nmyshuf() {\n  perl -MList::Util=shuffle -e 'print shuffle(<>);' \"$@\";\n}\n\nnormalize_text() {\n  tr '[:upper:]' '[:lower:]' | sed -e 's/^/__label__/g' | \\\n    sed -e \"s/'/ ' /g\" -e 's/\"//g' -e 's/\\./ \\. /g' -e 's/<br \\/>/ /g' \\\n        -e 's/,/ , /g' -e 's/(/ ( /g' -e 's/)/ ) /g' -e 's/\\!/ \\! /g' \\\n        -e 's/\\?/ \\? /g' -e 's/\\;/ /g' -e 's/\\:/ /g' | tr -s \" \" | myshuf\n}\n\nDATASET=(\n  ag_news\n  sogou_news\n  dbpedia\n  yelp_review_polarity\n  yelp_review_full\n  yahoo_answers\n  amazon_review_full\n  amazon_review_polarity\n)\n\nID=(\n  0Bz8a_Dbh9QhbUDNpeUdjb0wxRms # ag_news\n  0Bz8a_Dbh9QhbUkVqNEszd0pHaFE # sogou_news\n  0Bz8a_Dbh9QhbQ2Vic1kxMmZZQ1k # dbpedia\n  0Bz8a_Dbh9QhbNUpYQ2N3SGlFaDg # yelp_review_polarity\n  0Bz8a_Dbh9QhbZlU4dXhHTFhZQU0 # yelp_review_full\n  0Bz8a_Dbh9Qhbd2JNdDBsQUdocVU # yahoo_answers\n  0Bz8a_Dbh9QhbZVhsUnRWRDhETzA # amazon_review_full\n  0Bz8a_Dbh9QhbaW12WVVZS2drcnM # amazon_review_polarity\n)\n\n# These learning rates were chosen by validation on a subset of the training set.\nLR=( 0.25 0.5 0.5 0.1 0.1 0.1 0.05 0.05 )\n\nRESULTDIR=result\nDATADIR=data\n\nmkdir -p \"${RESULTDIR}\"\nmkdir -p \"${DATADIR}\"\n\n# Small datasets first\n\nfor i in {0..0}\ndo\n  echo \"Downloading dataset ${DATASET[i]}\"\n  if [ ! -f \"${DATADIR}/${DATASET[i]}.train\" ]\n  then\n    wget -c \"https://drive.google.com/uc?export=download&id=${ID[i]}\" -O \"${DATADIR}/${DATASET[i]}_csv.tar.gz\"\n    tar -xzvf \"${DATADIR}/${DATASET[i]}_csv.tar.gz\" -C \"${DATADIR}\"\n    cat \"${DATADIR}/${DATASET[i]}_csv/train.csv\" | normalize_text > \"${DATADIR}/${DATASET[i]}.train\"\n    cat \"${DATADIR}/${DATASET[i]}_csv/test.csv\" | normalize_text > \"${DATADIR}/${DATASET[i]}.test\"\n  fi\ndone\n\n# Large datasets require a bit more work due to the extra request page\n\nfor i in {1..7}\ndo\n  echo \"Downloading dataset ${DATASET[i]}\"\n  if [ ! -f \"${DATADIR}/${DATASET[i]}.train\" ]\n  then\n    curl -c /tmp/cookies \"https://drive.google.com/uc?export=download&id=${ID[i]}\" > /tmp/intermezzo.html\n    curl -L -b /tmp/cookies \"https://drive.google.com$(cat /tmp/intermezzo.html | grep -Po 'uc-download-link\" [^>]* href=\"\\K[^\"]*' | sed 's/\\&amp;/\\&/g')\" > \"${DATADIR}/${DATASET[i]}_csv.tar.gz\"\n    tar -xzvf \"${DATADIR}/${DATASET[i]}_csv.tar.gz\" -C \"${DATADIR}\"\n    cat \"${DATADIR}/${DATASET[i]}_csv/train.csv\" | normalize_text > \"${DATADIR}/${DATASET[i]}.train\"\n    cat \"${DATADIR}/${DATASET[i]}_csv/test.csv\" | normalize_text > \"${DATADIR}/${DATASET[i]}.test\"\n  fi\ndone\n\nmake\n\nfor i in {0..7}\ndo\n  echo \"Working on dataset ${DATASET[i]}\"\n  ./fasttext supervised -input \"${DATADIR}/${DATASET[i]}.train\" \\\n    -output \"${RESULTDIR}/${DATASET[i]}\" -dim 10 -lr \"${LR[i]}\" -wordNgrams 2 \\\n    -minCount 1 -bucket 10000000 -epoch 5 -thread 4 > /dev/null\n  ./fasttext test \"${RESULTDIR}/${DATASET[i]}.bin\" \\\n    \"${DATADIR}/${DATASET[i]}.test\"\ndone\n"
        },
        {
          "name": "crawl",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "download_model.py",
          "type": "blob",
          "size": 1.26,
          "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2017-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\n\nimport fasttext.util\n\n\nargs = None\n\n\ndef command_download(lang_id, if_exists):\n    \"\"\"\n        Download pre-trained common-crawl vectors from fastText's website\n        https://fasttext.cc/docs/en/crawl-vectors.html\n    \"\"\"\n    fasttext.util.download_model(lang_id, if_exists)\n\n\ndef main():\n    global args\n\n    parser = argparse.ArgumentParser(\n        description='fastText helper tool to reduce model dimensions.')\n    parser.add_argument(\"language\", type=str, default=\"en\",\n                        help=\"language identifier of the pre-trained vectors. For example `en` or `fr`.\")\n    parser.add_argument(\"--overwrite\", action=\"store_true\",\n                        help=\"overwrite if file exists.\")\n\n    args = parser.parse_args()\n\n    command_download(args.language, if_exists=(\n        'overwrite' if args.overwrite else 'strict'))\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "eval.py",
          "type": "blob",
          "size": 2.05,
          "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2016-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\nimport numpy as np\nfrom scipy import stats\nimport os\nimport math\nimport argparse\n\n\ndef compat_splitting(line):\n    return line.decode('utf8').split()\n\n\ndef similarity(v1, v2):\n    n1 = np.linalg.norm(v1)\n    n2 = np.linalg.norm(v2)\n    return np.dot(v1, v2) / n1 / n2\n\n\nparser = argparse.ArgumentParser(description='Process some integers.')\nparser.add_argument(\n    '--model',\n    '-m',\n    dest='modelPath',\n    action='store',\n    required=True,\n    help='path to model'\n)\nparser.add_argument(\n    '--data',\n    '-d',\n    dest='dataPath',\n    action='store',\n    required=True,\n    help='path to data'\n)\nargs = parser.parse_args()\n\nvectors = {}\nfin = open(args.modelPath, 'rb')\nfor _, line in enumerate(fin):\n    try:\n        tab = compat_splitting(line)\n        vec = np.array(tab[1:], dtype=float)\n        word = tab[0]\n        if np.linalg.norm(vec) == 0:\n            continue\n        if not word in vectors:\n            vectors[word] = vec\n    except ValueError:\n        continue\n    except UnicodeDecodeError:\n        continue\nfin.close()\n\nmysim = []\ngold = []\ndrop = 0.0\nnwords = 0.0\n\nfin = open(args.dataPath, 'rb')\nfor line in fin:\n    tline = compat_splitting(line)\n    word1 = tline[0].lower()\n    word2 = tline[1].lower()\n    nwords = nwords + 1.0\n\n    if (word1 in vectors) and (word2 in vectors):\n        v1 = vectors[word1]\n        v2 = vectors[word2]\n        d = similarity(v1, v2)\n        mysim.append(d)\n        gold.append(float(tline[2]))\n    else:\n        drop = drop + 1.0\nfin.close()\n\ncorr = stats.spearmanr(mysim, gold)\ndataset = os.path.basename(args.dataPath)\nprint(\n    \"{0:20s}: {1:2.0f}  (OOV: {2:2.0f}%)\"\n    .format(dataset, corr[0] * 100, math.ceil(drop / nwords * 100.0))\n)\n"
        },
        {
          "name": "fasttext.pc.in",
          "type": "blob",
          "size": 0.33,
          "content": "prefix=@CMAKE_INSTALL_PREFIX@\nexec_prefix=@CMAKE_INSTALL_FULL_LIBEXECDIR@\nlibdir=@CMAKE_INSTALL_FULL_LIBDIR@\nincludedir=@CMAKE_INSTALL_FULL_INCLUDEDIR@\n\nName: @PROJECT_NAME@\nDescription: Efficient learning of word representations and sentence classification\nVersion: @PROJECT_VERSION@\nLibs: -L${libdir} -lfasttext\nCflags: -I${includedir}\n"
        },
        {
          "name": "get-wikimedia.sh",
          "type": "blob",
          "size": 3.23,
          "content": "#!/usr/bin/env bash\n#\n# Copyright (c) 2016-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nset -e\n\nnormalize_text() {\n    sed -e \"s/’/'/g\" -e \"s/′/'/g\" -e \"s/''/ /g\" -e \"s/'/ ' /g\" -e \"s/“/\\\"/g\" -e \"s/”/\\\"/g\" \\\n        -e 's/\"/ \" /g' -e 's/\\./ \\. /g' -e 's/<br \\/>/ /g' -e 's/, / , /g' -e 's/(/ ( /g' -e 's/)/ ) /g' -e 's/\\!/ \\! /g' \\\n        -e 's/\\?/ \\? /g' -e 's/\\;/ /g' -e 's/\\:/ /g' -e 's/-/ - /g' -e 's/=/ /g' -e 's/=/ /g' -e 's/*/ /g' -e 's/|/ /g' \\\n        -e 's/«/ /g' | tr 0-9 \" \"\n}\n\nexport LANGUAGE=en_US.UTF-8\nexport LC_ALL=en_US.UTF-8\nexport LANG=en_US.UTF-8\n\nNOW=$(date +\"%Y%m%d\")\n\nROOT=\"data/wikimedia/${NOW}\"\nmkdir -p \"${ROOT}\"\necho \"Saving data in \"\"$ROOT\"\nread -r -p \"Choose a language (e.g. en, bh, fr, etc.): \" choice\nLANG=\"$choice\"\necho \"Chosen language: \"\"$LANG\"\nread -r -p \"Continue to download (WARNING: This might be big and can take a long time!)(y/n)? \" choice\ncase \"$choice\" in\n  y|Y ) echo \"Starting download...\";;\n  n|N ) echo \"Exiting\";exit 1;;\n  * ) echo \"Invalid answer\";exit 1;;\nesac\nwget -c \"https://dumps.wikimedia.org/\"\"$LANG\"\"wiki/latest/\"\"${LANG}\"\"wiki-latest-pages-articles.xml.bz2\" -P \"${ROOT}\"\necho \"Processing \"\"$ROOT\"/\"$LANG\"\"wiki-latest-pages-articles.xml.bz2\"\nbzip2 -c -d \"$ROOT\"/\"$LANG\"\"wiki-latest-pages-articles.xml.bz2\" | awk '{print tolower($0);}' | perl -e '\n# Program to filter Wikipedia XML dumps to \"clean\" text consisting only of lowercase\n# letters (a-z, converted from A-Z), and spaces (never consecutive)...\n# All other characters are converted to spaces.  Only text which normally appears.\n# in the web browser is displayed.  Tables are removed.  Image captions are.\n# preserved.  Links are converted to normal text.  Digits are spelled out.\n# *** Modified to not spell digits or throw away non-ASCII characters ***\n# Written by Matt Mahoney, June 10, 2006.  This program is released to the public domain.\n$/=\">\";                     # input record separator\nwhile (<>) {\n  if (/<text /) {$text=1;}  # remove all but between <text> ... </text>\n  if (/#redirect/i) {$text=0;}  # remove #REDIRECT\n  if ($text) {\n    # Remove any text not normally visible\n    if (/<\\/text>/) {$text=0;}\n    s/<.*>//;               # remove xml tags\n    s/&amp;/&/g;            # decode URL encoded chars\n    s/&lt;/</g;\n    s/&gt;/>/g;\n    s/<ref[^<]*<\\/ref>//g;  # remove references <ref...> ... </ref>\n    s/<[^>]*>//g;           # remove xhtml tags\n    s/\\[http:[^] ]*/[/g;    # remove normal url, preserve visible text\n    s/\\|thumb//ig;          # remove images links, preserve caption\n    s/\\|left//ig;\n    s/\\|right//ig;\n    s/\\|\\d+px//ig;\n    s/\\[\\[image:[^\\[\\]]*\\|//ig;\n    s/\\[\\[category:([^|\\]]*)[^]]*\\]\\]/[[$1]]/ig;  # show categories without markup\n    s/\\[\\[[a-z\\-]*:[^\\]]*\\]\\]//g;  # remove links to other languages\n    s/\\[\\[[^\\|\\]]*\\|/[[/g;  # remove wiki url, preserve visible text\n    s/{{[^}]*}}//g;         # remove {{icons}} and {tables}\n    s/{[^}]*}//g;\n    s/\\[//g;                # remove [ and ]\n    s/\\]//g;\n    s/&[^;]*;/ /g;          # remove URL encoded chars\n    $_=\" $_ \";\n    chop;\n    print $_;\n  }\n}\n' | normalize_text | awk '{if (NF>1) print;}' | tr -s \" \" | shuf > \"${ROOT}\"/wiki.\"${LANG}\".txt\n"
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.06,
          "content": "[build-system]\nrequires = [\"setuptools\", \"wheel\", \"pybind11\"]\n"
        },
        {
          "name": "python",
          "type": "tree",
          "content": null
        },
        {
          "name": "quantization-example.sh",
          "type": "blob",
          "size": 1.54,
          "content": "myshuf() {\n  perl -MList::Util=shuffle -e 'print shuffle(<>);' \"$@\";\n}\n\nnormalize_text() {\n  tr '[:upper:]' '[:lower:]' | sed -e 's/^/__label__/g' | \\\n    sed -e \"s/'/ ' /g\" -e 's/\"//g' -e 's/\\./ \\. /g' -e 's/<br \\/>/ /g' \\\n        -e 's/,/ , /g' -e 's/(/ ( /g' -e 's/)/ ) /g' -e 's/\\!/ \\! /g' \\\n        -e 's/\\?/ \\? /g' -e 's/\\;/ /g' -e 's/\\:/ /g' | tr -s \" \" | myshuf\n}\n\nRESULTDIR=result\nDATADIR=data\n\nmkdir -p \"${RESULTDIR}\"\nmkdir -p \"${DATADIR}\"\n\nif [ ! -f \"${DATADIR}/dbpedia.train\" ]\nthen\n  wget -c \"https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbQ2Vic1kxMmZZQ1k\" -O \"${DATADIR}/dbpedia_csv.tar.gz\"\n  tar -xzvf \"${DATADIR}/dbpedia_csv.tar.gz\" -C \"${DATADIR}\"\n  cat \"${DATADIR}/dbpedia_csv/train.csv\" | normalize_text > \"${DATADIR}/dbpedia.train\"\n  cat \"${DATADIR}/dbpedia_csv/test.csv\" | normalize_text > \"${DATADIR}/dbpedia.test\"\nfi\n\nmake\n\necho \"Training...\"\n./fasttext supervised -input \"${DATADIR}/dbpedia.train\" -output \"${RESULTDIR}/dbpedia\" -dim 10 -lr 0.1 -wordNgrams 2 -minCount 1 -bucket 10000000 -epoch 5 -thread 4\n\necho \"Quantizing...\"\n./fasttext quantize -output \"${RESULTDIR}/dbpedia\" -input \"${DATADIR}/dbpedia.train\" -qnorm -retrain -epoch 1 -cutoff 100000\n\necho \"Testing original model...\"\n./fasttext test \"${RESULTDIR}/dbpedia.bin\" \"${DATADIR}/dbpedia.test\"\necho \"Testing quantized model...\"\n./fasttext test \"${RESULTDIR}/dbpedia.ftz\" \"${DATADIR}/dbpedia.test\"\n\nwc -c < \"${RESULTDIR}/dbpedia.bin\" | awk '{print \"Size of the original model:\\t\",$1;}'\nwc -c < \"${RESULTDIR}/dbpedia.ftz\" | awk '{print \"Size of the quantized model:\\t\",$1;}'\n"
        },
        {
          "name": "reduce_model.py",
          "type": "blob",
          "size": 2.79,
          "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2017-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\nimport os\nimport re\nimport sys\n\nimport fasttext\nimport fasttext.util\n\nargs = None\n\n\ndef eprint(*args, **kwargs):\n    print(*args, file=sys.stderr, **kwargs)\n\n\ndef guess_target_name(model_file, initial_dim, target_dim):\n    \"\"\"\n    Given a model name with the convention a.<dim>.b, this function\n    returns the model's name with `target_dim` value.\n    For example model_file name `cc.en.300.bin` with initial dim 300 becomes\n    `cc.en.100.bin` when the `target_dim` is 100.\n    \"\"\"\n    prg = re.compile(\"(.*).%s.(.*)\" % initial_dim)\n    m = prg.match(model_file)\n    if m:\n        return \"%s.%d.%s\" % (m.group(1), target_dim, m.group(2))\n\n    sp_ext = os.path.splitext(model_file)\n    return \"%s.%d%s\" % (sp_ext[0], target_dim, sp_ext[1])\n\n\ndef command_reduce(model_file, target_dim, if_exists):\n    \"\"\"\n    Given a `model_file`, this function reduces its dimension to `target_dim`\n    by applying a PCA.\n    \"\"\"\n    eprint(\"Loading model\")\n\n    ft = fasttext.load_model(model_file)\n    initial_dim = ft.get_dimension()\n    if target_dim >= initial_dim:\n        raise Exception(\"Target dimension (%d) should be less than initial dimension (%d).\" % (\n            target_dim, initial_dim))\n\n    result_filename = guess_target_name(model_file, initial_dim, target_dim)\n    if os.path.isfile(result_filename):\n        if if_exists == 'overwrite':\n            pass\n        elif if_exists == 'strict':\n            raise Exception(\n                \"File already exists. Use --overwrite to overwrite.\")\n        elif if_exists == 'ignore':\n            return result_filename\n\n    eprint(\"Reducing matrix dimensions\")\n    fasttext.util.reduce_model(ft, target_dim)\n\n    eprint(\"Saving model\")\n    ft.save_model(result_filename)\n    eprint(\"%s saved\" % result_filename)\n\n    return result_filename\n\n\ndef main():\n    global args\n\n    parser = argparse.ArgumentParser(\n        description='fastText helper tool to reduce model dimensions.')\n    parser.add_argument(\"model\", type=str,\n                        help=\"model file to reduce. model.bin\")\n    parser.add_argument(\"dim\", type=int,\n                        help=\"targeted dimension of word vectors.\")\n    parser.add_argument(\"--overwrite\", action=\"store_true\",\n                        help=\"overwrite if file exists.\")\n\n    args = parser.parse_args()\n\n    command_reduce(args.model, args.dim, if_exists=(\n        'overwrite' if args.overwrite else 'strict'))\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "runtests.py",
          "type": "blob",
          "size": 1.78,
          "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2016-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\n# To run the integration tests you must first fetch all the required test data.\n# Have a look at tests/fetch_test_data.sh\n# You will then need to point this script to the corresponding folder\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport unittest\nimport argparse\nfrom fasttext.tests import gen_tests\nfrom fasttext.tests import gen_unit_tests\n\n\ndef run_tests(tests):\n    suite = unittest.TestLoader().loadTestsFromTestCase(tests)\n    unittest.TextTestRunner(verbosity=3).run(suite)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"-u\", \"--unit-tests\", help=\"run unit tests\", action=\"store_true\"\n    )\n    parser.add_argument(\n        \"-i\",\n        \"--integration-tests\",\n        help=\"run integration tests\",\n        action=\"store_true\"\n    )\n    parser.add_argument(\n        \"-v\",\n        \"--verbose\",\n        default=1,\n        help=\"verbosity level (default 1)\",\n        type=int,\n    )\n    parser.add_argument(\"--data-dir\", help=\"Full path to data directory\")\n    args = parser.parse_args()\n    if args.unit_tests:\n        run_tests(gen_unit_tests(verbose=args.verbose))\n    if args.integration_tests:\n        if args.data_dir is None:\n            raise ValueError(\n                \"Need data directory! Consult tests/fetch_test_data.sh\"\n            )\n        run_tests(gen_tests(args.data_dir, verbose=args.verbose))\n    if not args.unit_tests and not args.integration_tests:\n        print(\"Ran no tests\")\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.04,
          "content": "[metadata]\ndescription-file = README.md\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 6.21,
          "content": "#!/usr/bin/env python\n\n# Copyright (c) 2017-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom setuptools import setup, Extension\nfrom setuptools.command.build_ext import build_ext\nimport sys\nimport setuptools\nimport os\nimport subprocess\nimport platform\nimport io\n\n__version__ = \"0.9.2\"\nFASTTEXT_SRC = \"src\"\n\n# Based on https://github.com/pybind/python_example\n\n\nclass get_pybind_include:\n    \"\"\"Helper class to determine the pybind11 include path\n\n    The purpose of this class is to postpone importing pybind11\n    until it is actually installed, so that the ``get_include()``\n    method can be invoked.\"\"\"\n\n    def __init__(self, user=False):\n        try:\n            pass\n        except ImportError:\n            if subprocess.call([sys.executable, \"-m\", \"pip\", \"install\", \"pybind11\"]):\n                raise RuntimeError(\"pybind11 install failed.\")\n\n        self.user = user\n\n    def __str__(self):\n        import pybind11\n\n        return pybind11.get_include(self.user)\n\n\ntry:\n    coverage_index = sys.argv.index(\"--coverage\")\nexcept ValueError:\n    coverage = False\nelse:\n    del sys.argv[coverage_index]\n    coverage = True\n\nfasttext_src_files = map(str, os.listdir(FASTTEXT_SRC))\nfasttext_src_cc = list(filter(lambda x: x.endswith(\".cc\"), fasttext_src_files))\n\nfasttext_src_cc = list(\n    map(lambda x: str(os.path.join(FASTTEXT_SRC, x)), fasttext_src_cc)\n)\n\next_modules = [\n    Extension(\n        str(\"fasttext_pybind\"),\n        [\n            str(\"python/fasttext_module/fasttext/pybind/fasttext_pybind.cc\"),\n        ]\n        + fasttext_src_cc,\n        include_dirs=[\n            # Path to pybind11 headers\n            get_pybind_include(),\n            get_pybind_include(user=True),\n            # Path to fasttext source code\n            FASTTEXT_SRC,\n        ],\n        language=\"c++\",\n        extra_compile_args=[\n            \"-O0 -fno-inline -fprofile-arcs -pthread -march=native\"\n            if coverage\n            else \"-O3 -funroll-loops -pthread -march=native\"\n        ],\n    ),\n]\n\n\n# As of Python 3.6, CCompiler has a `has_flag` method.\n# cf http://bugs.python.org/issue26689\ndef has_flag(compiler, flags):\n    \"\"\"Return a boolean indicating whether a flag name is supported on\n    the specified compiler.\n    \"\"\"\n    import tempfile\n\n    with tempfile.NamedTemporaryFile(\"w\", suffix=\".cpp\") as f:\n        f.write(\"int main (int argc, char **argv) { return 0; }\")\n        try:\n            compiler.compile([f.name], extra_postargs=flags)\n        except setuptools.distutils.errors.CompileError:\n            return False\n    return True\n\n\ndef cpp_flag(compiler):\n    \"\"\"Return the -std=c++17 compiler flag.\"\"\"\n    standards = [\"-std=c++17\"]\n    for standard in standards:\n        if has_flag(compiler, [standard]):\n            return standard\n    raise RuntimeError(\"Unsupported compiler -- at least C++17 support \" \"is needed!\")\n\n\nclass BuildExt(build_ext):\n    \"\"\"A custom build extension for adding compiler-specific options.\"\"\"\n\n    c_opts = {\n        \"msvc\": [\"/EHsc\"],\n        \"unix\": [],\n    }\n\n    def build_extensions(self):\n        if sys.platform == \"darwin\":\n            mac_osx_version = float(\".\".join(platform.mac_ver()[0].split(\".\")[:2]))\n            os.environ[\"MACOSX_DEPLOYMENT_TARGET\"] = str(mac_osx_version)\n            all_flags = [\"-stdlib=libc++\", \"-mmacosx-version-min=10.7\"]\n            if has_flag(self.compiler, [all_flags[0]]):\n                self.c_opts[\"unix\"] += [all_flags[0]]\n            elif has_flag(self.compiler, all_flags):\n                self.c_opts[\"unix\"] += all_flags\n            else:\n                raise RuntimeError(\n                    \"libc++ is needed! Failed to compile with {} and {}.\".format(\n                        \" \".join(all_flags), all_flags[0]\n                    )\n                )\n        ct = self.compiler.compiler_type\n        opts = self.c_opts.get(ct, [])\n        extra_link_args = []\n\n        if coverage:\n            coverage_option = \"--coverage\"\n            opts.append(coverage_option)\n            extra_link_args.append(coverage_option)\n\n        if ct == \"unix\":\n            opts.append('-DVERSION_INFO=\"%s\"' % self.distribution.get_version())\n            opts.append(cpp_flag(self.compiler))\n            if has_flag(self.compiler, [\"-fvisibility=hidden\"]):\n                opts.append(\"-fvisibility=hidden\")\n        elif ct == \"msvc\":\n            opts.append('/DVERSION_INFO=\\\\\"%s\\\\\"' % self.distribution.get_version())\n        for ext in self.extensions:\n            ext.extra_compile_args = opts\n            ext.extra_link_args = extra_link_args\n        build_ext.build_extensions(self)\n\n\ndef _get_readme():\n    \"\"\"\n    Use pandoc to generate rst from md.\n    pandoc --from=markdown --to=rst --output=python/README.rst python/README.md\n    \"\"\"\n    with io.open(\"python/README.rst\", encoding=\"utf-8\") as fid:\n        return fid.read()\n\n\nsetup(\n    name=\"fasttext\",\n    version=__version__,\n    author=\"Onur Celebi\",\n    author_email=\"celebio@fb.com\",\n    description=\"fasttext Python bindings\",\n    long_description=_get_readme(),\n    ext_modules=ext_modules,\n    url=\"https://github.com/facebookresearch/fastText\",\n    license=\"MIT\",\n    classifiers=[\n        \"Development Status :: 3 - Alpha\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Science/Research\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Programming Language :: Python :: 2.7\",\n        \"Programming Language :: Python :: 3.4\",\n        \"Programming Language :: Python :: 3.5\",\n        \"Programming Language :: Python :: 3.6\",\n        \"Topic :: Software Development\",\n        \"Topic :: Scientific/Engineering\",\n        \"Operating System :: Microsoft :: Windows\",\n        \"Operating System :: POSIX\",\n        \"Operating System :: Unix\",\n        \"Operating System :: MacOS\",\n    ],\n    install_requires=[\"pybind11>=2.2\", \"setuptools >= 0.7.0\", \"numpy\"],\n    cmdclass={\"build_ext\": BuildExt},\n    packages=[\n        str(\"fasttext\"),\n        str(\"fasttext.util\"),\n        str(\"fasttext.tests\"),\n    ],\n    package_dir={str(\"\"): str(\"python/fasttext_module\")},\n    zip_safe=False,\n)\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "webassembly",
          "type": "tree",
          "content": null
        },
        {
          "name": "website",
          "type": "tree",
          "content": null
        },
        {
          "name": "wikifil.pl",
          "type": "blob",
          "size": 1.95,
          "content": "#!/usr/bin/perl\n\n# Program to filter Wikipedia XML dumps to \"clean\" text consisting only of lowercase\n# letters (a-z, converted from A-Z), and spaces (never consecutive).  \n# All other characters are converted to spaces.  Only text which normally appears \n# in the web browser is displayed.  Tables are removed.  Image captions are \n# preserved.  Links are converted to normal text.  Digits are spelled out.\n\n# Written by Matt Mahoney, June 10, 2006.  This program is released to the public domain.\n\n$/=\">\";                     # input record separator\nwhile (<>) {\n  if (/<text /) {$text=1;}  # remove all but between <text> ... </text>\n  if (/#redirect/i) {$text=0;}  # remove #REDIRECT\n  if ($text) {\n\n    # Remove any text not normally visible\n    if (/<\\/text>/) {$text=0;}\n    s/<.*>//;               # remove xml tags\n    s/&amp;/&/g;            # decode URL encoded chars\n    s/&lt;/</g;\n    s/&gt;/>/g;\n    s/<ref[^<]*<\\/ref>//g;  # remove references <ref...> ... </ref>\n    s/<[^>]*>//g;           # remove xhtml tags\n    s/\\[http:[^] ]*/[/g;    # remove normal url, preserve visible text\n    s/\\|thumb//ig;          # remove images links, preserve caption\n    s/\\|left//ig;\n    s/\\|right//ig;\n    s/\\|\\d+px//ig;\n    s/\\[\\[image:[^\\[\\]]*\\|//ig;\n    s/\\[\\[category:([^|\\]]*)[^]]*\\]\\]/[[$1]]/ig;  # show categories without markup\n    s/\\[\\[[a-z\\-]*:[^\\]]*\\]\\]//g;  # remove links to other languages\n    s/\\[\\[[^\\|\\]]*\\|/[[/g;  # remove wiki url, preserve visible text\n    s/\\{\\{[^\\}]*\\}\\}//g;         # remove {{icons}} and {tables}\n    s/\\{[^\\}]*\\}//g;\n    s/\\[//g;                # remove [ and ]\n    s/\\]//g;\n    s/&[^;]*;/ /g;          # remove URL encoded chars\n\n    # convert to lowercase letters and spaces, spell digits\n    $_=\" $_ \";\n    tr/A-Z/a-z/;\n    s/0/ zero /g;\n    s/1/ one /g;\n    s/2/ two /g;\n    s/3/ three /g;\n    s/4/ four /g;\n    s/5/ five /g;\n    s/6/ six /g;\n    s/7/ seven /g;\n    s/8/ eight /g;\n    s/9/ nine /g;\n    tr/a-z/ /cs;\n    chop;\n    print $_;\n  }\n}\n"
        },
        {
          "name": "word-vector-example.sh",
          "type": "blob",
          "size": 1.15,
          "content": "#!/usr/bin/env bash\n#\n# Copyright (c) 2016-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nRESULTDIR=result\nDATADIR=data\n\nmkdir -p \"${RESULTDIR}\"\nmkdir -p \"${DATADIR}\"\n\nif [ ! -f \"${DATADIR}/fil9\" ]\nthen\n  wget -c http://mattmahoney.net/dc/enwik9.zip -P \"${DATADIR}\"\n  unzip \"${DATADIR}/enwik9.zip\" -d \"${DATADIR}\"\n  perl wikifil.pl \"${DATADIR}/enwik9\" > \"${DATADIR}\"/fil9\nfi\n\nif [ ! -f \"${DATADIR}/rw/rw.txt\" ]\nthen\n  wget -c https://nlp.stanford.edu/~lmthang/morphoNLM/rw.zip -P \"${DATADIR}\"\n  unzip \"${DATADIR}/rw.zip\" -d \"${DATADIR}\"\nfi\n\nmake\n\n./fasttext skipgram -input \"${DATADIR}\"/fil9 -output \"${RESULTDIR}\"/fil9 -lr 0.025 -dim 100 \\\n  -ws 5 -epoch 1 -minCount 5 -neg 5 -loss ns -bucket 2000000 \\\n  -minn 3 -maxn 6 -thread 4 -t 1e-4 -lrUpdateRate 100\n\ncut -f 1,2 \"${DATADIR}\"/rw/rw.txt | awk '{print tolower($0)}' | tr '\\t' '\\n' > \"${DATADIR}\"/queries.txt\n\ncat \"${DATADIR}\"/queries.txt | ./fasttext print-word-vectors \"${RESULTDIR}\"/fil9.bin > \"${RESULTDIR}\"/vectors.txt\n\npython eval.py -m \"${RESULTDIR}\"/vectors.txt -d \"${DATADIR}\"/rw/rw.txt\n"
        }
      ]
    }
  ]
}