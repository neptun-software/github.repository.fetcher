{
  "metadata": {
    "timestamp": 1736557590441,
    "page": 646,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjY3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "jax-ml/jax",
      "stars": 30944,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".bazelrc",
          "type": "blob",
          "size": 18.59,
          "content": "# #############################################################################\n# All default build options below. These apply to all build commands.\n# #############################################################################\n# Make Bazel print out all options from rc files.\nbuild --announce_rc\n\n# By default, execute all actions locally.\nbuild --spawn_strategy=local\n\n# Enable host OS specific configs. For instance, \"build:linux\" will be used\n# automatically when building on Linux.\nbuild --enable_platform_specific_config\n\nbuild --experimental_cc_shared_library\n\n# Do not use C-Ares when building gRPC.\nbuild --define=grpc_no_ares=true\n\nbuild --define=tsl_link_protobuf=true\n\n# Enable optimization.\nbuild -c opt\n\n# Suppress all warning messages.\nbuild --output_filter=DONT_MATCH_ANYTHING\n\nbuild --copt=-DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir.\n\n# #############################################################################\n# Platform Specific configs below. These are automatically picked up by Bazel\n# depending on the platform that is running the build.\n# #############################################################################\nbuild:linux --config=posix\nbuild:linux --copt=-Wno-unknown-warning-option\n\n# Workaround for gcc 10+ warnings related to upb.\n# See https://github.com/tensorflow/tensorflow/issues/39467\nbuild:linux --copt=-Wno-stringop-truncation\nbuild:linux --copt=-Wno-array-parameter\n\nbuild:macos --config=posix\nbuild:macos --apple_platform_type=macos\n\n# Windows has a relatively short command line limit, which JAX has begun to hit.\n# See https://docs.bazel.build/versions/main/windows.html\nbuild:windows --features=compiler_param_file\nbuild:windows --features=archive_param_file\n\n# XLA uses M_* math constants that only get defined by MSVC headers if\n# _USE_MATH_DEFINES is defined.\nbuild:windows --copt=/D_USE_MATH_DEFINES\nbuild:windows --host_copt=/D_USE_MATH_DEFINES\n# Make sure to include as little of windows.h as possible\nbuild:windows --copt=-DWIN32_LEAN_AND_MEAN\nbuild:windows --host_copt=-DWIN32_LEAN_AND_MEAN\nbuild:windows --copt=-DNOGDI\nbuild:windows --host_copt=-DNOGDI\n# https://devblogs.microsoft.com/cppblog/announcing-full-support-for-a-c-c-conformant-preprocessor-in-msvc/\n# otherwise, there will be some compiling error due to preprocessing.\nbuild:windows --copt=/Zc:preprocessor\nbuild:windows --cxxopt=/std:c++17\nbuild:windows --host_cxxopt=/std:c++17\n# Generate PDB files, to generate useful PDBs, in opt compilation_mode\n# --copt /Z7 is needed.\nbuild:windows --linkopt=/DEBUG\nbuild:windows --host_linkopt=/DEBUG\nbuild:windows --linkopt=/OPT:REF\nbuild:windows --host_linkopt=/OPT:REF\nbuild:windows --linkopt=/OPT:ICF\nbuild:windows --host_linkopt=/OPT:ICF\nbuild:windows --incompatible_strict_action_env=true\n\n# #############################################################################\n# Feature-specific configurations. These are used by the CI configs below\n# depending on the type of build. E.g. `ci_linux_x86_64` inherits the Linux x86\n# configs such as `avx_linux` and `mkl_open_source_only`, `ci_linux_x86_64_cuda`\n# inherits `cuda` and `build_cuda_with_nvcc`, etc.\n# #############################################################################\nbuild:nonccl --define=no_nccl_support=true\n\nbuild:posix --copt=-fvisibility=hidden\nbuild:posix --copt=-Wno-sign-compare\nbuild:posix --cxxopt=-std=c++17\nbuild:posix --host_cxxopt=-std=c++17\n\nbuild:avx_posix --copt=-mavx\nbuild:avx_posix --host_copt=-mavx\n\nbuild:native_arch_posix --copt=-march=native\nbuild:native_arch_posix --host_copt=-march=native\n\nbuild:avx_linux --copt=-mavx\nbuild:avx_linux --host_copt=-mavx\n\nbuild:avx_windows --copt=/arch:AVX\n\nbuild:mkl_open_source_only --define=tensorflow_mkldnn_contraction_kernel=1\n\n# Config setting to build oneDNN with Compute Library for the Arm Architecture (ACL).\nbuild:mkl_aarch64_threadpool --define=build_with_mkl_aarch64=true\nbuild:mkl_aarch64_threadpool --@compute_library//:openmp=false\nbuild:mkl_aarch64_threadpool -c opt\n\n# Disable clang extention that rejects type definitions within offsetof.\n# This was added in clang-16 by https://reviews.llvm.org/D133574.\n# Can be removed once upb is updated, since a type definition is used within\n# offset of in the current version of ubp.\n# See https://github.com/protocolbuffers/upb/blob/9effcbcb27f0a665f9f345030188c0b291e32482/upb/upb.c#L183.\nbuild:clang --copt=-Wno-gnu-offsetof-extensions\n# Disable clang extention that rejects unknown arguments.\nbuild:clang --copt=-Qunused-arguments\n# Error on struct/class mismatches, since this causes link failures on Windows.\nbuild:clang --copt=-Werror=mismatched-tags\n\n# Configs for CUDA\nbuild:cuda --repo_env TF_NEED_CUDA=1\nbuild:cuda --repo_env TF_NCCL_USE_STUB=1\n# \"sm\" means we emit only cubin, which is forward compatible within a GPU generation.\n# \"compute\" means we emit both cubin and PTX, which is larger but also forward compatible to future GPU generations.\nbuild:cuda --repo_env HERMETIC_CUDA_COMPUTE_CAPABILITIES=\"sm_50,sm_60,sm_70,sm_80,compute_90\"\nbuild:cuda --crosstool_top=@local_config_cuda//crosstool:toolchain\nbuild:cuda --@local_config_cuda//:enable_cuda\nbuild:cuda --@xla//xla/python:jax_cuda_pip_rpaths=true\n\n# Default hermetic CUDA and CUDNN versions.\nbuild:cuda --repo_env=HERMETIC_CUDA_VERSION=\"12.3.2\"\nbuild:cuda --repo_env=HERMETIC_CUDNN_VERSION=\"9.1.1\"\n\n# This flag is needed to include CUDA libraries for bazel tests.\ntest:cuda --@local_config_cuda//cuda:include_cuda_libs=true\n\n# Force the linker to set RPATH, not RUNPATH. When resolving dynamic libraries,\n# ld.so prefers in order: RPATH, LD_LIBRARY_PATH, RUNPATH. JAX sets RPATH to\n# point to the $ORIGIN-relative location of the pip-installed NVIDIA CUDA\n# packages.\n# This has pros and cons:\n# * pro: we'll ignore other CUDA installations, which has frequently confused\n#   users in the past. By setting RPATH, we'll always use the NVIDIA pip\n#   packages if they are installed.\n# * con: the user cannot override the CUDA installation location\n#   via LD_LIBRARY_PATH, if the nvidia-... pip packages are installed. This is\n#   acceptable, because the workaround is \"remove the nvidia-...\" pip packages.\n# The list of CUDA pip packages that JAX depends on are present in setup.py.\nbuild:cuda --linkopt=-Wl,--disable-new-dtags\n\n# Build CUDA and other C++ targets with Clang\nbuild:build_cuda_with_clang --@local_config_cuda//:cuda_compiler=clang\n\n# Build CUDA with NVCC and other C++ targets with Clang\nbuild:build_cuda_with_nvcc --action_env=TF_NVCC_CLANG=\"1\"\nbuild:build_cuda_with_nvcc --@local_config_cuda//:cuda_compiler=nvcc\n\n# Requires MSVC and LLVM to be installed\nbuild:win_clang --extra_toolchains=@local_config_cc//:cc-toolchain-x64_windows-clang-cl\nbuild:win_clang --extra_execution_platforms=//jax/tools/toolchains:x64_windows-clang-cl\nbuild:win_clang --compiler=clang-cl\n\nbuild:rocm_base --crosstool_top=@local_config_rocm//crosstool:toolchain\nbuild:rocm_base --define=using_rocm=true --define=using_rocm_hipcc=true\nbuild:rocm_base --repo_env TF_NEED_ROCM=1\nbuild:rocm_base --action_env TF_ROCM_AMDGPU_TARGETS=\"gfx900,gfx906,gfx908,gfx90a,gfx940,gfx941,gfx942,gfx1030,gfx1100\"\n\n# Build with hipcc for ROCm and clang for the host.\nbuild:rocm --config=rocm_base\nbuild:rocm --action_env=TF_ROCM_CLANG=\"1\"\nbuild:rocm --action_env=CLANG_COMPILER_PATH=\"/usr/lib/llvm-18/bin/clang\"\nbuild:rocm --copt=-Wno-gnu-offsetof-extensions\nbuild:rocm --copt=-Qunused-arguments\nbuild:rocm --action_env=TF_HIPCC_CLANG=\"1\"\n\n# #############################################################################\n# Cache options below.\n# #############################################################################\n# Public read-only cache for Mac builds. JAX uses a GCS bucket to store cache\n# from JAX's Mac CI build. By applying --config=macos_cache, any local Mac build\n# should be able to read from this cache and potentially see a speedup. The\n# \"oct2023\" in the URL is just the date when the bucket was created and can be\n# disregarded. It still contains the latest cache that is being used.\nbuild:macos_cache --remote_cache=\"https://storage.googleapis.com/tensorflow-macos-bazel-cache/oct2023\" --remote_upload_local_results=false\n\n# Cache pushes are limited to JAX's CI system.\nbuild:macos_cache_push --config=macos_cache --remote_upload_local_results=true --google_default_credentials\n\n# #############################################################################\n# CI Build config options below.\n# JAX uses these configs in CI builds for building artifacts and when running\n# Bazel tests.\n# #############################################################################\n# Linux x86 CI configs\nbuild:ci_linux_x86_64 --config=avx_linux --config=avx_posix\nbuild:ci_linux_x86_64 --config=mkl_open_source_only\nbuild:ci_linux_x86_64 --config=clang --verbose_failures=true\nbuild:ci_linux_x86_64 --color=yes\n\n# TODO(b/356695103): We do not have a CPU only toolchain so we use the CUDA\n# toolchain for both CPU and GPU builds.\nbuild:ci_linux_x86_64 --host_crosstool_top=\"@local_config_cuda//crosstool:toolchain\"\nbuild:ci_linux_x86_64 --crosstool_top=\"@local_config_cuda//crosstool:toolchain\"\nbuild:ci_linux_x86_64 --extra_toolchains=\"@local_config_cuda//crosstool:toolchain-linux-x86_64\"\nbuild:ci_linux_x86_64 --repo_env=TF_SYSROOT=\"/dt9\"\n\n# Clang path needs to be set for remote toolchain to be configured correctly.\nbuild:ci_linux_x86_64 --action_env=CLANG_CUDA_COMPILER_PATH=\"/usr/lib/llvm-18/bin/clang\"\n\n# The toolchain in `--config=cuda` needs to be read before the toolchain in\n# `--config=ci_linux_x86_64`. Otherwise, we run into issues with manylinux\n# compliance.\nbuild:ci_linux_x86_64_cuda --config=cuda --config=build_cuda_with_nvcc\nbuild:ci_linux_x86_64_cuda --config=ci_linux_x86_64\n\n# Linux Aarch64 CI configs\nbuild:ci_linux_aarch64_base --config=clang --verbose_failures=true\nbuild:ci_linux_aarch64_base --action_env=TF_SYSROOT=\"/dt10\"\nbuild:ci_linux_aarch64_base --color=yes\n\nbuild:ci_linux_aarch64 --config=ci_linux_aarch64_base\nbuild:ci_linux_aarch64 --host_crosstool_top=\"@ml2014_clang_aarch64_config_aarch64//crosstool:toolchain\"\nbuild:ci_linux_aarch64 --crosstool_top=\"@ml2014_clang_aarch64_config_aarch64//crosstool:toolchain\"\n\n# CUDA configs for Linux Aarch64 do not pass in the crosstool_top flag from\n# above because the Aarch64 toolchain rule does not support building with NVCC.\n# Instead, we use `@local_config_cuda//crosstool:toolchain` from --config=cuda\n# and set `CLANG_CUDA_COMPILER_PATH` to define the toolchain so that we can\n# use Clang for the C++ targets and NVCC to build CUDA targets.\nbuild:ci_linux_aarch64_cuda --config=ci_linux_aarch64_base\nbuild:ci_linux_aarch64_cuda --config=cuda --config=build_cuda_with_nvcc\nbuild:ci_linux_aarch64_cuda --action_env=CLANG_CUDA_COMPILER_PATH=\"/usr/lib/llvm-18/bin/clang\"\n\n# Mac x86 CI configs\nbuild:ci_darwin_x86_64 --macos_minimum_os=10.14\nbuild:ci_darwin_x86_64 --config=macos_cache_push\nbuild:ci_darwin_x86_64 --verbose_failures=true\nbuild:ci_darwin_x86_64 --color=yes\n\n# Mac Arm64 CI configs\nbuild:ci_darwin_arm64 --macos_minimum_os=11.0\nbuild:ci_darwin_arm64 --config=macos_cache_push\nbuild:ci_darwin_arm64 --verbose_failures=true\nbuild:ci_darwin_arm64 --color=yes\n\n# Windows x86 CI configs\nbuild:ci_windows_amd64 --config=avx_windows\nbuild:ci_windows_amd64 --compiler=clang-cl --config=clang --verbose_failures=true\nbuild:ci_windows_amd64 --crosstool_top=\"@xla//tools/toolchains/win/20240424:toolchain\"\nbuild:ci_windows_amd64 --extra_toolchains=\"@xla//tools/toolchains/win/20240424:cc-toolchain-x64_windows-clang-cl\"\nbuild:ci_windows_amd64 --host_linkopt=/FORCE:MULTIPLE --linkopt=/FORCE:MULTIPLE\nbuild:ci_windows_amd64 --color=yes\n\n# #############################################################################\n# RBE config options below. These inherit the CI configs above and set the\n# remote execution backend and authentication options required to run builds\n# with RBE. Linux x86 and Windows builds use RBE.\n# #############################################################################\n# Flag to enable remote config\ncommon --experimental_repo_remote_exec\n\n# Allow creation of resultstore URLs for any bazel invocation\nbuild:resultstore --google_default_credentials\nbuild:resultstore --bes_backend=buildeventservice.googleapis.com\nbuild:resultstore --bes_instance_name=\"tensorflow-testing\"\nbuild:resultstore --bes_results_url=\"https://source.cloud.google.com/results/invocations\"\nbuild:resultstore --bes_timeout=600s\n\nbuild:rbe --config=resultstore\nbuild:rbe --repo_env=BAZEL_DO_NOT_DETECT_CPP_TOOLCHAIN=1\nbuild:rbe --define=EXECUTOR=remote\nbuild:rbe --flaky_test_attempts=3\nbuild:rbe --jobs=200\nbuild:rbe --remote_executor=grpcs://remotebuildexecution.googleapis.com\nbuild:rbe --remote_timeout=3600\nbuild:rbe --spawn_strategy=remote,worker,standalone,local\n# Attempt to minimize the amount of data transfer between bazel and the remote\n# workers:\nbuild:rbe --remote_download_toplevel\ntest:rbe --test_env=USER=anon\n\n# RBE configs for Linux x86\n# Set the remote worker pool\ncommon:rbe_linux_x86_64_base --remote_instance_name=projects/tensorflow-testing/instances/default_instance\n\nbuild:rbe_linux_x86_64_base --config=rbe\nbuild:rbe_linux_x86_64_base --action_env=PATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/go/bin\"\nbuild:rbe_linux_x86_64_base --linkopt=-lrt\nbuild:rbe_linux_x86_64_base --host_linkopt=-lrt\nbuild:rbe_linux_x86_64_base --linkopt=-lm\nbuild:rbe_linux_x86_64_base --host_linkopt=-lm\n\n# Set the host, execution, and target platform\nbuild:rbe_linux_x86_64_base --host_platform=\"@ubuntu20.04-clang_manylinux2014-cuda12.3-cudnn9.1_config_platform//:platform\"\nbuild:rbe_linux_x86_64_base --extra_execution_platforms=\"@ubuntu20.04-clang_manylinux2014-cuda12.3-cudnn9.1_config_platform//:platform\"\nbuild:rbe_linux_x86_64_base --platforms=\"@ubuntu20.04-clang_manylinux2014-cuda12.3-cudnn9.1_config_platform//:platform\"\n\nbuild:rbe_linux_x86_64 --config=rbe_linux_x86_64_base\nbuild:rbe_linux_x86_64 --config=ci_linux_x86_64\n\nbuild:rbe_linux_x86_64_cuda --config=rbe_linux_x86_64_base\nbuild:rbe_linux_x86_64_cuda --config=ci_linux_x86_64_cuda\nbuild:rbe_linux_x86_64_cuda --repo_env=REMOTE_GPU_TESTING=1\n\n# RBE configs for Windows\n# Set the remote worker pool\ncommon:rbe_windows_amd64 --remote_instance_name=projects/tensorflow-testing/instances/windows\n\nbuild:rbe_windows_amd64 --config=rbe\n\n# Set the host, execution, and target platform\nbuild:rbe_windows_amd64 --host_platform=\"@xla//tools/toolchains/win:x64_windows-clang-cl\"\nbuild:rbe_windows_amd64 --extra_execution_platforms=\"@xla//tools/toolchains/win:x64_windows-clang-cl\"\nbuild:rbe_windows_amd64 --platforms=\"@xla//tools/toolchains/win:x64_windows-clang-cl\"\n\nbuild:rbe_windows_amd64 --shell_executable=C:\\\\tools\\\\msys64\\\\usr\\\\bin\\\\bash.exe\nbuild:rbe_windows_amd64 --enable_runfiles\nbuild:rbe_windows_amd64 --define=override_eigen_strong_inline=true\n\n# Don't build the python zip archive in the RBE build.\nbuild:rbe_windows_amd64 --nobuild_python_zip\n\nbuild:rbe_windows_amd64 --config=ci_windows_amd64\n\n# #############################################################################\n# Cross-compile config options below. Native RBE support does not exist for\n# Linux Aarch64 and Mac x86. So, we use a cross-compile toolchain to build\n# targets for Linux Aarch64 and Mac x86 on the Linux x86 RBE pool.\n# #############################################################################\n# Set execution platform to Linux x86\n# Note: Lot of the \"host_\" flags such as \"host_cpu\" and \"host_crosstool_top\"\n# flags seem to be actually used to specify the execution platform details. It\n# seems it is this way because these flags are old and predate the distinction\n# between host and execution platform.\nbuild:cross_compile_base --host_cpu=k8\nbuild:cross_compile_base --host_crosstool_top=@xla//tools/toolchains/cross_compile/cc:cross_compile_toolchain_suite\nbuild:cross_compile_base --extra_execution_platforms=@xla//tools/toolchains/cross_compile/config:linux_x86_64\n\n# Linux Aarch64\nbuild:cross_compile_linux_aarch64 --config=cross_compile_base\n\n# Set the target CPU to Aarch64\nbuild:cross_compile_linux_aarch64 --platforms=@xla//tools/toolchains/cross_compile/config:linux_aarch64\nbuild:cross_compile_linux_aarch64 --cpu=aarch64\nbuild:cross_compile_linux_aarch64 --crosstool_top=@xla//tools/toolchains/cross_compile/cc:cross_compile_toolchain_suite\n\nbuild:rbe_cross_compile_base --config=rbe\nbuild:rbe_cross_compile_base --remote_instance_name=projects/tensorflow-testing/instances/default_instance\n\n# RBE cross-compile configs for Linux Aarch64\nbuild:rbe_cross_compile_linux_aarch64 --config=cross_compile_linux_aarch64\nbuild:rbe_cross_compile_linux_aarch64 --config=rbe_cross_compile_base\n\n# Mac x86\nbuild:cross_compile_darwin_x86_64 --config=cross_compile_base\nbuild:cross_compile_darwin_x86_64 --config=nonccl\n# Target Catalina (10.15) as the minimum supported OS\nbuild:cross_compile_darwin_x86_64 --action_env  MACOSX_DEPLOYMENT_TARGET=10.15\n\n# Set the target CPU to Darwin x86\nbuild:cross_compile_darwin_x86_64 --platforms=@xla//tools/toolchains/cross_compile/config:darwin_x86_64\nbuild:cross_compile_darwin_x86_64 --cpu=darwin\nbuild:cross_compile_darwin_x86_64 --crosstool_top=@xla//tools/toolchains/cross_compile/cc:cross_compile_toolchain_suite\n# When RBE cross-compiling for macOS, we need to explicitly register the\n# toolchain. Otherwise, oddly, RBE complains that a \"docker container must be\n# specified\".\nbuild:cross_compile_darwin_x86_64 --extra_toolchains=@xla//tools/toolchains/cross_compile/config:macos-x86-cross-compile-cc-toolchain\n# Map --platforms=darwin_x86_64 to --cpu=darwin and vice-versa to make selects()\n# and transistions that use these flags work. The flag --platform_mappings needs\n# to be set to a file that exists relative to the package path roots.\nbuild:cross_compile_darwin_x86_64 --platform_mappings=platform_mappings\n\n# RBE cross-compile configs for Darwin x86\nbuild:rbe_cross_compile_darwin_x86_64 --config=cross_compile_darwin_x86_64\nbuild:rbe_cross_compile_darwin_x86_64 --config=rbe_cross_compile_base\n\n#############################################################################\n# Some configs to make getting some forms of debug builds. In general, the\n# codebase is only regularly built with optimizations. Use 'debug_symbols' to\n# just get symbols for the parts of XLA/PJRT that jaxlib uses.\n# Or try 'debug' to get a build with assertions enabled and minimal\n# optimizations.\n# Include these in a local .bazelrc.user file as:\n#   build --config=debug_symbols\n# Or:\n#   build --config=debug\n#\n# Additional files can be opted in for debug symbols by adding patterns\n# to a per_file_copt similar to below.\n#############################################################################\n\nbuild:debug_symbols --strip=never --per_file_copt=\"xla/pjrt|xla/python@-g3\"\nbuild:debug --config debug_symbols -c fastbuild\n\n# Load `.jax_configure.bazelrc` file written by build.py\ntry-import %workspace%/.jax_configure.bazelrc\n\n# Load rc file with user-specific options.\ntry-import %workspace%/.bazelrc.user\n"
        },
        {
          "name": ".bazelversion",
          "type": "blob",
          "size": 0.01,
          "content": "6.5.0\n"
        },
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 0.29,
          "content": "root = true\n\n[*.{py,rst,md,yml}]\nindent_style = space\nend_of_line = lf\ntrim_trailing_whitespace = true\ninsert_final_newline = true\n\n[*.py]\nmax_line_length = 79\nindent_size = 2\n\n[*.rst]\nmax_line_length = 79\nindent_size = 2\n\n[*.md]\nmax_line_length = 79\nindent_size = 2\n\n[*.yml]\nindent_size = 2\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.35,
          "content": "*.pyc\n*.so\n*.egg-info\n*.whl\n/build/lib\n/build/bazel*\n/dist/\n.ipynb_checkpoints\n/bazel-*\n.jax_configure.bazelrc\n/tensorflow\n.DS_Store\n.mypy_cache/\n.pytype/\n/docs/build\n*_pb2.py\n/docs/notebooks/.ipynb_checkpoints/\n/docs/_autosummary\n.idea\n.vscode\n.envrc\njax.iml\n.bazelrc.user\n.hypothesis/\n\n# virtualenv/venv directories\n/venv/\n/bin/\n/include/\n/lib/\n/share/\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 1.3,
          "content": "# Install the pre-commit hooks below with\n# 'pre-commit install'\n\n# Auto-update the version of the hooks with\n# 'pre-commit autoupdate'\n\n# Run the hooks on all files with\n# 'pre-commit run --all'\n\nrepos:\n- repo: https://github.com/pre-commit/pre-commit-hooks\n  rev: 2c9f875913ee60ca25ce70243dc24d5b6415598c  # frozen: v4.6.0\n  hooks:\n  - id: check-ast\n  - id: check-merge-conflict\n  - id: check-toml\n  - id: check-yaml\n  - id: end-of-file-fixer\n    # only include python files\n    files: \\.py$\n  - id: debug-statements\n    # only include python files\n    files: \\.py$\n  - id: trailing-whitespace\n    # only include python files\n    files: \\.py$\n\n- repo: https://github.com/astral-sh/ruff-pre-commit\n  rev: 8983acb92ee4b01924893632cf90af926fa608f0  # frozen: v0.7.0\n  hooks:\n  - id: ruff\n\n- repo: https://github.com/pre-commit/mirrors-mypy\n  rev: '102bbee94061ff02fd361ec29c27b7cb26582f5f'  # frozen: v1.12.2\n  hooks:\n  - id: mypy\n    files: (jax/|tests/typing_test\\.py)\n    exclude: jax/_src/basearray.py|jax/numpy/__init__.py  # Use pyi instead\n    additional_dependencies: [types-requests==2.31.0, jaxlib, numpy>=2.2.0]\n    args: [--config=pyproject.toml]\n\n- repo: https://github.com/mwouts/jupytext\n  rev: 8ed836db64ad5d304f2315e6bfd9049c9142e190  # frozen: v1.16.4\n  hooks:\n  - id: jupytext\n    files: docs/\n    args: [--sync]\n"
        },
        {
          "name": ".readthedocs.yml",
          "type": "blob",
          "size": 0.56,
          "content": "# .readthedocs.yml\n# Read the Docs configuration file\n# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details\n\n# Required\nversion: 2\n\nbuild:\n  os: \"ubuntu-22.04\"\n  tools:\n    python: \"3.10\"\n\n# Build documentation in the docs/ directory with Sphinx\nsphinx:\n  configuration: docs/conf.py\n  fail_on_warning: true\n\n# Optionally build your docs in additional formats such as PDF and ePub\nformats:\n  - htmlzip\n\n# Optionally set the version of Python and requirements required to build your docs\npython:\n  install:\n    - requirements: docs/requirements.txt\n"
        },
        {
          "name": "AUTHORS",
          "type": "blob",
          "size": 0.31,
          "content": "# This is the list of JAX's significant contributors.\n#\n# This does not necessarily list everyone who has contributed code,\n# especially since many employees of one corporation may be contributing.\n# To see the full list of contributors, see the revision history in\n# source control.\nGoogle LLC\nNVIDIA Corporation"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 152.68,
          "content": "# Change log\n\nBest viewed [here](https://jax.readthedocs.io/en/latest/changelog.html).\nFor the changes specific to the experimental Pallas APIs,\nsee {ref}`pallas-changelog`.\n\nJAX follows Effort-based versioning; for a discussion of this and JAX's API\ncompatibility policy, refer to {ref}`api-compatibility`. For the Python and\nNumPy version support policy, refer to {ref}`version-support-policy`.\n\n<!--\nRemember to align the itemized text with the first line of an item within a list.\n\nWhen releasing, please add the new-release-boilerplate to docs/pallas/CHANGELOG.md.\n-->\n\n## Unreleased\n\n* Changes:\n  * The minimum NumPy version is now 1.25. NumPy 1.25 will remain the minimum\n    supported version until June 2025.\n  * The minimum SciPy version is now 1.11. SciPy 1.11 will remain the minimum\n    supported version until June 2025.\n  * {func}`jax.numpy.einsum` now defaults to `optimize='auto'` rather than\n    `optimize='optimal'`. This avoids exponentially-scaling trace-time in\n    the case of many arguments ({jax-issue}`#25214`).\n  * {func}`jax.numpy.linalg.solve` no longer supports batched 1D arguments\n    on the right hand side. To recover the previous behavior in these cases,\n    use `solve(a, b[..., None]).squeeze(-1)`.\n\n* New Features\n  * {func}`jax.numpy.fft.fftn`, {func}`jax.numpy.fft.rfftn`,\n    {func}`jax.numpy.fft.ifftn`, and {func}`jax.numpy.fft.irfftn` now support\n    transforms in more than 3 dimensions, which was previously the limit. See\n    {jax-issue}`#25606` for more details.\n  * Support added for user defined state in the FFI via the new\n    {func}`jax.ffi.register_ffi_type_id` function.\n  * The AOT lowering `.as_text()` method now supports the `debug_info` option\n    to include debugging information, e.g., source location, in the output.\n* Deprecations\n  * From {mod}`jax.interpreters.xla`, `abstractify` and `pytype_aval_mappings`\n    are now deprecated, having been replaced by symbols of the same name\n    in {mod}`jax.core`.\n  * {func}`jax.scipy.special.lpmn` and {func}`jax.scipy.special.lpmn_values`\n    are deprecated, following their deprecation in SciPy v1.15.0. There are\n    no plans to replace these deprecated functions with new APIs.\n  * The {mod}`jax.extend.ffi` submodule was moved to {mod}`jax.ffi`, and the\n    previous import path is deprecated.\n\n* Deletions\n  * `jax_enable_memories` flag has been deleted and the behavior of that flag\n    is on by default.\n  * From `jax.lib.xla_client`, the previously-deprecated `Device` and\n    `XlaRuntimeError` symbols have been removed; instead use `jax.Device`\n    and `jax.errors.JaxRuntimeError` respectively.\n  * The `jax.experimental.array_api` module has been removed after being\n    deprecated in JAX v0.4.32. Since that release, {mod}`jax.numpy` supports\n    the array API directly.\n\n## jax 0.4.38 (Dec 17, 2024)\n\n* Changes:\n  * `jax.tree.flatten_with_path` and `jax.tree.map_with_path` are added\n    as shortcuts of the corresponding `tree_util` functions.\n\n* Deprecations\n  * a number of APIs in the internal `jax.core` namespace have been deprecated.\n    Most were no-ops, were little-used, or can be replaced by APIs of the same\n    name in {mod}`jax.extend.core`; see the documentation for {mod}`jax.extend`\n    for information on the compatibility guarantees of these semi-public extensions.\n  * Several previously-deprecated APIs have been removed, including:\n    * from {mod}`jax.core`: `check_eqn`, `check_type`,  `check_valid_jaxtype`, and\n      `non_negative_dim`.\n    * from {mod}`jax.lib.xla_bridge`: `xla_client` and `default_backend`.\n    * from {mod}`jax.lib.xla_client`: `_xla` and `bfloat16`.\n    * from {mod}`jax.numpy`: `round_`.\n\n* New Features\n  * {func}`jax.export.export` can be used for device-polymorphic export with\n    shardings constructed with {func}`jax.sharding.AbstractMesh`.\n    See the [jax.export documentation](https://jax.readthedocs.io/en/latest/export/export.html#device-polymorphic-export).\n  * Added {func}`jax.lax.split`. This is a primitive version of\n    {func}`jax.numpy.split`, added because it yields a more compact\n    transpose during automatic differentiation.\n\n## jax 0.4.37 (Dec 9, 2024)\n\nThis is a patch release of jax 0.4.36. Only \"jax\" was released at this version.\n\n* Bug fixes\n  * Fixed a bug where `jit` would error if an argument was named `f` (#25329).\n  * Fix a bug that will throw `index out of range` error in\n    {func}`jax.lax.while_loop` if the user register pytree node class with\n    different aux data for the flatten and flatten_with_path.\n  * Pinned a new libtpu release (0.0.6) that fixes a compiler bug on TPU v6e.\n\n## jax 0.4.36 (Dec 5, 2024)\n\n* Breaking Changes\n  * This release lands \"stackless\", an internal change to JAX's tracing\n    machinery. We made trace dispatch purely a function of context rather than a\n    function of both context and data. This let us delete a lot of machinery for\n    managing data-dependent tracing: levels, sublevels, `post_process_call`,\n    `new_base_main`, `custom_bind`, and so on. The change should only affect\n    users that use JAX internals.\n\n    If you do use JAX internals then you may need to\n    update your code (see\n    https://github.com/jax-ml/jax/commit/c36e1f7c1ad4782060cbc8e8c596d85dfb83986f\n    for clues about how to do this). There might also be version skew\n    issues with JAX libraries that do this. If you find this change breaks your\n    non-JAX-internals-using code then try the\n    `config.jax_data_dependent_tracing_fallback` flag as a workaround, and if\n    you need help updating your code then please file a bug.\n  * {func}`jax.experimental.jax2tf.convert` with `native_serialization=False`\n    or with `enable_xla=False` have been deprecated since July 2024, with\n    JAX version 0.4.31. Now we removed support for these use cases. `jax2tf`\n    with native serialization will still be supported.\n  * In `jax.interpreters.xla`, the `xb`, `xc`, and `xe` symbols have been removed\n    after being deprecated in JAX v0.4.31. Instead use `xb = jax.lib.xla_bridge`,\n    `xc = jax.lib.xla_client`, and `xe = jax.lib.xla_extension`.\n  * The deprecated module `jax.experimental.export` has been removed. It was replaced\n    by {mod}`jax.export` in JAX v0.4.30. See the [migration guide](https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export)\n    for information on migrating to the new API.\n  * The `initial` argument to {func}`jax.nn.softmax` and {func}`jax.nn.log_softmax`\n    has been removed, after being deprecated in v0.4.27.\n  * Calling `np.asarray` on typed PRNG keys (i.e. keys produced by :func:`jax.random.key`)\n    now raises an error. Previously, this returned a scalar object array.\n  * The following deprecated methods and functions in {mod}`jax.export` have\n    been removed:\n      * `jax.export.DisabledSafetyCheck.shape_assertions`: it had no effect\n        already.\n      * `jax.export.Exported.lowering_platforms`: use `platforms`.\n      * `jax.export.Exported.mlir_module_serialization_version`:\n        use `calling_convention_version`.\n      * `jax.export.Exported.uses_shape_polymorphism`:\n         use `uses_global_constants`.\n      * the `lowering_platforms` kwarg for {func}`jax.export.export`: use\n        `platforms` instead.\n  * The kwargs `symbolic_scope` and `symbolic_constraints` from\n    {func}`jax.export.symbolic_args_specs` have been removed. They were\n    deprecated in June 2024. Use `scope` and `constraints` instead.\n  * Hashing of tracers, which has been deprecated since version 0.4.30, now\n    results in a `TypeError`.\n  * Refactor: JAX build CLI (build/build.py) now uses a subcommand structure and\n    replaces previous build.py usage. Run `python build/build.py --help` for\n    more details. Brief overview of the new subcommand options:\n    * `build`: Builds JAX wheel packages. For e.g., `python build/build.py build --wheels=jaxlib,jax-cuda-pjrt`\n    * `requirements_update`: Updates requirements_lock.txt files.\n  * {func}`jax.scipy.linalg.toeplitz` now does implicit batching on multi-dimensional\n    inputs. To recover the previous behavior, you can call {func}`jax.numpy.ravel`\n    on the function inputs.\n  * {func}`jax.scipy.special.gamma` and {func}`jax.scipy.special.gammasgn` now\n    return NaN for negative integer inputs, to match the behavior of SciPy from\n    https://github.com/scipy/scipy/pull/21827.\n  * `jax.clear_backends` was removed after being deprecated in v0.4.26.\n  * We removed the custom call \"__gpu$xla.gpu.triton\" from the list of custom\n    call that we guarantee export stability. This is because this custom call\n    relies on Triton IR, which is not guaranteed to be stable. If you need\n    to export code that uses this custom call, you can use the `disabled_checks`\n    parameter. See more details in the [documentation](https://jax.readthedocs.io/en/latest/export/export.html#compatibility-guarantees-for-custom-calls).\n\n* New Features\n  * {func}`jax.jit` got a new `compiler_options: dict[str, Any]` argument, for\n    passing compilation options to XLA. For the moment it's undocumented and\n    may be in flux.\n  * {func}`jax.tree_util.register_dataclass` now allows metadata fields to be\n    declared inline via {func}`dataclasses.field`. See the function documentation\n    for examples.\n  * Added {func}`jax.numpy.put_along_axis`.\n  * {func}`jax.lax.linalg.eig` and the related `jax.numpy` functions\n    ({func}`jax.numpy.linalg.eig` and {func}`jax.numpy.linalg.eigvals`) are now\n    supported on GPU. See {jax-issue}`#24663` for more details.\n  * Added two new configuration flags, `jax_exec_time_optimization_effort` and `jax_memory_fitting_effort`, to control the amount of effort the compiler spends minimizing execution time and memory usage, respectively.  Valid values are between -1.0 and 1.0, default is 0.0.\n\n* Bug fixes\n  * Fixed a bug where the GPU implementations of LU and QR decomposition would\n    result in an indexing overflow for batch sizes close to int32 max. See\n    {jax-issue}`#24843` for more details.\n\n* Deprecations\n  * `jax.lib.xla_extension.ArrayImpl` and `jax.lib.xla_client.ArrayImpl` are deprecated;\n    use `jax.Array` instead.\n  * `jax.lib.xla_extension.XlaRuntimeError` is deprecated; use `jax.errors.JaxRuntimeError`\n    instead.\n\n## jax 0.4.35 (Oct 22, 2024)\n\n* Breaking Changes\n  * {func}`jax.numpy.isscalar` now returns True for any array-like object with\n    zero dimensions. Previously it only returned True for zero-dimensional\n    array-like objects with a weak dtype.\n  * `jax.experimental.host_callback` has been deprecated since March 2024, with\n    JAX version 0.4.26. Now we removed it.\n    See {jax-issue}`#20385` for a discussion of alternatives.\n\n* Changes:\n  * `jax.lax.FftType` was introduced as a public name for the enum of FFT\n    operations. The semi-public API `jax.lib.xla_client.FftType` has been\n    deprecated.\n  * TPU: JAX now installs TPU support from the `libtpu` package rather than\n    `libtpu-nightly`. For the next few releases JAX will pin an empty version of\n    `libtpu-nightly` as well as `libtpu` to ease the transition; that dependency\n    will be removed in Q1 2025.\n\n* Deprecations:\n  * The semi-public API `jax.lib.xla_client.PaddingType` has been deprecated.\n    No JAX APIs consume this type, so there is no replacement.\n  * The default behavior of {func}`jax.pure_callback` and\n    {func}`jax.extend.ffi.ffi_call` under `vmap` has been deprecated and so has\n    the `vectorized` parameter to those functions. The `vmap_method` parameter\n    should be used instead for better defined behavior. See the discussion in\n    {jax-issue}`#23881` for more details.\n  * The semi-public API `jax.lib.xla_client.register_custom_call_target` has\n    been deprecated. Use the JAX FFI instead.\n  * The semi-public APIs `jax.lib.xla_client.dtype_to_etype`,\n    `jax.lib.xla_client.ops`,\n    `jax.lib.xla_client.shape_from_pyval`, `jax.lib.xla_client.PrimitiveType`,\n    `jax.lib.xla_client.Shape`, `jax.lib.xla_client.XlaBuilder`, and\n    `jax.lib.xla_client.XlaComputation` have been deprecated. Use StableHLO\n    instead.\n\n## jax 0.4.34 (October 4, 2024)\n\n* New Functionality\n  * This release includes wheels for Python 3.13. Free-threading mode is not yet\n    supported.\n  * `jax.errors.JaxRuntimeError` has been added as a public alias for the\n    formerly private `XlaRuntimeError` type.\n\n* Breaking changes\n  * `jax_pmap_no_rank_reduction` flag is set to `True` by default.\n    * array[0] on a pmap result now introduces a reshape (use array[0:1]\n      instead).\n    * The per-shard shape (accessable via jax_array.addressable_shards or\n      jax_array.addressable_data(0)) now has a leading (1, ...). Update code\n      that directly accesses shards accordingly. The rank of the per-shard-shape\n      now matches that of the global shape which is the same behavior as jit.\n      This avoids costly reshapes when passing results from pmap into jit.\n  * `jax.experimental.host_callback` has been deprecated since March 2024, with\n    JAX version 0.4.26. Now we set the default value of the\n    `--jax_host_callback_legacy` configuration value to `True`, which means that\n    if your code uses `jax.experimental.host_callback` APIs, those API calls\n    will be implemented in terms of the new `jax.experimental.io_callback` API.\n    If this breaks your code, for a very limited time, you can set the\n    `--jax_host_callback_legacy` to `True`. Soon we will remove that\n    configuration option, so you should instead transition to using the\n    new JAX callback APIs. See {jax-issue}`#20385` for a discussion.\n\n* Deprecations\n  * In {func}`jax.numpy.trim_zeros`, non-arraylike arguments or arraylike\n    arguments with `ndim != 1` are now deprecated, and in the future will result\n    in an error.\n  * Internal pretty-printing tools `jax.core.pp_*` have been removed, after\n    being deprecated in JAX v0.4.30.\n  * `jax.lib.xla_client.Device` is deprecated; use `jax.Device` instead.\n  * `jax.lib.xla_client.XlaRuntimeError` has been deprecated. Use\n    `jax.errors.JaxRuntimeError` instead.\n\n* Deletion:\n  * `jax.xla_computation` is deleted. It's been 3 months since it's deprecation\n    in 0.4.30 JAX release.\n    Please use the AOT APIs to get the same functionality as `jax.xla_computation`.\n    * `jax.xla_computation(fn)(*args, **kwargs)` can be replaced with\n      `jax.jit(fn).lower(*args, **kwargs).compiler_ir('hlo')`.\n    * You can also use `.out_info` property of `jax.stages.Lowered` to get the\n      output information (like tree structure, shape and dtype).\n    * For cross-backend lowering, you can replace\n      `jax.xla_computation(fn, backend='tpu')(*args, **kwargs)` with\n      `jax.jit(fn).trace(*args, **kwargs).lower(lowering_platforms=('tpu',)).compiler_ir('hlo')`.\n  * {class}`jax.ShapeDtypeStruct` no longer accepts the `named_shape` argument.\n    The argument was only used by `xmap` which was removed in 0.4.31.\n  * `jax.tree.map(f, None, non-None)`, which previously emitted a\n    `DeprecationWarning`, now raises an error in a future version of jax. `None`\n    is only a tree-prefix of itself. To preserve the current behavior, you can\n    ask `jax.tree.map` to treat `None` as a leaf value by writing:\n    `jax.tree.map(lambda x, y: None if x is None else f(x, y), a, b, is_leaf=lambda x: x is None)`.\n  * `jax.sharding.XLACompatibleSharding` has been removed. Please use\n    `jax.sharding.Sharding`.\n\n* Bug fixes\n  * Fixed a bug where {func}`jax.numpy.cumsum` would produce incorrect outputs\n    if a non-boolean input was provided and `dtype=bool` was specified.\n  * Edit implementation of {func}`jax.numpy.ldexp` to get correct gradient.\n\n## jax 0.4.33 (September 16, 2024)\n\nThis is a patch release on top of jax 0.4.32, that fixes two bugs found in that\nrelease.\n\nA TPU-only data corruption bug was found in the version of libtpu pinned by\nJAX 0.4.32, which manifested only if multiple TPU slices were present in the\nsame job, for example, if training on multiple v5e slices.\nThis release fixes that issue by pinning a fixed version of `libtpu`.\n\nThis release fixes an inaccurate result for F64 tanh on CPU (#23590).\n\n## jax 0.4.32 (September 11, 2024)\n\nNote: This release was yanked from PyPi because of a data corruption bug on TPU.\nSee the 0.4.33 release notes for more details.\n\n* New Functionality\n  * Added {func}`jax.extend.ffi.ffi_call` and {func}`jax.extend.ffi.ffi_lowering`\n    to support the use of the new {ref}`ffi-tutorial` to interface with custom\n    C++ and CUDA code from JAX.\n\n* Changes\n  * `jax_enable_memories` flag is set to `True` by default.\n  * {mod}`jax.numpy` now supports v2023.12 of the Python Array API Standard.\n    See {ref}`python-array-api` for more information.\n  * Computations on the CPU backend may now be dispatched asynchronously in\n    more cases. Previously non-parallel computations were always dispatched\n    synchronously. You can recover the old behavior by setting\n    `jax.config.update('jax_cpu_enable_async_dispatch', False)`.\n  * Added new {func}`jax.process_indices` function to replace the\n    `jax.host_ids()` function that was deprecated in JAX v0.2.13.\n  * To align with the behavior of `numpy.fabs`, `jax.numpy.fabs` has been\n    modified to no longer support `complex dtypes`.\n  * ``jax.tree_util.register_dataclass`` now checks that ``data_fields``\n    and ``meta_fields`` includes all dataclass fields with ``init=True``\n    and only them, if ``nodetype`` is a dataclass.\n  * Several {mod}`jax.numpy` functions now have full {class}`~jax.numpy.ufunc`\n    interfaces, including {obj}`~jax.numpy.add`, {obj}`~jax.numpy.multiply`,\n    {obj}`~jax.numpy.bitwise_and`, {obj}`~jax.numpy.bitwise_or`,\n    {obj}`~jax.numpy.bitwise_xor`, {obj}`~jax.numpy.logical_and`,\n    {obj}`~jax.numpy.logical_and`, and {obj}`~jax.numpy.logical_and`.\n    In future releases we plan to expand these to other ufuncs.\n  * Added {func}`jax.lax.optimization_barrier`, which allows users to prevent\n    compiler optimizations such as common-subexpression elimination and to\n    control scheduling.\n\n* Breaking changes\n  * The MHLO MLIR dialect (`jax.extend.mlir.mhlo`) has been removed. Use the\n    `stablehlo` dialect instead.\n\n* Deprecations\n  * Complex inputs to {func}`jax.numpy.clip` and {func}`jax.numpy.hypot` are\n    no longer allowed, after being deprecated since JAX v0.4.27.\n  * Deprecated the following APIs:\n    * `jax.lib.xla_bridge.xla_client`: use {mod}`jax.lib.xla_client` directly.\n    * `jax.lib.xla_bridge.get_backend`: use {func}`jax.extend.backend.get_backend`.\n    * `jax.lib.xla_bridge.default_backend`: use {func}`jax.extend.backend.default_backend`.\n  * The `jax.experimental.array_api` module is deprecated, and importing it is no\n    longer required to use the Array API. `jax.numpy` supports the array API\n    directly; see {ref}`python-array-api` for more information.\n  * The internal utilities `jax.core.check_eqn`, `jax.core.check_type`, and\n    `jax.core.check_valid_jaxtype` are now deprecated, and will be removed in\n    the future.\n  * `jax.numpy.round_` has been deprecated, following removal of the corresponding\n    API in NumPy 2.0. Use {func}`jax.numpy.round` instead.\n  * Passing a DLPack capsule to {func}`jax.dlpack.from_dlpack` is deprecated.\n    The argument to {func}`jax.dlpack.from_dlpack` should be an array from\n    another framework that implements the ``__dlpack__`` protocol.\n\n## jaxlib 0.4.32 (September 11, 2024)\n\nNote: This release was yanked from PyPi because of a data corruption bug on TPU.\nSee the 0.4.33 release notes for more details.\n\n* Breaking changes\n  * This release of jaxlib switched to a new version of the CPU backend, which\n    should compile faster and leverage parallelism better. If you experience\n    any problems due to this change, you can temporarily enable the old CPU\n    backend by setting the environment variable\n    `XLA_FLAGS=--xla_cpu_use_thunk_runtime=false`. If you need to do this,\n    please file a JAX bug with instructions to reproduce.\n  * Hermetic CUDA support is added.\n    Hermetic CUDA uses a specific downloadable version of CUDA instead of the\n    user’s locally installed CUDA. Bazel will download CUDA, CUDNN and NCCL\n    distributions, and then use CUDA libraries and tools as dependencies in\n    various Bazel targets. This enables more reproducible builds for JAX and its\n    supported CUDA versions.\n\n* Changes\n  * SparseCore profiling is added.\n    * JAX now supports profiling [SparseCore](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#sparsecore) on TPUv5p chips. These traces will be viewable in Tensorboard Profiler's [TraceViewer](https://www.tensorflow.org/guide/profiler#trace_viewer).\n\n## jax 0.4.31 (July 29, 2024)\n\n* Deletion\n  * xmap has been deleted. Please use {func}`shard_map` as the replacement.\n\n* Changes\n  * The minimum CuDNN version is v9.1. This was true in previous releases also,\n    but we now declare this version constraint formally.\n  * The minimum Python version is now 3.10. 3.10 will remain the minimum\n    supported version until July 2025.\n  * The minimum NumPy version is now 1.24. NumPy 1.24 will remain the minimum\n    supported version until December 2024.\n  * The minimum SciPy version is now 1.10. SciPy 1.10 will remain the minimum\n    supported version until January 2025.\n  * {func}`jax.numpy.ceil`, {func}`jax.numpy.floor` and {func}`jax.numpy.trunc` now return the output\n    of the same dtype as the input, i.e. no longer upcast integer or boolean inputs to floating point.\n  * `libdevice.10.bc` is no longer bundled with CUDA wheels. It must be\n    installed either as a part of local CUDA installation, or via NVIDIA's CUDA\n    pip wheels.\n  * {class}`jax.experimental.pallas.BlockSpec` now expects `block_shape` to\n    be passed *before* `index_map`. The old argument order is deprecated and\n    will be removed in a future release.\n  * Updated the repr of gpu devices to be more consistent\n    with TPUs/CPUs. For example, `cuda(id=0)` will now be `CudaDevice(id=0)`.\n  * Added the `device` property and `to_device` method to {class}`jax.Array`, as\n    part of JAX's [Array API](https://data-apis.org/array-api) support.\n* Deprecations\n  * Removed a number of previously-deprecated internal APIs related to\n    polymorphic shapes. From {mod}`jax.core`: removed `canonicalize_shape`,\n    `dimension_as_value`, `definitely_equal`, and `symbolic_equal_dim`.\n  * HLO lowering rules should no longer wrap singleton ir.Values in tuples.\n    Instead, return singleton ir.Values unwrapped. Support for wrapped values\n    will be removed in a future version of JAX.\n  * {func}`jax.experimental.jax2tf.convert` with `native_serialization=False`\n    or `enable_xla=False` is now deprecated and this support will be removed in\n    a future version.\n    Native serialization has been the default since JAX 0.4.16 (September 2023).\n  * The previously-deprecated function `jax.random.shuffle` has been removed;\n    instead use `jax.random.permutation` with `independent=True`.\n\n## jaxlib 0.4.31 (July 29, 2024)\n\n* Bug fixes\n  * Fixed a bug that meant that negative static_argnums to a jit were mishandled\n    by the jit dispatch fast path.\n  * Fixed a bug that meant triangular solves of batches of singular matrices\n    produce nonsensical finite values, instead of inf or nan (#3589, #15429).\n\n## jax 0.4.30 (June 18, 2024)\n\n* Changes\n  * JAX supports ml_dtypes >= 0.2. In 0.4.29 release, the ml_dtypes version was\n    bumped to 0.4.0 but this has been rolled back in this release to give users\n    of both TensorFlow and JAX more time to migrate to a newer TensorFlow\n    release.\n  * `jax.experimental.mesh_utils` can now create an efficient mesh for TPU v5e.\n  * jax now depends on jaxlib directly. This change was enabled by the CUDA\n    plugin switch: there are no longer multiple jaxlib variants. You can install\n    a CPU-only jax with `pip install jax`, no extras required.\n  * Added an API for exporting and serializing JAX functions. This used\n    to exist in `jax.experimental.export` (which is being deprecated),\n    and will now live in `jax.export`.\n    See the [documentation](https://jax.readthedocs.io/en/latest/export/index.html).\n\n* Deprecations\n  * Internal pretty-printing tools `jax.core.pp_*` are deprecated, and will be removed\n    in a future release.\n  * Hashing of tracers is deprecated, and will lead to a `TypeError` in a future JAX\n    release. This previously was the case, but there was an inadvertent regression in\n    the last several JAX releases.\n  * `jax.experimental.export` is deprecated. Use {mod}`jax.export` instead.\n    See the [migration guide](https://jax.readthedocs.io/en/latest/export/export.html#migration-guide-from-jax-experimental-export).\n  * Passing an array in place of a dtype is now deprecated in most cases; e.g. for arrays\n    `x` and `y`, `x.astype(y)` will raise a warning. To silence it use `x.astype(y.dtype)`.\n  * `jax.xla_computation` is deprecated and will be removed in a future release.\n    Please use the AOT APIs to get the same functionality as `jax.xla_computation`.\n    * `jax.xla_computation(fn)(*args, **kwargs)` can be replaced with\n      `jax.jit(fn).lower(*args, **kwargs).compiler_ir('hlo')`.\n    * You can also use `.out_info` property of `jax.stages.Lowered` to get the\n      output information (like tree structure, shape and dtype).\n    * For cross-backend lowering, you can replace\n      `jax.xla_computation(fn, backend='tpu')(*args, **kwargs)` with\n      `jax.jit(fn).trace(*args, **kwargs).lower(lowering_platforms=('tpu',)).compiler_ir('hlo')`.\n\n\n## jaxlib 0.4.30 (June 18, 2024)\n\n  * Support for monolithic CUDA jaxlibs has been dropped. You must use the\n    plugin-based installation (`pip install jax[cuda12]` or\n    `pip install jax[cuda12_local]`).\n\n## jax 0.4.29 (June 10, 2024)\n\n* Changes\n  * We anticipate that this will be the last release of JAX and jaxlib\n    supporting a monolithic CUDA jaxlib. Future releases will use the CUDA\n    plugin jaxlib (e.g. `pip install jax[cuda12]`).\n  * JAX now requires ml_dtypes version 0.4.0 or newer.\n  * Removed backwards-compatibility support for old usage of the\n    `jax.experimental.export` API. It is not possible anymore to use\n    `from jax.experimental.export import export`, and instead you should use\n    `from jax.experimental import export`.\n    The removed functionality has been deprecated since 0.4.24.\n  * Added `is_leaf` argument to {func}`jax.tree.all` & {func}`jax.tree_util.tree_all`.\n\n* Deprecations\n  * `jax.sharding.XLACompatibleSharding` is deprecated. Please use\n    `jax.sharding.Sharding`.\n  * `jax.experimental.Exported.in_shardings` has been renamed as\n    `jax.experimental.Exported.in_shardings_hlo`. Same for `out_shardings`.\n    The old names will be removed after 3 months.\n  * Removed a number of previously-deprecated APIs:\n    * from {mod}`jax.core`: `non_negative_dim`, `DimSize`, `Shape`\n    * from {mod}`jax.lax`: `tie_in`\n    * from {mod}`jax.nn`: `normalize`\n    * from {mod}`jax.interpreters.xla`: `backend_specific_translations`,\n      `translations`, `register_translation`, `xla_destructure`,\n      `TranslationRule`, `TranslationContext`, `XlaOp`.\n  * The ``tol`` argument of {func}`jax.numpy.linalg.matrix_rank` is being\n    deprecated and will soon be removed. Use `rtol` instead.\n  * The ``rcond`` argument of {func}`jax.numpy.linalg.pinv` is being\n    deprecated and will soon be removed. Use `rtol` instead.\n  * The deprecated `jax.config` submodule has been removed. To configure JAX\n    use `import jax` and then reference the config object via `jax.config`.\n  * {mod}`jax.random` APIs no longer accept batched keys, where previously\n    some did unintentionally. Going forward, we recommend explicit use of\n    {func}`jax.vmap` in such cases.\n  * In {func}`jax.scipy.special.beta`, the `x` and `y` parameters have been\n    renamed to `a` and `b` for consistency with other `beta` APIs.\n\n* New Functionality\n  * Added {func}`jax.experimental.Exported.in_shardings_jax` to construct\n    shardings that can be used with the JAX APIs from the HloShardings\n    that are stored in the `Exported` objects.\n\n## jaxlib 0.4.29 (June 10, 2024)\n\n* Bug fixes\n  * Fixed a bug where XLA sharded some concatenation operations incorrectly,\n    which manifested as an incorrect output for cumulative reductions (#21403).\n  * Fixed a bug where XLA:CPU miscompiled certain matmul fusions\n    (https://github.com/openxla/xla/pull/13301).\n  * Fixes a compiler crash on GPU (https://github.com/jax-ml/jax/issues/21396).\n\n* Deprecations\n  * `jax.tree.map(f, None, non-None)` now emits a `DeprecationWarning`, and will\n    raise an error in a future version of jax. `None` is only a tree-prefix of\n    itself. To preserve the current behavior, you can ask `jax.tree.map` to\n    treat `None` as a leaf value by writing:\n    `jax.tree.map(lambda x, y: None if x is None else f(x, y), a, b, is_leaf=lambda x: x is None)`.\n\n## jax 0.4.28 (May 9, 2024)\n\n* Bug fixes\n  * Reverted a change to `make_jaxpr` that was breaking Equinox (#21116).\n\n* Deprecations & removals\n  * The ``kind`` argument to {func}`jax.numpy.sort` and {func}`jax.numpy.argsort`\n    is now removed. Use `stable=True` or `stable=False` instead.\n  * Removed ``get_compute_capability`` from the ``jax.experimental.pallas.gpu``\n    module. Use the ``compute_capability`` attribute of a GPU device, returned\n    by {func}`jax.devices` or {func}`jax.local_devices`, instead.\n  * The ``newshape`` argument to {func}`jax.numpy.reshape`is being deprecated\n    and will soon be removed. Use `shape` instead.\n\n* Changes\n  * The minimum jaxlib version of this release is 0.4.27.\n\n## jaxlib 0.4.28 (May 9, 2024)\n\n* Bug fixes\n  * Fixes a memory corruption bug in the type name of Array and JIT Python\n    objects in Python 3.10 or earlier.\n  * Fixed a warning `'+ptx84' is not a recognized feature for this target`\n    under CUDA 12.4.\n  * Fixed a slow compilation problem on CPU.\n\n* Changes\n  * The Windows build is now built with Clang instead of MSVC.\n\n\n## jax 0.4.27 (May 7, 2024)\n\n* New Functionality\n  * Added {func}`jax.numpy.unstack` and {func}`jax.numpy.cumulative_sum`,\n    following their addition in the array API 2023 standard, soon to be\n    adopted by NumPy.\n  * Added a new config option `jax_cpu_collectives_implementation` to select the\n    implementation of cross-process collective operations used by the CPU backend.\n    Choices available are `'none'`(default), `'gloo'` and `'mpi'` (requires jaxlib 0.4.26).\n    If set to `'none'`, cross-process collective operations are disabled.\n\n* Changes\n  * {func}`jax.pure_callback`, {func}`jax.experimental.io_callback`\n    and {func}`jax.debug.callback` now use {class}`jax.Array` instead\n    of {class}`np.ndarray`. You can recover the old behavior by transforming\n    the arguments via `jax.tree.map(np.asarray, args)` before passing them\n    to the callback.\n  * `complex_arr.astype(bool)` now follows the same semantics as NumPy, returning\n    False where `complex_arr` is equal to `0 + 0j`, and True otherwise.\n  * `core.Token` now is a non-trivial class which wraps a `jax.Array`. It could\n    be created and threaded in and out of computations to build up dependency.\n    The singleton object `core.token` has been removed, users now should create\n    and use fresh `core.Token` objects instead.\n  * On GPU, the Threefry PRNG implementation no longer lowers to a kernel call\n    by default. This choice can improve runtime memory usage at a compile-time\n    cost. Prior behavior, which produces a kernel call, can be recovered with\n    `jax.config.update('jax_threefry_gpu_kernel_lowering', True)`. If the new\n    default causes issues, please file a bug. Otherwise, we intend to remove\n    this flag in a future release.\n\n* Deprecations & Removals\n  * Pallas now exclusively uses XLA for compiling kernels on GPU. The old\n    lowering pass via Triton Python APIs has been removed and the\n    `JAX_TRITON_COMPILE_VIA_XLA` environment variable no longer has any effect.\n  * {func}`jax.numpy.clip` has a new argument signature: `a`, `a_min`, and\n    `a_max` are deprecated in favor of `x` (positional only), `min`, and\n    `max` ({jax-issue}`20550`).\n  * The `device()` method of JAX arrays has been removed, after being deprecated\n    since JAX v0.4.21. Use `arr.devices()` instead.\n  * The `initial` argument to {func}`jax.nn.softmax` and {func}`jax.nn.log_softmax`\n    is deprecated; empty inputs to softmax are now supported without setting this.\n  * In {func}`jax.jit`, passing invalid `static_argnums` or `static_argnames`\n    now leads to an error rather than a warning.\n  * The minimum jaxlib version is now 0.4.23.\n  * The {func}`jax.numpy.hypot` function now issues a deprecation warning when\n    passing complex-valued inputs to it. This will raise an error when the\n    deprecation is completed.\n  * Scalar arguments to {func}`jax.numpy.nonzero`, {func}`jax.numpy.where`, and\n    related functions now raise an error, following a similar change in NumPy.\n  * The config option `jax_cpu_enable_gloo_collectives` is deprecated.\n    Use `jax.config.update('jax_cpu_collectives_implementation', 'gloo')` instead.\n  * The `jax.Array.device_buffer` and `jax.Array.device_buffers` methods have\n    been removed after being deprecated in JAX v0.4.22. Instead use\n    {attr}`jax.Array.addressable_shards` and {meth}`jax.Array.addressable_data`.\n  * The `condition`, `x`, and `y` parameters of `jax.numpy.where` are now\n    positional-only, following deprecation of the keywords in JAX v0.4.21.\n  * Non-array arguments to functions in {mod}`jax.lax.linalg` now must be\n    specified by keyword. Previously, this raised a DeprecationWarning.\n  * Array-like arguments are now required in several :func:`jax.numpy` APIs,\n    including {func}`~jax.numpy.apply_along_axis`,\n    {func}`~jax.numpy.apply_over_axes`, {func}`~jax.numpy.inner`,\n    {func}`~jax.numpy.outer`, {func}`~jax.numpy.cross`,\n    {func}`~jax.numpy.kron`, and {func}`~jax.numpy.lexsort`.\n\n* Bug fixes\n  * {func}`jax.numpy.astype` will now always return a copy when `copy=True`.\n    Previously, no copy would be made when the output array would have the same\n    dtype as the input array. This may result in some increased memory usage.\n    The default value is set to `copy=False` to preserve backwards compatibility.\n\n## jaxlib 0.4.27 (May 7, 2024)\n\n## jax 0.4.26 (April 3, 2024)\n\n* New Functionality\n  * Added {func}`jax.numpy.trapezoid`, following the addition of this function in\n    NumPy 2.0.\n\n* Changes\n  * Complex-valued {func}`jax.numpy.geomspace` now chooses the logarithmic spiral\n    branch consistent with that of NumPy 2.0.\n  * The behavior of `lax.rng_bit_generator`, and in turn the `'rbg'`\n    and `'unsafe_rbg'` PRNG implementations, under `jax.vmap` [has\n    changed](https://github.com/jax-ml/jax/issues/19085) so that\n    mapping over keys results in random generation only from the first\n    key in the batch.\n  * Docs now use `jax.random.key` for construction of PRNG key arrays\n    rather than `jax.random.PRNGKey`.\n\n* Deprecations & Removals\n  * {func}`jax.tree_map` is deprecated; use `jax.tree.map` instead, or for backward\n    compatibility with older JAX versions, use {func}`jax.tree_util.tree_map`.\n  * {func}`jax.clear_backends` is deprecated as it does not necessarily do what\n    its name suggests and can lead to unexpected consequences, e.g., it will not\n    destroy existing backends and release corresponding owned resources. Use\n    {func}`jax.clear_caches` if you only want to clean up compilation caches.\n    For backward compatibility or you really need to switch/reinitialize the\n    default backend, use {func}`jax.extend.backend.clear_backends`.\n  * The `jax.experimental.maps` module and `jax.experimental.maps.xmap` are\n    deprecated. Use `jax.experimental.shard_map` or `jax.vmap` with the\n    `spmd_axis_name` argument for expressing SPMD device-parallel computations.\n  * The `jax.experimental.host_callback` module is deprecated.\n    Use instead the [new JAX external callbacks](https://jax.readthedocs.io/en/latest/notebooks/external_callbacks.html).\n    Added `JAX_HOST_CALLBACK_LEGACY` flag to assist in the transition to the\n    new callbacks. See {jax-issue}`#20385` for a discussion.\n  * Passing arguments to {func}`jax.numpy.array_equal` and {func}`jax.numpy.array_equiv`\n    that cannot be converted to a JAX array now results in an exception.\n  * The deprecated flag `jax_parallel_functions_output_gda` has been removed.\n    This flag was long deprecated and did nothing; its use was a no-op.\n  * The previously-deprecated imports `jax.interpreters.ad.config` and\n    `jax.interpreters.ad.source_info_util` have now been removed. Use `jax.config`\n    and `jax.extend.source_info_util` instead.\n  * JAX export does not support older serialization versions anymore. Version 9\n    has been supported since October 27th, 2023 and has become the default\n    since February 1, 2024.\n    See [a description of the versions](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#native-serialization-versions).\n    This change could break clients that set a specific\n    JAX serialization version lower than 9.\n\n## jaxlib 0.4.26 (April 3, 2024)\n\n* Changes\n  * JAX now supports CUDA 12.1 or newer only. Support for CUDA 11.8 has been\n    dropped.\n  * JAX now supports NumPy 2.0.\n\n## jax 0.4.25 (Feb 26, 2024)\n\n* New Features\n  * Added [CUDA Array\n    Interface](https://numba.readthedocs.io/en/stable/cuda/cuda_array_interface.html)\n    import support (requires jaxlib 0.4.24).\n  * JAX arrays now support NumPy-style scalar boolean indexing, e.g. `x[True]` or `x[False]`.\n  * Added {mod}`jax.tree` module, with a more convenient interface for referencing functions\n    in {mod}`jax.tree_util`.\n  * {func}`jax.tree.transpose` (i.e. {func}`jax.tree_util.tree_transpose`) now accepts\n    `inner_treedef=None`, in which case the inner treedef will be automatically inferred.\n\n* Changes\n  * Pallas now uses XLA instead of the Triton Python APIs to compile Triton\n    kernels. You can revert to the old behavior by setting the\n    `JAX_TRITON_COMPILE_VIA_XLA` environment variable to `\"0\"`.\n  * Several deprecated APIs in {mod}`jax.interpreters.xla` that were removed in v0.4.24\n    have been re-added in v0.4.25, including `backend_specific_translations`,\n    `translations`, `register_translation`, `xla_destructure`, `TranslationRule`,\n    `TranslationContext`, and `XLAOp`. These are still considered deprecated, and\n    will be removed again in the future when better replacements are available.\n    Refer to {jax-issue}`#19816` for discussion.\n\n* Deprecations & Removals\n  * {func}`jax.numpy.linalg.solve` now shows a deprecation warning for batched 1D\n    solves with `b.ndim > 1`. In the future these will be treated as batched 2D\n    solves.\n  * Conversion of a non-scalar array to a Python scalar now raises an error, regardless\n    of the size of the array. Previously a deprecation warning was raised in the case of\n    non-scalar arrays of size 1. This follows a similar deprecation in NumPy.\n  * The previously deprecated configuration APIs have been removed\n    following a standard 3 months deprecation cycle (see {ref}`api-compatibility`).\n    These include\n    * the `jax.config.config` object and\n    * the `define_*_state` and `DEFINE_*` methods of {data}`jax.config`.\n  * Importing the `jax.config` submodule via `import jax.config` is deprecated.\n    To configure JAX use `import jax` and then reference the config object\n    via `jax.config`.\n  * The minimum jaxlib version is now 0.4.20.\n\n## jaxlib 0.4.25 (Feb 26, 2024)\n\n## jax 0.4.24 (Feb 6, 2024)\n\n* Changes\n\n  * JAX lowering to StableHLO does not depend on physical devices anymore.\n    If your primitive wraps custom_partitioning or JAX callbacks in the lowering\n    rule i.e. function passed to `rule` parameter of `mlir.register_lowering` then add your\n    primitive to `jax._src.dispatch.prim_requires_devices_during_lowering` set.\n    This is needed because custom_partitioning and JAX callbacks need physical\n    devices to create `Sharding`s during lowering.\n    This is a temporary state until we can create `Sharding`s without physical\n    devices.\n  * {func}`jax.numpy.argsort` and {func}`jax.numpy.sort` now support the `stable`\n    and `descending` arguments.\n  * Several changes to the handling of shape polymorphism (used in\n    {mod}`jax.experimental.jax2tf` and {mod}`jax.experimental.export`):\n    * cleaner pretty-printing of symbolic expressions ({jax-issue}`#19227`)\n    * added the ability to specify symbolic constraints on the dimension variables.\n      This makes shape polymorphism more expressive, and gives a way to workaround\n      limitations in the reasoning about inequalities.\n      See https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    * with the addition of symbolic constraints ({jax-issue}`#19235`) we now\n      consider dimension variables from different scopes to be different, even\n      if they have the same name. Symbolic expressions from different scopes\n      cannot interact, e.g., in arithmetic operations.\n      Scopes are introduced by {func}`jax.experimental.jax2tf.convert`,\n      {func}`jax.experimental.export.symbolic_shape`, {func}`jax.experimental.export.symbolic_args_specs`.\n      The scope of a symbolic expression `e` can be read with `e.scope` and passed\n      into the above functions to direct them to construct symbolic expressions in\n      a given scope.\n      See https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    * simplified and faster equality comparisons, where we consider two symbolic dimensions\n      to be equal if the normalized form of their difference reduces to 0\n      ({jax-issue}`#19231`; note that this may result in user-visible behavior\n        changes)\n    * improved the error messages for inconclusive inequality comparisons\n      ({jax-issue}`#19235`).\n    * the `core.non_negative_dim` API (introduced recently)\n      was deprecated and `core.max_dim` and `core.min_dim` were introduced\n      ({jax-issue}`#18953`) to express `max` and `min` for symbolic dimensions.\n      You can use `core.max_dim(d, 0)` instead of `core.non_negative_dim(d)`.\n    * the `shape_poly.is_poly_dim` is deprecated in favor of `export.is_symbolic_dim`\n      ({jax-issue}`#19282`).\n    * the `export.args_specs` is deprecated in favor of `export.symbolic_args_specs\n      ({jax-issue}`#19283`).\n    * the `shape_poly.PolyShape` and `jax2tf.PolyShape` are deprecated, use\n      strings for polymorphic shapes specifications ({jax-issue}`#19284`).\n    * JAX default native serialization version is now 9. This is relevant\n      for {mod}`jax.experimental.jax2tf` and {mod}`jax.experimental.export`.\n      See [description of version numbers](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#native-serialization-versions).\n  * Refactored the API for `jax.experimental.export`. Instead of\n    `from jax.experimental.export import export` you should use now\n    `from jax.experimental import export`. The old way of importing will\n    continue to work for a deprecation period of 3 months.\n  * Added {func}`jax.scipy.stats.sem`.\n  * {func}`jax.numpy.unique` with `return_inverse = True` returns inverse indices\n    reshaped to the dimension of the input, following a similar change to\n    {func}`numpy.unique` in NumPy 2.0.\n  * {func}`jax.numpy.sign` now returns `x / abs(x)` for nonzero complex inputs. This is\n    consistent with the behavior of {func}`numpy.sign` in NumPy version 2.0.\n  * {func}`jax.scipy.special.logsumexp` with `return_sign=True` now uses the NumPy 2.0\n    convention for the complex sign, `x / abs(x)`. This is consistent with the behavior\n    of {func}`scipy.special.logsumexp` in SciPy v1.13.\n  * JAX now supports the bool DLPack type for both import and export.\n    Previously bool values could not be imported and were exported as integers.\n\n* Deprecations & Removals\n  * A number of previously deprecated functions have been removed, following a\n    standard 3+ month deprecation cycle (see {ref}`api-compatibility`).\n    This includes:\n    * From {mod}`jax.core`: `TracerArrayConversionError`,\n      `TracerIntegerConversionError`, `UnexpectedTracerError`,\n      `as_hashable_function`, `collections`, `dtypes`, `lu`, `map`,\n      `namedtuple`, `partial`, `pp`, `ref`, `safe_zip`, `safe_map`,\n      `source_info_util`, `total_ordering`, `traceback_util`, `tuple_delete`,\n      `tuple_insert`, and `zip`.\n    * From {mod}`jax.lax`: `dtypes`, `itertools`, `naryop`, `naryop_dtype_rule`,\n      `standard_abstract_eval`, `standard_naryop`, `standard_primitive`,\n      `standard_unop`, `unop`, and `unop_dtype_rule`.\n    * The `jax.linear_util` submodule and all its contents.\n    * The `jax.prng` submodule and all its contents.\n    * From {mod}`jax.random`: `PRNGKeyArray`, `KeyArray`, `default_prng_impl`,\n      `threefry_2x32`, `threefry2x32_key`, `threefry2x32_p`, `rbg_key`, and\n      `unsafe_rbg_key`.\n    * From {mod}`jax.tree_util`: `register_keypaths`, `AttributeKeyPathEntry`, and\n      `GetItemKeyPathEntry`.\n    * from {mod}`jax.interpreters.xla`: `backend_specific_translations`, `translations`,\n      `register_translation`, `xla_destructure`, `TranslationRule`, `TranslationContext`,\n      `axis_groups`, `ShapedArray`, `ConcreteArray`, `AxisEnv`, `backend_compile`,\n      and `XLAOp`.\n    * from {mod}`jax.numpy`: `NINF`, `NZERO`, `PZERO`, `row_stack`, `issubsctype`,\n      `trapz`, and `in1d`.\n    * from {mod}`jax.scipy.linalg`: `tril` and `triu`.\n  * The previously-deprecated method `PRNGKeyArray.unsafe_raw_array` has been\n    removed. Use {func}`jax.random.key_data` instead.\n  * `bool(empty_array)` now raises an error rather than returning `False`. This\n    previously raised a deprecation warning, and follows a similar change in NumPy.\n  * Support for the mhlo MLIR dialect has been deprecated. JAX no longer uses\n    the mhlo dialect, in favor of stablehlo. APIs that refer to \"mhlo\" will be\n    removed in the future. Use the \"stablehlo\" dialect instead.\n  * {mod}`jax.random`: passing batched keys directly to random number generation functions,\n    such as {func}`~jax.random.bits`, {func}`~jax.random.gamma`, and others, is deprecated\n    and will emit a `FutureWarning`.  Use `jax.vmap` for explicit batching.\n  * {func}`jax.lax.tie_in` is deprecated: it has been a no-op since JAX v0.2.0.\n\n## jaxlib 0.4.24 (Feb 6, 2024)\n\n* Changes\n\n  * JAX now supports CUDA 12.3 and CUDA 11.8. Support for CUDA 12.2 has been\n    dropped.\n  * `cost_analysis` now works with cross-compiled `Compiled` objects (i.e. when\n    using `.lower().compile()` with a topology object, e.g., to compile for\n    Cloud TPU from a non-TPU computer).\n  * Added [CUDA Array\n    Interface](https://numba.readthedocs.io/en/stable/cuda/cuda_array_interface.html)\n    import support (requires jax 0.4.25).\n\n## jax 0.4.23 (Dec 13, 2023)\n\n## jaxlib 0.4.23 (Dec 13, 2023)\n\n* Fixed a bug that caused verbose logging from the GPU compiler during\n  compilation.\n\n## jax 0.4.22 (Dec 13, 2023)\n\n* Deprecations\n  * The `device_buffer` and `device_buffers` properties of JAX arrays are deprecated.\n    Explicit buffers have been replaced by the more flexible array sharding interface,\n    but the previous outputs can be recovered this way:\n    * `arr.device_buffer` becomes `arr.addressable_data(0)`\n    * `arr.device_buffers` becomes `[x.data for x in arr.addressable_shards]`\n\n## jaxlib 0.4.22 (Dec 13, 2023)\n\n## jax 0.4.21 (Dec 4 2023)\n\n* New Features\n  * Added {obj}`jax.nn.squareplus`.\n\n* Changes\n  * The minimum jaxlib version is now 0.4.19.\n  * Released wheels are built now with clang instead of gcc.\n  * Enforce that the device backend has not been initialized prior to calling `jax.distributed.initialize()`.\n  * Automate arguments to `jax.distributed.initialize()` in cloud TPU environments.\n\n* Deprecations\n  * The previously-deprecated `sym_pos` argument has been removed from\n    {func}`jax.scipy.linalg.solve`. Use `assume_a='pos'` instead.\n  * Passing `None` to {func}`jax.array` or {func}`jax.asarray`, either directly or\n    within a list or tuple, is deprecated and now raises a {obj}`FutureWarning`.\n    It currently is converted to NaN, and in the future will raise a {obj}`TypeError`.\n  * Passing the `condition`, `x`, and `y` parameters to `jax.numpy.where` by\n    keyword arguments has been deprecated, to match `numpy.where`.\n  * Passing arguments to {func}`jax.numpy.array_equal` and {func}`jax.numpy.array_equiv`\n    that cannot be converted to a JAX array is deprecated and now raises a\n    {obj}`DeprecationWaning`. Currently the functions return False, in the future this\n    will raise an exception.\n  * The `device()` method of JAX arrays is deprecated. Depending on the context, it may\n    be replaced with one of the following:\n    - {meth}`jax.Array.devices` returns the set of all devices used by the array.\n    - {attr}`jax.Array.sharding` gives the sharding configuration used by the array.\n\n## jaxlib 0.4.21 (Dec 4 2023)\n\n* Changes\n  * In preparation for adding distributed CPU support, JAX now treats CPU\n    devices identically to GPU and TPU devices, that is:\n\n    * `jax.devices()` includes all devices present in a distributed job, even\n      those not local to the current process. `jax.local_devices()` still only\n      includes devices local to the current process, so if the change to\n      `jax.devices()` breaks you, you most likely want to use\n      `jax.local_devices()` instead.\n    * CPU devices now receive a globally unique ID number within a distributed\n      job; previously CPU devices would receive a process-local ID number.\n    * The `process_index` of each CPU device will now match any GPU or TPU\n      devices within the same process; previously the `process_index` of a CPU\n      device was always 0.\n\n  * On NVIDIA GPU, JAX now prefers a Jacobi SVD solver for matrices up to\n    1024x1024. The Jacobi solver appears faster than the non-Jacobi version.\n\n* Bug fixes\n  * Fixed error/hang when an array with non-finite values is passed to a\n    non-symmetric eigendecomposition (#18226). Arrays with non-finite values now\n    produce arrays full of NaNs as outputs.\n\n## jax 0.4.20 (Nov 2, 2023)\n\n## jaxlib 0.4.20 (Nov 2, 2023)\n\n* Bug fixes\n  * Fixed some type confusion between E4M3 and E5M2 float8 types.\n\n## jax 0.4.19 (Oct 19, 2023)\n\n* New Features\n  * Added {obj}`jax.typing.DTypeLike`, which can be used to annotate objects that\n    are convertible to JAX dtypes.\n  * Added `jax.numpy.fill_diagonal`.\n\n* Changes\n  * JAX now requires SciPy 1.9 or newer.\n\n* Bug fixes\n  * Only process 0 in a multicontroller distributed JAX program will write\n    persistent compilation cache entries. This fixes write contention if the\n    cache is placed on a network file system such as GCS.\n  * The version check for cusolver and cufft no longer considers the patch\n    versions when determining if the installed version of these libraries is at\n    least as new as the versions against which JAX was built.\n\n## jaxlib 0.4.19 (Oct 19, 2023)\n\n* Changes\n  * jaxlib will now always prefer pip-installed NVIDIA CUDA libraries\n    (nvidia-... packages) over any other CUDA installation if they are\n    installed, including installations named in `LD_LIBRARY_PATH`. If this\n    causes problems and the intent is to use a system-installed CUDA, the fix is\n    to remove the pip installed CUDA library packages.\n\n## jax 0.4.18 (Oct 6, 2023)\n\n## jaxlib 0.4.18 (Oct 6, 2023)\n\n* Changes\n  * CUDA jaxlibs now depend on the user to install a compatible NCCL version.\n    If using the recommended `cuda12_pip` installation, NCCL should be installed\n    automatically. Currently, NCCL 2.16 or newer is required.\n  * We now provide Linux aarch64 wheels, both with and without NVIDIA GPU\n    support.\n  * {meth}`jax.Array.item` now supports optional index arguments.\n\n* Deprecations\n  * A number of internal utilities and inadvertent exports in {mod}`jax.lax` have\n    been deprecated, and will be removed in a future release.\n    * `jax.lax.dtypes`: use `jax.dtypes` instead.\n    * `jax.lax.itertools`: use `itertools` instead.\n    * `naryop`, `naryop_dtype_rule`, `standard_abstract_eval`, `standard_naryop`,\n      `standard_primitive`, `standard_unop`, `unop`, and `unop_dtype_rule` are\n      internal utilities, now deprecated without replacement.\n\n* Bug fixes\n  * Fixed Cloud TPU regression where compilation would OOM due to smem.\n\n## jax 0.4.17 (Oct 3, 2023)\n\n* New features\n  * Added new {func}`jax.numpy.bitwise_count` function, matching the API of the similar\n    function recently added to NumPy.\n* Deprecations\n  * Removed the deprecated module `jax.abstract_arrays` and all its contents.\n  * Named key constructors in {mod}`jax.random` are deprecated. Pass the `impl` argument\n    to {func}`jax.random.PRNGKey` or {func}`jax.random.key` instead:\n    * `random.threefry2x32_key(seed)` becomes `random.PRNGKey(seed, impl='threefry2x32')`\n    * `random.rbg_key(seed)` becomes `random.PRNGKey(seed, impl='rbg')`\n    * `random.unsafe_rbg_key(seed)` becomes `random.PRNGKey(seed, impl='unsafe_rbg')`\n* Changes:\n  * CUDA: JAX now verifies that the CUDA libraries it finds are at least as new\n    as the CUDA libraries that JAX was built against. If older libraries are\n    found, JAX raises an exception since that is preferable to mysterious\n    failures and crashes.\n  * Removed the \"No GPU/TPU\" found warning. Instead warn if, on Linux, an\n    NVIDIA GPU or a Google TPU are found but not used and `--jax_platforms` was\n    not specified.\n  * {func}`jax.scipy.stats.mode` now returns a 0 count if the mode is taken\n    across a size-0 axis, matching the behavior of `scipy.stats.mode` in SciPy\n    1.11.\n  * Most `jax.numpy` functions and attributes now have fully-defined type stubs.\n    Previously many of these were treated as `Any` by static type checkers like\n    `mypy` and `pytype`.\n\n## jaxlib 0.4.17 (Oct 3, 2023)\n\n* Changes:\n  * Python 3.12 wheels were added in this release.\n  * The CUDA 12 wheels now require CUDA 12.2 or newer and cuDNN 8.9.4 or newer.\n\n* Bug fixes:\n  * Fixed log spam from ABSL when the JAX CPU backend was initialized.\n\n## jax 0.4.16 (Sept 18, 2023)\n\n* Changes\n  * Added {class}`jax.numpy.ufunc`, as well as {func}`jax.numpy.frompyfunc`, which can convert\n    any scalar-valued function into a {func}`numpy.ufunc`-like object, with methods such as\n    {meth}`~jax.numpy.ufunc.outer`, {meth}`~jax.numpy.ufunc.reduce`,\n    {meth}`~jax.numpy.ufunc.accumulate`, {meth}`~jax.numpy.ufunc.at`, and\n    {meth}`~jax.numpy.ufunc.reduceat` ({jax-issue}`#17054`).\n  * Added {func}`jax.scipy.integrate.trapezoid`.\n  * When not running under IPython: when an exception is raised, JAX now filters out the\n    entirety of its internal frames from tracebacks. (Without the \"unfiltered stack trace\"\n    that previously appeared.) This should produce much friendlier-looking tracebacks. See\n    [here](https://github.com/jax-ml/jax/pull/16949) for an example.\n    This behavior can be changed by setting `JAX_TRACEBACK_FILTERING=remove_frames` (for two\n    separate unfiltered/filtered tracebacks, which was the old behavior) or\n    `JAX_TRACEBACK_FILTERING=off` (for one unfiltered traceback).\n  * jax2tf default serialization version is now 7, which introduces new shape\n    [safety assertions](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#errors-in-presence-of-shape-polymorphism).\n  * Devices passed to `jax.sharding.Mesh` should be hashable. This specifically\n    applies to mock devices or user created devices. `jax.devices()` are\n    already hashable.\n\n* Breaking changes:\n  * jax2tf now uses native serialization by default. See\n    the [jax2tf documentation](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n    for details and for mechanisms to override the default.\n  * The option `--jax_coordination_service` has been removed. It is now always\n    `True`.\n  * `jax.jaxpr_util` has been removed from the public JAX namespace.\n  * `JAX_USE_PJRT_C_API_ON_TPU` no longer has an effect (i.e. it always defaults to true).\n  * The backwards compatibility flag `--jax_host_callback_ad_transforms`\n    introduced in December 2021, has been removed.\n\n* Deprecations:\n  * Several `jax.numpy` APIs have been deprecated following\n    [NumPy NEP-52](https://numpy.org/neps/nep-0052-python-api-cleanup.html):\n    * `jax.numpy.NINF` has been deprecated. Use `-jax.numpy.inf` instead.\n    * `jax.numpy.PZERO` has been deprecated. Use `0.0` instead.\n    * `jax.numpy.NZERO` has been deprecated. Use `-0.0` instead.\n    * `jax.numpy.issubsctype(x, t)` has been deprecated. Use `jax.numpy.issubdtype(x.dtype, t)`.\n    * `jax.numpy.row_stack` has been deprecated. Use `jax.numpy.vstack` instead.\n    * `jax.numpy.in1d` has been deprecated. Use `jax.numpy.isin` instead.\n    * `jax.numpy.trapz` has been deprecated. Use `jax.scipy.integrate.trapezoid` instead.\n  * `jax.scipy.linalg.tril` and `jax.scipy.linalg.triu` have been deprecated,\n    following SciPy. Use `jax.numpy.tril` and `jax.numpy.triu` instead.\n  * `jax.lax.prod` has been removed after being deprecated in JAX v0.4.11.\n    Use the built-in `math.prod` instead.\n  * A number of exports from `jax.interpreters.xla` related to defining\n    HLO lowering rules for custom JAX primitives have been deprecated. Custom\n    primitives should be defined using the StableHLO lowering utilities in\n    `jax.interpreters.mlir` instead.\n  * The following previously-deprecated functions have been removed after a\n    three-month deprecation period:\n    * `jax.abstract_arrays.ShapedArray`: use `jax.core.ShapedArray`.\n    * `jax.abstract_arrays.raise_to_shaped`: use `jax.core.raise_to_shaped`.\n    * `jax.numpy.alltrue`: use `jax.numpy.all`.\n    * `jax.numpy.sometrue`: use `jax.numpy.any`.\n    * `jax.numpy.product`: use `jax.numpy.prod`.\n    * `jax.numpy.cumproduct`: use `jax.numpy.cumprod`.\n\n* Deprecations/removals:\n  * The internal submodule `jax.prng` is now deprecated. Its contents are available at\n    {mod}`jax.extend.random`.\n  * The internal submodule path `jax.linear_util` has been deprecated. Use\n    {mod}`jax.extend.linear_util` instead (Part of {ref}`jax-extend-jep`)\n  * `jax.random.PRNGKeyArray` and `jax.random.KeyArray` are deprecated.  Use {class}`jax.Array`\n    for type annotations, and `jax.dtypes.issubdtype(arr.dtype, jax.dtypes.prng_key)` for\n    runtime detection of typed prng keys.\n  * The method `PRNGKeyArray.unsafe_raw_array` is deprecated. Use\n    {func}`jax.random.key_data` instead.\n  * `jax.experimental.pjit.with_sharding_constraint` is deprecated. Use\n    `jax.lax.with_sharding_constraint` instead.\n  * The internal utilities `jax.core.is_opaque_dtype` and `jax.core.has_opaque_dtype`\n    have been removed. Opaque dtypes have been renamed to Extended dtypes; use\n    `jnp.issubdtype(dtype, jax.dtypes.extended)` instead (available since jax v0.4.14).\n  * The utility `jax.interpreters.xla.register_collective_primitive` has been\n    removed. This utility did nothing useful in recent JAX releases and calls\n    to it can be safely removed.\n  * The internal submodule path `jax.linear_util` has been deprecated. Use\n    {mod}`jax.extend.linear_util` instead (Part of {ref}`jax-extend-jep`)\n\n## jaxlib 0.4.16 (Sept 18, 2023)\n\n* Changes:\n  * Sparse CSR matrix multiplications via the experimental jax sparse APIs\n    no longer uses a deterministic algorithm on NVIDIA GPUs. This change was\n    made to improve compatibility with CUDA 12.2.1.\n\n* Bug fixes:\n  * Fixed a crash on Windows due to a fatal LLVM error related to out-of-order\n    sections and IMAGE_REL_AMD64_ADDR32NB relocations\n    (https://github.com/openxla/xla/commit/cb732a921f0c4184995cbed82394931011d12bd4).\n\n## jax 0.4.14 (July 27, 2023)\n\n* Changes\n  * `jax.jit` takes `donate_argnames` as an argument. It's semantics are similar\n    to `static_argnames`.\n    If neither donate_argnums nor donate_argnames is provided, no\n    arguments are donated. If donate_argnums is not provided but\n    donate_argnames is, or vice versa, JAX uses\n    `inspect.signature(fun)` to find any positional arguments that\n    correspond to donate_argnames (or vice versa). If both donate_argnums and donate_argnames are provided, inspect.signature is not used, and only actual\n    parameters listed in either donate_argnums or donate_argnames will\n    be donated.\n  * {func}`jax.random.gamma` has been re-factored to a more efficient algorithm\n    with more robust endpoint behavior ({jax-issue}`#16779`). This means that the\n    sequence of values returned for a given `key` will change between JAX v0.4.13\n    and v0.4.14 for `gamma` and related samplers (including {func}`jax.random.ball`,\n    {func}`jax.random.beta`, {func}`jax.random.chisquare`, {func}`jax.random.dirichlet`,\n    {func}`jax.random.generalized_normal`, {func}`jax.random.loggamma`, {func}`jax.random.t`).\n\n* Deletions\n  * `in_axis_resources` and `out_axis_resources` have been deleted from pjit since\n    it has been more than 3 months since their deprecation. Please use\n    `in_shardings` and `out_shardings` as the replacement.\n    This is a safe and trivial name replacement. It does not change any of the\n    current pjit semantics and doesn't break any code.\n    You can still pass in `PartitionSpecs` to in_shardings and out_shardings.\n\n\n* Deprecations\n  * Python 3.8 support has been dropped as per\n    https://jax.readthedocs.io/en/latest/deprecation.html\n  * JAX now requires NumPy 1.22 or newer as per\n    https://jax.readthedocs.io/en/latest/deprecation.html\n  * Passing optional arguments to {func}`jax.numpy.ndarray.at` by position is\n    no longer supported, after being deprecated in JAX version 0.4.7.\n    For example, instead of `x.at[i].get(True)`, use `x.at[i].get(indices_are_sorted=True)`\n  * The following `jax.Array` methods have been removed, after being deprecated\n    in JAX v0.4.5:\n    * `jax.Array.broadcast`: use {func}`jax.lax.broadcast` instead.\n    * `jax.Array.broadcast_in_dim`: use {func}`jax.lax.broadcast_in_dim` instead.\n    * `jax.Array.split`: use {func}`jax.numpy.split` instead.\n  * The following APIs have been removed after previous deprecation:\n    * `jax.ad`: use {mod}`jax.interpreters.ad`.\n    * `jax.curry`: use ``curry = lambda f: partial(partial, f)``.\n    * `jax.partial_eval`: use {mod}`jax.interpreters.partial_eval`.\n    * `jax.pxla`: use {mod}`jax.interpreters.pxla`.\n    * `jax.xla`: use {mod}`jax.interpreters.xla`.\n    * `jax.ShapedArray`: use {class}`jax.core.ShapedArray`.\n    * `jax.interpreters.pxla.device_put`: use {func}`jax.device_put`.\n    * `jax.interpreters.pxla.make_sharded_device_array`: use {func}`jax.make_array_from_single_device_arrays`.\n    * `jax.interpreters.pxla.ShardedDeviceArray`: use {class}`jax.Array`.\n    * `jax.numpy.DeviceArray`: use {class}`jax.Array`.\n    * `jax.stages.Compiled.compiler_ir`: use {func}`jax.stages.Compiled.as_text`.\n\n* Breaking changes\n  * JAX now requires ml_dtypes version 0.2.0 or newer.\n  * To fix a corner case, calls to {func}`jax.lax.cond` with five\n    arguments will always resolve to the \"common operands\" `cond`\n    behavior (as documented) if the second and third arguments are\n    callable, even if other operands are callable as well. See\n    [#16413](https://github.com/jax-ml/jax/issues/16413).\n  * The deprecated config options `jax_array` and `jax_jit_pjit_api_merge`,\n    which did nothing, have been removed. These options have been true by\n    default for many releases.\n\n* New features\n  * JAX now supports a configuration flag --jax_serialization_version\n    and a JAX_SERIALIZATION_VERSION environment variable to control the\n    serialization version ({jax-issue}`#16746`).\n  * jax2tf in presence of shape polymorphism now generates code that checks\n    certain shape constraints, if the serialization version is at least 7.\n    See https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#errors-in-presence-of-shape-polymorphism.\n\n## jaxlib 0.4.14 (July 27, 2023)\n\n* Deprecations\n  * Python 3.8 support has been dropped as per\n      https://jax.readthedocs.io/en/latest/deprecation.html\n\n## jax 0.4.13 (June 22, 2023)\n\n* Changes\n  * `jax.jit` now allows `None` to be passed to `in_shardings` and\n    `out_shardings`. The semantics are as follows:\n      * For in_shardings, JAX will mark is as replicated but this behavior\n        can change in the future.\n      * For out_shardings, we will rely on the XLA GSPMD partitioner to\n        determine the output shardings.\n  * `jax.experimental.pjit.pjit` also allows `None` to be passed to\n    `in_shardings` and `out_shardings`. The semantics are as follows:\n    * If the mesh context manager is *not* provided, JAX has the freedom to\n      choose whatever sharding it wants.\n      * For in_shardings, JAX will mark is as replicated but this behavior\n        can change in the future.\n      * For out_shardings, we will rely on the XLA GSPMD partitioner to\n        determine the output shardings.\n    * If the mesh context manager is provided, None will imply that the value\n      will be replicated on all devices of the mesh.\n  * Executable.cost_analysis() works on Cloud TPU\n  * Added a warning if a non-allowlisted `jaxlib` plugin is in use.\n  * Added `jax.tree_util.tree_leaves_with_path`.\n  * `None` is not a valid input to\n    `jax.experimental.multihost_utils.host_local_array_to_global_array` or\n    `jax.experimental.multihost_utils.global_array_to_host_local_array`.\n    Please use `jax.sharding.PartitionSpec()` if you wanted to replicate your\n    input.\n\n* Bug fixes\n  * Fixed incorrect wheel name in CUDA 12 releases (#16362); the correct wheel\n    is named `cudnn89` instead of `cudnn88`.\n\n* Deprecations\n  * The `native_serialization_strict_checks` parameter to\n    {func}`jax.experimental.jax2tf.convert` is deprecated in favor of the\n    new `native_serializaation_disabled_checks` ({jax-issue}`#16347`).\n\n## jaxlib 0.4.13 (June 22, 2023)\n\n* Changes\n  * Added Windows CPU-only wheels to the `jaxlib` Pypi release.\n\n* Bug fixes\n  * `__cuda_array_interface__` was broken in previous jaxlib versions and is now\n    fixed ({jax-issue}`16440`).\n  * Concurrent CUDA kernel tracing is now enabled by default on NVIDIA GPUs.\n\n## jax 0.4.12 (June 8, 2023)\n\n* Changes\n  * Added {class}`scipy.spatial.transform.Rotation` and {class}`scipy.spatial.transform.Slerp`\n\n* Deprecations\n  * `jax.abstract_arrays` and its contents are now deprecated. See related\n    functionality in :mod:`jax.core`.\n  * `jax.numpy.alltrue`: use `jax.numpy.all`. This follows the deprecation\n    of `numpy.alltrue` in NumPy version 1.25.0.\n  * `jax.numpy.sometrue`: use `jax.numpy.any`. This follows the deprecation\n    of `numpy.sometrue` in NumPy version 1.25.0.\n  * `jax.numpy.product`: use `jax.numpy.prod`. This follows the deprecation\n    of `numpy.product` in NumPy version 1.25.0.\n  * `jax.numpy.cumproduct`: use `jax.numpy.cumprod`. This follows the deprecation\n    of `numpy.cumproduct` in NumPy version 1.25.0.\n  * `jax.sharding.OpShardingSharding` has been removed since it has been 3\n    months since it was deprecated.\n\n## jaxlib 0.4.12 (June 8, 2023)\n\n* Changes\n  * Includes PTX/SASS for Hopper (SM version 9.0+) GPUs. Previous\n    versions of jaxlib should work on Hopper but would have a long\n    JIT-compilation delay the first time a JAX operation was executed.\n\n* Bug fixes\n  * Fixes incorrect source line information in JAX-generated Python tracebacks\n    under Python 3.11.\n  * Fixes crash when printing local variables of frames in JAX-generated Python\n    tracebacks (#16027).\n\n## jax 0.4.11 (May 31, 2023)\n\n* Deprecations\n  * The following APIs have been removed after a 3 month deprecation period, in\n    accordance with the {ref}`api-compatibility` policy:\n    * `jax.experimental.PartitionSpec`: use `jax.sharding.PartitionSpec`.\n    * `jax.experimental.maps.Mesh`: use `jax.sharding.Mesh`\n    * `jax.experimental.pjit.NamedSharding`: use `jax.sharding.NamedSharding`.\n    * `jax.experimental.pjit.PartitionSpec`: use `jax.sharding.PartitionSpec`.\n    * `jax.experimental.pjit.FROM_GDA`. Instead pass sharded `jax.Array` objects\n      as input and remove the optional `in_shardings` argument to `pjit`.\n    * `jax.interpreters.pxla.PartitionSpec`: use `jax.sharding.PartitionSpec`.\n    * `jax.interpreters.pxla.Mesh`: use `jax.sharding.Mesh`\n    * `jax.interpreters.xla.Buffer`: use `jax.Array`.\n    * `jax.interpreters.xla.Device`: use `jax.Device`.\n    * `jax.interpreters.xla.DeviceArray`: use `jax.Array`.\n    * `jax.interpreters.xla.device_put`: use `jax.device_put`.\n    * `jax.interpreters.xla.xla_call_p`: use `jax.experimental.pjit.pjit_p`.\n    * `axis_resources` argument of `with_sharding_constraint` is removed. Please\n      use `shardings` instead.\n\n\n## jaxlib 0.4.11 (May 31, 2023)\n\n* Changes\n  * Added `memory_stats()` method to `Device`s. If supported, this returns a\n    dict of string stat names with int values, e.g. `\"bytes_in_use\"`, or None if\n    the platform doesn't support memory statistics. The exact stats returned may\n    vary across platforms. Currently only implemented on Cloud TPU.\n  * Readded support for the Python buffer protocol (`memoryview`) on CPU\n    devices.\n\n## jax 0.4.10 (May 11, 2023)\n\n## jaxlib 0.4.10 (May 11, 2023)\n\n* Changes\n  * Fixed `'apple-m1' is not a recognized processor for this target (ignoring\n    processor)` issue that prevented previous release from running on Mac M1.\n\n## jax 0.4.9 (May 9, 2023)\n\n* Changes\n  * The flags experimental_cpp_jit, experimental_cpp_pjit and\n    experimental_cpp_pmap have been removed.\n    They are now always on.\n  * Accuracy of singular value decomposition (SVD) on TPU has been improved\n    (requires jaxlib 0.4.9).\n\n* Deprecations\n  * `jax.experimental.gda_serialization` is deprecated and has been renamed to\n    `jax.experimental.array_serialization`.\n    Please change your imports to use `jax.experimental.array_serialization`.\n  * The `in_axis_resources` and `out_axis_resources` arguments of pjit have been\n    deprecated. Please use `in_shardings` and `out_shardings` respectively.\n  * The function `jax.numpy.msort` has been removed. It has been deprecated since\n    JAX v0.4.1. Use `jnp.sort(a, axis=0)` instead.\n  * `in_parts` and `out_parts` arguments have been removed from `jax.xla_computation`\n    since they were only used with sharded_jit and sharded_jit is long gone.\n  * `instantiate_const_outputs` argument has been removed from `jax.xla_computation`\n    since it has been unused for a very long time.\n\n## jaxlib 0.4.9 (May 9, 2023)\n\n## jax 0.4.8 (March 29, 2023)\n\n* Breaking changes\n  * A major component of the Cloud TPU runtime has been upgraded. This enables\n    the following new features on Cloud TPU:\n    * {func}`jax.debug.print`, {func}`jax.debug.callback`, and\n      {func}`jax.debug.breakpoint()` now work on Cloud TPU\n    * Automatic TPU memory defragmentation\n\n    {func}`jax.experimental.host_callback` is no longer supported on Cloud TPU\n    with the new runtime component. Please file an issue on the [JAX issue\n    tracker](https://github.com/jax-ml/jax/issues) if the new `jax.debug` APIs\n    are insufficient for your use case.\n\n    The old runtime component will be available for at least the next three\n    months by setting the environment variable\n    `JAX_USE_PJRT_C_API_ON_TPU=false`. If you find you need to disable the new\n    runtime for any reason, please let us know on the [JAX issue\n    tracker](https://github.com/jax-ml/jax/issues).\n\n* Changes\n  * The minimum jaxlib version has been bumped from 0.4.6 to 0.4.7.\n\n* Deprecations\n  * CUDA 11.4 support has been dropped. JAX GPU wheels only support\n    CUDA 11.8 and CUDA 12. Older CUDA versions may work if jaxlib is built\n    from source.\n  * `global_arg_shapes` argument of pmap only worked with sharded_jit and has\n    been removed from pmap. Please migrate to pjit and remove global_arg_shapes\n    from pmap.\n\n## jax 0.4.7 (March 27, 2023)\n\n* Changes\n  * As per https://jax.readthedocs.io/en/latest/jax_array_migration.html#jax-array-migration\n    `jax.config.jax_array` cannot be disabled anymore.\n  * `jax.config.jax_jit_pjit_api_merge` cannot be disabled anymore.\n  * {func}`jax.experimental.jax2tf.convert` now supports the `native_serialization`\n    parameter to use JAX's native lowering to StableHLO to obtain a\n    StableHLO module for the entire JAX function instead of lowering each JAX\n    primitive to a TensorFlow op. This simplifies the internals and increases\n    the confidence that what you serialize matches the JAX native semantics.\n    See [documentation](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md).\n    As part of this change the config flag `--jax2tf_default_experimental_native_lowering`\n    has been renamed to `--jax2tf_native_serialization`.\n  * JAX now depends on `ml_dtypes`, which contains definitions of NumPy types\n    like bfloat16. These definitions were previously internal to JAX, but have\n    been split into a separate package to facilitate sharing them with other\n    projects.\n  * JAX now requires NumPy 1.21 or newer and SciPy 1.7 or newer.\n\n* Deprecations\n  * The type `jax.numpy.DeviceArray` is deprecated. Use `jax.Array` instead,\n    for which it is an alias.\n  * The type `jax.interpreters.pxla.ShardedDeviceArray` is deprecated. Use\n    `jax.Array` instead.\n  * Passing additional arguments to {func}`jax.numpy.ndarray.at` by position is deprecated.\n    For example, instead of `x.at[i].get(True)`, use `x.at[i].get(indices_are_sorted=True)`\n  * `jax.interpreters.xla.device_put` is deprecated. Please use `jax.device_put`.\n  * `jax.interpreters.pxla.device_put` is deprecated. Please use `jax.device_put`.\n  * `jax.experimental.pjit.FROM_GDA` is deprecated. Please pass in sharded\n    jax.Arrays as input and remove the `in_shardings` argument to pjit since\n    it is optional.\n\n## jaxlib 0.4.7 (March 27, 2023)\n\nChanges:\n  * jaxlib now depends on `ml_dtypes`, which contains definitions of NumPy types\n    like bfloat16. These definitions were previously internal to JAX, but have\n    been split into a separate package to facilitate sharing them with other\n    projects.\n\n## jax 0.4.6 (Mar 9, 2023)\n\n* Changes\n  * `jax.tree_util` now contain a set of APIs that allow user to define keys for their\n    custom pytree node. This includes:\n    * `tree_flatten_with_path` that flattens a tree and return not only each leaf but\n      also their key paths.\n    * `tree_map_with_path` that can map a function that takes the key path as an argument.\n    * `register_pytree_with_keys` to register how the key path and leaves should looks\n      like in a custom pytree node.\n    * `keystr` that pretty-prints a key path.\n\n  * {func}`jax2tf.call_tf` has a new parameter `output_shape_dtype` (default `None`)\n    that can be used to declare the output shape and type of the result. This enables\n    {func}`jax2tf.call_tf` to work in the presence of shape polymorphism. ({jax-issue}`#14734`).\n\n* Deprecations\n  * The old key-path APIs in `jax.tree_util` are deprecated and will be removed 3 months\n    from Mar 10 2023:\n    * `register_keypaths`: use {func}`jax.tree_util.register_pytree_with_keys` instead.\n    * `AttributeKeyPathEntry` : use `GetAttrKey` instead.\n    * `GetitemKeyPathEntry` : use `SequenceKey` or `DictKey` instead.\n\n## jaxlib 0.4.6 (Mar 9, 2023)\n\n## jax 0.4.5 (Mar 2, 2023)\n\n* Deprecations\n  * `jax.sharding.OpShardingSharding` has been renamed to `jax.sharding.GSPMDSharding`.\n    `jax.sharding.OpShardingSharding` will be removed in 3 months from Feb 17, 2023.\n  * The following `jax.Array` methods are deprecated and will be removed 3 months from\n    Feb 23 2023:\n    * `jax.Array.broadcast`: use {func}`jax.lax.broadcast` instead.\n    * `jax.Array.broadcast_in_dim`: use {func}`jax.lax.broadcast_in_dim` instead.\n    * `jax.Array.split`: use {func}`jax.numpy.split` instead.\n\n## jax 0.4.4 (Feb 16, 2023)\n\n* Changes\n  * The implementation of `jit` and `pjit` has been merged. Merging jit and pjit\n    changes the internals of JAX without affecting the public API of JAX.\n    Before, `jit` was a final style primitive. Final style means that the creation\n    of jaxpr was delayed as much as possible and transformations were stacked\n    on top of each other. With the `jit`-`pjit` implementation merge, `jit`\n    becomes an initial style primitive which means that we trace to jaxpr\n    as early as possible. For more information see\n    [this section in autodidax](https://jax.readthedocs.io/en/latest/autodidax.html#on-the-fly-final-style-and-staged-initial-style-processing).\n    Moving to initial style should simplify JAX's internals and make\n    development of features like dynamic shapes, etc easier.\n    You can disable it only via the environment variable i.e.\n    `os.environ['JAX_JIT_PJIT_API_MERGE'] = '0'`.\n    The merge must be disabled via an environment variable since it affects JAX\n    at import time so it needs to be disabled before jax is imported.\n  * `axis_resources` argument of `with_sharding_constraint` is deprecated.\n    Please use `shardings` instead. There is no change needed if you were using\n    `axis_resources` as an arg. If you were using it as a kwarg, then please\n    use `shardings` instead. `axis_resources` will be removed after 3 months\n    from Feb 13, 2023.\n  * added the {mod}`jax.typing` module, with tools for type annotations of JAX\n    functions.\n  * The following names have been deprecated:\n    * `jax.xla.Device` and `jax.interpreters.xla.Device`: use `jax.Device`.\n    * `jax.experimental.maps.Mesh`. Use `jax.sharding.Mesh`\n    instead.\n    * `jax.experimental.pjit.NamedSharding`: use `jax.sharding.NamedSharding`.\n    * `jax.experimental.pjit.PartitionSpec`: use `jax.sharding.PartitionSpec`.\n    * `jax.interpreters.pxla.Mesh`: use `jax.sharding.Mesh`.\n    * `jax.interpreters.pxla.PartitionSpec`: use `jax.sharding.PartitionSpec`.\n* Breaking Changes\n  * the `initial` argument to reduction functions like :func:`jax.numpy.sum`\n    is now required to be a scalar, consistent with the corresponding NumPy API.\n    The previous behavior of broadcasting the output against non-scalar `initial`\n    values was an unintentional implementation detail ({jax-issue}`#14446`).\n\n## jaxlib 0.4.4 (Feb 16, 2023)\n  * Breaking changes\n    * Support for NVIDIA Kepler series GPUs has been removed from the default\n      `jaxlib` builds. If Kepler support is needed, it is still possible to\n      build `jaxlib` from source with Kepler support (via the\n      `--cuda_compute_capabilities=sm_35` option to `build.py`), however note\n      that CUDA 12 has completely dropped support for Kepler GPUs.\n\n## jax 0.4.3 (Feb 8, 2023)\n  * Breaking changes\n    * Deleted {func}`jax.scipy.linalg.polar_unitary`, which was a deprecated JAX\n      extension to the scipy API. Use {func}`jax.scipy.linalg.polar` instead.\n\n  * Changes\n    * Added {func}`jax.scipy.stats.rankdata`.\n\n## jaxlib 0.4.3 (Feb 8, 2023)\n  * `jax.Array` now has the non-blocking `is_ready()` method, which returns `True`\n    if the array is ready (see also {func}`jax.block_until_ready`).\n\n## jax 0.4.2 (Jan 24, 2023)\n\n* Breaking changes\n  * Deleted `jax.experimental.callback`\n  * Operations with dimensions in presence of jax2tf shape polymorphism have\n    been generalized to work in more scenarios, by converting the symbolic\n    dimension to JAX arrays. Operations involving symbolic dimensions and\n    `np.ndarray` now can raise errors when the result is used as a shape value\n    ({jax-issue}`#14106`).\n  * jaxpr objects now raise an error on attribute setting in order to avoid\n    problematic mutations ({jax-issue}`14102`)\n\n* Changes\n  * {func}`jax2tf.call_tf` has a new parameter `has_side_effects` (default `True`)\n    that can be used to declare whether an instance can be removed or replicated\n    by JAX optimizations such as dead-code elimination ({jax-issue}`#13980`).\n  * Added more support for floordiv and mod for jax2tf shape polymorphism. Previously,\n    certain division operations resulted in errors in presence of symbolic dimensions\n    ({jax-issue}`#14108`).\n\n## jaxlib 0.4.2 (Jan 24, 2023)\n\n* Changes\n  * Set JAX_USE_PJRT_C_API_ON_TPU=1 to enable new Cloud TPU runtime, featuring\n    automatic device memory defragmentation.\n\n## jax 0.4.1 (Dec 13, 2022)\n\n* Changes\n  * Support for Python 3.7 has been dropped, in accordance with JAX's\n    {ref}`version-support-policy`.\n  * We introduce `jax.Array` which is a unified array type that subsumes\n    `DeviceArray`, `ShardedDeviceArray`, and `GlobalDeviceArray` types in JAX.\n    The `jax.Array` type helps make parallelism a core feature of JAX,\n    simplifies and unifies JAX internals, and allows us to unify `jit` and\n    `pjit`.  `jax.Array` has been enabled by default in JAX 0.4 and makes some\n    breaking change to the `pjit` API.  The [jax.Array migration\n    guide](https://jax.readthedocs.io/en/latest/jax_array_migration.html) can\n    help you migrate your codebase to `jax.Array`. You can also look at the\n    [Distributed arrays and automatic parallelization](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html)\n    tutorial to understand the new concepts.\n  * `PartitionSpec` and `Mesh` are now out of experimental. The new API endpoints\n    are `jax.sharding.PartitionSpec` and `jax.sharding.Mesh`.\n    `jax.experimental.maps.Mesh` and `jax.experimental.PartitionSpec` are\n    deprecated and will be removed in 3 months.\n  * `with_sharding_constraint`s new public endpoint is\n    `jax.lax.with_sharding_constraint`.\n  * If using ABSL flags together with `jax.config`, the ABSL flag values are no\n    longer read or written after the JAX configuration options are initially\n    populated from the ABSL flags. This change improves performance of reading\n    `jax.config` options, which are used pervasively in JAX.\n  * The jax2tf.call_tf function now uses for TF lowering the first TF\n    device of the same platform as used by the embedding JAX computation.\n    Before, it was using the 0th device for the JAX-default backend.\n  * A number of `jax.numpy` functions now have their arguments marked as\n    positional-only, matching NumPy.\n  * `jnp.msort` is now deprecated, following the deprecation of `np.msort` in numpy 1.24.\n    It will be removed in a future release, in accordance with the {ref}`api-compatibility`\n    policy. It can be replaced with `jnp.sort(a, axis=0)`.\n\n## jaxlib 0.4.1 (Dec 13, 2022)\n\n* Changes\n  * Support for Python 3.7 has been dropped, in accordance with JAX's\n    {ref}`version-support-policy`.\n  * The behavior of `XLA_PYTHON_CLIENT_MEM_FRACTION=.XX` has been changed to allocate XX% of\n    the total GPU memory instead of the previous behavior of using currently available GPU memory\n    to calculate preallocation. Please refer to\n    [GPU memory allocation](https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html) for\n    more details.\n  * The deprecated method `.block_host_until_ready()` has been removed. Use\n    `.block_until_ready()` instead.\n\n## jax 0.4.0 (Dec 12, 2022)\n\n* The release was yanked.\n\n## jaxlib 0.4.0 (Dec 12, 2022)\n\n* The release was yanked.\n\n## jax 0.3.25 (Nov 15, 2022)\n* Changes\n  * {func}`jax.numpy.linalg.pinv` now supports the `hermitian` option.\n  * {func}`jax.scipy.linalg.hessenberg` is now supported on CPU only. Requires\n    jaxlib > 0.3.24.\n  * New functions {func}`jax.lax.linalg.hessenberg`,\n    {func}`jax.lax.linalg.tridiagonal`, and\n    {func}`jax.lax.linalg.householder_product` were added. Householder reduction\n    is currently CPU-only and tridiagonal reductions are supported on CPU and\n    GPU only.\n  * The gradients of `svd` and `jax.numpy.linalg.pinv` are now computed more\n    economically for non-square matrices.\n* Breaking Changes\n  * Deleted the `jax_experimental_name_stack` config option.\n  * Convert a string `axis_names` arguments to the\n    {class}`jax.experimental.maps.Mesh` constructor into a singleton tuple\n    instead of unpacking the string into a sequence of character axis names.\n\n## jaxlib 0.3.25 (Nov 15, 2022)\n* Changes\n  * Added support for tridiagonal reductions on CPU and GPU.\n  * Added support for upper Hessenberg reductions on CPU.\n* Bugs\n  * Fixed a bug that meant that frames in tracebacks captured by JAX were\n    incorrectly mapped to source lines under Python 3.10+\n\n## jax 0.3.24 (Nov 4, 2022)\n* Changes\n  * JAX should be faster to import. We now import scipy lazily, which accounted\n    for a significant fraction of JAX's import time.\n  * Setting the env var `JAX_PERSISTENT_CACHE_MIN_COMPILE_TIME_SECS=$N` can be\n    used to limit the number of cache entries written to the persistent cache.\n    By default, computations that take 1 second or more to compile will be\n    cached.\n    * Added {func}`jax.scipy.stats.mode`.\n  * The default device order used by `pmap` on TPU if no order is specified now\n    matches `jax.devices()` for single-process jobs. Previously the\n    two orderings differed, which could lead to unnecessary copies or\n    out-of-memory errors. Requiring the orderings to agree simplifies matters.\n* Breaking Changes\n    * {func}`jax.numpy.gradient` now behaves like most other functions in {mod}`jax.numpy`,\n      and forbids passing lists or tuples in place of arrays ({jax-issue}`#12958`)\n    * Functions in {mod}`jax.numpy.linalg` and {mod}`jax.numpy.fft` now uniformly\n      require inputs to be array-like: i.e. lists and tuples cannot be used in place\n      of arrays. Part of {jax-issue}`#7737`.\n* Deprecations\n  * `jax.sharding.MeshPspecSharding` has been renamed to `jax.sharding.NamedSharding`.\n    `jax.sharding.MeshPspecSharding` name will be removed in 3 months.\n\n## jaxlib 0.3.24 (Nov 4, 2022)\n* Changes\n  * Buffer donation now works on CPU. This may break code that marked buffers\n    for donation on CPU but relied on donation not being implemented.\n\n## jax 0.3.23 (Oct 12, 2022)\n* Changes\n  * Update Colab TPU driver version for new jaxlib release.\n\n## jax 0.3.22 (Oct 11, 2022)\n* Changes\n  * Add `JAX_PLATFORMS=tpu,cpu` as default setting in TPU initialization,\n  so JAX will raise an error if TPU cannot be initialized instead of falling\n  back to CPU. Set `JAX_PLATFORMS=''` to override this behavior and automatically\n  choose an available backend (the original default), or set `JAX_PLATFORMS=cpu`\n  to always use CPU regardless of if the TPU is available.\n* Deprecations\n  * Several test utilities deprecated in JAX v0.3.8 are now removed from\n    {mod}`jax.test_util`.\n\n## jaxlib 0.3.22 (Oct 11, 2022)\n\n## jax 0.3.21 (Sep 30, 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.3.20...jax-v0.3.21).\n* Changes\n  * The persistent compilation cache will now warn instead of raising an\n    exception on error ({jax-issue}`#12582`), so program execution can continue\n    if something goes wrong with the cache. Set\n    `JAX_RAISE_PERSISTENT_CACHE_ERRORS=true` to revert this behavior.\n\n## jax 0.3.20 (Sep 28, 2022)\n* Bug fixes:\n  * Adds missing `.pyi` files that were missing from the previous release ({jax-issue}`#12536`).\n  * Fixes an incompatibility between `jax` 0.3.19 and the libtpu version it pinned ({jax-issue}`#12550`). Requires jaxlib 0.3.20.\n  * Fix incorrect `pip` url in `setup.py` comment ({jax-issue}`#12528`).\n\n## jaxlib 0.3.20 (Sep 28, 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jaxlib-v0.3.15...jaxlib-v0.3.20).\n* Bug fixes\n  * Fixes support for limiting the visible CUDA devices via\n   `jax_cuda_visible_devices` in distributed jobs. This functionality is needed for\n   the JAX/SLURM integration on GPU ({jax-issue}`#12533`).\n\n## jax 0.3.19 (Sep 27, 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.3.18...jax-v0.3.19).\n* Fixes required jaxlib version.\n\n## jax 0.3.18 (Sep 26, 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.3.17...jax-v0.3.18).\n* Changes\n  * Ahead-of-time lowering and compilation functionality (tracked in\n    {jax-issue}`#7733`) is stable and public. See [the\n    overview](https://jax.readthedocs.io/en/latest/aot.html) and the API docs\n    for {mod}`jax.stages`.\n  * Introduced {class}`jax.Array`, intended to be used for both `isinstance` checks\n    and type annotations for array types in JAX. Notice that this included some subtle\n    changes to how `isinstance` works for {class}`jax.numpy.ndarray` for jax-internal\n    objects, as {class}`jax.numpy.ndarray` is now a simple alias of {class}`jax.Array`.\n* Breaking changes\n  * `jax._src` is no longer imported into the public `jax` namespace.\n    This may break users that were using JAX internals.\n  * `jax.soft_pmap` has been deleted. Please use `pjit` or `xmap` instead.\n    `jax.soft_pmap` is undocumented. If it were documented, a deprecation period\n    would have been provided.\n\n## jax 0.3.17 (Aug 31, 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.3.16...jax-v0.3.17).\n* Bugs\n  * Fix corner case issue in gradient of `lax.pow` with an exponent of zero\n    ({jax-issue}`12041`)\n* Breaking changes\n  * {func}`jax.checkpoint`, also known as {func}`jax.remat`, no longer supports\n    the `concrete` option, following the previous version's deprecation; see\n    [JEP 11830](https://jax.readthedocs.io/en/latest/jep/11830-new-remat-checkpoint.html).\n* Changes\n  * Added {func}`jax.pure_callback` that enables calling back to pure Python functions from compiled functions (e.g. functions decorated with `jax.jit` or `jax.pmap`).\n* Deprecations:\n  * The deprecated `DeviceArray.tile()` method has been removed. Use {func}`jax.numpy.tile`\n    ({jax-issue}`#11944`).\n  * `DeviceArray.to_py()` has been deprecated. Use `np.asarray(x)` instead.\n\n## jax 0.3.16\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.3.15...main).\n* Breaking changes\n  * Support for NumPy 1.19 has been dropped, per the\n    [deprecation policy](https://jax.readthedocs.io/en/latest/deprecation.html).\n    Please upgrade to NumPy 1.20 or newer.\n* Changes\n  * Added {mod}`jax.debug` that includes utilities for runtime value debugging such at {func}`jax.debug.print` and {func}`jax.debug.breakpoint`.\n  * Added new documentation for [runtime value debugging](debugging/index)\n* Deprecations\n  * {func}`jax.mask` {func}`jax.shapecheck` APIs have been removed.\n    See {jax-issue}`#11557`.\n  * {mod}`jax.experimental.loops` has been removed. See {jax-issue}`#10278`\n    for an alternative API.\n  * {func}`jax.tree_util.tree_multimap` has been removed. It has been deprecated since\n    JAX release 0.3.5, and {func}`jax.tree_util.tree_map` is a direct replacement.\n  * Removed `jax.experimental.stax`; it has long been a deprecated alias of\n    {mod}`jax.example_libraries.stax`.\n  * Removed `jax.experimental.optimizers`; it has long been a deprecated alias of\n    {mod}`jax.example_libraries.optimizers`.\n  * {func}`jax.checkpoint`, also known as {func}`jax.remat`, has a new\n    implementation switched on by default, meaning the old implementation is\n    deprecated; see [JEP 11830](https://jax.readthedocs.io/en/latest/jep/11830-new-remat-checkpoint.html).\n\n## jax 0.3.15 (July 22, 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.3.14...jax-v0.3.15).\n* Changes\n  * `JaxTestCase` and `JaxTestLoader` have been removed from `jax.test_util`. These\n    classes have been deprecated since v0.3.1 ({jax-issue}`#11248`).\n  * Added {class}`jax.scipy.gaussian_kde` ({jax-issue}`#11237`).\n  * Binary operations between JAX arrays and built-in collections (`dict`, `list`, `set`, `tuple`)\n    now raise a `TypeError` in all cases. Previously some cases (particularly equality and inequality)\n    would return boolean scalars inconsistent with similar operations in NumPy ({jax-issue}`#11234`).\n  * Several {mod}`jax.tree_util` routines accessed as top-level JAX package imports are now\n    deprecated, and will be removed in a future JAX release in accordance with the\n    {ref}`api-compatibility` policy:\n    * {func}`jax.treedef_is_leaf` is deprecated in favor of {func}`jax.tree_util.treedef_is_leaf`\n    * {func}`jax.tree_flatten` is deprecated in favor of {func}`jax.tree_util.tree_flatten`\n    * {func}`jax.tree_leaves` is deprecated in favor of {func}`jax.tree_util.tree_leaves`\n    * {func}`jax.tree_structure` is deprecated in favor of {func}`jax.tree_util.tree_structure`\n    * {func}`jax.tree_transpose` is deprecated in favor of {func}`jax.tree_util.tree_transpose`\n    * {func}`jax.tree_unflatten` is deprecated in favor of {func}`jax.tree_util.tree_unflatten`\n  * The `sym_pos` argument of {func}`jax.scipy.linalg.solve` is deprecated in favor of `assume_a='pos'`,\n    following a similar deprecation in {func}`scipy.linalg.solve`.\n\n## jaxlib 0.3.15 (July 22, 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jaxlib-v0.3.14...jaxlib-v0.3.15).\n\n## jax 0.3.14 (June 27, 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.3.13...jax-v0.3.14).\n* Breaking changes\n  * {func}`jax.experimental.compilation_cache.initialize_cache` does not support\n    `max_cache_size_  bytes` anymore and will not get that as an input.\n  * `JAX_PLATFORMS` now raises an exception when platform initialization fails.\n* Changes\n  * Fixed compatibility problems with NumPy 1.23.\n  * {func}`jax.numpy.linalg.slogdet` now accepts an optional `method` argument\n    that allows selection between an LU-decomposition based implementation and\n    an implementation based on QR decomposition.\n  * {func}`jax.numpy.linalg.qr` now supports `mode=\"raw\"`.\n  * `pickle`, `copy.copy`, and `copy.deepcopy` now have more complete support when\n    used on jax arrays ({jax-issue}`#10659`). In particular:\n    - `pickle` and `deepcopy` previously returned `np.ndarray` objects when used\n      on a `DeviceArray`; now `DeviceArray` objects are returned. For `deepcopy`,\n      the copied array is on the same device as the original. For `pickle` the\n      deserialized array will be on the default device.\n    - Within function transformations (i.e. traced code), `deepcopy` and `copy`\n      previously were no-ops. Now they use the same mechanism as `DeviceArray.copy()`.\n    - Calling `pickle` on a traced array now results in an explicit\n      `ConcretizationTypeError`.\n  * The implementation of singular value decomposition (SVD) and\n    symmetric/Hermitian eigendecomposition should be significantly faster on\n    TPU, especially for matrices above 1000x1000 or so. Both now use a spectral\n    divide-and-conquer algorithm for eigendecomposition (QDWH-eig).\n  * {func}`jax.numpy.ldexp` no longer silently promotes all inputs to float64,\n    instead it promotes to float32 for integer inputs of size int32 or smaller\n    ({jax-issue}`#10921`).\n  * Add a `create_perfetto_link` option to {func}`jax.profiler.start_trace` and\n    {func}`jax.profiler.start_trace`. When used, the profiler will generate a\n    link to the Perfetto UI to view the trace.\n  * Changed the semantics of {func}`jax.profiler.start_server(...)` to store the\n    keepalive globally, rather than requiring the user to keep a reference to\n    it.\n  * Added {func}`jax.random.generalized_normal`.\n  * Added {func}`jax.random.ball`.\n  * Added {func}`jax.default_device`.\n  * Added a `python -m jax.collect_profile` script to manually capture program\n    traces as an alternative to the TensorBoard UI.\n  * Added a `jax.named_scope` context manager that adds profiler metadata to\n    Python programs (similar to `jax.named_call`).\n  * In scatter-update operations (i.e. :attr:`jax.numpy.ndarray.at`), unsafe implicit\n    dtype casts are deprecated, and now result in a `FutureWarning`.\n    In a future release, this will become an error. An example of an unsafe implicit\n    cast is `jnp.zeros(4, dtype=int).at[0].set(1.5)`, in which `1.5` previously was\n    silently truncated to `1`.\n  * {func}`jax.experimental.compilation_cache.initialize_cache` now supports gcs\n    bucket path as input.\n  * Added {func}`jax.scipy.stats.gennorm`.\n  * {func}`jax.numpy.roots` is now better behaved when `strip_zeros=False` when\n    coefficients have leading zeros ({jax-issue}`#11215`).\n\n## jaxlib 0.3.14 (June 27, 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jaxlib-v0.3.10...jaxlib-v0.3.14).\n  * x86-64 Mac wheels now require Mac OS 10.14 (Mojave) or newer. Mac OS 10.14\n    was released in 2018, so this should not be a very onerous requirement.\n  * The bundled version of NCCL was updated to 2.12.12, fixing some deadlocks.\n  * The Python flatbuffers package is no longer a dependency of jaxlib.\n\n## jax 0.3.13 (May 16, 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.3.12...jax-v0.3.13).\n\n## jax 0.3.12 (May 15, 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.3.11...jax-v0.3.12).\n* Changes\n  * Fixes [#10717](https://github.com/jax-ml/jax/issues/10717).\n\n## jax 0.3.11 (May 15, 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.3.10...jax-v0.3.11).\n* Changes\n  * {func}`jax.lax.eigh` now accepts an optional `sort_eigenvalues` argument\n    that allows users to opt out of eigenvalue sorting on TPU.\n* Deprecations\n  * Non-array arguments to functions in {mod}`jax.lax.linalg` are now marked\n    keyword-only. As a backward-compatibility step passing keyword-only\n    arguments positionally yields a warning, but in a future JAX release passing\n    keyword-only arguments positionally will fail.\n    However, most users should prefer to use {mod}`jax.numpy.linalg` instead.\n  * {func}`jax.scipy.linalg.polar_unitary`, which was a JAX extension to the\n    scipy API, is deprecated. Use {func}`jax.scipy.linalg.polar` instead.\n\n## jax 0.3.10 (May 3, 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.3.9...jax-v0.3.10).\n\n## jaxlib 0.3.10 (May 3, 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jaxlib-v0.3.7...jaxlib-v0.3.10).\n* Changes\n  * [TF commit](https://github.com/tensorflow/tensorflow/commit/207d50d253e11c3a3430a700af478a1d524a779a)\n    fixes an issue in the MHLO canonicalizer that caused constant folding to\n    take a long time or crash for certain programs.\n\n## jax 0.3.9 (May 2, 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.3.8...jax-v0.3.9).\n* Changes\n  * Added support for fully asynchronous checkpointing for GlobalDeviceArray.\n\n## jax 0.3.8 (April 29 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.3.7...jax-v0.3.8).\n* Changes\n  * {func}`jax.numpy.linalg.svd` on TPUs uses a qdwh-svd solver.\n  * {func}`jax.numpy.linalg.cond` on TPUs now accepts complex input.\n  * {func}`jax.numpy.linalg.pinv` on TPUs now accepts complex input.\n  * {func}`jax.numpy.linalg.matrix_rank` on TPUs now accepts complex input.\n  * {func}`jax.scipy.cluster.vq.vq` has been added.\n  * `jax.experimental.maps.mesh` has been deleted.\n    Please use `jax.experimental.maps.Mesh`. Please see https://jax.readthedocs.io/en/latest/_autosummary/jax.experimental.maps.Mesh.html#jax.experimental.maps.Mesh\n    for more information.\n  * {func}`jax.scipy.linalg.qr` now returns a length-1 tuple rather than the raw array when\n    `mode='r'`, in order to match the behavior of `scipy.linalg.qr` ({jax-issue}`#10452`)\n  * {func}`jax.numpy.take_along_axis` now takes an optional `mode` parameter\n    that specifies the behavior of out-of-bounds indexing. By default,\n    invalid values (e.g., NaN) will be returned for out-of-bounds indices. In\n    previous versions of JAX, invalid indices were clamped into range. The\n    previous behavior can be restored by passing `mode=\"clip\"`.\n  * {func}`jax.numpy.take` now defaults to `mode=\"fill\"`, which returns\n    invalid values (e.g., NaN) for out-of-bounds indices.\n  * Scatter operations, such as `x.at[...].set(...)`, now have `\"drop\"` semantics.\n    This has no effect on the scatter operation itself, but it means that when\n    differentiated the gradient of a scatter will yield zero cotangents for\n    out-of-bounds indices. Previously out-of-bounds indices were clamped into\n    range for the gradient, which was not mathematically correct.\n  * {func}`jax.numpy.take_along_axis` now raises a `TypeError` if its indices\n    are not of an integer type, matching the behavior of\n    {func}`numpy.take_along_axis`. Previously non-integer indices were silently\n    cast to integers.\n  * {func}`jax.numpy.ravel_multi_index` now raises a `TypeError` if its `dims` argument\n    is not of an integer type, matching the behavior of\n    {func}`numpy.ravel_multi_index`. Previously non-integer `dims` was silently\n    cast to integers.\n  * {func}`jax.numpy.split` now raises a `TypeError` if its `axis` argument\n    is not of an integer type, matching the behavior of\n    {func}`numpy.split`. Previously non-integer `axis` was silently\n    cast to integers.\n  * {func}`jax.numpy.indices` now raises a `TypeError` if its dimensions\n    are not of an integer type, matching the behavior of\n    {func}`numpy.indices`. Previously non-integer dimensions were silently\n    cast to integers.\n  * {func}`jax.numpy.diag` now raises a `TypeError` if its `k` argument\n    is not of an integer type, matching the behavior of\n    {func}`numpy.diag`. Previously non-integer `k` was silently\n    cast to integers.\n  * Added {func}`jax.random.orthogonal`.\n* Deprecations\n  * Many functions and objects available in {mod}`jax.test_util` are now deprecated and will raise a\n    warning on import. This includes `cases_from_list`, `check_close`, `check_eq`, `device_under_test`,\n    `format_shape_dtype_string`, `rand_uniform`, `skip_on_devices`, `with_config`, `xla_bridge`, and\n    `_default_tolerance` ({jax-issue}`#10389`). These, along with previously-deprecated `JaxTestCase`,\n    `JaxTestLoader`, and `BufferDonationTestCase`, will be removed in a future JAX release.\n    Most of these utilities can be replaced by calls to standard python & numpy testing utilities found\n    in e.g.  {mod}`unittest`, {mod}`absl.testing`, {mod}`numpy.testing`, etc. JAX-specific functionality\n    such as device checking can be replaced through the use of public APIs such as {func}`jax.devices`.\n    Many of the deprecated utilities will still exist in {mod}`jax._src.test_util`, but these are not\n    public APIs and as such may be changed or removed without notice in future releases.\n\n## jax 0.3.7 (April 15, 2022)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.3.6...jax-v0.3.7).\n* Changes:\n  * Fixed a performance problem if the indices passed to\n    {func}`jax.numpy.take_along_axis` were broadcasted ({jax-issue}`#10281`).\n  * {func}`jax.scipy.special.expit` and {func}`jax.scipy.special.logit` now\n    require their arguments to be scalars or JAX arrays. They also now promote\n    integer arguments to floating point.\n  * The `DeviceArray.tile()` method is deprecated, because numpy arrays do not have a\n    `tile()` method. As a replacement for this, use {func}`jax.numpy.tile`\n    ({jax-issue}`#10266`).\n\n## jaxlib 0.3.7 (April 15, 2022)\n* Changes:\n  * Linux wheels are now built conforming to the `manylinux2014` standard, instead\n    of `manylinux2010`.\n\n## jax 0.3.6 (April 12, 2022)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.3.5...jax-v0.3.6).\n* Changes:\n  * Upgraded libtpu wheel to a version that fixes a hang when initializing a TPU\n    pod. Fixes [#10218](https://github.com/jax-ml/jax/issues/10218).\n* Deprecations:\n  * {mod}`jax.experimental.loops` is being deprecated. See {jax-issue}`#10278`\n    for an alternative API.\n\n## jax 0.3.5 (April 7, 2022)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.3.4...jax-v0.3.5).\n* Changes:\n  * added {func}`jax.random.loggamma` & improved behavior of {func}`jax.random.beta`\n    and {func}`jax.random.dirichlet` for small parameter values ({jax-issue}`#9906`).\n  * the private `lax_numpy` submodule is no longer exposed in the `jax.numpy` namespace ({jax-issue}`#10029`).\n  * added array creation routines {func}`jax.numpy.frombuffer`, {func}`jax.numpy.fromfunction`,\n    and {func}`jax.numpy.fromstring` ({jax-issue}`#10049`).\n  * `DeviceArray.copy()` now returns a `DeviceArray` rather than a `np.ndarray` ({jax-issue}`#10069`)\n  * added {func}`jax.scipy.linalg.rsf2csf`\n  * `jax.experimental.sharded_jit` has been deprecated and will be removed soon.\n* Deprecations:\n  * {func}`jax.nn.normalize` is being deprecated. Use {func}`jax.nn.standardize` instead ({jax-issue}`#9899`).\n  * {func}`jax.tree_util.tree_multimap` is deprecated. Use {func}`jax.tree_util.tree_map` instead ({jax-issue}`#5746`).\n  * `jax.experimental.sharded_jit` is deprecated. Use `pjit` instead.\n\n## jaxlib 0.3.5 (April 7, 2022)\n* Bug fixes\n  * Fixed a bug where double-precision complex-to-real IRFFTs would mutate their\n    input buffers on GPU ({jax-issue}`#9946`).\n  * Fixed incorrect constant-folding of complex scatters ({jax-issue}`#10159`)\n\n## jax 0.3.4 (March 18, 2022)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.3.3...jax-v0.3.4).\n\n\n## jax 0.3.3 (March 17, 2022)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.3.2...jax-v0.3.3).\n\n\n## jax 0.3.2 (March 16, 2022)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.3.1...jax-v0.3.2).\n* Changes:\n  * The functions `jax.ops.index_update`, `jax.ops.index_add`, which were\n    deprecated in 0.2.22, have been removed. Please use\n    [the `.at` property on JAX arrays](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html)\n    instead, e.g., `x.at[idx].set(y)`.\n  * Moved `jax.experimental.ann.approx_*_k` into `jax.lax`. These functions are\n    optimized alternatives to `jax.lax.top_k`.\n  * {func}`jax.numpy.broadcast_arrays` and {func}`jax.numpy.broadcast_to` now require scalar\n    or array-like inputs, and will fail if they are passed lists (part of {jax-issue}`#7737`).\n  * The standard jax[tpu] install can now be used with Cloud TPU v4 VMs.\n  * `pjit` now works on CPU (in addition to previous TPU and GPU support).\n\n\n## jaxlib 0.3.2 (March 16, 2022)\n* Changes\n  * ``XlaComputation.as_hlo_text()`` now supports printing large constants by\n    passing boolean flag ``print_large_constants=True``.\n* Deprecations:\n  * The ``.block_host_until_ready()`` method on JAX arrays has been deprecated.\n    Use ``.block_until_ready()`` instead.\n\n## jax 0.3.1 (Feb 18, 2022)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.3.0...jax-v0.3.1).\n\n* Changes:\n  * `jax.test_util.JaxTestCase` and `jax.test_util.JaxTestLoader` are now deprecated.\n    The suggested replacement is to use `parametrized.TestCase` directly. For tests that\n    rely on custom asserts such as `JaxTestCase.assertAllClose()`, the suggested replacement\n    is to use standard numpy testing utilities such as {func}`numpy.testing.assert_allclose()`,\n    which work directly with JAX arrays ({jax-issue}`#9620`).\n  * `jax.test_util.JaxTestCase` now sets `jax_numpy_rank_promotion='raise'` by default\n    ({jax-issue}`#9562`). To recover the previous behavior, use the new\n    `jax.test_util.with_config` decorator:\n    ```python\n    @jtu.with_config(jax_numpy_rank_promotion='allow')\n    class MyTestCase(jtu.JaxTestCase):\n      ...\n    ```\n  * Added {func}`jax.scipy.linalg.schur`, {func}`jax.scipy.linalg.sqrtm`,\n    {func}`jax.scipy.signal.csd`, {func}`jax.scipy.signal.stft`,\n    {func}`jax.scipy.signal.welch`.\n\n\n## jax 0.3.0 (Feb 10, 2022)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.2.28...jax-v0.3.0).\n\n* Changes\n  * jax version has been bumped to 0.3.0. Please see the [design doc](https://jax.readthedocs.io/en/latest/design_notes/jax_versioning.html)\n    for the explanation.\n\n## jaxlib 0.3.0 (Feb 10, 2022)\n* Changes\n  * Bazel 5.0.0 is now required to build jaxlib.\n  * jaxlib version has been bumped to 0.3.0. Please see the [design doc](https://jax.readthedocs.io/en/latest/design_notes/jax_versioning.html)\n    for the explanation.\n\n## jax 0.2.28 (Feb 1, 2022)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.2.27...jax-v0.2.28).\n  * `jax.jit(f).lower(...).compiler_ir()` now defaults to the MHLO dialect if no\n    `dialect=` is passed.\n  * The `jax.jit(f).lower(...).compiler_ir(dialect='mhlo')` now returns an MLIR\n    `ir.Module` object instead of its string representation.\n\n## jaxlib 0.1.76 (Jan 27, 2022)\n\n* New features\n  * Includes precompiled SASS for NVidia compute capability 8.0 GPUS\n    (e.g. A100). Removes precompiled SASS for compute capability 6.1 so as not\n    to increase the number of compute capabilities: GPUs with compute capability\n    6.1 can use the 6.0 SASS.\n  * With jaxlib 0.1.76, JAX uses the MHLO MLIR dialect as its primary target compiler IR\n    by default.\n* Breaking changes\n  * Support for NumPy 1.18 has been dropped, per the\n    [deprecation policy](https://jax.readthedocs.io/en/latest/deprecation.html).\n    Please upgrade to a supported NumPy version.\n* Bug fixes\n  * Fixed a bug where apparently identical pytreedef objects constructed by different routes\n    do not compare as equal (#9066).\n  * The JAX jit cache requires two static arguments to have identical types for a cache hit (#9311).\n\n## jax 0.2.27 (Jan 18 2022)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.26...jax-v0.2.27).\n\n* Breaking changes:\n  * Support for NumPy 1.18 has been dropped, per the\n    [deprecation policy](https://jax.readthedocs.io/en/latest/deprecation.html).\n    Please upgrade to a supported NumPy version.\n  * The host_callback primitives have been simplified to drop the\n    special autodiff handling for hcb.id_tap and id_print.\n    From now on, only the primals are tapped. The old behavior can be\n    obtained (for a limited time) by setting the ``JAX_HOST_CALLBACK_AD_TRANSFORMS``\n    environment variable, or the ```--jax_host_callback_ad_transforms``` flag.\n    Additionally, added documentation for how to implement the old behavior\n    using JAX custom AD APIs ({jax-issue}`#8678`).\n  * Sorting now matches the behavior of NumPy for ``0.0`` and ``NaN`` regardless of the\n    bit representation. In particular, ``0.0`` and ``-0.0`` are now treated as equivalent,\n    where previously ``-0.0`` was treated as less than ``0.0``. Additionally all ``NaN``\n    representations are now treated as equivalent and sorted to the end of the array.\n    Previously negative ``NaN`` values were sorted to the front of the array, and ``NaN``\n    values with different internal bit representations were not treated as equivalent, and\n    were sorted according to those bit patterns ({jax-issue}`#9178`).\n  * {func}`jax.numpy.unique` now treats ``NaN`` values in the same way as `np.unique` in\n    NumPy versions 1.21 and newer: at most one ``NaN`` value will appear in the uniquified\n    output ({jax-issue}`9184`).\n\n* Bug fixes:\n  * host_callback now supports ad_checkpoint.checkpoint ({jax-issue}`#8907`).\n\n* New features:\n  * add `jax.block_until_ready` ({jax-issue}`#8941)\n  * Added a new debugging flag/environment variable `JAX_DUMP_IR_TO=/path`.\n    If set, JAX dumps the MHLO/HLO IR it generates for each computation to a\n    file under the given path.\n  * Added `jax.ensure_compile_time_eval` to the public api ({jax-issue}`#7987`).\n  * jax2tf now supports a flag jax2tf_associative_scan_reductions to change\n    the lowering for associative reductions, e.g., jnp.cumsum, to behave\n    like JAX on CPU and GPU (to use an associative scan). See the jax2tf README\n    for more details ({jax-issue}`#9189`).\n\n\n## jaxlib 0.1.75 (Dec 8, 2021)\n* New features:\n  * Support for python 3.10.\n\n## jax 0.2.26 (Dec 8, 2021)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.2.25...jax-v0.2.26).\n\n* Bug fixes:\n  * Out-of-bounds indices to `jax.ops.segment_sum` will now be handled with\n    `FILL_OR_DROP` semantics, as documented. This primarily affects the\n    reverse-mode derivative, where gradients corresponding to out-of-bounds\n    indices will now be returned as 0. (#8634).\n  * jax2tf will force the converted code to use XLA for the code fragments\n    under jax.jit, e.g., most jax.numpy functions ({jax-issue}`#7839`).\n\n## jaxlib 0.1.74 (Nov 17, 2021)\n* Enabled peer-to-peer copies between GPUs. Previously, GPU copies were bounced via\n  the host, which is usually slower.\n* Added experimental MLIR Python bindings for use by JAX.\n\n## jax 0.2.25 (Nov 10, 2021)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.2.24...jax-v0.2.25).\n\n* New features:\n  * (Experimental) `jax.distributed.initialize` exposes multi-host GPU backend.\n  * `jax.random.permutation` supports new `independent` keyword argument\n    ({jax-issue}`#8430`)\n* Breaking changes\n  * Moved `jax.experimental.stax` to `jax.example_libraries.stax`\n  * Moved `jax.experimental.optimizers` to `jax.example_libraries.optimizers`\n* New features:\n  * Added `jax.lax.linalg.qdwh`.\n\n## jax 0.2.24 (Oct 19, 2021)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.2.22...jax-v0.2.24).\n\n* New features:\n  * `jax.random.choice` and `jax.random.permutation` now support\n    multidimensional arrays and an optional `axis` argument ({jax-issue}`#8158`)\n* Breaking changes:\n  * `jax.numpy.take` and `jax.numpy.take_along_axis` now require array-like inputs\n    (see {jax-issue}`#7737`)\n\n## jaxlib 0.1.73 (Oct 18, 2021)\n\n* Multiple cuDNN versions are now supported for jaxlib GPU `cuda11` wheels.\n  * cuDNN 8.2 or newer. We recommend using the cuDNN 8.2 wheel if your cuDNN\n    installation is new enough, since it supports additional functionality.\n  * cuDNN 8.0.5 or newer.\n\n* Breaking changes:\n  * The install commands for GPU jaxlib are as follows:\n\n    ```bash\n    pip install --upgrade pip\n\n    # Installs the wheel compatible with CUDA 11 and cuDNN 8.2 or newer.\n    pip install --upgrade \"jax[cuda]\" -f https://storage.googleapis.com/jax-releases/jax_releases.html\n\n    # Installs the wheel compatible with Cuda 11 and cudnn 8.2 or newer.\n    pip install jax[cuda11_cudnn82] -f https://storage.googleapis.com/jax-releases/jax_releases.html\n\n    # Installs the wheel compatible with Cuda 11 and cudnn 8.0.5 or newer.\n    pip install jax[cuda11_cudnn805] -f https://storage.googleapis.com/jax-releases/jax_releases.html\n    ```\n\n## jax 0.2.22 (Oct 12, 2021)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.2.21...jax-v0.2.22).\n* Breaking Changes\n  * Static arguments to `jax.pmap` must now be hashable.\n\n    Unhashable static arguments have long been disallowed on `jax.jit`, but they\n    were still permitted on `jax.pmap`; `jax.pmap` compared unhashable static\n    arguments using object identity.\n\n    This behavior is a footgun, since comparing arguments using\n    object identity leads to recompilation each time the object identity\n    changes. Instead, we now ban unhashable arguments: if a user of `jax.pmap`\n    wants to compare static arguments by object identity, they can define\n    `__hash__` and `__eq__` methods on their objects that do that, or wrap their\n    objects in an object that has those operations with object identity\n    semantics. Another option is to use `functools.partial` to encapsulate the\n    unhashable static arguments into the function object.\n  * `jax.util.partial` was an accidental export that has now been removed. Use\n    `functools.partial` from the Python standard library instead.\n* Deprecations\n  * The functions `jax.ops.index_update`, `jax.ops.index_add` etc. are\n    deprecated and will be removed in a future JAX release. Please use\n    [the `.at` property on JAX arrays](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html)\n    instead, e.g., `x.at[idx].set(y)`. For now, these functions produce a\n    `DeprecationWarning`.\n* New features:\n  * An optimized C++ code-path improving the dispatch time for `pmap` is now the\n    default when using jaxlib 0.1.72 or newer. The feature can be disabled using\n    the `--experimental_cpp_pmap` flag (or `JAX_CPP_PMAP` environment variable).\n  * `jax.numpy.unique` now supports an optional `fill_value` argument ({jax-issue}`#8121`)\n\n## jaxlib 0.1.72 (Oct 12, 2021)\n  * Breaking changes:\n    * Support for CUDA 10.2 and CUDA 10.1 has been dropped. Jaxlib now supports\n      CUDA 11.1+.\n  * Bug fixes:\n    * Fixes https://github.com/jax-ml/jax/issues/7461, which caused wrong\n      outputs on all platforms due to incorrect buffer aliasing inside the XLA\n      compiler.\n\n## jax 0.2.21 (Sept 23, 2021)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.2.20...jax-v0.2.21).\n* Breaking Changes\n  * `jax.api` has been removed. Functions that were available as `jax.api.*`\n    were aliases for functions in `jax.*`; please use the functions in\n    `jax.*` instead.\n  * `jax.partial`, and `jax.lax.partial` were accidental exports that have now\n    been removed. Use `functools.partial` from the Python standard library\n    instead.\n  * Boolean scalar indices now raise a `TypeError`; previously this silently\n    returned wrong results ({jax-issue}`#7925`).\n  * Many more `jax.numpy` functions now require array-like inputs, and will error\n    if passed a list ({jax-issue}`#7747` {jax-issue}`#7802` {jax-issue}`#7907`).\n    See {jax-issue}`#7737` for a discussion of the rationale behind this change.\n  * When inside a transformation such as `jax.jit`, `jax.numpy.array` always\n    stages the array it produces into the traced computation. Previously\n    `jax.numpy.array` would sometimes produce a on-device array, even under\n    a `jax.jit` decorator. This change may break code that used JAX arrays to\n    perform shape or index computations that must be known statically; the\n    workaround is to perform such computations using classic NumPy arrays\n    instead.\n  * `jnp.ndarray` is now a true base-class for JAX arrays. In particular, this\n    means that for a standard numpy array `x`, `isinstance(x, jnp.ndarray)` will\n    now return `False` ({jax-issue}`7927`).\n* New features:\n  * Added {func}`jax.numpy.insert` implementation ({jax-issue}`#7936`).\n\n## jax 0.2.20 (Sept 2, 2021)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.2.19...jax-v0.2.20).\n* Breaking Changes\n  * `jnp.poly*` functions now require array-like inputs ({jax-issue}`#7732`)\n  * `jnp.unique` and other set-like operations now require array-like inputs\n    ({jax-issue}`#7662`)\n\n## jaxlib 0.1.71 (Sep 1, 2021)\n* Breaking changes:\n  * Support for CUDA 11.0 and CUDA 10.1 has been dropped. Jaxlib now supports\n    CUDA 10.2 and CUDA 11.1+.\n\n## jax 0.2.19 (Aug 12, 2021)\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.2.18...jax-v0.2.19).\n* Breaking changes:\n  * Support for NumPy 1.17 has been dropped, per the\n    [deprecation policy](https://jax.readthedocs.io/en/latest/deprecation.html).\n    Please upgrade to a supported NumPy version.\n  * The `jit` decorator has been added around the implementation of a number of\n    operators on JAX arrays. This speeds up dispatch times for common\n    operators such as `+`.\n\n    This change should largely be transparent to most users. However, there is\n    one known behavioral change, which is that large integer constants may now\n    produce an error when passed directly to a JAX operator\n    (e.g., `x + 2**40`). The workaround is to cast the constant to an\n    explicit type (e.g., `np.float64(2**40)`).\n* New features:\n  * Improved the support for shape polymorphism in jax2tf for operations that\n    need to use a dimension size in array computation, e.g., `jnp.mean`.\n    ({jax-issue}`#7317`)\n* Bug fixes:\n  * Some leaked trace errors from the previous release ({jax-issue}`#7613`)\n\n## jaxlib 0.1.70 (Aug 9, 2021)\n* Breaking changes:\n  * Support for Python 3.6 has been dropped, per the\n    [deprecation policy](https://jax.readthedocs.io/en/latest/deprecation.html).\n    Please upgrade to a supported Python version.\n  * Support for NumPy 1.17 has been dropped, per the\n    [deprecation policy](https://jax.readthedocs.io/en/latest/deprecation.html).\n    Please upgrade to a supported NumPy version.\n\n  * The host_callback mechanism now uses one thread per local device for\n    making the calls to the Python callbacks. Previously there was a single\n    thread for all devices. This means that the callbacks may now be called\n    interleaved. The callbacks corresponding to one device will still be\n    called in sequence.\n\n## jax 0.2.18 (July 21 2021)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.17...jax-v0.2.18).\n\n* Breaking changes:\n  * Support for Python 3.6 has been dropped, per the\n    [deprecation policy](https://jax.readthedocs.io/en/latest/deprecation.html).\n    Please upgrade to a supported Python version.\n  * The minimum jaxlib version is now 0.1.69.\n  * The `backend` argument to {py:func}`jax.dlpack.from_dlpack` has been\n    removed.\n\n* New features:\n  * Added a polar decomposition ({py:func}`jax.scipy.linalg.polar`).\n\n* Bug fixes:\n  * Tightened the checks for lax.argmin and lax.argmax to ensure they are\n    not used with an invalid `axis` value, or with an empty reduction dimension.\n    ({jax-issue}`#7196`)\n\n\n## jaxlib 0.1.69 (July 9 2021)\n* Fix bugs in TFRT CPU backend that results in incorrect results.\n\n## jax 0.2.17 (July 9 2021)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.16...jax-v0.2.17).\n* Bug fixes:\n  * Default to the older \"stream_executor\" CPU runtime for jaxlib <= 0.1.68\n    to work around #7229, which caused wrong outputs on CPU due to a concurrency\n    problem.\n* New features:\n  * New SciPy function {py:func}`jax.scipy.special.sph_harm`.\n  * Reverse-mode autodiff functions ({func}`jax.grad`,\n    {func}`jax.value_and_grad`, {func}`jax.vjp`, and\n    {func}`jax.linear_transpose`) support a parameter that indicates which named\n    axes should be summed over in the backward pass if they were broadcasted\n    over in the forward pass. This enables use of these APIs in a\n    non-per-example way inside maps (initially only\n    {func}`jax.experimental.maps.xmap`) ({jax-issue}`#6950`).\n\n\n## jax 0.2.16 (June 23 2021)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.15...jax-v0.2.16).\n\n## jax 0.2.15 (June 23 2021)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.14...jax-v0.2.15).\n* New features:\n  * [#7042](https://github.com/jax-ml/jax/pull/7042) Turned on TFRT CPU backend\n    with significant dispatch performance improvements on CPU.\n  * The {func}`jax2tf.convert` supports inequalities and min/max for booleans\n    ({jax-issue}`#6956`).\n  * New SciPy function {py:func}`jax.scipy.special.lpmn_values`.\n\n* Breaking changes:\n  * Support for NumPy 1.16 has been dropped, per the\n    [deprecation policy](https://jax.readthedocs.io/en/latest/deprecation.html).\n\n* Bug fixes:\n  * Fixed bug that prevented round-tripping from JAX to TF and back:\n    `jax2tf.call_tf(jax2tf.convert)` ({jax-issue}`#6947`).\n\n## jaxlib 0.1.68 (June 23 2021)\n* Bug fixes:\n  * Fixed bug in TFRT CPU backend that gets nans when transfer TPU buffer to\n    CPU.\n\n## jax 0.2.14 (June 10 2021)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.13...jax-v0.2.14).\n* New features:\n  * The {func}`jax2tf.convert` now has support for `pjit` and `sharded_jit`.\n  * A new configuration option JAX_TRACEBACK_FILTERING controls how JAX filters\n    tracebacks.\n  * A new traceback filtering mode using `__tracebackhide__` is now enabled by\n    default in sufficiently recent versions of IPython.\n  * The {func}`jax2tf.convert` supports shape polymorphism even when the\n    unknown dimensions are used in arithmetic operations, e.g., `jnp.reshape(-1)`\n    ({jax-issue}`#6827`).\n  * The {func}`jax2tf.convert` generates custom attributes with location information\n   in TF ops. The code that XLA generates after jax2tf\n   has the same location information as JAX/XLA.\n  * New SciPy function {py:func}`jax.scipy.special.lpmn`.\n\n* Bug fixes:\n  * The {func}`jax2tf.convert` now ensures that it uses the same typing rules\n    for Python scalars and for choosing 32-bit vs. 64-bit computations\n    as JAX ({jax-issue}`#6883`).\n  * The {func}`jax2tf.convert` now scopes the `enable_xla` conversion parameter\n    properly to apply only during the just-in-time conversion\n    ({jax-issue}`#6720`).\n  * The {func}`jax2tf.convert` now converts `lax.dot_general` using the\n    `XlaDot` TensorFlow op, for better fidelity w.r.t. JAX numerical precision\n    ({jax-issue}`#6717`).\n  * The {func}`jax2tf.convert` now has support for inequality comparisons and\n    min/max for complex numbers ({jax-issue}`#6892`).\n\n## jaxlib 0.1.67 (May 17 2021)\n\n## jaxlib 0.1.66 (May 11 2021)\n\n* New features:\n  * CUDA 11.1 wheels are now supported on all CUDA 11 versions 11.1 or higher.\n\n    NVidia now promises compatibility between CUDA minor releases starting with\n    CUDA 11.1. This means that JAX can release a single CUDA 11.1 wheel that\n    is compatible with CUDA 11.2 and 11.3.\n\n    There is no longer a separate jaxlib release for CUDA 11.2 (or higher); use\n    the CUDA 11.1 wheel for those versions (cuda111).\n  * Jaxlib now bundles `libdevice.10.bc` in CUDA wheels. There should be no need\n    to point JAX to a CUDA installation to find this file.\n  * Added automatic support for static keyword arguments to the {func}`jit`\n    implementation.\n  * Added support for pretransformation exception traces.\n  * Initial support for pruning unused arguments from {func}`jit` -transformed\n    computations.\n    Pruning is still a work in progress.\n  * Improved the string representation of {class}`PyTreeDef` objects.\n  * Added support for XLA's variadic ReduceWindow.\n* Bug fixes:\n  * Fixed a bug in the remote cloud TPU support when large numbers of arguments\n    are passed to a computation.\n  * Fix a bug that meant that JAX garbage collection was not triggered by\n    {func}`jit` transformed functions.\n\n## jax 0.2.13 (May 3 2021)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.12...jax-v0.2.13).\n* New features:\n  * When combined with jaxlib 0.1.66, {func}`jax.jit` now supports static\n    keyword arguments. A new `static_argnames` option has been added to specify\n    keyword arguments as static.\n  * {func}`jax.nonzero` has a new optional `size` argument that allows it to\n    be used within `jit` ({jax-issue}`#6501`)\n  * {func}`jax.numpy.unique` now supports the `axis` argument ({jax-issue}`#6532`).\n  * {func}`jax.experimental.host_callback.call` now supports `pjit.pjit` ({jax-issue}`#6569`).\n  * Added {func}`jax.scipy.linalg.eigh_tridiagonal` that computes the\n    eigenvalues of a tridiagonal matrix. Only eigenvalues are supported at\n    present.\n  * The order of the filtered and unfiltered stack traces in exceptions has been\n    changed. The traceback attached to an exception thrown from JAX-transformed\n    code is now filtered, with an `UnfilteredStackTrace` exception\n    containing the original trace as the `__cause__` of the filtered exception.\n    Filtered stack traces now also work with Python 3.6.\n  * If an exception is thrown by code that has been transformed by reverse-mode\n    automatic differentiation, JAX now attempts to attach as a `__cause__` of\n    the exception a `JaxStackTraceBeforeTransformation` object that contains the\n    stack trace that created the original operation in the forward pass.\n    Requires jaxlib 0.1.66.\n\n* Breaking changes:\n  * The following function names have changed. There are still aliases, so this\n    should not break existing code, but the aliases will eventually be removed\n    so please change your code.\n    * `host_id` --> {func}`~jax.process_index`\n    * `host_count` --> {func}`~jax.process_count`\n    * `host_ids` --> `range(jax.process_count())`\n  * Similarly, the argument to {func}`~jax.local_devices` has been renamed from\n    `host_id` to `process_index`.\n  * Arguments to {func}`jax.jit` other than the function are now marked as\n    keyword-only. This change is to prevent accidental breakage when arguments\n    are added to `jit`.\n* Bug fixes:\n  * The {func}`jax2tf.convert` now works in presence of gradients for functions\n    with integer inputs ({jax-issue}`#6360`).\n  * Fixed assertion failure in {func}`jax2tf.call_tf` when used with captured\n    `tf.Variable` ({jax-issue}`#6572`).\n\n## jaxlib 0.1.65 (April 7 2021)\n\n## jax 0.2.12 (April 1 2021)\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.11...v0.2.12).\n* New features\n  * New profiling APIs: {func}`jax.profiler.start_trace`,\n    {func}`jax.profiler.stop_trace`, and {func}`jax.profiler.trace`\n  * {func}`jax.lax.reduce` is now differentiable.\n* Breaking changes:\n  * The minimum jaxlib version is now 0.1.64.\n  * Some profiler APIs names have been changed. There are still aliases, so this\n    should not break existing code, but the aliases will eventually be removed\n    so please change your code.\n    * `TraceContext` --> {func}`~jax.profiler.TraceAnnotation`\n    * `StepTraceContext` --> {func}`~jax.profiler.StepTraceAnnotation`\n    * `trace_function` --> {func}`~jax.profiler.annotate_function`\n  * Omnistaging can no longer be disabled. See [omnistaging](https://github.com/jax-ml/jax/blob/main/docs/design_notes/omnistaging.md)\n    for more information.\n  * Python integers larger than the maximum `int64` value will now lead to an overflow\n    in all cases, rather than being silently converted to `uint64` in some cases ({jax-issue}`#6047`).\n  * Outside X64 mode, Python integers outside the range representable by `int32` will now lead to an\n    `OverflowError` rather than having their value silently truncated.\n* Bug fixes:\n  * `host_callback` now supports empty arrays in arguments and results ({jax-issue}`#6262`).\n  * {func}`jax.random.randint` clips rather than wraps of out-of-bounds limits, and can now generate\n    integers in the full range of the specified dtype ({jax-issue}`#5868`)\n\n## jax 0.2.11 (March 23 2021)\n\n* [GitHub\n  commits](https://github.com/jax-ml/jax/compare/jax-v0.2.10...jax-v0.2.11).\n* New features:\n  * [#6112](https://github.com/jax-ml/jax/pull/6112) added context managers:\n    `jax.enable_checks`, `jax.check_tracer_leaks`, `jax.debug_nans`,\n    `jax.debug_infs`, `jax.log_compiles`.\n  * [#6085](https://github.com/jax-ml/jax/pull/6085) added `jnp.delete`\n\n* Bug fixes:\n  * [#6136](https://github.com/jax-ml/jax/pull/6136) generalized\n    `jax.flatten_util.ravel_pytree` to handle integer dtypes.\n  * [#6129](https://github.com/jax-ml/jax/issues/6129) fixed a bug with handling\n    some constants like `enum.IntEnums`\n  * [#6145](https://github.com/jax-ml/jax/pull/6145) fixed batching issues with\n    incomplete beta functions\n  * [#6014](https://github.com/jax-ml/jax/pull/6014) fixed H2D transfers during\n    tracing\n  * [#6165](https://github.com/jax-ml/jax/pull/6165) avoids OverflowErrors when\n    converting some large Python integers to floats\n* Breaking changes:\n  * The minimum jaxlib version is now 0.1.62.\n\n\n## jaxlib 0.1.64 (March 18 2021)\n\n## jaxlib 0.1.63 (March 17 2021)\n\n## jax 0.2.10 (March 5 2021)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.9...jax-v0.2.10).\n* New features:\n  * {func}`jax.scipy.stats.chi2` is now available as a distribution with logpdf and pdf methods.\n  * {func}`jax.scipy.stats.betabinom` is now available as a distribution with logpmf and pmf methods.\n  * Added {func}`jax.experimental.jax2tf.call_tf` to call TensorFlow functions\n    from JAX ({jax-issue}`#5627`)\n    and [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax)).\n  * Extended the batching rule for `lax.pad` to support batching of the padding values.\n* Bug fixes:\n  * {func}`jax.numpy.take` properly handles negative indices ({jax-issue}`#5768`)\n* Breaking changes:\n  * JAX's promotion rules were adjusted to make promotion more consistent and\n    invariant to JIT. In particular, binary operations can now result in weakly-typed\n    values when appropriate. The main user-visible effect of the change is that\n    some operations result in outputs of different precision than before; for\n    example the expression `jnp.bfloat16(1) + 0.1 * jnp.arange(10)`\n    previously returned a `float64` array, and now returns a `bfloat16` array.\n    JAX's type promotion behavior is described at {ref}`type-promotion`.\n  * {func}`jax.numpy.linspace` now computes the floor of integer values, i.e.,\n    rounding towards -inf rather than 0. This change was made to match NumPy\n    1.20.0.\n  * {func}`jax.numpy.i0` no longer accepts complex numbers. Previously the\n    function computed the absolute value of complex arguments. This change was\n    made to match the semantics of NumPy 1.20.0.\n  * Several {mod}`jax.numpy` functions no longer accept tuples or lists in place\n    of array arguments: {func}`jax.numpy.pad`, :func`jax.numpy.ravel`,\n    {func}`jax.numpy.repeat`, {func}`jax.numpy.reshape`.\n    In general, {mod}`jax.numpy` functions should be used with scalars or array arguments.\n\n## jaxlib 0.1.62 (March 9 2021)\n\n* New features:\n  * jaxlib wheels are now built to require AVX instructions on x86-64 machines\n    by default. If you want to use JAX on a machine that doesn't support AVX,\n    you can build a jaxlib from source using the `--target_cpu_features` flag\n    to `build.py`. `--target_cpu_features` also replaces\n    `--enable_march_native`.\n\n## jaxlib 0.1.61 (February 12 2021)\n\n## jaxlib 0.1.60 (February 3 2021)\n\n* Bug fixes:\n  * Fixed a memory leak when converting CPU DeviceArrays to NumPy arrays. The\n    memory leak was present in jaxlib releases 0.1.58 and 0.1.59.\n  * `bool`, `int8`, and `uint8` are now considered safe to cast to\n    `bfloat16` NumPy extension type.\n\n## jax 0.2.9 (January 26 2021)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.8...jax-v0.2.9).\n* New features:\n  * Extend the {mod}`jax.experimental.loops` module with support for pytrees. Improved\n    error checking and error messages.\n  * Add {func}`jax.experimental.enable_x64` and {func}`jax.experimental.disable_x64`.\n    These are context managers which allow X64 mode to be temporarily enabled/disabled\n    within a session.\n* Breaking changes:\n  * {func}`jax.ops.segment_sum` now drops segment IDs that are out of range rather\n    than wrapping them into the segment ID space. This was done for performance\n    reasons.\n\n## jaxlib 0.1.59 (January 15 2021)\n\n## jax 0.2.8 (January 12 2021)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.7...jax-v0.2.8).\n* New features:\n  * Add {func}`jax.closure_convert` for use with higher-order custom\n    derivative functions. ({jax-issue}`#5244`)\n  * Add {func}`jax.experimental.host_callback.call` to call a custom Python\n    function on the host and return a result to the device computation.\n    ({jax-issue}`#5243`)\n* Bug fixes:\n  * `jax.numpy.arccosh` now returns the same branch as `numpy.arccosh` for\n    complex inputs ({jax-issue}`#5156`)\n  * `host_callback.id_tap` now works for `jax.pmap` also. There is an\n    optional parameter for `id_tap` and `id_print` to request that the\n    device from which the value is tapped be passed as a keyword argument\n    to the tap function ({jax-issue}`#5182`).\n* Breaking changes:\n  * `jax.numpy.pad` now takes keyword arguments. Positional argument `constant_values`\n    has been removed. In addition, passing unsupported keyword arguments raises an error.\n  * Changes for {func}`jax.experimental.host_callback.id_tap` ({jax-issue}`#5243`):\n    * Removed support for `kwargs` for {func}`jax.experimental.host_callback.id_tap`.\n      (This support has been deprecated for a few months.)\n    * Changed the printing of tuples for {func}`jax.experimental.host_callback.id_print`\n      to use '(' instead of '['.\n    * Changed the {func}`jax.experimental.host_callback.id_print` in presence of JVP\n      to print a pair of primal and tangent. Previously, there were two separate\n      print operations for the primals and the tangent.\n    * `host_callback.outfeed_receiver` has been removed (it is not necessary,\n      and was deprecated a few months ago).\n* New features:\n  * New flag for debugging `inf`, analogous to that for `NaN` ({jax-issue}`#5224`).\n\n## jax 0.2.7 (Dec 4 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.6...jax-v0.2.7).\n* New features:\n  * Add `jax.device_put_replicated`\n  * Add multi-host support to `jax.experimental.sharded_jit`\n  * Add support for differentiating eigenvalues computed by `jax.numpy.linalg.eig`\n  * Add support for building on Windows platforms\n  * Add support for general in_axes and out_axes in `jax.pmap`\n  * Add complex support for `jax.numpy.linalg.slogdet`\n* Bug fixes:\n  * Fix higher-than-second order derivatives of `jax.numpy.sinc` at zero\n  * Fix some hard-to-hit bugs around symbolic zeros in transpose rules\n* Breaking changes:\n  * `jax.experimental.optix` has been deleted, in favor of the standalone\n    `optax` Python package.\n  * indexing of JAX arrays with non-tuple sequences now raises a `TypeError`. This type of indexing\n    has been deprecated in Numpy since v1.16, and in JAX since v0.2.4.\n    See {jax-issue}`#4564`.\n\n## jax 0.2.6 (Nov 18 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.5...jax-v0.2.6).\n* New Features:\n  * Add support for shape-polymorphic tracing for the jax.experimental.jax2tf converter.\n    See [README.md](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md).\n* Breaking change cleanup\n\n  * Raise an error on non-hashable static arguments for jax.jit and\n    xla_computation.  See [cb48f42](https://github.com/jax-ml/jax/commit/cb48f42).\n  * Improve consistency of type promotion behavior ({jax-issue}`#4744`):\n    * Adding a complex Python scalar to a JAX floating point number respects the precision of\n      the JAX float. For example, `jnp.float32(1) + 1j` now returns `complex64`, where previously\n      it returned `complex128`.\n    * Results of type promotion with 3 or more terms involving uint64, a signed int, and a third type\n      are now independent of the order of arguments. For example:\n      `jnp.result_type(jnp.uint64, jnp.int64, jnp.float16)` and\n      `jnp.result_type(jnp.float16, jnp.uint64, jnp.int64)` both return `float16`, where previously\n      the first returned `float64` and the second returned `float16`.\n  * The contents of the (undocumented) `jax.lax_linalg` linear algebra module\n    are now exposed publicly as `jax.lax.linalg`.\n  * `jax.random.PRNGKey` now produces the same results in and out of JIT compilation\n    ({jax-issue}`#4877`).\n    This required changing the result for a given seed in a few particular cases:\n    * With `jax_enable_x64=False`, negative seeds passed as Python integers now return a different result\n      outside JIT mode. For example, `jax.random.PRNGKey(-1)` previously returned\n      `[4294967295, 4294967295]`, and now returns `[0, 4294967295]`. This matches the behavior in JIT.\n    * Seeds outside the range representable by `int64` outside JIT now result in an `OverflowError`\n      rather than a `TypeError`. This matches the behavior in JIT.\n\n    To recover the keys returned previously for negative integers with `jax_enable_x64=False`\n    outside JIT, you can use:\n\n    ```\n    key = random.PRNGKey(-1).at[0].set(0xFFFFFFFF)\n    ```\n  * DeviceArray now raises `RuntimeError` instead of `ValueError` when trying\n    to access its value while it has been deleted.\n\n## jaxlib 0.1.58 (January 12ish 2021)\n\n* Fixed a bug that meant JAX sometimes return platform-specific types (e.g.,\n  `np.cint`) instead of standard types (e.g., `np.int32`). (#4903)\n* Fixed a crash when constant-folding certain int16 operations. (#4971)\n* Added an `is_leaf` predicate to {func}`pytree.flatten`.\n\n## jaxlib 0.1.57 (November 12 2020)\n\n* Fixed manylinux2010 compliance issues in GPU wheels.\n* Switched the CPU FFT implementation from Eigen to PocketFFT.\n* Fixed a bug where the hash of bfloat16 values was not correctly initialized\n  and could change (#4651).\n* Add support for retaining ownership when passing arrays to DLPack (#4636).\n* Fixed a bug for batched triangular solves with sizes greater than 128 but not\n  a multiple of 128.\n* Fixed a bug when performing concurrent FFTs on multiple GPUs (#3518).\n* Fixed a bug in profiler where tools are missing (#4427).\n* Dropped support for CUDA 10.0.\n\n## jax 0.2.5 (October 27 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.4...jax-v0.2.5).\n* Improvements:\n  * Ensure that `check_jaxpr` does not perform FLOPS.  See {jax-issue}`#4650`.\n  * Expanded the set of JAX primitives converted by jax2tf.\n    See [primitives_with_limited_support.md](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/primitives_with_limited_support.md).\n\n## jax 0.2.4 (October 19 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.3...jax-v0.2.4).\n* Improvements:\n  * Add support for `remat` to jax.experimental.host_callback.  See {jax-issue}`#4608`.\n* Deprecations\n\n  * Indexing with non-tuple sequences is now deprecated, following a similar deprecation in Numpy.\n    In a future release, this will result in a TypeError. See {jax-issue}`#4564`.\n\n## jaxlib 0.1.56 (October 14, 2020)\n\n## jax 0.2.3 (October 14 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.2...jax-v0.2.3).\n* The reason for another release so soon is we need to temporarily roll back a\n  new jit fastpath while we look into a performance degradation\n\n## jax 0.2.2 (October 13 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.1...jax-v0.2.2).\n\n## jax 0.2.1 (October 6 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.2.0...jax-v0.2.1).\n* Improvements:\n  * As a benefit of omnistaging, the host_callback functions are executed (in program\n    order) even if the result of the {py:func}`jax.experimental.host_callback.id_print`/\n    {py:func}`jax.experimental.host_callback.id_tap` is not used in the computation.\n\n## jax (0.2.0) (September 23 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.77...jax-v0.2.0).\n* Improvements:\n  * Omnistaging on by default. See {jax-issue}`#3370` and\n    [omnistaging](https://github.com/jax-ml/jax/blob/main/docs/design_notes/omnistaging.md)\n\n## jax (0.1.77) (September 15 2020)\n\n* Breaking changes:\n  * New simplified interface for {py:func}`jax.experimental.host_callback.id_tap` (#4101)\n\n## jaxlib 0.1.55 (September 8, 2020)\n\n* Update XLA:\n  * Fix bug in DLPackManagedTensorToBuffer (#4196)\n\n## jax 0.1.76 (September 8, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.75...jax-v0.1.76).\n\n## jax 0.1.75 (July 30, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.74...jax-v0.1.75).\n* Bug Fixes:\n  * make jnp.abs() work for unsigned inputs (#3914)\n* Improvements:\n  * \"Omnistaging\" behavior added behind a flag, disabled by default (#3370)\n\n## jax 0.1.74 (July 29, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.73...jax-v0.1.74).\n* New Features:\n  * BFGS (#3101)\n  * TPU support for half-precision arithmetic (#3878)\n* Bug Fixes:\n  * Prevent some accidental dtype warnings (#3874)\n  * Fix a multi-threading bug in custom derivatives (#3845, #3869)\n* Improvements:\n  * Faster searchsorted implementation (#3873)\n  * Better test coverage for jax.numpy sorting algorithms (#3836)\n\n## jaxlib 0.1.52 (July 22, 2020)\n\n* Update XLA.\n\n## jax 0.1.73 (July 22, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.72...jax-v0.1.73).\n* The minimum jaxlib version is now 0.1.51.\n* New Features:\n  * jax.image.resize. (#3703)\n  * hfft and ihfft (#3664)\n  * jax.numpy.intersect1d (#3726)\n  * jax.numpy.lexsort (#3812)\n  * `lax.scan` and the `scan` primitive support an `unroll`\n    parameter for loop unrolling when lowering to XLA\n    ({jax-issue}`#3738`).\n* Bug Fixes:\n  * Fix reduction repeated axis error (#3618)\n  * Fix shape rule for lax.pad for input dimensions of size 0. (#3608)\n  * make psum transpose handle zero cotangents (#3653)\n  * Fix shape error when taking JVP of reduce-prod over size 0 axis. (#3729)\n  * Support differentiation through jax.lax.all_to_all (#3733)\n  * address nan issue in jax.scipy.special.zeta (#3777)\n* Improvements:\n  * Many improvements to jax2tf\n  * Reimplement argmin/argmax using a single pass variadic reduction. (#3611)\n  * Enable XLA SPMD partitioning by default. (#3151)\n  * Add support for 0d transpose convolution (#3643)\n  * Make LU gradient work for low-rank matrices (#3610)\n  * support multiple_results and custom JVPs in jet (#3657)\n  * Generalize reduce-window padding to support (lo, hi) pairs. (#3728)\n  * Implement complex convolutions on CPU and GPU. (#3735)\n  * Make jnp.take work for empty slices of empty arrays. (#3751)\n  * Relax dimension ordering rules for dot_general. (#3778)\n  * Enable buffer donation for GPU. (#3800)\n  * Add support for base dilation and window dilation to reduce window op… (#3803)\n\n## jaxlib 0.1.51 (July 2, 2020)\n\n* Update XLA.\n* Add new runtime support for host_callback.\n\n## jax 0.1.72 (June 28, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.71...jax-v0.1.72).\n* Bug fixes:\n  * Fix an odeint bug introduced in the previous release, see\n    {jax-issue}`#3587`.\n\n## jax 0.1.71 (June 25, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.70...jax-v0.1.71).\n* The minimum jaxlib version is now 0.1.48.\n* Bug fixes:\n  * Allow `jax.experimental.ode.odeint` dynamics functions to close over\n    values with respect to which we're differentiating\n    {jax-issue}`#3562`.\n\n## jaxlib 0.1.50 (June 25, 2020)\n\n* Add support for CUDA 11.0.\n* Drop support for CUDA 9.2 (we only maintain support for the last four CUDA\n  versions.)\n* Update XLA.\n\n## jaxlib 0.1.49 (June 19, 2020)\n\n* Bug fixes:\n  * Fix build issue that could result in slow compiles\n    (<https://github.com/tensorflow/tensorflow/commit/f805153a25b00d12072bd728e91bb1621bfcf1b1>)\n\n## jaxlib 0.1.48 (June 12, 2020)\n\n* New features:\n  * Adds support for fast traceback collection.\n  * Adds preliminary support for on-device heap profiling.\n  * Implements `np.nextafter` for `bfloat16` types.\n  * Complex128 support for FFTs on CPU and GPU.\n* Bug fixes:\n  * Improved float64 `tanh` accuracy on GPU.\n  * float64 scatters on GPU are much faster.\n  * Complex matrix multiplication on CPU should be much faster.\n  * Stable sorts on CPU should actually be stable now.\n  * Concurrency bug fix in CPU backend.\n\n## jax 0.1.70 (June 8, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.69...jax-v0.1.70).\n* New features:\n  * `lax.switch` introduces indexed conditionals with multiple\n    branches, together with a generalization of the `cond`\n    primitive\n    {jax-issue}`#3318`.\n\n## jax 0.1.69 (June 3, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.68...jax-v0.1.69).\n\n## jax 0.1.68 (May 21, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.67...jax-v0.1.68).\n* New features:\n  * {func}`lax.cond` supports a single-operand form, taken as the argument\n    to both branches\n    {jax-issue}`#2993`.\n* Notable changes:\n  * The format of the `transforms` keyword for the {func}`jax.experimental.host_callback.id_tap`\n    primitive has changed {jax-issue}`#3132`.\n\n## jax 0.1.67 (May 12, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.66...jax-v0.1.67).\n* New features:\n  * Support for reduction over subsets of a pmapped axis using `axis_index_groups`\n    {jax-issue}`#2382`.\n  * Experimental support for printing and calling host-side Python function from\n    compiled code. See [id_print and id_tap](https://jax.readthedocs.io/en/latest/jax.experimental.host_callback.html)\n    ({jax-issue}`#3006`).\n* Notable changes:\n  * The visibility of names exported from {mod}`jax.numpy` has been\n    tightened. This may break code that was making use of names that were\n    previously exported accidentally.\n\n## jaxlib 0.1.47 (May 8, 2020)\n\n* Fixes crash for outfeed.\n\n## jax 0.1.66 (May 5, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.65...jax-v0.1.66).\n* New features:\n  * Support for `in_axes=None` on {func}`pmap`\n    {jax-issue}`#2896`.\n\n## jaxlib 0.1.46 (May 5, 2020)\n\n* Fixes crash for linear algebra functions on Mac OS X (#432).\n* Fixes an illegal instruction crash caused by using AVX512 instructions when\n  an operating system or hypervisor disabled them (#2906).\n\n## jax 0.1.65 (April 30, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.64...jax-v0.1.65).\n* New features:\n  * Differentiation of determinants of singular matrices\n    {jax-issue}`#2809`.\n* Bug fixes:\n  * Fix {func}`odeint` differentiation with respect to time of ODEs with\n    time-dependent dynamics {jax-issue}`#2817`,\n    also add ODE CI testing.\n  * Fix {func}`lax_linalg.qr` differentiation\n    {jax-issue}`#2867`.\n\n## jaxlib 0.1.45 (April 21, 2020)\n\n* Fixes segfault: {jax-issue}`#2755`\n* Plumb is_stable option on Sort HLO through to Python.\n\n## jax 0.1.64 (April 21, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.63...jax-v0.1.64).\n* New features:\n  * Add syntactic sugar for functional indexed updates\n    {jax-issue}`#2684`.\n  * Add {func}`jax.numpy.linalg.multi_dot` {jax-issue}`#2726`.\n  * Add {func}`jax.numpy.unique` {jax-issue}`#2760`.\n  * Add {func}`jax.numpy.rint` {jax-issue}`#2724`.\n  * Add {func}`jax.numpy.rint` {jax-issue}`#2724`.\n  * Add more primitive rules for {func}`jax.experimental.jet`.\n* Bug fixes:\n  * Fix {func}`logaddexp` and {func}`logaddexp2` differentiation at zero {jax-issue}`#2107`.\n  * Improve memory usage in reverse-mode autodiff without {func}`jit`\n    {jax-issue}`#2719`.\n* Better errors:\n  * Improves error message for reverse-mode differentiation of {func}`lax.while_loop`\n    {jax-issue}`#2129`.\n\n## jaxlib 0.1.44 (April 16, 2020)\n\n* Fixes a bug where if multiple GPUs of different models were present, JAX\n  would only compile programs suitable for the first GPU.\n* Bugfix for `batch_group_count` convolutions.\n* Added precompiled SASS for more GPU versions to avoid startup PTX compilation\n  hang.\n\n## jax 0.1.63 (April 12, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.62...jax-v0.1.63).\n* Added `jax.custom_jvp` and `jax.custom_vjp` from {jax-issue}`#2026`, see the [tutorial notebook](https://jax.readthedocs.io/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html). Deprecated `jax.custom_transforms` and removed it from the docs (though it still works).\n* Add `scipy.sparse.linalg.cg` {jax-issue}`#2566`.\n* Changed how Tracers are printed to show more useful information for debugging {jax-issue}`#2591`.\n* Made `jax.numpy.isclose` handle `nan` and `inf` correctly {jax-issue}`#2501`.\n* Added several new rules for `jax.experimental.jet` {jax-issue}`#2537`.\n* Fixed `jax.experimental.stax.BatchNorm` when `scale`/`center` isn't provided.\n* Fix some missing cases of broadcasting in `jax.numpy.einsum` {jax-issue}`#2512`.\n* Implement `jax.numpy.cumsum` and `jax.numpy.cumprod` in terms of a parallel prefix scan {jax-issue}`#2596` and make `reduce_prod` differentiable to arbitrary order {jax-issue}`#2597`.\n* Add `batch_group_count` to `conv_general_dilated` {jax-issue}`#2635`.\n* Add docstring for `test_util.check_grads` {jax-issue}`#2656`.\n* Add `callback_transform` {jax-issue}`#2665`.\n* Implement `rollaxis`, `convolve`/`correlate` 1d & 2d, `copysign`,\n  `trunc`, `roots`, and `quantile`/`percentile` interpolation options.\n\n## jaxlib 0.1.43 (March 31, 2020)\n\n* Fixed a performance regression for Resnet-50 on GPU.\n\n## jax 0.1.62 (March 21, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.61...jax-v0.1.62).\n* JAX has dropped support for Python 3.5. Please upgrade to Python 3.6 or newer.\n* Removed the internal function `lax._safe_mul`, which implemented the\n  convention `0. * nan == 0.`. This change means some programs when\n  differentiated will produce nans when they previously produced correct\n  values, though it ensures nans rather than silently incorrect results are\n  produced for other programs. See #2447 and #1052 for details.\n* Added an `all_gather` parallel convenience function.\n* More type annotations in core code.\n\n## jaxlib 0.1.42 (March 19, 2020)\n\n* jaxlib 0.1.41 broke cloud TPU support due to an API incompatibility. This\n  release fixes it again.\n* JAX has dropped support for Python 3.5. Please upgrade to Python 3.6 or newer.\n\n## jax 0.1.61 (March 17, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.60...jax-v0.1.61).\n* Fixes Python 3.5 support. This will be the last JAX or jaxlib release that\n  supports Python 3.5.\n\n## jax 0.1.60 (March 17, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.59...jax-v0.1.60).\n* New features:\n  * {py:func}`jax.pmap` has `static_broadcast_argnums` argument which allows\n    the user to specify arguments that should be treated as compile-time\n    constants and should be broadcasted to all devices. It works analogously to\n    `static_argnums` in {py:func}`jax.jit`.\n  * Improved error messages for when tracers are mistakenly saved in global state.\n  * Added {py:func}`jax.nn.one_hot` utility function.\n  * Added {mod}`jax.experimental.jet` for exponentially faster\n    higher-order automatic differentiation.\n  * Added more correctness checking to arguments of {py:func}`jax.lax.broadcast_in_dim`.\n* The minimum jaxlib version is now 0.1.41.\n\n## jaxlib 0.1.40 (March 4, 2020)\n\n* Adds experimental support in Jaxlib for TensorFlow profiler, which allows\n  tracing of CPU and GPU computations from TensorBoard.\n* Includes prototype support for multihost GPU computations that communicate via\n  NCCL.\n* Improves performance of NCCL collectives on GPU.\n* Adds TopK, CustomCallWithoutLayout, CustomCallWithLayout, IGammaGradA and\n  RandomGamma implementations.\n* Supports device assignments known at XLA compilation time.\n\n## jax 0.1.59 (February 11, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/jax-v0.1.58...jax-v0.1.59).\n* Breaking changes\n\n  * The minimum jaxlib version is now 0.1.38.\n  * Simplified {py:class}`Jaxpr` by removing the `Jaxpr.freevars` and\n    `Jaxpr.bound_subjaxprs`. The call primitives (`xla_call`, `xla_pmap`,\n    `sharded_call`, and `remat_call`) get a new parameter `call_jaxpr` with a\n    fully-closed (no `constvars`) jaxpr. Also, added a new field `call_primitive`\n    to primitives.\n* New features:\n  * Reverse-mode automatic differentiation (e.g. `grad`) of `lax.cond`, making it\n    now differentiable in both modes ({jax-issue}`#2091`)\n  * JAX now supports DLPack, which allows sharing CPU and GPU arrays in a\n    zero-copy way with other libraries, such as PyTorch.\n  * JAX GPU DeviceArrays now support `__cuda_array_interface__`, which is another\n    zero-copy protocol for sharing GPU arrays with other libraries such as CuPy\n    and Numba.\n  * JAX CPU device buffers now implement the Python buffer protocol, which allows\n    zero-copy buffer sharing between JAX and NumPy.\n  * Added JAX_SKIP_SLOW_TESTS environment variable to skip tests known as slow.\n\n## jaxlib 0.1.39 (February 11, 2020)\n\n* Updates XLA.\n\n## jaxlib 0.1.38 (January 29, 2020)\n\n* CUDA 9.0 is no longer supported.\n* CUDA 10.2 wheels are now built by default.\n\n## jax 0.1.58 (January 28, 2020)\n\n* [GitHub commits](https://github.com/jax-ml/jax/compare/46014da21...jax-v0.1.58).\n* Breaking changes\n\n  * JAX has dropped Python 2 support, because Python 2 reached its end of life on\n    January 1, 2020. Please update to Python 3.5 or newer.\n* New features\n\n  >   > * Forward-mode automatic differentiation (`jvp`) of while loop\n  >   ({jax-issue}`#1980`)\n  > * New NumPy and SciPy functions:\n  >\n  >   * {py:func}`jax.numpy.fft.fft2`\n  >   * {py:func}`jax.numpy.fft.ifft2`\n  >   * {py:func}`jax.numpy.fft.rfft`\n  >   * {py:func}`jax.numpy.fft.irfft`\n  >   * {py:func}`jax.numpy.fft.rfft2`\n  >   * {py:func}`jax.numpy.fft.irfft2`\n  >   * {py:func}`jax.numpy.fft.rfftn`\n  >   * {py:func}`jax.numpy.fft.irfftn`\n  >   * {py:func}`jax.numpy.fft.fftfreq`\n  >   * {py:func}`jax.numpy.fft.rfftfreq`\n  >   * {py:func}`jax.numpy.linalg.matrix_rank`\n  >   * {py:func}`jax.numpy.linalg.matrix_power`\n  >   * {py:func}`jax.scipy.special.betainc`\n  > * Batched Cholesky decomposition on GPU now uses a more efficient batched\n  >   kernel.\n\n### Notable bug fixes\n\n* With the Python 3 upgrade, JAX no longer depends on `fastcache`, which should\n  help with installation.\n"
        },
        {
          "name": "CITATION.bib",
          "type": "blob",
          "size": 0.4,
          "content": "@software{jax2018github,\n  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},\n  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},\n  url = {http://github.com/jax-ml/jax},\n  version = {0.3.13},\n  year = {2018},\n}\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 0.15,
          "content": "# Contributing to JAX\n\nFor information on how to contribute to JAX, see\n[Contributing to JAX](https://jax.readthedocs.io/en/latest/contributing.html)\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.09,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 20.26,
          "content": "<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/jax-ml/jax/main/images/jax_logo_250px.png\" alt=\"logo\"></img>\n</div>\n\n# Transformable numerical computing at scale\n\n[![Continuous integration](https://github.com/jax-ml/jax/actions/workflows/ci-build.yaml/badge.svg)](https://github.com/jax-ml/jax/actions/workflows/ci-build.yaml)\n[![PyPI version](https://img.shields.io/pypi/v/jax)](https://pypi.org/project/jax/)\n\n[**Quickstart**](#quickstart-colab-in-the-cloud)\n| [**Transformations**](#transformations)\n| [**Install guide**](#installation)\n| [**Neural net libraries**](#neural-network-libraries)\n| [**Change logs**](https://jax.readthedocs.io/en/latest/changelog.html)\n| [**Reference docs**](https://jax.readthedocs.io/en/latest/)\n\n\n## What is JAX?\n\nJAX is a Python library for accelerator-oriented array computation and program transformation,\ndesigned for high-performance numerical computing and large-scale machine learning.\n\nWith its updated version of [Autograd](https://github.com/hips/autograd),\nJAX can automatically differentiate native\nPython and NumPy functions. It can differentiate through loops, branches,\nrecursion, and closures, and it can take derivatives of derivatives of\nderivatives. It supports reverse-mode differentiation (a.k.a. backpropagation)\nvia [`grad`](#automatic-differentiation-with-grad) as well as forward-mode differentiation,\nand the two can be composed arbitrarily to any order.\n\nWhat’s new is that JAX uses [XLA](https://www.tensorflow.org/xla)\nto compile and run your NumPy programs on GPUs and TPUs. Compilation happens\nunder the hood by default, with library calls getting just-in-time compiled and\nexecuted. But JAX also lets you just-in-time compile your own Python functions\ninto XLA-optimized kernels using a one-function API,\n[`jit`](#compilation-with-jit). Compilation and automatic differentiation can be\ncomposed arbitrarily, so you can express sophisticated algorithms and get\nmaximal performance without leaving Python. You can even program multiple GPUs\nor TPU cores at once using [`pmap`](#spmd-programming-with-pmap), and\ndifferentiate through the whole thing.\n\nDig a little deeper, and you'll see that JAX is really an extensible system for\n[composable function transformations](#transformations). Both\n[`grad`](#automatic-differentiation-with-grad) and [`jit`](#compilation-with-jit)\nare instances of such transformations. Others are\n[`vmap`](#auto-vectorization-with-vmap) for automatic vectorization and\n[`pmap`](#spmd-programming-with-pmap) for single-program multiple-data (SPMD)\nparallel programming of multiple accelerators, with more to come.\n\nThis is a research project, not an official Google product. Expect\n[sharp edges](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html).\nPlease help by trying it out, [reporting\nbugs](https://github.com/jax-ml/jax/issues), and letting us know what you\nthink!\n\n```python\nimport jax.numpy as jnp\nfrom jax import grad, jit, vmap\n\ndef predict(params, inputs):\n  for W, b in params:\n    outputs = jnp.dot(inputs, W) + b\n    inputs = jnp.tanh(outputs)  # inputs to the next layer\n  return outputs                # no activation on last layer\n\ndef loss(params, inputs, targets):\n  preds = predict(params, inputs)\n  return jnp.sum((preds - targets)**2)\n\ngrad_loss = jit(grad(loss))  # compiled gradient evaluation function\nperex_grads = jit(vmap(grad_loss, in_axes=(None, 0, 0)))  # fast per-example grads\n```\n\n### Contents\n* [Quickstart: Colab in the Cloud](#quickstart-colab-in-the-cloud)\n* [Transformations](#transformations)\n* [Current gotchas](#current-gotchas)\n* [Installation](#installation)\n* [Neural net libraries](#neural-network-libraries)\n* [Citing JAX](#citing-jax)\n* [Reference documentation](#reference-documentation)\n\n## Quickstart: Colab in the Cloud\nJump right in using a notebook in your browser, connected to a Google Cloud GPU.\nHere are some starter notebooks:\n- [The basics: NumPy on accelerators, `grad` for differentiation, `jit` for compilation, and `vmap` for vectorization](https://jax.readthedocs.io/en/latest/quickstart.html)\n- [Training a Simple Neural Network, with TensorFlow Dataset Data Loading](https://colab.research.google.com/github/jax-ml/jax/blob/main/docs/notebooks/neural_network_with_tfds_data.ipynb)\n\n**JAX now runs on Cloud TPUs.** To try out the preview, see the [Cloud TPU\nColabs](https://github.com/jax-ml/jax/tree/main/cloud_tpu_colabs).\n\nFor a deeper dive into JAX:\n- [The Autodiff Cookbook, Part 1: easy and powerful automatic differentiation in JAX](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html)\n- [Common gotchas and sharp edges](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html)\n- See the [full list of\nnotebooks](https://github.com/jax-ml/jax/tree/main/docs/notebooks).\n\n## Transformations\n\nAt its core, JAX is an extensible system for transforming numerical functions.\nHere are four transformations of primary interest: `grad`, `jit`, `vmap`, and\n`pmap`.\n\n### Automatic differentiation with `grad`\n\nJAX has roughly the same API as [Autograd](https://github.com/hips/autograd).\nThe most popular function is\n[`grad`](https://jax.readthedocs.io/en/latest/jax.html#jax.grad)\nfor reverse-mode gradients:\n\n```python\nfrom jax import grad\nimport jax.numpy as jnp\n\ndef tanh(x):  # Define a function\n  y = jnp.exp(-2.0 * x)\n  return (1.0 - y) / (1.0 + y)\n\ngrad_tanh = grad(tanh)  # Obtain its gradient function\nprint(grad_tanh(1.0))   # Evaluate it at x = 1.0\n# prints 0.4199743\n```\n\nYou can differentiate to any order with `grad`.\n\n```python\nprint(grad(grad(grad(tanh)))(1.0))\n# prints 0.62162673\n```\n\nFor more advanced autodiff, you can use\n[`jax.vjp`](https://jax.readthedocs.io/en/latest/jax.html#jax.vjp) for\nreverse-mode vector-Jacobian products and\n[`jax.jvp`](https://jax.readthedocs.io/en/latest/jax.html#jax.jvp) for\nforward-mode Jacobian-vector products. The two can be composed arbitrarily with\none another, and with other JAX transformations. Here's one way to compose those\nto make a function that efficiently computes [full Hessian\nmatrices](https://jax.readthedocs.io/en/latest/_autosummary/jax.hessian.html#jax.hessian):\n\n```python\nfrom jax import jit, jacfwd, jacrev\n\ndef hessian(fun):\n  return jit(jacfwd(jacrev(fun)))\n```\n\nAs with [Autograd](https://github.com/hips/autograd), you're free to use\ndifferentiation with Python control structures:\n\n```python\ndef abs_val(x):\n  if x > 0:\n    return x\n  else:\n    return -x\n\nabs_val_grad = grad(abs_val)\nprint(abs_val_grad(1.0))   # prints 1.0\nprint(abs_val_grad(-1.0))  # prints -1.0 (abs_val is re-evaluated)\n```\n\nSee the [reference docs on automatic\ndifferentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)\nand the [JAX Autodiff\nCookbook](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html)\nfor more.\n\n### Compilation with `jit`\n\nYou can use XLA to compile your functions end-to-end with\n[`jit`](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit),\nused either as an `@jit` decorator or as a higher-order function.\n\n```python\nimport jax.numpy as jnp\nfrom jax import jit\n\ndef slow_f(x):\n  # Element-wise ops see a large benefit from fusion\n  return x * x + x * 2.0\n\nx = jnp.ones((5000, 5000))\nfast_f = jit(slow_f)\n%timeit -n10 -r3 fast_f(x)  # ~ 4.5 ms / loop on Titan X\n%timeit -n10 -r3 slow_f(x)  # ~ 14.5 ms / loop (also on GPU via JAX)\n```\n\nYou can mix `jit` and `grad` and any other JAX transformation however you like.\n\nUsing `jit` puts constraints on the kind of Python control flow\nthe function can use; see\nthe tutorial on [Control Flow and Logical Operators with JIT](https://jax.readthedocs.io/en/latest/control-flow.html)\nfor more.\n\n### Auto-vectorization with `vmap`\n\n[`vmap`](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap) is\nthe vectorizing map.\nIt has the familiar semantics of mapping a function along array axes, but\ninstead of keeping the loop on the outside, it pushes the loop down into a\nfunction’s primitive operations for better performance.\n\nUsing `vmap` can save you from having to carry around batch dimensions in your\ncode. For example, consider this simple *unbatched* neural network prediction\nfunction:\n\n```python\ndef predict(params, input_vec):\n  assert input_vec.ndim == 1\n  activations = input_vec\n  for W, b in params:\n    outputs = jnp.dot(W, activations) + b  # `activations` on the right-hand side!\n    activations = jnp.tanh(outputs)        # inputs to the next layer\n  return outputs                           # no activation on last layer\n```\n\nWe often instead write `jnp.dot(activations, W)` to allow for a batch dimension on the\nleft side of `activations`, but we’ve written this particular prediction function to\napply only to single input vectors. If we wanted to apply this function to a\nbatch of inputs at once, semantically we could just write\n\n```python\nfrom functools import partial\npredictions = jnp.stack(list(map(partial(predict, params), input_batch)))\n```\n\nBut pushing one example through the network at a time would be slow! It’s better\nto vectorize the computation, so that at every layer we’re doing matrix-matrix\nmultiplication rather than matrix-vector multiplication.\n\nThe `vmap` function does that transformation for us. That is, if we write\n\n```python\nfrom jax import vmap\npredictions = vmap(partial(predict, params))(input_batch)\n# or, alternatively\npredictions = vmap(predict, in_axes=(None, 0))(params, input_batch)\n```\n\nthen the `vmap` function will push the outer loop inside the function, and our\nmachine will end up executing matrix-matrix multiplications exactly as if we’d\ndone the batching by hand.\n\nIt’s easy enough to manually batch a simple neural network without `vmap`, but\nin other cases manual vectorization can be impractical or impossible. Take the\nproblem of efficiently computing per-example gradients: that is, for a fixed set\nof parameters, we want to compute the gradient of our loss function evaluated\nseparately at each example in a batch. With `vmap`, it’s easy:\n\n```python\nper_example_gradients = vmap(partial(grad(loss), params))(inputs, targets)\n```\n\nOf course, `vmap` can be arbitrarily composed with `jit`, `grad`, and any other\nJAX transformation! We use `vmap` with both forward- and reverse-mode automatic\ndifferentiation for fast Jacobian and Hessian matrix calculations in\n`jax.jacfwd`, `jax.jacrev`, and `jax.hessian`.\n\n### SPMD programming with `pmap`\n\nFor parallel programming of multiple accelerators, like multiple GPUs, use\n[`pmap`](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap).\nWith `pmap` you write single-program multiple-data (SPMD) programs, including\nfast parallel collective communication operations. Applying `pmap` will mean\nthat the function you write is compiled by XLA (similarly to `jit`), then\nreplicated and executed in parallel across devices.\n\nHere's an example on an 8-GPU machine:\n\n```python\nfrom jax import random, pmap\nimport jax.numpy as jnp\n\n# Create 8 random 5000 x 6000 matrices, one per GPU\nkeys = random.split(random.key(0), 8)\nmats = pmap(lambda key: random.normal(key, (5000, 6000)))(keys)\n\n# Run a local matmul on each device in parallel (no data transfer)\nresult = pmap(lambda x: jnp.dot(x, x.T))(mats)  # result.shape is (8, 5000, 5000)\n\n# Compute the mean on each device in parallel and print the result\nprint(pmap(jnp.mean)(result))\n# prints [1.1566595 1.1805978 ... 1.2321935 1.2015157]\n```\n\nIn addition to expressing pure maps, you can use fast [collective communication\noperations](https://jax.readthedocs.io/en/latest/jax.lax.html#parallel-operators)\nbetween devices:\n\n```python\nfrom functools import partial\nfrom jax import lax\n\n@partial(pmap, axis_name='i')\ndef normalize(x):\n  return x / lax.psum(x, 'i')\n\nprint(normalize(jnp.arange(4.)))\n# prints [0.         0.16666667 0.33333334 0.5       ]\n```\n\nYou can even [nest `pmap` functions](https://colab.research.google.com/github/jax-ml/jax/blob/main/cloud_tpu_colabs/Pmap_Cookbook.ipynb#scrollTo=MdRscR5MONuN) for more\nsophisticated communication patterns.\n\nIt all composes, so you're free to differentiate through parallel computations:\n\n```python\nfrom jax import grad\n\n@pmap\ndef f(x):\n  y = jnp.sin(x)\n  @pmap\n  def g(z):\n    return jnp.cos(z) * jnp.tan(y.sum()) * jnp.tanh(x).sum()\n  return grad(lambda w: jnp.sum(g(w)))(x)\n\nprint(f(x))\n# [[ 0.        , -0.7170853 ],\n#  [-3.1085174 , -0.4824318 ],\n#  [10.366636  , 13.135289  ],\n#  [ 0.22163185, -0.52112055]]\n\nprint(grad(lambda x: jnp.sum(f(x)))(x))\n# [[ -3.2369726,  -1.6356447],\n#  [  4.7572474,  11.606951 ],\n#  [-98.524414 ,  42.76499  ],\n#  [ -1.6007166,  -1.2568436]]\n```\n\nWhen reverse-mode differentiating a `pmap` function (e.g. with `grad`), the\nbackward pass of the computation is parallelized just like the forward pass.\n\nSee the [SPMD\nCookbook](https://colab.research.google.com/github/jax-ml/jax/blob/main/cloud_tpu_colabs/Pmap_Cookbook.ipynb)\nand the [SPMD MNIST classifier from scratch\nexample](https://github.com/jax-ml/jax/blob/main/examples/spmd_mnist_classifier_fromscratch.py)\nfor more.\n\n## Current gotchas\n\nFor a more thorough survey of current gotchas, with examples and explanations,\nwe highly recommend reading the [Gotchas\nNotebook](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html).\nSome standouts:\n\n1. JAX transformations only work on [pure functions](https://en.wikipedia.org/wiki/Pure_function), which don't have side-effects and respect [referential transparency](https://en.wikipedia.org/wiki/Referential_transparency) (i.e. object identity testing with `is` isn't preserved). If you use a JAX transformation on an impure Python function, you might see an error like `Exception: Can't lift Traced...`  or `Exception: Different traces at same level`.\n1. [In-place mutating updates of\n   arrays](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#in-place-updates), like `x[i] += y`, aren't supported, but [there are functional alternatives](https://jax.readthedocs.io/en/latest/jax.ops.html). Under a `jit`, those functional alternatives will reuse buffers in-place automatically.\n1. [Random numbers are\n   different](https://jax.readthedocs.io/en/latest/random-numbers.html), but for [good reasons](https://github.com/jax-ml/jax/blob/main/docs/jep/263-prng.md).\n1. If you're looking for [convolution\n   operators](https://jax.readthedocs.io/en/latest/notebooks/convolutions.html),\n   they're in the `jax.lax` package.\n1. JAX enforces single-precision (32-bit, e.g. `float32`) values by default, and\n   [to enable\n   double-precision](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#double-64bit-precision)\n   (64-bit, e.g. `float64`) one needs to set the `jax_enable_x64` variable at\n   startup (or set the environment variable `JAX_ENABLE_X64=True`).\n   On TPU, JAX uses 32-bit values by default for everything _except_ internal\n   temporary variables in 'matmul-like' operations, such as `jax.numpy.dot` and `lax.conv`.\n   Those ops have a `precision` parameter which can be used to approximate 32-bit operations\n   via three bfloat16 passes, with a cost of possibly slower runtime.\n   Non-matmul operations on TPU lower to implementations that often emphasize speed over\n   accuracy, so in practice computations on TPU will be less precise than similar\n   computations on other backends.\n1. Some of NumPy's dtype promotion semantics involving a mix of Python scalars\n   and NumPy types aren't preserved, namely `np.add(1, np.array([2],\n   np.float32)).dtype` is `float64` rather than `float32`.\n1. Some transformations, like `jit`, [constrain how you can use Python control\n   flow](https://jax.readthedocs.io/en/latest/control-flow.html).\n   You'll always get loud errors if something goes wrong. You might have to use\n   [`jit`'s `static_argnums`\n   parameter](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit),\n   [structured control flow\n   primitives](https://jax.readthedocs.io/en/latest/jax.lax.html#control-flow-operators)\n   like\n   [`lax.scan`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scan.html#jax.lax.scan),\n   or just use `jit` on smaller subfunctions.\n\n## Installation\n\n### Supported platforms\n\n|            | Linux x86_64 | Linux aarch64 | Mac x86_64   | Mac aarch64  | Windows x86_64 | Windows WSL2 x86_64 |\n|------------|--------------|---------------|--------------|--------------|----------------|---------------------|\n| CPU        | yes          | yes           | yes          | yes          | yes            | yes                 |\n| NVIDIA GPU | yes          | yes           | no           | n/a          | no             | experimental        |\n| Google TPU | yes          | n/a           | n/a          | n/a          | n/a            | n/a                 |\n| AMD GPU    | yes          | no            | experimental | n/a          | no             | no                  |\n| Apple GPU  | n/a          | no            | n/a          | experimental | n/a            | n/a                 |\n| Intel GPU  | experimental | n/a           | n/a          | n/a          | no             | no                  |\n\n\n### Instructions\n\n| Platform        | Instructions                                                                                                    |\n|-----------------|-----------------------------------------------------------------------------------------------------------------|\n| CPU             | `pip install -U jax`                                                                                            |\n| NVIDIA GPU      | `pip install -U \"jax[cuda12]\"`                                                                                  |\n| Google TPU      | `pip install -U \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html`                 |\n| AMD GPU (Linux) | Follow [AMD's instructions](https://github.com/jax-ml/jax/blob/main/build/rocm/README.md).                                       |\n| Mac GPU         | Follow [Apple's instructions](https://developer.apple.com/metal/jax/).                                          |\n| Intel GPU       | Follow [Intel's instructions](https://github.com/intel/intel-extension-for-openxla/blob/main/docs/acc_jax.md).  |\n\nSee [the documentation](https://jax.readthedocs.io/en/latest/installation.html)\nfor information on alternative installation strategies. These include compiling\nfrom source, installing with Docker, using other versions of CUDA, a\ncommunity-supported conda build, and answers to some frequently-asked questions.\n\n\n\n## Neural network libraries\n\nMultiple Google research groups at Google DeepMind and Alphabet develop and share libraries\nfor training neural networks in JAX. If you want a fully featured library for neural network\ntraining with examples and how-to guides, try\n[Flax](https://github.com/google/flax) and its [documentation site](https://flax.readthedocs.io/en/latest/nnx/index.html).\n\nCheck out the [JAX Ecosystem section](https://jax.readthedocs.io/en/latest/#ecosystem)\non the JAX documentation site for a list of JAX-based network libraries, which includes\n[Optax](https://github.com/deepmind/optax) for gradient processing and\noptimization, [chex](https://github.com/deepmind/chex) for reliable code and testing, and\n[Equinox](https://github.com/patrick-kidger/equinox) for neural networks.\n(Watch the NeurIPS 2020 JAX Ecosystem at DeepMind talk\n[here](https://www.youtube.com/watch?v=iDxJxIyzSiM) for additional details.)\n\n## Citing JAX\n\nTo cite this repository:\n\n```\n@software{jax2018github,\n  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},\n  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},\n  url = {http://github.com/jax-ml/jax},\n  version = {0.3.13},\n  year = {2018},\n}\n```\n\nIn the above bibtex entry, names are in alphabetical order, the version number\nis intended to be that from [jax/version.py](../main/jax/version.py), and\nthe year corresponds to the project's open-source release.\n\nA nascent version of JAX, supporting only automatic differentiation and\ncompilation to XLA, was described in a [paper that appeared at SysML\n2018](https://mlsys.org/Conferences/2019/doc/2018/146.pdf). We're currently working on\ncovering JAX's ideas and capabilities in a more comprehensive and up-to-date\npaper.\n\n## Reference documentation\n\nFor details about the JAX API, see the\n[reference documentation](https://jax.readthedocs.io/).\n\nFor getting started as a JAX developer, see the\n[developer documentation](https://jax.readthedocs.io/en/latest/developer.html).\n"
        },
        {
          "name": "WORKSPACE",
          "type": "blob",
          "size": 2.92,
          "content": "# The XLA commit is determined by third_party/xla/workspace.bzl.\nload(\"//third_party/xla:workspace.bzl\", jax_xla_workspace = \"repo\")\njax_xla_workspace()\n\n# Initialize hermetic Python\nload(\"@xla//third_party/py:python_init_rules.bzl\", \"python_init_rules\")\npython_init_rules()\n\nload(\"@xla//third_party/py:python_init_repositories.bzl\", \"python_init_repositories\")\npython_init_repositories(\n    requirements = {\n        \"3.10\": \"//build:requirements_lock_3_10.txt\",\n        \"3.11\": \"//build:requirements_lock_3_11.txt\",\n        \"3.12\": \"//build:requirements_lock_3_12.txt\",\n        \"3.13\": \"//build:requirements_lock_3_13.txt\",\n        \"3.13-ft\": \"//build:requirements_lock_3_13_ft.txt\",\n    },\n    local_wheel_inclusion_list = [\n        \"jaxlib*\",\n        \"jax_cuda*\",\n        \"jax-cuda*\",\n    ],\n    local_wheel_workspaces = [\"//jaxlib:jax.bzl\"],\n    local_wheel_dist_folder = \"../dist\",\n    default_python_version = \"system\",\n)\n\nload(\"@xla//third_party/py:python_init_toolchains.bzl\", \"python_init_toolchains\")\npython_init_toolchains()\n\nload(\"@xla//third_party/py:python_init_pip.bzl\", \"python_init_pip\")\npython_init_pip()\n\nload(\"@pypi//:requirements.bzl\", \"install_deps\")\ninstall_deps()\n\n# Optional, to facilitate testing against newest versions of Python\nload(\"@xla//third_party/py:python_repo.bzl\", \"custom_python_interpreter\")\ncustom_python_interpreter(\n    name = \"python_dev\",\n    urls = [\"https://www.python.org/ftp/python/{version}/Python-{version_variant}.tgz\"],\n    strip_prefix = \"Python-{version_variant}\",\n    version = \"3.13.0\",\n    version_variant = \"3.13.0rc2\",\n)\n\nload(\"@xla//:workspace4.bzl\", \"xla_workspace4\")\nxla_workspace4()\n\nload(\"@xla//:workspace3.bzl\", \"xla_workspace3\")\nxla_workspace3()\n\nload(\"@xla//:workspace2.bzl\", \"xla_workspace2\")\nxla_workspace2()\n\nload(\"@xla//:workspace1.bzl\", \"xla_workspace1\")\nxla_workspace1()\n\nload(\"@xla//:workspace0.bzl\", \"xla_workspace0\")\nxla_workspace0()\n\nload(\"//third_party/flatbuffers:workspace.bzl\", flatbuffers = \"repo\")\nflatbuffers()\n\nload(\n    \"@tsl//third_party/gpus/cuda/hermetic:cuda_json_init_repository.bzl\",\n    \"cuda_json_init_repository\",\n)\n\ncuda_json_init_repository()\n\nload(\n    \"@cuda_redist_json//:distributions.bzl\",\n    \"CUDA_REDISTRIBUTIONS\",\n    \"CUDNN_REDISTRIBUTIONS\",\n)\nload(\n    \"@tsl//third_party/gpus/cuda/hermetic:cuda_redist_init_repositories.bzl\",\n    \"cuda_redist_init_repositories\",\n    \"cudnn_redist_init_repository\",\n)\n\ncuda_redist_init_repositories(\n    cuda_redistributions = CUDA_REDISTRIBUTIONS,\n)\n\ncudnn_redist_init_repository(\n    cudnn_redistributions = CUDNN_REDISTRIBUTIONS,\n)\n\nload(\n    \"@tsl//third_party/gpus/cuda/hermetic:cuda_configure.bzl\",\n    \"cuda_configure\",\n)\n\ncuda_configure(name = \"local_config_cuda\")\n\nload(\n    \"@tsl//third_party/nccl/hermetic:nccl_redist_init_repository.bzl\",\n    \"nccl_redist_init_repository\",\n)\n\nnccl_redist_init_repository()\n\nload(\n    \"@tsl//third_party/nccl/hermetic:nccl_configure.bzl\",\n    \"nccl_configure\",\n)\n\nnccl_configure(name = \"local_config_nccl\")\n"
        },
        {
          "name": "benchmarks",
          "type": "tree",
          "content": null
        },
        {
          "name": "build",
          "type": "tree",
          "content": null
        },
        {
          "name": "ci",
          "type": "tree",
          "content": null
        },
        {
          "name": "cloud_tpu_colabs",
          "type": "tree",
          "content": null
        },
        {
          "name": "conftest.py",
          "type": "blob",
          "size": 2.46,
          "content": "# Copyright 2021 The JAX Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"pytest configuration\"\"\"\n\nimport os\nimport pytest\n\n\n@pytest.fixture(autouse=True)\ndef add_imports(doctest_namespace):\n  import jax\n  import numpy\n  doctest_namespace[\"jax\"] = jax\n  doctest_namespace[\"lax\"] = jax.lax\n  doctest_namespace[\"jnp\"] = jax.numpy\n  doctest_namespace[\"np\"] = numpy\n\n\n# A pytest hook that runs immediately before test collection (i.e. when pytest\n# loads all the test cases to run). When running parallel tests via xdist on\n# Cloud TPU, we use this hook to set the env vars needed to run multiple test\n# processes across different TPU chips.\n#\n# It's important that the hook runs before test collection, since jax tests end\n# up initializing the TPU runtime on import (e.g. to query supported test\n# types). It's also important that the hook gets called by each xdist worker\n# process. Luckily each worker does its own test collection.\n#\n# The pytest_collection hook can be used to overwrite the collection logic, but\n# we only use it to set the env vars and fall back to the default collection\n# logic by always returning None. See\n# https://docs.pytest.org/en/latest/how-to/writing_hook_functions.html#firstresult-stop-at-first-non-none-result\n# for details.\n#\n# The env var JAX_ENABLE_TPU_XDIST must be set for this hook to have an\n# effect. We do this to minimize any effect on non-TPU tests, and as a pointer\n# in test code to this \"magic\" hook. TPU tests should not specify more xdist\n# workers than the number of TPU chips.\ndef pytest_collection() -> None:\n  if not os.environ.get(\"JAX_ENABLE_TPU_XDIST\", None):\n    return\n  # When running as an xdist worker, will be something like \"gw0\"\n  xdist_worker_name = os.environ.get(\"PYTEST_XDIST_WORKER\", \"\")\n  if not xdist_worker_name.startswith(\"gw\"):\n    return\n  xdist_worker_number = int(xdist_worker_name[len(\"gw\"):])\n  os.environ.setdefault(\"TPU_VISIBLE_CHIPS\", str(xdist_worker_number))\n  os.environ.setdefault(\"ALLOW_MULTIPLE_LIBTPU_LOAD\", \"true\")\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "jax",
          "type": "tree",
          "content": null
        },
        {
          "name": "jax_plugins",
          "type": "tree",
          "content": null
        },
        {
          "name": "jaxlib",
          "type": "tree",
          "content": null
        },
        {
          "name": "platform_mappings",
          "type": "blob",
          "size": 0.36,
          "content": "platforms:\n#  Maps \"--platforms=//tools/toolchains/cross_compile/config:darwin_x86_64\"\n#  to \"--cpu=darwin\".\n  @xla//tools/toolchains/cross_compile/config:darwin_x86_64\n    --cpu=darwin\n\nflags:\n  # Maps \"--cpu=darwin\" to\n  # \"--platforms=//tools/toolchains/cross_compile/config:darwin_x86_64\".\n  --cpu=darwin\n    @xla//tools/toolchains/cross_compile/config:darwin_x86_64\n"
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 3.12,
          "content": "[build-system]\nrequires = [\"setuptools\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.mypy]\nshow_error_codes = true\ndisable_error_code = \"attr-defined, name-defined, annotation-unchecked\"\nno_implicit_optional = true\nwarn_redundant_casts = true\n\n[[tool.mypy.overrides]]\nmodule = [\n    \"IPython.*\",\n    \"absl.*\",\n    \"colorama.*\",\n    \"etils.*\",\n    \"filelock.*\",\n    \"flatbuffers.*\",\n    \"flax.*\",\n    \"google.colab.*\",\n    \"hypothesis.*\",\n    \"jax.experimental.jax2tf.tests.back_compat_testdata\",\n    \"jax.experimental.jax2tf.tests.flax_models\",\n    \"jax_cuda12_plugin.*\",\n    \"jaxlib.*\",\n    \"jaxlib.mlir.*\",\n    \"jraph.*\",\n    \"libtpu.*\",\n    \"matplotlib.*\",\n    \"nvidia.*\",\n    \"numpy.*\",\n    \"opt_einsum.*\",\n    \"optax.*\",\n    \"pygments.*\",\n    \"pytest.*\",\n    \"rich.*\",\n    \"scipy.*\",\n    \"setuptools.*\",\n    \"tensorboard_plugin_profile.convert.*\",\n    \"tensorflow.*\",\n    \"tensorflow.io.*\",\n    \"tensorflowjs.*\",\n    \"tensorstore.*\",\n    \"web_pdb.*\",\n    \"zstandard.*\",\n    \"kubernetes.*\"\n]\nignore_missing_imports = true\n\n[tool.pytest.ini_options]\nmarkers = [\n    \"multiaccelerator: indicates that a test can make use of and possibly requires multiple accelerators\",\n    \"SlurmMultiNodeGpuTest: mark a test for Slurm multinode GPU nightly CI\"\n]\nfilterwarnings = [\n    \"error\",\n\n    # TODO(jakevdp): remove when array_api_tests stabilize\n    \"default:.*not machine-readable.*:UserWarning\",\n    \"default:Special cases found for .* but none were parsed.*:UserWarning\",\n\n    # NOTE: this is probably not where you want to add code to suppress a\n    # warning. Only pytest tests look at this list, whereas Bazel tests also\n    # check for warnings and do not check this list. Most likely, you should\n    # add a @jtu.ignore_warning decorator to your test instead.\n]\ndoctest_optionflags = [\n    \"NUMBER\",\n    \"NORMALIZE_WHITESPACE\"\n]\naddopts = \"--doctest-glob='*.rst' --ignore='examples/ffi'\"\n\n[tool.ruff]\npreview = true\nexclude = [\n    \".git\",\n    \"build\",\n    \"__pycache__\",\n]\nline-length = 88\nindent-width = 2\ntarget-version = \"py310\"\n\n[tool.ruff.lint]\nignore = [\n    # Unnecessary collection call\n    \"C408\",\n    # Unnecessary map usage\n    \"C417\",\n    # Unnecessary dict comprehension for iterable\n    \"C420\",\n    # Object names too complex\n    \"C901\",\n    # Local variable is assigned to but never used\n    \"F841\",\n    # Raise with from clause inside except block\n    \"B904\",\n    # Zip without explicit strict parameter\n    \"B905\",\n]\nselect = [\n    \"B9\",\n    \"C\",\n    \"F\",\n    \"W\",\n    \"YTT\",\n    \"ASYNC\",\n    \"E101\",\n    \"E112\",\n    \"E113\",\n    \"E115\",\n    \"E117\",\n    \"E225\",\n    \"E227\",\n    \"E228\",\n]\n\n[tool.ruff.lint.mccabe]\nmax-complexity = 18\n\n[tool.ruff.lint.per-file-ignores]\n# F811: Redefinition of unused name.\n# F821: Undefined name.\n\"docs/autodidax.py\" = [\"F811\"]\n\"docs/pallas/tpu/matmul.ipynb\" = [\"F811\"]\n\"docs/pallas/tpu/distributed.ipynb\" = [\"F811\"]\n\"docs/pallas/quickstart.ipynb\" = [\"F811\"]\n\"docs/notebooks/autodiff_cookbook.ipynb\" = [\"F811\", \"F821\"]\n\"docs/notebooks/autodiff_remat.ipynb\" = [\"F811\", \"F821\"]\n\"docs/notebooks/Custom_derivative_rules_for_Python_code.ipynb\" = [\"F811\"]\n\"docs/jep/9407-type-promotion.ipynb\" = [\"F811\"]\n\"docs/autodidax.ipynb\" = [\"F811\"]\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 4.56,
          "content": "# Copyright 2018 The JAX Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport importlib\nimport os\n\nfrom setuptools import setup, find_packages\n\nproject_name = 'jax'\n\n_current_jaxlib_version = '0.4.38'\n# The following should be updated after each new jaxlib release.\n_latest_jaxlib_version_on_pypi = '0.4.38'\n\n_libtpu_version = '0.0.7'\n_libtpu_nightly_terminal_version = '0.1.dev20241010+nightly.cleanup'\n\ndef load_version_module(pkg_path):\n  spec = importlib.util.spec_from_file_location(\n    'version', os.path.join(pkg_path, 'version.py'))\n  module = importlib.util.module_from_spec(spec)\n  spec.loader.exec_module(module)\n  return module\n\n_version_module = load_version_module(project_name)\n__version__ = _version_module._get_version_for_build()\n_jax_version = _version_module._version  # JAX version, with no .dev suffix.\n_cmdclass = _version_module._get_cmdclass(project_name)\n_minimum_jaxlib_version = _version_module._minimum_jaxlib_version\n\nwith open('README.md', encoding='utf-8') as f:\n  _long_description = f.read()\n\nsetup(\n    name=project_name,\n    version=__version__,\n    cmdclass=_cmdclass,\n    description='Differentiate, compile, and transform Numpy code.',\n    long_description=_long_description,\n    long_description_content_type='text/markdown',\n    author='JAX team',\n    author_email='jax-dev@google.com',\n    packages=find_packages(exclude=[\"examples\", \"jax/src/internal_test_util\"]),\n    package_data={'jax': ['py.typed', \"*.pyi\", \"**/*.pyi\"]},\n    python_requires='>=3.10',\n    install_requires=[\n        f'jaxlib >={_minimum_jaxlib_version}, <={_jax_version}',\n        'ml_dtypes>=0.4.0',\n        'numpy>=1.25',\n        \"numpy>=1.26.0; python_version>='3.12'\",\n        'opt_einsum',\n        'scipy>=1.11.1',\n    ],\n    extras_require={\n        # Minimum jaxlib version; used in testing.\n        'minimum-jaxlib': [f'jaxlib=={_minimum_jaxlib_version}'],\n\n        # A CPU-only jax doesn't require any extras, but we keep this extra\n        # around for compatibility.\n        'cpu': [],\n\n        # Used only for CI builds that install JAX from github HEAD.\n        'ci': [f'jaxlib=={_latest_jaxlib_version_on_pypi}'],\n\n        # Cloud TPU VM jaxlib can be installed via:\n        # $ pip install jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n        'tpu': [\n          f'jaxlib>={_current_jaxlib_version},<={_jax_version}',\n          # TODO(phawkins): remove the libtpu-nightly dependency in Q1 2025.\n          f'libtpu-nightly=={_libtpu_nightly_terminal_version}',\n          f'libtpu=={_libtpu_version}',\n          'requests',  # necessary for jax.distributed.initialize\n        ],\n\n        'cuda': [\n          f\"jaxlib=={_current_jaxlib_version}\",\n          f\"jax-cuda12-plugin[with_cuda]>={_current_jaxlib_version},<={_jax_version}\",\n        ],\n\n        'cuda12': [\n          f\"jaxlib=={_current_jaxlib_version}\",\n          f\"jax-cuda12-plugin[with_cuda]>={_current_jaxlib_version},<={_jax_version}\",\n        ],\n\n        # Deprecated alias for cuda12, kept to avoid breaking users who wrote\n        # cuda12_pip in their CI.\n        'cuda12_pip': [\n          f\"jaxlib=={_current_jaxlib_version}\",\n          f\"jax-cuda12-plugin[with_cuda]>={_current_jaxlib_version},<={_jax_version}\",\n        ],\n\n        # Target that does not depend on the CUDA pip wheels, for those who want\n        # to use a preinstalled CUDA.\n        'cuda12_local': [\n          f\"jaxlib=={_current_jaxlib_version}\",\n          f\"jax-cuda12-plugin=={_current_jaxlib_version}\",\n        ],\n\n        # ROCm support for ROCm 6.0 and above.\n        'rocm': [\n          f\"jaxlib=={_current_jaxlib_version}\",\n          f\"jax-rocm60-plugin>={_current_jaxlib_version},<={_jax_version}\",\n        ],\n\n        # For automatic bootstrapping distributed jobs in Kubernetes\n        'k8s': [\n          'kubernetes',\n        ],\n    },\n    url='https://github.com/jax-ml/jax',\n    license='Apache-2.0',\n    classifiers=[\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\",\n        \"Programming Language :: Python :: 3.13\",\n    ],\n    zip_safe=False,\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "third_party",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}